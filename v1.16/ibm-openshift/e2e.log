I0526 21:26:30.632546      23 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-297629347
I0526 21:26:30.632803      23 e2e.go:92] Starting e2e run "0c282475-98ec-4464-a834-167e881d9705" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1590528388 - Will randomize all specs
Will run 276 of 4897 specs

May 26 21:26:30.647: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 21:26:30.651: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 26 21:26:30.703: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 26 21:26:30.785: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 26 21:26:30.785: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
May 26 21:26:30.786: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 26 21:26:30.802: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
May 26 21:26:30.802: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
May 26 21:26:30.803: INFO: e2e test version: v1.16.2
May 26 21:26:30.807: INFO: kube-apiserver version: v1.16.2
May 26 21:26:30.807: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 21:26:30.821: INFO: Cluster IP family: ipv4
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:26:30.822: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubectl
May 26 21:26:30.961: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
May 26 21:26:30.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 create -f - --namespace=kubectl-4329'
May 26 21:26:31.624: INFO: stderr: ""
May 26 21:26:31.624: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 26 21:26:31.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4329'
May 26 21:26:31.799: INFO: stderr: ""
May 26 21:26:31.799: INFO: stdout: "update-demo-nautilus-7rnd8 update-demo-nautilus-9q8g9 "
May 26 21:26:31.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-nautilus-7rnd8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4329'
May 26 21:26:31.937: INFO: stderr: ""
May 26 21:26:31.937: INFO: stdout: ""
May 26 21:26:31.937: INFO: update-demo-nautilus-7rnd8 is created but not running
May 26 21:26:36.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4329'
May 26 21:26:37.098: INFO: stderr: ""
May 26 21:26:37.099: INFO: stdout: "update-demo-nautilus-7rnd8 update-demo-nautilus-9q8g9 "
May 26 21:26:37.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-nautilus-7rnd8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4329'
May 26 21:26:37.247: INFO: stderr: ""
May 26 21:26:37.247: INFO: stdout: "true"
May 26 21:26:37.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-nautilus-7rnd8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4329'
May 26 21:26:37.388: INFO: stderr: ""
May 26 21:26:37.388: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 26 21:26:37.388: INFO: validating pod update-demo-nautilus-7rnd8
May 26 21:26:37.407: INFO: got data: {
  "image": "nautilus.jpg"
}

May 26 21:26:37.407: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 26 21:26:37.407: INFO: update-demo-nautilus-7rnd8 is verified up and running
May 26 21:26:37.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-nautilus-9q8g9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4329'
May 26 21:26:37.536: INFO: stderr: ""
May 26 21:26:37.536: INFO: stdout: "true"
May 26 21:26:37.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-nautilus-9q8g9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4329'
May 26 21:26:37.691: INFO: stderr: ""
May 26 21:26:37.691: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 26 21:26:37.691: INFO: validating pod update-demo-nautilus-9q8g9
May 26 21:26:37.715: INFO: got data: {
  "image": "nautilus.jpg"
}

May 26 21:26:37.715: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 26 21:26:37.715: INFO: update-demo-nautilus-9q8g9 is verified up and running
STEP: using delete to clean up resources
May 26 21:26:37.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 delete --grace-period=0 --force -f - --namespace=kubectl-4329'
May 26 21:26:37.867: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 26 21:26:37.867: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 26 21:26:37.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4329'
May 26 21:26:38.032: INFO: stderr: "No resources found in kubectl-4329 namespace.\n"
May 26 21:26:38.032: INFO: stdout: ""
May 26 21:26:38.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods -l name=update-demo --namespace=kubectl-4329 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 26 21:26:38.194: INFO: stderr: ""
May 26 21:26:38.194: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:26:38.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4329" for this suite.
May 26 21:27:10.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:27:12.450: INFO: namespace kubectl-4329 deletion completed in 34.241095589s

â€¢ [SLOW TEST:41.628 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:27:12.450: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 21:27:13.635: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-960a8d0a-0222-4f99-8bbb-b8a016ddf66b" in namespace "security-context-test-2487" to be "success or failure"
May 26 21:27:13.645: INFO: Pod "busybox-readonly-false-960a8d0a-0222-4f99-8bbb-b8a016ddf66b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.933084ms
May 26 21:27:15.656: INFO: Pod "busybox-readonly-false-960a8d0a-0222-4f99-8bbb-b8a016ddf66b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021083132s
May 26 21:27:17.667: INFO: Pod "busybox-readonly-false-960a8d0a-0222-4f99-8bbb-b8a016ddf66b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032133241s
May 26 21:27:19.678: INFO: Pod "busybox-readonly-false-960a8d0a-0222-4f99-8bbb-b8a016ddf66b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043448706s
May 26 21:27:21.689: INFO: Pod "busybox-readonly-false-960a8d0a-0222-4f99-8bbb-b8a016ddf66b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.054559048s
May 26 21:27:21.689: INFO: Pod "busybox-readonly-false-960a8d0a-0222-4f99-8bbb-b8a016ddf66b" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:27:21.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2487" for this suite.
May 26 21:27:29.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:27:31.601: INFO: namespace security-context-test-2487 deletion completed in 9.89385467s

â€¢ [SLOW TEST:19.151 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:27:31.602: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 21:27:31.749: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Creating first CR 
May 26 21:27:31.924: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-26T21:27:31Z generation:1 name:name1 resourceVersion:40239 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:b13c9269-e688-426a-97ae-232705b677ca] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
May 26 21:27:41.944: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-26T21:27:41Z generation:1 name:name2 resourceVersion:40277 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:93c551ae-5ce9-4b6d-afcd-29f0a3c59455] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
May 26 21:27:51.959: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-26T21:27:31Z generation:2 name:name1 resourceVersion:40315 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:b13c9269-e688-426a-97ae-232705b677ca] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
May 26 21:28:01.979: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-26T21:27:41Z generation:2 name:name2 resourceVersion:40351 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:93c551ae-5ce9-4b6d-afcd-29f0a3c59455] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
May 26 21:28:12.015: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-26T21:27:31Z generation:2 name:name1 resourceVersion:40387 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:b13c9269-e688-426a-97ae-232705b677ca] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
May 26 21:28:22.041: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-26T21:27:41Z generation:2 name:name2 resourceVersion:40423 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:93c551ae-5ce9-4b6d-afcd-29f0a3c59455] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:28:32.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9201" for this suite.
May 26 21:28:40.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:28:42.485: INFO: namespace crd-watch-9201 deletion completed in 9.888728869s

â€¢ [SLOW TEST:70.884 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:28:42.486: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:28:46.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3265" for this suite.
May 26 21:28:54.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:28:56.576: INFO: namespace kubelet-test-3265 deletion completed in 9.886940109s

â€¢ [SLOW TEST:14.090 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:28:56.577: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
May 26 21:28:56.727: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
May 26 21:28:57.771: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May 26 21:28:59.902: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 26 21:29:01.916: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 26 21:29:03.917: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 26 21:29:05.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 26 21:29:07.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 26 21:29:09.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 26 21:29:11.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125337, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 26 21:29:14.096: INFO: Waited 163.999737ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:29:16.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5279" for this suite.
May 26 21:29:24.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:29:26.268: INFO: namespace aggregator-5279 deletion completed in 9.953775856s

â€¢ [SLOW TEST:29.691 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:29:26.268: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 26 21:29:27.423: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 26 21:29:29.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125367, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125367, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125367, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125367, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 26 21:29:31.462: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125367, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125367, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125367, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125367, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 26 21:29:33.461: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125367, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125367, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125367, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125367, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 26 21:29:35.463: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125367, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125367, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125367, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125367, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 26 21:29:38.504: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:29:50.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5803" for this suite.
May 26 21:29:58.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:30:00.789: INFO: namespace webhook-5803 deletion completed in 9.905708418s
STEP: Destroying namespace "webhook-5803-markers" for this suite.
May 26 21:30:08.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:30:10.898: INFO: namespace webhook-5803-markers deletion completed in 10.108925994s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:44.689 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:30:10.960: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
May 26 21:30:11.192: INFO: Waiting up to 5m0s for pod "pod-7aba1538-946a-4750-b7aa-531e2e43a825" in namespace "emptydir-422" to be "success or failure"
May 26 21:30:11.214: INFO: Pod "pod-7aba1538-946a-4750-b7aa-531e2e43a825": Phase="Pending", Reason="", readiness=false. Elapsed: 21.828565ms
May 26 21:30:13.226: INFO: Pod "pod-7aba1538-946a-4750-b7aa-531e2e43a825": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034340754s
May 26 21:30:15.238: INFO: Pod "pod-7aba1538-946a-4750-b7aa-531e2e43a825": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046460923s
May 26 21:30:17.251: INFO: Pod "pod-7aba1538-946a-4750-b7aa-531e2e43a825": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.05860659s
STEP: Saw pod success
May 26 21:30:17.251: INFO: Pod "pod-7aba1538-946a-4750-b7aa-531e2e43a825" satisfied condition "success or failure"
May 26 21:30:17.262: INFO: Trying to get logs from node 10.215.60.34 pod pod-7aba1538-946a-4750-b7aa-531e2e43a825 container test-container: <nil>
STEP: delete the pod
May 26 21:30:17.357: INFO: Waiting for pod pod-7aba1538-946a-4750-b7aa-531e2e43a825 to disappear
May 26 21:30:17.366: INFO: Pod pod-7aba1538-946a-4750-b7aa-531e2e43a825 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:30:17.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-422" for this suite.
May 26 21:30:25.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:30:27.550: INFO: namespace emptydir-422 deletion completed in 10.155932192s

â€¢ [SLOW TEST:16.590 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:30:27.550: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
May 26 21:30:27.715: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-297629347 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:30:27.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1847" for this suite.
May 26 21:30:35.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:30:37.768: INFO: namespace kubectl-1847 deletion completed in 9.912824958s

â€¢ [SLOW TEST:10.218 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:30:37.768: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-163d6555-e805-4448-becf-4d5c4eab79c2
STEP: Creating a pod to test consume configMaps
May 26 21:30:37.947: INFO: Waiting up to 5m0s for pod "pod-configmaps-43fc867f-d3eb-4fa0-9c4b-6fbcb26a05cf" in namespace "configmap-1097" to be "success or failure"
May 26 21:30:37.956: INFO: Pod "pod-configmaps-43fc867f-d3eb-4fa0-9c4b-6fbcb26a05cf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.283965ms
May 26 21:30:39.973: INFO: Pod "pod-configmaps-43fc867f-d3eb-4fa0-9c4b-6fbcb26a05cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026168569s
STEP: Saw pod success
May 26 21:30:39.973: INFO: Pod "pod-configmaps-43fc867f-d3eb-4fa0-9c4b-6fbcb26a05cf" satisfied condition "success or failure"
May 26 21:30:39.987: INFO: Trying to get logs from node 10.215.60.34 pod pod-configmaps-43fc867f-d3eb-4fa0-9c4b-6fbcb26a05cf container configmap-volume-test: <nil>
STEP: delete the pod
May 26 21:30:40.057: INFO: Waiting for pod pod-configmaps-43fc867f-d3eb-4fa0-9c4b-6fbcb26a05cf to disappear
May 26 21:30:40.075: INFO: Pod pod-configmaps-43fc867f-d3eb-4fa0-9c4b-6fbcb26a05cf no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:30:40.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1097" for this suite.
May 26 21:30:48.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:30:49.992: INFO: namespace configmap-1097 deletion completed in 9.892644376s

â€¢ [SLOW TEST:12.225 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:30:49.995: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-f2646583-fddb-4fc2-bb20-656a1b6a6849
STEP: Creating secret with name secret-projected-all-test-volume-0a1d1de1-b117-4fed-8209-3c6458a5b56d
STEP: Creating a pod to test Check all projections for projected volume plugin
May 26 21:30:50.195: INFO: Waiting up to 5m0s for pod "projected-volume-63e28f75-d34b-42b9-ae36-25bd0018c623" in namespace "projected-1468" to be "success or failure"
May 26 21:30:50.207: INFO: Pod "projected-volume-63e28f75-d34b-42b9-ae36-25bd0018c623": Phase="Pending", Reason="", readiness=false. Elapsed: 11.414636ms
May 26 21:30:52.220: INFO: Pod "projected-volume-63e28f75-d34b-42b9-ae36-25bd0018c623": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024436794s
May 26 21:30:54.231: INFO: Pod "projected-volume-63e28f75-d34b-42b9-ae36-25bd0018c623": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035495485s
STEP: Saw pod success
May 26 21:30:54.231: INFO: Pod "projected-volume-63e28f75-d34b-42b9-ae36-25bd0018c623" satisfied condition "success or failure"
May 26 21:30:54.242: INFO: Trying to get logs from node 10.215.60.34 pod projected-volume-63e28f75-d34b-42b9-ae36-25bd0018c623 container projected-all-volume-test: <nil>
STEP: delete the pod
May 26 21:30:54.308: INFO: Waiting for pod projected-volume-63e28f75-d34b-42b9-ae36-25bd0018c623 to disappear
May 26 21:30:54.321: INFO: Pod projected-volume-63e28f75-d34b-42b9-ae36-25bd0018c623 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:30:54.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1468" for this suite.
May 26 21:31:02.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:31:04.241: INFO: namespace projected-1468 deletion completed in 9.901186167s

â€¢ [SLOW TEST:14.246 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:31:04.243: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:31:19.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2463" for this suite.
May 26 21:31:27.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:31:29.692: INFO: namespace namespaces-2463 deletion completed in 9.893446561s
STEP: Destroying namespace "nsdeletetest-3708" for this suite.
May 26 21:31:29.700: INFO: Namespace nsdeletetest-3708 was already deleted
STEP: Destroying namespace "nsdeletetest-3074" for this suite.
May 26 21:31:37.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:31:39.589: INFO: namespace nsdeletetest-3074 deletion completed in 9.888508304s

â€¢ [SLOW TEST:35.346 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:31:39.589: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 21:31:39.751: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-f9c40b8c-798b-4b5d-9e66-c1a21c1295c8
STEP: Creating secret with name s-test-opt-upd-deddf811-9267-4d3b-b11a-c429b30c62bc
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-f9c40b8c-798b-4b5d-9e66-c1a21c1295c8
STEP: Updating secret s-test-opt-upd-deddf811-9267-4d3b-b11a-c429b30c62bc
STEP: Creating secret with name s-test-opt-create-413dc8e5-d702-436b-9d4a-bdc722625420
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:31:48.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8804" for this suite.
May 26 21:32:20.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:32:22.228: INFO: namespace projected-8804 deletion completed in 34.10033101s

â€¢ [SLOW TEST:42.639 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:32:22.229: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 26 21:32:23.639: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 26 21:32:25.668: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125543, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125543, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125543, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125543, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 26 21:32:28.713: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:32:28.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3038" for this suite.
May 26 21:32:37.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:32:38.874: INFO: namespace webhook-3038 deletion completed in 9.892992244s
STEP: Destroying namespace "webhook-3038-markers" for this suite.
May 26 21:32:46.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:32:48.808: INFO: namespace webhook-3038-markers deletion completed in 9.933330361s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:26.646 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:32:48.874: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5238.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5238.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5238.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5238.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5238.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5238.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 26 21:33:05.242: INFO: DNS probes using dns-5238/dns-test-c7e1b6cf-683f-4427-ac11-f7eefcaea369 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:33:05.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5238" for this suite.
May 26 21:33:13.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:33:15.189: INFO: namespace dns-5238 deletion completed in 9.889842099s

â€¢ [SLOW TEST:26.315 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:33:15.189: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 26 21:33:15.405: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f94a0084-99da-426a-af36-d5faa97df59f" in namespace "projected-6607" to be "success or failure"
May 26 21:33:15.420: INFO: Pod "downwardapi-volume-f94a0084-99da-426a-af36-d5faa97df59f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.57188ms
May 26 21:33:17.431: INFO: Pod "downwardapi-volume-f94a0084-99da-426a-af36-d5faa97df59f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0263708s
STEP: Saw pod success
May 26 21:33:17.431: INFO: Pod "downwardapi-volume-f94a0084-99da-426a-af36-d5faa97df59f" satisfied condition "success or failure"
May 26 21:33:17.442: INFO: Trying to get logs from node 10.215.60.34 pod downwardapi-volume-f94a0084-99da-426a-af36-d5faa97df59f container client-container: <nil>
STEP: delete the pod
May 26 21:33:17.532: INFO: Waiting for pod downwardapi-volume-f94a0084-99da-426a-af36-d5faa97df59f to disappear
May 26 21:33:17.545: INFO: Pod downwardapi-volume-f94a0084-99da-426a-af36-d5faa97df59f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:33:17.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6607" for this suite.
May 26 21:33:25.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:33:27.451: INFO: namespace projected-6607 deletion completed in 9.886117153s

â€¢ [SLOW TEST:12.262 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:33:27.453: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:33:43.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9863" for this suite.
May 26 21:33:51.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:33:53.581: INFO: namespace resourcequota-9863 deletion completed in 9.89021056s

â€¢ [SLOW TEST:26.129 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:33:53.589: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-8954
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 26 21:33:53.713: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 26 21:34:24.073: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.99.174:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8954 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 26 21:34:24.073: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 21:34:24.298: INFO: Found all expected endpoints: [netserver-0]
May 26 21:34:24.315: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.182.107:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8954 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 26 21:34:24.316: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 21:34:24.529: INFO: Found all expected endpoints: [netserver-1]
May 26 21:34:24.540: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.141.189:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8954 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 26 21:34:24.540: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 21:34:24.728: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:34:24.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8954" for this suite.
May 26 21:34:32.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:34:34.648: INFO: namespace pod-network-test-8954 deletion completed in 9.899630806s

â€¢ [SLOW TEST:41.060 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:34:34.651: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
May 26 21:34:34.822: INFO: Waiting up to 5m0s for pod "pod-65725fd8-eecd-4803-b2a3-c2825d992981" in namespace "emptydir-8095" to be "success or failure"
May 26 21:34:34.837: INFO: Pod "pod-65725fd8-eecd-4803-b2a3-c2825d992981": Phase="Pending", Reason="", readiness=false. Elapsed: 14.930238ms
May 26 21:34:36.848: INFO: Pod "pod-65725fd8-eecd-4803-b2a3-c2825d992981": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026043887s
May 26 21:34:38.860: INFO: Pod "pod-65725fd8-eecd-4803-b2a3-c2825d992981": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038507801s
May 26 21:34:40.871: INFO: Pod "pod-65725fd8-eecd-4803-b2a3-c2825d992981": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049263386s
STEP: Saw pod success
May 26 21:34:40.871: INFO: Pod "pod-65725fd8-eecd-4803-b2a3-c2825d992981" satisfied condition "success or failure"
May 26 21:34:40.881: INFO: Trying to get logs from node 10.215.60.34 pod pod-65725fd8-eecd-4803-b2a3-c2825d992981 container test-container: <nil>
STEP: delete the pod
May 26 21:34:40.936: INFO: Waiting for pod pod-65725fd8-eecd-4803-b2a3-c2825d992981 to disappear
May 26 21:34:40.947: INFO: Pod pod-65725fd8-eecd-4803-b2a3-c2825d992981 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:34:40.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8095" for this suite.
May 26 21:34:48.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:34:50.867: INFO: namespace emptydir-8095 deletion completed in 9.900030701s

â€¢ [SLOW TEST:16.216 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:34:50.868: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
May 26 21:34:51.592: INFO: created pod pod-service-account-defaultsa
May 26 21:34:51.592: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 26 21:34:51.620: INFO: created pod pod-service-account-mountsa
May 26 21:34:51.620: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 26 21:34:51.645: INFO: created pod pod-service-account-nomountsa
May 26 21:34:51.645: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 26 21:34:51.675: INFO: created pod pod-service-account-defaultsa-mountspec
May 26 21:34:51.675: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 26 21:34:51.704: INFO: created pod pod-service-account-mountsa-mountspec
May 26 21:34:51.704: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 26 21:34:51.737: INFO: created pod pod-service-account-nomountsa-mountspec
May 26 21:34:51.737: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 26 21:34:51.770: INFO: created pod pod-service-account-defaultsa-nomountspec
May 26 21:34:51.770: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 26 21:34:51.801: INFO: created pod pod-service-account-mountsa-nomountspec
May 26 21:34:51.801: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 26 21:34:51.827: INFO: created pod pod-service-account-nomountsa-nomountspec
May 26 21:34:51.827: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:34:51.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6819" for this suite.
May 26 21:34:59.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:35:02.679: INFO: namespace svcaccounts-6819 deletion completed in 10.835189381s

â€¢ [SLOW TEST:11.811 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:35:02.680: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
May 26 21:35:02.824: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 21:35:11.675: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:35:44.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3830" for this suite.
May 26 21:35:52.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:35:54.440: INFO: namespace crd-publish-openapi-3830 deletion completed in 9.888412693s

â€¢ [SLOW TEST:51.760 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:35:54.443: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 21:35:54.571: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:35:57.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1996" for this suite.
May 26 21:36:45.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:36:47.730: INFO: namespace pods-1996 deletion completed in 49.887353545s

â€¢ [SLOW TEST:53.288 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:36:47.731: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-gnq5
STEP: Creating a pod to test atomic-volume-subpath
May 26 21:36:47.965: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-gnq5" in namespace "subpath-4556" to be "success or failure"
May 26 21:36:47.974: INFO: Pod "pod-subpath-test-secret-gnq5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.471457ms
May 26 21:36:49.987: INFO: Pod "pod-subpath-test-secret-gnq5": Phase="Running", Reason="", readiness=true. Elapsed: 2.022126048s
May 26 21:36:51.997: INFO: Pod "pod-subpath-test-secret-gnq5": Phase="Running", Reason="", readiness=true. Elapsed: 4.032064587s
May 26 21:36:54.007: INFO: Pod "pod-subpath-test-secret-gnq5": Phase="Running", Reason="", readiness=true. Elapsed: 6.041803988s
May 26 21:36:56.017: INFO: Pod "pod-subpath-test-secret-gnq5": Phase="Running", Reason="", readiness=true. Elapsed: 8.052426079s
May 26 21:36:58.029: INFO: Pod "pod-subpath-test-secret-gnq5": Phase="Running", Reason="", readiness=true. Elapsed: 10.06373196s
May 26 21:37:00.039: INFO: Pod "pod-subpath-test-secret-gnq5": Phase="Running", Reason="", readiness=true. Elapsed: 12.074058011s
May 26 21:37:02.051: INFO: Pod "pod-subpath-test-secret-gnq5": Phase="Running", Reason="", readiness=true. Elapsed: 14.085695647s
May 26 21:37:04.062: INFO: Pod "pod-subpath-test-secret-gnq5": Phase="Running", Reason="", readiness=true. Elapsed: 16.096822864s
May 26 21:37:06.072: INFO: Pod "pod-subpath-test-secret-gnq5": Phase="Running", Reason="", readiness=true. Elapsed: 18.107040832s
May 26 21:37:08.082: INFO: Pod "pod-subpath-test-secret-gnq5": Phase="Running", Reason="", readiness=true. Elapsed: 20.117195905s
May 26 21:37:10.093: INFO: Pod "pod-subpath-test-secret-gnq5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.128279416s
STEP: Saw pod success
May 26 21:37:10.093: INFO: Pod "pod-subpath-test-secret-gnq5" satisfied condition "success or failure"
May 26 21:37:10.106: INFO: Trying to get logs from node 10.215.60.34 pod pod-subpath-test-secret-gnq5 container test-container-subpath-secret-gnq5: <nil>
STEP: delete the pod
May 26 21:37:10.193: INFO: Waiting for pod pod-subpath-test-secret-gnq5 to disappear
May 26 21:37:10.203: INFO: Pod pod-subpath-test-secret-gnq5 no longer exists
STEP: Deleting pod pod-subpath-test-secret-gnq5
May 26 21:37:10.203: INFO: Deleting pod "pod-subpath-test-secret-gnq5" in namespace "subpath-4556"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:37:10.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4556" for this suite.
May 26 21:37:18.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:37:20.227: INFO: namespace subpath-4556 deletion completed in 9.99409001s

â€¢ [SLOW TEST:32.496 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:37:20.228: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-2610204e-3b27-4840-88cf-ef1123aae547
STEP: Creating a pod to test consume configMaps
May 26 21:37:20.400: INFO: Waiting up to 5m0s for pod "pod-configmaps-3408959d-eb15-4b9f-b0ae-bf505a3f01dc" in namespace "configmap-3646" to be "success or failure"
May 26 21:37:20.410: INFO: Pod "pod-configmaps-3408959d-eb15-4b9f-b0ae-bf505a3f01dc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.584033ms
May 26 21:37:22.420: INFO: Pod "pod-configmaps-3408959d-eb15-4b9f-b0ae-bf505a3f01dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01906344s
STEP: Saw pod success
May 26 21:37:22.420: INFO: Pod "pod-configmaps-3408959d-eb15-4b9f-b0ae-bf505a3f01dc" satisfied condition "success or failure"
May 26 21:37:22.429: INFO: Trying to get logs from node 10.215.60.34 pod pod-configmaps-3408959d-eb15-4b9f-b0ae-bf505a3f01dc container configmap-volume-test: <nil>
STEP: delete the pod
May 26 21:37:22.483: INFO: Waiting for pod pod-configmaps-3408959d-eb15-4b9f-b0ae-bf505a3f01dc to disappear
May 26 21:37:22.492: INFO: Pod pod-configmaps-3408959d-eb15-4b9f-b0ae-bf505a3f01dc no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:37:22.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3646" for this suite.
May 26 21:37:30.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:37:32.395: INFO: namespace configmap-3646 deletion completed in 9.887441128s

â€¢ [SLOW TEST:12.167 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:37:32.396: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
May 26 21:37:32.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 cluster-info'
May 26 21:37:32.889: INFO: stderr: ""
May 26 21:37:32.889: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:37:32.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9523" for this suite.
May 26 21:37:40.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:37:42.793: INFO: namespace kubectl-9523 deletion completed in 9.888272598s

â€¢ [SLOW TEST:10.397 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:37:42.793: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 26 21:37:42.933: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:38:00.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3570" for this suite.
May 26 21:38:08.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:38:10.380: INFO: namespace pods-3570 deletion completed in 9.898668006s

â€¢ [SLOW TEST:27.587 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:38:10.382: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-1579
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 26 21:38:10.519: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 26 21:38:30.843: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.99.181:8080/dial?request=hostName&protocol=http&host=172.30.141.191&port=8080&tries=1'] Namespace:pod-network-test-1579 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 26 21:38:30.843: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 21:38:31.038: INFO: Waiting for endpoints: map[]
May 26 21:38:31.047: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.99.181:8080/dial?request=hostName&protocol=http&host=172.30.99.180&port=8080&tries=1'] Namespace:pod-network-test-1579 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 26 21:38:31.047: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 21:38:31.228: INFO: Waiting for endpoints: map[]
May 26 21:38:31.237: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.99.181:8080/dial?request=hostName&protocol=http&host=172.30.182.117&port=8080&tries=1'] Namespace:pod-network-test-1579 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 26 21:38:31.237: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 21:38:31.411: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:38:31.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1579" for this suite.
May 26 21:38:39.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:38:41.335: INFO: namespace pod-network-test-1579 deletion completed in 9.906642188s

â€¢ [SLOW TEST:30.953 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:38:41.337: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 26 21:38:42.130: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 26 21:38:44.155: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125922, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125922, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125922, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726125922, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 26 21:38:47.195: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 21:38:47.205: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5694-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:38:48.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9154" for this suite.
May 26 21:38:56.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:38:58.348: INFO: namespace webhook-9154 deletion completed in 9.888769565s
STEP: Destroying namespace "webhook-9154-markers" for this suite.
May 26 21:39:06.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:39:08.246: INFO: namespace webhook-9154-markers deletion completed in 9.898281025s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:26.954 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:39:08.291: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 26 21:39:08.470: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7060 /api/v1/namespaces/watch-7060/configmaps/e2e-watch-test-configmap-a 32a07e8e-c861-44d5-a9ce-6a03d031e078 45510 0 2020-05-26 21:39:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 26 21:39:08.470: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7060 /api/v1/namespaces/watch-7060/configmaps/e2e-watch-test-configmap-a 32a07e8e-c861-44d5-a9ce-6a03d031e078 45510 0 2020-05-26 21:39:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 26 21:39:18.491: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7060 /api/v1/namespaces/watch-7060/configmaps/e2e-watch-test-configmap-a 32a07e8e-c861-44d5-a9ce-6a03d031e078 45550 0 2020-05-26 21:39:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 26 21:39:18.491: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7060 /api/v1/namespaces/watch-7060/configmaps/e2e-watch-test-configmap-a 32a07e8e-c861-44d5-a9ce-6a03d031e078 45550 0 2020-05-26 21:39:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 26 21:39:28.514: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7060 /api/v1/namespaces/watch-7060/configmaps/e2e-watch-test-configmap-a 32a07e8e-c861-44d5-a9ce-6a03d031e078 45586 0 2020-05-26 21:39:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 26 21:39:28.515: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7060 /api/v1/namespaces/watch-7060/configmaps/e2e-watch-test-configmap-a 32a07e8e-c861-44d5-a9ce-6a03d031e078 45586 0 2020-05-26 21:39:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 26 21:39:38.538: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7060 /api/v1/namespaces/watch-7060/configmaps/e2e-watch-test-configmap-a 32a07e8e-c861-44d5-a9ce-6a03d031e078 45635 0 2020-05-26 21:39:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 26 21:39:38.538: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7060 /api/v1/namespaces/watch-7060/configmaps/e2e-watch-test-configmap-a 32a07e8e-c861-44d5-a9ce-6a03d031e078 45635 0 2020-05-26 21:39:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 26 21:39:48.558: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7060 /api/v1/namespaces/watch-7060/configmaps/e2e-watch-test-configmap-b 60fcafb3-af19-4a63-8c65-36cb09a6a20e 45680 0 2020-05-26 21:39:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 26 21:39:48.558: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7060 /api/v1/namespaces/watch-7060/configmaps/e2e-watch-test-configmap-b 60fcafb3-af19-4a63-8c65-36cb09a6a20e 45680 0 2020-05-26 21:39:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 26 21:39:58.579: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7060 /api/v1/namespaces/watch-7060/configmaps/e2e-watch-test-configmap-b 60fcafb3-af19-4a63-8c65-36cb09a6a20e 45723 0 2020-05-26 21:39:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 26 21:39:58.580: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7060 /api/v1/namespaces/watch-7060/configmaps/e2e-watch-test-configmap-b 60fcafb3-af19-4a63-8c65-36cb09a6a20e 45723 0 2020-05-26 21:39:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:40:08.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7060" for this suite.
May 26 21:40:16.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:40:18.496: INFO: namespace watch-7060 deletion completed in 9.901546589s

â€¢ [SLOW TEST:70.205 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:40:18.496: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
May 26 21:40:26.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec pod-sharedvolume-d1d368e4-cc2c-4810-81e7-2c585a8a2622 -c busybox-main-container --namespace=emptydir-6304 -- cat /usr/share/volumeshare/shareddata.txt'
May 26 21:40:27.108: INFO: stderr: ""
May 26 21:40:27.108: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:40:27.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6304" for this suite.
May 26 21:40:35.162: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:40:37.015: INFO: namespace emptydir-6304 deletion completed in 9.887134545s

â€¢ [SLOW TEST:18.519 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:40:37.016: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-bq48
STEP: Creating a pod to test atomic-volume-subpath
May 26 21:40:37.202: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-bq48" in namespace "subpath-7262" to be "success or failure"
May 26 21:40:37.214: INFO: Pod "pod-subpath-test-projected-bq48": Phase="Pending", Reason="", readiness=false. Elapsed: 11.649963ms
May 26 21:40:39.225: INFO: Pod "pod-subpath-test-projected-bq48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022725918s
May 26 21:40:41.236: INFO: Pod "pod-subpath-test-projected-bq48": Phase="Running", Reason="", readiness=true. Elapsed: 4.033941964s
May 26 21:40:43.246: INFO: Pod "pod-subpath-test-projected-bq48": Phase="Running", Reason="", readiness=true. Elapsed: 6.043956938s
May 26 21:40:45.255: INFO: Pod "pod-subpath-test-projected-bq48": Phase="Running", Reason="", readiness=true. Elapsed: 8.053484368s
May 26 21:40:47.266: INFO: Pod "pod-subpath-test-projected-bq48": Phase="Running", Reason="", readiness=true. Elapsed: 10.063833082s
May 26 21:40:49.276: INFO: Pod "pod-subpath-test-projected-bq48": Phase="Running", Reason="", readiness=true. Elapsed: 12.073986536s
May 26 21:40:51.286: INFO: Pod "pod-subpath-test-projected-bq48": Phase="Running", Reason="", readiness=true. Elapsed: 14.084517403s
May 26 21:40:53.298: INFO: Pod "pod-subpath-test-projected-bq48": Phase="Running", Reason="", readiness=true. Elapsed: 16.095983375s
May 26 21:40:55.308: INFO: Pod "pod-subpath-test-projected-bq48": Phase="Running", Reason="", readiness=true. Elapsed: 18.105981084s
May 26 21:40:57.318: INFO: Pod "pod-subpath-test-projected-bq48": Phase="Running", Reason="", readiness=true. Elapsed: 20.115635885s
May 26 21:40:59.328: INFO: Pod "pod-subpath-test-projected-bq48": Phase="Running", Reason="", readiness=true. Elapsed: 22.126423139s
May 26 21:41:01.339: INFO: Pod "pod-subpath-test-projected-bq48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.136836437s
STEP: Saw pod success
May 26 21:41:01.339: INFO: Pod "pod-subpath-test-projected-bq48" satisfied condition "success or failure"
May 26 21:41:01.348: INFO: Trying to get logs from node 10.215.60.34 pod pod-subpath-test-projected-bq48 container test-container-subpath-projected-bq48: <nil>
STEP: delete the pod
May 26 21:41:01.451: INFO: Waiting for pod pod-subpath-test-projected-bq48 to disappear
May 26 21:41:01.463: INFO: Pod pod-subpath-test-projected-bq48 no longer exists
STEP: Deleting pod pod-subpath-test-projected-bq48
May 26 21:41:01.463: INFO: Deleting pod "pod-subpath-test-projected-bq48" in namespace "subpath-7262"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:41:01.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7262" for this suite.
May 26 21:41:09.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:41:11.380: INFO: namespace subpath-7262 deletion completed in 9.889370599s

â€¢ [SLOW TEST:34.364 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:41:11.380: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:41:27.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7447" for this suite.
May 26 21:41:35.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:41:37.674: INFO: namespace resourcequota-7447 deletion completed in 9.900089531s

â€¢ [SLOW TEST:26.294 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:41:37.674: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
May 26 21:41:37.869: INFO: Waiting up to 5m0s for pod "pod-92e384c6-e7ea-4f92-97f6-2c065d6a4055" in namespace "emptydir-4838" to be "success or failure"
May 26 21:41:37.880: INFO: Pod "pod-92e384c6-e7ea-4f92-97f6-2c065d6a4055": Phase="Pending", Reason="", readiness=false. Elapsed: 10.312284ms
May 26 21:41:39.889: INFO: Pod "pod-92e384c6-e7ea-4f92-97f6-2c065d6a4055": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019619066s
STEP: Saw pod success
May 26 21:41:39.889: INFO: Pod "pod-92e384c6-e7ea-4f92-97f6-2c065d6a4055" satisfied condition "success or failure"
May 26 21:41:39.898: INFO: Trying to get logs from node 10.215.60.34 pod pod-92e384c6-e7ea-4f92-97f6-2c065d6a4055 container test-container: <nil>
STEP: delete the pod
May 26 21:41:39.955: INFO: Waiting for pod pod-92e384c6-e7ea-4f92-97f6-2c065d6a4055 to disappear
May 26 21:41:39.966: INFO: Pod pod-92e384c6-e7ea-4f92-97f6-2c065d6a4055 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:41:39.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4838" for this suite.
May 26 21:41:48.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:41:49.877: INFO: namespace emptydir-4838 deletion completed in 9.891967461s

â€¢ [SLOW TEST:12.203 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:41:49.877: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 26 21:41:50.038: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd6cf7f8-b0ba-48cd-9d29-800d84e96b40" in namespace "downward-api-5603" to be "success or failure"
May 26 21:41:50.047: INFO: Pod "downwardapi-volume-fd6cf7f8-b0ba-48cd-9d29-800d84e96b40": Phase="Pending", Reason="", readiness=false. Elapsed: 8.412512ms
May 26 21:41:52.055: INFO: Pod "downwardapi-volume-fd6cf7f8-b0ba-48cd-9d29-800d84e96b40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01734404s
STEP: Saw pod success
May 26 21:41:52.056: INFO: Pod "downwardapi-volume-fd6cf7f8-b0ba-48cd-9d29-800d84e96b40" satisfied condition "success or failure"
May 26 21:41:52.064: INFO: Trying to get logs from node 10.215.60.34 pod downwardapi-volume-fd6cf7f8-b0ba-48cd-9d29-800d84e96b40 container client-container: <nil>
STEP: delete the pod
May 26 21:41:52.112: INFO: Waiting for pod downwardapi-volume-fd6cf7f8-b0ba-48cd-9d29-800d84e96b40 to disappear
May 26 21:41:52.122: INFO: Pod downwardapi-volume-fd6cf7f8-b0ba-48cd-9d29-800d84e96b40 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:41:52.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5603" for this suite.
May 26 21:42:00.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:42:02.028: INFO: namespace downward-api-5603 deletion completed in 9.891196622s

â€¢ [SLOW TEST:12.151 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:42:02.028: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:42:08.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1705" for this suite.
May 26 21:42:16.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:42:18.265: INFO: namespace emptydir-wrapper-1705 deletion completed in 9.900793894s

â€¢ [SLOW TEST:16.237 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:42:18.265: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
May 26 21:42:18.441: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-3962" to be "success or failure"
May 26 21:42:18.449: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 7.796433ms
May 26 21:42:20.462: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020209674s
STEP: Saw pod success
May 26 21:42:20.462: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
May 26 21:42:20.471: INFO: Trying to get logs from node 10.215.60.11 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
May 26 21:42:20.550: INFO: Waiting for pod pod-host-path-test to disappear
May 26 21:42:20.558: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:42:20.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-3962" for this suite.
May 26 21:42:28.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:42:30.457: INFO: namespace hostpath-3962 deletion completed in 9.885717268s

â€¢ [SLOW TEST:12.192 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:42:30.462: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-3c8d3472-b0c4-46e5-b500-1137d67b208f
STEP: Creating a pod to test consume configMaps
May 26 21:42:31.687: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4bb7c0ea-7ed6-4b07-a2ed-574696fefbdc" in namespace "projected-3914" to be "success or failure"
May 26 21:42:31.696: INFO: Pod "pod-projected-configmaps-4bb7c0ea-7ed6-4b07-a2ed-574696fefbdc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.017393ms
May 26 21:42:33.708: INFO: Pod "pod-projected-configmaps-4bb7c0ea-7ed6-4b07-a2ed-574696fefbdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02061476s
May 26 21:42:35.717: INFO: Pod "pod-projected-configmaps-4bb7c0ea-7ed6-4b07-a2ed-574696fefbdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029910728s
STEP: Saw pod success
May 26 21:42:35.717: INFO: Pod "pod-projected-configmaps-4bb7c0ea-7ed6-4b07-a2ed-574696fefbdc" satisfied condition "success or failure"
May 26 21:42:35.726: INFO: Trying to get logs from node 10.215.60.34 pod pod-projected-configmaps-4bb7c0ea-7ed6-4b07-a2ed-574696fefbdc container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 26 21:42:35.783: INFO: Waiting for pod pod-projected-configmaps-4bb7c0ea-7ed6-4b07-a2ed-574696fefbdc to disappear
May 26 21:42:35.795: INFO: Pod pod-projected-configmaps-4bb7c0ea-7ed6-4b07-a2ed-574696fefbdc no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:42:35.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3914" for this suite.
May 26 21:42:43.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:42:45.704: INFO: namespace projected-3914 deletion completed in 9.895694852s

â€¢ [SLOW TEST:15.242 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:42:45.705: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 21:42:45.845: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 26 21:42:50.856: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 26 21:42:58.874: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 26 21:43:00.883: INFO: Creating deployment "test-rollover-deployment"
May 26 21:43:00.907: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 26 21:43:02.926: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 26 21:43:02.945: INFO: Ensure that both replica sets have 1 created replica
May 26 21:43:02.967: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 26 21:43:02.987: INFO: Updating deployment test-rollover-deployment
May 26 21:43:02.987: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 26 21:43:05.004: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 26 21:43:05.023: INFO: Make sure deployment "test-rollover-deployment" is complete
May 26 21:43:05.041: INFO: all replica sets need to contain the pod-template-hash label
May 26 21:43:05.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126180, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126180, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126183, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126180, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 26 21:43:07.060: INFO: all replica sets need to contain the pod-template-hash label
May 26 21:43:07.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126180, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126180, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126183, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126180, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 26 21:43:09.061: INFO: all replica sets need to contain the pod-template-hash label
May 26 21:43:09.061: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126180, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126180, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126183, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126180, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 26 21:43:11.064: INFO: all replica sets need to contain the pod-template-hash label
May 26 21:43:11.064: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126180, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126180, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126180, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 26 21:43:13.060: INFO: all replica sets need to contain the pod-template-hash label
May 26 21:43:13.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126180, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126180, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126180, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 26 21:43:15.060: INFO: all replica sets need to contain the pod-template-hash label
May 26 21:43:15.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126180, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126180, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126180, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 26 21:43:17.061: INFO: all replica sets need to contain the pod-template-hash label
May 26 21:43:17.061: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126180, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126180, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126180, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 26 21:43:19.058: INFO: all replica sets need to contain the pod-template-hash label
May 26 21:43:19.059: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126180, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126180, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126190, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726126180, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 26 21:43:21.060: INFO: 
May 26 21:43:21.061: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May 26 21:43:21.095: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-932 /apis/apps/v1/namespaces/deployment-932/deployments/test-rollover-deployment 11f64434-fcec-408a-8eb6-e2800a102cfe 47282 2 2020-05-26 21:43:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0027ebb28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-05-26 21:43:00 +0000 UTC,LastTransitionTime:2020-05-26 21:43:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2020-05-26 21:43:20 +0000 UTC,LastTransitionTime:2020-05-26 21:43:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 26 21:43:21.117: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-932 /apis/apps/v1/namespaces/deployment-932/replicasets/test-rollover-deployment-7d7dc6548c f5d0bd32-f785-4057-bcf1-cb18ff12c401 47270 2 2020-05-26 21:43:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 11f64434-fcec-408a-8eb6-e2800a102cfe 0xc0027ebfe7 0xc0027ebfe8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003a3c048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 26 21:43:21.117: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 26 21:43:21.118: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-932 /apis/apps/v1/namespaces/deployment-932/replicasets/test-rollover-controller be29c004-786c-45c6-a00e-04afa2c0700a 47280 2 2020-05-26 21:42:45 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 11f64434-fcec-408a-8eb6-e2800a102cfe 0xc0027ebf17 0xc0027ebf18}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0027ebf78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 26 21:43:21.118: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-932 /apis/apps/v1/namespaces/deployment-932/replicasets/test-rollover-deployment-f6c94f66c e08ae104-8b9a-43f0-841e-a021945a562d 47180 2 2020-05-26 21:43:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 11f64434-fcec-408a-8eb6-e2800a102cfe 0xc003a3c0b0 0xc003a3c0b1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003a3c128 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 26 21:43:21.127: INFO: Pod "test-rollover-deployment-7d7dc6548c-hhd55" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-hhd55 test-rollover-deployment-7d7dc6548c- deployment-932 /api/v1/namespaces/deployment-932/pods/test-rollover-deployment-7d7dc6548c-hhd55 76b19884-bc48-468d-8d98-0117e0b964c1 47229 0 2020-05-26 21:43:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[cni.projectcalico.org/podIP:172.30.99.185/32 cni.projectcalico.org/podIPs:172.30.99.185/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.99.185"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c f5d0bd32-f785-4057-bcf1-cb18ff12c401 0xc00297f7d7 0xc00297f7d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2bmmb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2bmmb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2bmmb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gjqjf,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 21:43:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 21:43:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 21:43:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 21:43:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.11,PodIP:172.30.99.185,StartTime:2020-05-26 21:43:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-26 21:43:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://fa847c47799ffa6cf81e5a1655fb6a592c056bbad47a8fae07db4a331e14004c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.99.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:43:21.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-932" for this suite.
May 26 21:43:29.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:43:31.035: INFO: namespace deployment-932 deletion completed in 9.888481933s

â€¢ [SLOW TEST:45.330 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:43:31.036: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 26 21:43:31.288: INFO: Number of nodes with available pods: 0
May 26 21:43:31.288: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 21:43:32.324: INFO: Number of nodes with available pods: 0
May 26 21:43:32.324: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 21:43:33.312: INFO: Number of nodes with available pods: 1
May 26 21:43:33.312: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 21:43:34.319: INFO: Number of nodes with available pods: 1
May 26 21:43:34.319: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 21:43:35.312: INFO: Number of nodes with available pods: 1
May 26 21:43:35.312: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 21:43:36.319: INFO: Number of nodes with available pods: 1
May 26 21:43:36.319: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 21:43:37.325: INFO: Number of nodes with available pods: 1
May 26 21:43:37.325: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 21:43:38.330: INFO: Number of nodes with available pods: 1
May 26 21:43:38.330: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 21:43:39.310: INFO: Number of nodes with available pods: 1
May 26 21:43:39.311: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 21:43:40.321: INFO: Number of nodes with available pods: 1
May 26 21:43:40.321: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 21:43:41.311: INFO: Number of nodes with available pods: 1
May 26 21:43:41.311: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 21:43:42.323: INFO: Number of nodes with available pods: 3
May 26 21:43:42.323: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 26 21:43:42.381: INFO: Number of nodes with available pods: 2
May 26 21:43:42.381: INFO: Node 10.215.60.11 is running more than one daemon pod
May 26 21:43:43.404: INFO: Number of nodes with available pods: 2
May 26 21:43:43.404: INFO: Node 10.215.60.11 is running more than one daemon pod
May 26 21:43:44.404: INFO: Number of nodes with available pods: 2
May 26 21:43:44.405: INFO: Node 10.215.60.11 is running more than one daemon pod
May 26 21:43:45.403: INFO: Number of nodes with available pods: 2
May 26 21:43:45.403: INFO: Node 10.215.60.11 is running more than one daemon pod
May 26 21:43:46.406: INFO: Number of nodes with available pods: 2
May 26 21:43:46.406: INFO: Node 10.215.60.11 is running more than one daemon pod
May 26 21:43:47.407: INFO: Number of nodes with available pods: 2
May 26 21:43:47.407: INFO: Node 10.215.60.11 is running more than one daemon pod
May 26 21:43:48.402: INFO: Number of nodes with available pods: 2
May 26 21:43:48.402: INFO: Node 10.215.60.11 is running more than one daemon pod
May 26 21:43:49.403: INFO: Number of nodes with available pods: 2
May 26 21:43:49.403: INFO: Node 10.215.60.11 is running more than one daemon pod
May 26 21:43:50.406: INFO: Number of nodes with available pods: 2
May 26 21:43:50.407: INFO: Node 10.215.60.11 is running more than one daemon pod
May 26 21:43:51.406: INFO: Number of nodes with available pods: 2
May 26 21:43:51.406: INFO: Node 10.215.60.11 is running more than one daemon pod
May 26 21:43:52.405: INFO: Number of nodes with available pods: 2
May 26 21:43:52.405: INFO: Node 10.215.60.11 is running more than one daemon pod
May 26 21:43:53.415: INFO: Number of nodes with available pods: 2
May 26 21:43:53.415: INFO: Node 10.215.60.11 is running more than one daemon pod
May 26 21:43:54.406: INFO: Number of nodes with available pods: 3
May 26 21:43:54.406: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4749, will wait for the garbage collector to delete the pods
May 26 21:43:54.498: INFO: Deleting DaemonSet.extensions daemon-set took: 22.809316ms
May 26 21:43:54.898: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.397603ms
May 26 21:44:07.921: INFO: Number of nodes with available pods: 0
May 26 21:44:07.921: INFO: Number of running nodes: 0, number of available pods: 0
May 26 21:44:07.932: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4749/daemonsets","resourceVersion":"47647"},"items":null}

May 26 21:44:07.942: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4749/pods","resourceVersion":"47647"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:44:07.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4749" for this suite.
May 26 21:44:16.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:44:17.911: INFO: namespace daemonsets-4749 deletion completed in 9.914420103s

â€¢ [SLOW TEST:46.875 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:44:17.912: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-11438a28-f6d1-4777-b5aa-d497b597f18e
STEP: Creating a pod to test consume secrets
May 26 21:44:18.104: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-01cb4dfc-ccf7-4dd0-a1f5-340df991df1d" in namespace "projected-9232" to be "success or failure"
May 26 21:44:18.112: INFO: Pod "pod-projected-secrets-01cb4dfc-ccf7-4dd0-a1f5-340df991df1d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.966579ms
May 26 21:44:20.125: INFO: Pod "pod-projected-secrets-01cb4dfc-ccf7-4dd0-a1f5-340df991df1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020482342s
May 26 21:44:22.135: INFO: Pod "pod-projected-secrets-01cb4dfc-ccf7-4dd0-a1f5-340df991df1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030288457s
STEP: Saw pod success
May 26 21:44:22.135: INFO: Pod "pod-projected-secrets-01cb4dfc-ccf7-4dd0-a1f5-340df991df1d" satisfied condition "success or failure"
May 26 21:44:22.146: INFO: Trying to get logs from node 10.215.60.34 pod pod-projected-secrets-01cb4dfc-ccf7-4dd0-a1f5-340df991df1d container projected-secret-volume-test: <nil>
STEP: delete the pod
May 26 21:44:22.222: INFO: Waiting for pod pod-projected-secrets-01cb4dfc-ccf7-4dd0-a1f5-340df991df1d to disappear
May 26 21:44:22.230: INFO: Pod pod-projected-secrets-01cb4dfc-ccf7-4dd0-a1f5-340df991df1d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:44:22.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9232" for this suite.
May 26 21:44:30.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:44:32.131: INFO: namespace projected-9232 deletion completed in 9.888099536s

â€¢ [SLOW TEST:14.219 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:44:32.131: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 26 21:44:34.373: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:44:34.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6931" for this suite.
May 26 21:44:42.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:44:44.340: INFO: namespace container-runtime-6931 deletion completed in 9.896247437s

â€¢ [SLOW TEST:12.209 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:44:44.341: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-6c368a56-c195-43db-83a7-fd97191652c3 in namespace container-probe-180
May 26 21:44:46.523: INFO: Started pod busybox-6c368a56-c195-43db-83a7-fd97191652c3 in namespace container-probe-180
STEP: checking the pod's current state and verifying that restartCount is present
May 26 21:44:46.533: INFO: Initial restart count of pod busybox-6c368a56-c195-43db-83a7-fd97191652c3 is 0
May 26 21:45:38.822: INFO: Restart count of pod container-probe-180/busybox-6c368a56-c195-43db-83a7-fd97191652c3 is now 1 (52.288833543s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:45:38.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-180" for this suite.
May 26 21:45:46.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:45:48.761: INFO: namespace container-probe-180 deletion completed in 9.890258919s

â€¢ [SLOW TEST:64.420 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:45:48.761: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 21:45:49.000: INFO: (0) /api/v1/nodes/10.215.60.10/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 47.316604ms)
May 26 21:45:49.015: INFO: (1) /api/v1/nodes/10.215.60.10/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 14.975306ms)
May 26 21:45:49.035: INFO: (2) /api/v1/nodes/10.215.60.10/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 19.916699ms)
May 26 21:45:49.052: INFO: (3) /api/v1/nodes/10.215.60.10/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 17.714812ms)
May 26 21:45:49.067: INFO: (4) /api/v1/nodes/10.215.60.10/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 14.120834ms)
May 26 21:45:49.084: INFO: (5) /api/v1/nodes/10.215.60.10/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 17.089358ms)
May 26 21:45:49.099: INFO: (6) /api/v1/nodes/10.215.60.10/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 14.847949ms)
May 26 21:45:49.114: INFO: (7) /api/v1/nodes/10.215.60.10/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 15.511833ms)
May 26 21:45:49.130: INFO: (8) /api/v1/nodes/10.215.60.10/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 15.617189ms)
May 26 21:45:49.145: INFO: (9) /api/v1/nodes/10.215.60.10/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 15.282488ms)
May 26 21:45:49.160: INFO: (10) /api/v1/nodes/10.215.60.10/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 14.578609ms)
May 26 21:45:49.175: INFO: (11) /api/v1/nodes/10.215.60.10/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 15.224465ms)
May 26 21:45:49.192: INFO: (12) /api/v1/nodes/10.215.60.10/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 16.434843ms)
May 26 21:45:49.207: INFO: (13) /api/v1/nodes/10.215.60.10/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 15.706884ms)
May 26 21:45:49.223: INFO: (14) /api/v1/nodes/10.215.60.10/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 15.323893ms)
May 26 21:45:49.239: INFO: (15) /api/v1/nodes/10.215.60.10/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 16.308454ms)
May 26 21:45:49.254: INFO: (16) /api/v1/nodes/10.215.60.10/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 14.500778ms)
May 26 21:45:49.269: INFO: (17) /api/v1/nodes/10.215.60.10/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 15.175276ms)
May 26 21:45:49.285: INFO: (18) /api/v1/nodes/10.215.60.10/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 16.299362ms)
May 26 21:45:49.303: INFO: (19) /api/v1/nodes/10.215.60.10/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 17.220304ms)
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:45:49.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9714" for this suite.
May 26 21:45:57.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:45:58.359: INFO: namespace proxy-9714 deletion completed in 9.041257189s

â€¢ [SLOW TEST:9.598 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:45:58.359: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 26 21:45:58.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-8838'
May 26 21:45:58.653: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 26 21:45:58.654: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
May 26 21:46:00.682: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-jjf2s]
May 26 21:46:00.682: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-jjf2s" in namespace "kubectl-8838" to be "running and ready"
May 26 21:46:00.692: INFO: Pod "e2e-test-httpd-rc-jjf2s": Phase="Running", Reason="", readiness=true. Elapsed: 9.569567ms
May 26 21:46:00.692: INFO: Pod "e2e-test-httpd-rc-jjf2s" satisfied condition "running and ready"
May 26 21:46:00.692: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-jjf2s]
May 26 21:46:00.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 logs rc/e2e-test-httpd-rc --namespace=kubectl-8838'
May 26 21:46:00.902: INFO: stderr: ""
May 26 21:46:00.902: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.30.182.124. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.30.182.124. Set the 'ServerName' directive globally to suppress this message\n[Tue May 26 21:45:59.968182 2020] [mpm_event:notice] [pid 1:tid 140261914291048] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Tue May 26 21:45:59.968265 2020] [core:notice] [pid 1:tid 140261914291048] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
May 26 21:46:00.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 delete rc e2e-test-httpd-rc --namespace=kubectl-8838'
May 26 21:46:01.096: INFO: stderr: ""
May 26 21:46:01.096: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:46:01.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8838" for this suite.
May 26 21:46:15.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:46:17.008: INFO: namespace kubectl-8838 deletion completed in 15.887519751s

â€¢ [SLOW TEST:18.650 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:46:17.013: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 21:46:17.128: INFO: Creating ReplicaSet my-hostname-basic-5cac9253-4ec9-4408-b346-d2c0703889b5
May 26 21:46:17.161: INFO: Pod name my-hostname-basic-5cac9253-4ec9-4408-b346-d2c0703889b5: Found 0 pods out of 1
May 26 21:46:22.171: INFO: Pod name my-hostname-basic-5cac9253-4ec9-4408-b346-d2c0703889b5: Found 1 pods out of 1
May 26 21:46:22.171: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-5cac9253-4ec9-4408-b346-d2c0703889b5" is running
May 26 21:46:22.179: INFO: Pod "my-hostname-basic-5cac9253-4ec9-4408-b346-d2c0703889b5-8t8jl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-26 21:46:17 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-26 21:46:18 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-26 21:46:18 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-26 21:46:17 +0000 UTC Reason: Message:}])
May 26 21:46:22.179: INFO: Trying to dial the pod
May 26 21:46:27.228: INFO: Controller my-hostname-basic-5cac9253-4ec9-4408-b346-d2c0703889b5: Got expected result from replica 1 [my-hostname-basic-5cac9253-4ec9-4408-b346-d2c0703889b5-8t8jl]: "my-hostname-basic-5cac9253-4ec9-4408-b346-d2c0703889b5-8t8jl", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:46:27.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2202" for this suite.
May 26 21:46:35.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:46:37.151: INFO: namespace replicaset-2202 deletion completed in 9.899890512s

â€¢ [SLOW TEST:20.139 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:46:37.155: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0526 21:46:47.483067      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 26 21:46:47.483: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:46:47.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2631" for this suite.
May 26 21:46:55.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:46:57.387: INFO: namespace gc-2631 deletion completed in 9.888610531s

â€¢ [SLOW TEST:20.232 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:46:57.387: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
May 26 21:46:57.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 create -f - --namespace=kubectl-8484'
May 26 21:46:58.081: INFO: stderr: ""
May 26 21:46:58.081: INFO: stdout: "pod/pause created\n"
May 26 21:46:58.081: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 26 21:46:58.081: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8484" to be "running and ready"
May 26 21:46:58.091: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 10.286845ms
May 26 21:47:00.101: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.020318189s
May 26 21:47:00.101: INFO: Pod "pause" satisfied condition "running and ready"
May 26 21:47:00.101: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
May 26 21:47:00.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 label pods pause testing-label=testing-label-value --namespace=kubectl-8484'
May 26 21:47:00.272: INFO: stderr: ""
May 26 21:47:00.272: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 26 21:47:00.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pod pause -L testing-label --namespace=kubectl-8484'
May 26 21:47:00.398: INFO: stderr: ""
May 26 21:47:00.398: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 26 21:47:00.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 label pods pause testing-label- --namespace=kubectl-8484'
May 26 21:47:00.572: INFO: stderr: ""
May 26 21:47:00.572: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 26 21:47:00.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pod pause -L testing-label --namespace=kubectl-8484'
May 26 21:47:00.706: INFO: stderr: ""
May 26 21:47:00.706: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
May 26 21:47:00.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 delete --grace-period=0 --force -f - --namespace=kubectl-8484'
May 26 21:47:00.903: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 26 21:47:00.904: INFO: stdout: "pod \"pause\" force deleted\n"
May 26 21:47:00.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get rc,svc -l name=pause --no-headers --namespace=kubectl-8484'
May 26 21:47:01.071: INFO: stderr: "No resources found in kubectl-8484 namespace.\n"
May 26 21:47:01.071: INFO: stdout: ""
May 26 21:47:01.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods -l name=pause --namespace=kubectl-8484 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 26 21:47:01.228: INFO: stderr: ""
May 26 21:47:01.228: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:47:01.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8484" for this suite.
May 26 21:47:09.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:47:11.138: INFO: namespace kubectl-8484 deletion completed in 9.887633395s

â€¢ [SLOW TEST:13.751 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:47:11.143: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-29668b18-f601-4b94-88ae-ff519f5739aa
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:47:11.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6802" for this suite.
May 26 21:47:19.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:47:21.236: INFO: namespace secrets-6802 deletion completed in 9.923389599s

â€¢ [SLOW TEST:10.093 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:47:21.238: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 26 21:47:22.456: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ced31816-7eb7-4210-979a-27524b5237b1" in namespace "downward-api-2720" to be "success or failure"
May 26 21:47:22.465: INFO: Pod "downwardapi-volume-ced31816-7eb7-4210-979a-27524b5237b1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.22095ms
May 26 21:47:24.475: INFO: Pod "downwardapi-volume-ced31816-7eb7-4210-979a-27524b5237b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019188966s
STEP: Saw pod success
May 26 21:47:24.475: INFO: Pod "downwardapi-volume-ced31816-7eb7-4210-979a-27524b5237b1" satisfied condition "success or failure"
May 26 21:47:24.485: INFO: Trying to get logs from node 10.215.60.34 pod downwardapi-volume-ced31816-7eb7-4210-979a-27524b5237b1 container client-container: <nil>
STEP: delete the pod
May 26 21:47:24.557: INFO: Waiting for pod downwardapi-volume-ced31816-7eb7-4210-979a-27524b5237b1 to disappear
May 26 21:47:24.568: INFO: Pod downwardapi-volume-ced31816-7eb7-4210-979a-27524b5237b1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:47:24.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2720" for this suite.
May 26 21:47:32.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:47:34.477: INFO: namespace downward-api-2720 deletion completed in 9.893629765s

â€¢ [SLOW TEST:13.239 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:47:34.477: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May 26 21:47:34.586: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 26 21:47:34.641: INFO: Waiting for terminating namespaces to be deleted...
May 26 21:47:34.654: INFO: 
Logging pods the kubelet thinks is on node 10.215.60.10 before test
May 26 21:47:34.740: INFO: dns-operator-c68448f89-lfqlv from openshift-dns-operator started at 2020-05-26 19:59:02 +0000 UTC (2 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container dns-operator ready: true, restart count 0
May 26 21:47:34.740: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:47:34.740: INFO: openshift-service-catalog-controller-manager-operator-86b49fb6n from openshift-service-catalog-controller-manager-operator started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container operator ready: true, restart count 1
May 26 21:47:34.740: INFO: downloads-8479fbbf57-z9pkl from openshift-console started at 2020-05-26 19:59:01 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container download-server ready: true, restart count 0
May 26 21:47:34.740: INFO: cluster-image-registry-operator-6f78cddbbc-qrdmf from openshift-image-registry started at 2020-05-26 19:59:03 +0000 UTC (2 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
May 26 21:47:34.740: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
May 26 21:47:34.740: INFO: service-ca-operator-7bb6cf7fbc-j7nvb from openshift-service-ca-operator started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container operator ready: true, restart count 0
May 26 21:47:34.740: INFO: openshift-service-catalog-apiserver-operator-5897998845-kpk9m from openshift-service-catalog-apiserver-operator started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container operator ready: true, restart count 1
May 26 21:47:34.740: INFO: calico-typha-f4f4dbb8c-s262v from calico-system started at 2020-05-26 20:00:43 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container calico-typha ready: true, restart count 0
May 26 21:47:34.740: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-05-26 20:05:06 +0000 UTC (3 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container alertmanager ready: true, restart count 0
May 26 21:47:34.740: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 26 21:47:34.740: INFO: 	Container config-reloader ready: true, restart count 0
May 26 21:47:34.740: INFO: multus-lh654 from openshift-multus started at 2020-05-26 19:58:00 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container kube-multus ready: true, restart count 0
May 26 21:47:34.740: INFO: cluster-storage-operator-8696454489-8wttw from openshift-cluster-storage-operator started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container cluster-storage-operator ready: true, restart count 0
May 26 21:47:34.740: INFO: olm-operator-9d9b9dc65-rg7n5 from openshift-operator-lifecycle-manager started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container olm-operator ready: true, restart count 0
May 26 21:47:34.740: INFO: node-ca-q2fgf from openshift-image-registry started at 2020-05-26 20:00:40 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container node-ca ready: true, restart count 0
May 26 21:47:34.740: INFO: certified-operators-7c7bd497cb-hjpfx from openshift-marketplace started at 2020-05-26 20:01:33 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container certified-operators ready: true, restart count 0
May 26 21:47:34.740: INFO: ibm-master-proxy-static-10.215.60.10 from kube-system started at 2020-05-26 19:57:58 +0000 UTC (2 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May 26 21:47:34.740: INFO: 	Container pause ready: true, restart count 0
May 26 21:47:34.740: INFO: openshift-kube-proxy-ns7pk from openshift-kube-proxy started at 2020-05-26 19:58:04 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container kube-proxy ready: true, restart count 0
May 26 21:47:34.740: INFO: ibm-cloud-provider-ip-159-122-104-134-84fdcfb5c5-cgj49 from ibm-system started at 2020-05-26 20:02:23 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container ibm-cloud-provider-ip-159-122-104-134 ready: true, restart count 0
May 26 21:47:34.740: INFO: thanos-querier-7f4dfb8d6f-5m6ft from openshift-monitoring started at 2020-05-26 20:05:38 +0000 UTC (4 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:47:34.740: INFO: 	Container oauth-proxy ready: true, restart count 0
May 26 21:47:34.740: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 26 21:47:34.740: INFO: 	Container thanos-querier ready: true, restart count 0
May 26 21:47:34.740: INFO: multus-admission-controller-4kd9c from openshift-multus started at 2020-05-26 19:59:01 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container multus-admission-controller ready: true, restart count 0
May 26 21:47:34.740: INFO: catalog-operator-6956d96f67-5xvls from openshift-operator-lifecycle-manager started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container catalog-operator ready: true, restart count 0
May 26 21:47:34.740: INFO: console-operator-56c4d6445c-brk5r from openshift-console-operator started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container console-operator ready: true, restart count 1
May 26 21:47:34.740: INFO: ingress-operator-f6594bf4d-m74mj from openshift-ingress-operator started at 2020-05-26 19:59:03 +0000 UTC (2 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container ingress-operator ready: true, restart count 0
May 26 21:47:34.740: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:47:34.740: INFO: tuned-22gmz from openshift-cluster-node-tuning-operator started at 2020-05-26 19:59:39 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container tuned ready: true, restart count 0
May 26 21:47:34.740: INFO: prometheus-adapter-5646d5d5dd-ql9h7 from openshift-monitoring started at 2020-05-26 20:05:35 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 26 21:47:34.740: INFO: registry-pvc-permissions-j5wnp from openshift-image-registry started at 2020-05-26 20:03:30 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container pvc-permissions ready: false, restart count 0
May 26 21:47:34.740: INFO: ibm-keepalived-watcher-bl295 from kube-system started at 2020-05-26 19:58:00 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container keepalived-watcher ready: true, restart count 0
May 26 21:47:34.740: INFO: downloads-8479fbbf57-qhklc from openshift-console started at 2020-05-26 19:59:01 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container download-server ready: true, restart count 0
May 26 21:47:34.740: INFO: calico-kube-controllers-599969f895-xfmxb from calico-system started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 26 21:47:34.740: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-05-26 20:05:48 +0000 UTC (7 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:47:34.740: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 26 21:47:34.740: INFO: 	Container prometheus ready: true, restart count 1
May 26 21:47:34.740: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May 26 21:47:34.740: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 26 21:47:34.740: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May 26 21:47:34.740: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 26 21:47:34.740: INFO: ibmcloud-block-storage-driver-sncrt from kube-system started at 2020-05-26 19:58:07 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May 26 21:47:34.740: INFO: calico-node-p2dwh from calico-system started at 2020-05-26 19:58:46 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container calico-node ready: true, restart count 0
May 26 21:47:34.740: INFO: ibm-file-plugin-5999bd7d7d-mcpqg from kube-system started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
May 26 21:47:34.740: INFO: cluster-node-tuning-operator-58cb5999f5-dqqxl from openshift-cluster-node-tuning-operator started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
May 26 21:47:34.740: INFO: cluster-monitoring-operator-64b79969cc-g94mm from openshift-monitoring started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
May 26 21:47:34.740: INFO: marketplace-operator-6d86c46f6b-j7hh9 from openshift-marketplace started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container marketplace-operator ready: true, restart count 0
May 26 21:47:34.740: INFO: dns-default-s9jv4 from openshift-dns started at 2020-05-26 20:00:40 +0000 UTC (2 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container dns ready: true, restart count 0
May 26 21:47:34.740: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 26 21:47:34.740: INFO: sonobuoy-systemd-logs-daemon-set-e1fd62b7d6894c57-wrl9n from sonobuoy started at 2020-05-26 21:25:54 +0000 UTC (2 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 26 21:47:34.740: INFO: 	Container systemd-logs ready: true, restart count 0
May 26 21:47:34.740: INFO: ibm-storage-watcher-5f6b5dbcd4-kn4xs from kube-system started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
May 26 21:47:34.740: INFO: ibmcloud-block-storage-plugin-9d877d7bc-kjrsl from kube-system started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
May 26 21:47:34.740: INFO: node-exporter-b6pqt from openshift-monitoring started at 2020-05-26 19:59:35 +0000 UTC (2 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:47:34.740: INFO: 	Container node-exporter ready: true, restart count 0
May 26 21:47:34.740: INFO: apiservice-cabundle-injector-5c88555f6d-r7p9b from openshift-service-ca started at 2020-05-26 19:59:45 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
May 26 21:47:34.740: INFO: redhat-operators-7b74fd75b7-8dvbx from openshift-marketplace started at 2020-05-26 20:01:34 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container redhat-operators ready: true, restart count 0
May 26 21:47:34.740: INFO: packageserver-7849f6fc4f-b6mlz from openshift-operator-lifecycle-manager started at 2020-05-26 20:02:55 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.740: INFO: 	Container packageserver ready: true, restart count 0
May 26 21:47:34.740: INFO: 
Logging pods the kubelet thinks is on node 10.215.60.11 before test
May 26 21:47:34.817: INFO: community-operators-674864f5c4-bwwnn from openshift-marketplace started at 2020-05-26 20:01:32 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.817: INFO: 	Container community-operators ready: true, restart count 0
May 26 21:47:34.817: INFO: router-default-6cfd8db4bf-jv7v4 from openshift-ingress started at 2020-05-26 20:01:45 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.817: INFO: 	Container router ready: true, restart count 0
May 26 21:47:34.817: INFO: ibm-keepalived-watcher-w8tv5 from kube-system started at 2020-05-26 19:57:34 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.817: INFO: 	Container keepalived-watcher ready: true, restart count 0
May 26 21:47:34.817: INFO: openshift-kube-proxy-h4l9g from openshift-kube-proxy started at 2020-05-26 19:58:05 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.817: INFO: 	Container kube-proxy ready: true, restart count 0
May 26 21:47:34.817: INFO: calico-typha-f4f4dbb8c-jg59b from calico-system started at 2020-05-26 20:00:43 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.817: INFO: 	Container calico-typha ready: true, restart count 0
May 26 21:47:34.817: INFO: multus-admission-controller-mqrcn from openshift-multus started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.817: INFO: 	Container multus-admission-controller ready: true, restart count 0
May 26 21:47:34.817: INFO: node-exporter-spjk9 from openshift-monitoring started at 2020-05-26 19:59:36 +0000 UTC (2 container statuses recorded)
May 26 21:47:34.817: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:47:34.817: INFO: 	Container node-exporter ready: true, restart count 0
May 26 21:47:34.817: INFO: calico-node-6tv2z from calico-system started at 2020-05-26 19:58:46 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.817: INFO: 	Container calico-node ready: true, restart count 0
May 26 21:47:34.817: INFO: dns-default-jlw4t from openshift-dns started at 2020-05-26 20:00:40 +0000 UTC (2 container statuses recorded)
May 26 21:47:34.817: INFO: 	Container dns ready: true, restart count 0
May 26 21:47:34.817: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 26 21:47:34.817: INFO: sonobuoy-systemd-logs-daemon-set-e1fd62b7d6894c57-gzfvj from sonobuoy started at 2020-05-26 21:25:54 +0000 UTC (2 container statuses recorded)
May 26 21:47:34.817: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 26 21:47:34.817: INFO: 	Container systemd-logs ready: true, restart count 0
May 26 21:47:34.817: INFO: network-operator-76d6fbdbb8-5hgl8 from openshift-network-operator started at 2020-05-26 19:57:36 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.817: INFO: 	Container network-operator ready: true, restart count 0
May 26 21:47:34.817: INFO: kube-state-metrics-7498bc479d-5pbn8 from openshift-monitoring started at 2020-05-26 19:59:34 +0000 UTC (3 container statuses recorded)
May 26 21:47:34.818: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 26 21:47:34.818: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 26 21:47:34.818: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 26 21:47:34.818: INFO: tuned-88qws from openshift-cluster-node-tuning-operator started at 2020-05-26 19:59:39 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.818: INFO: 	Container tuned ready: true, restart count 0
May 26 21:47:34.818: INFO: node-ca-jh22q from openshift-image-registry started at 2020-05-26 20:00:40 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.818: INFO: 	Container node-ca ready: true, restart count 0
May 26 21:47:34.818: INFO: vpn-6847db666c-dd6pq from kube-system started at 2020-05-26 20:03:07 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.818: INFO: 	Container vpn ready: true, restart count 0
May 26 21:47:34.818: INFO: thanos-querier-7f4dfb8d6f-w2z69 from openshift-monitoring started at 2020-05-26 20:05:34 +0000 UTC (4 container statuses recorded)
May 26 21:47:34.818: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:47:34.818: INFO: 	Container oauth-proxy ready: true, restart count 0
May 26 21:47:34.818: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 26 21:47:34.818: INFO: 	Container thanos-querier ready: true, restart count 0
May 26 21:47:34.818: INFO: ibm-master-proxy-static-10.215.60.11 from kube-system started at 2020-05-26 19:57:32 +0000 UTC (2 container statuses recorded)
May 26 21:47:34.818: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May 26 21:47:34.818: INFO: 	Container pause ready: true, restart count 0
May 26 21:47:34.818: INFO: tigera-operator-798cfbf7dd-q5frt from tigera-operator started at 2020-05-26 19:57:36 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.818: INFO: 	Container tigera-operator ready: true, restart count 2
May 26 21:47:34.818: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-05-26 20:05:57 +0000 UTC (7 container statuses recorded)
May 26 21:47:34.818: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:47:34.818: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 26 21:47:34.818: INFO: 	Container prometheus ready: true, restart count 1
May 26 21:47:34.818: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May 26 21:47:34.818: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 26 21:47:34.818: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May 26 21:47:34.818: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 26 21:47:34.818: INFO: image-registry-766b5fd974-j2s4m from openshift-image-registry started at 2020-05-26 20:03:30 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.818: INFO: 	Container registry ready: true, restart count 0
May 26 21:47:34.818: INFO: prometheus-operator-9d5b5788b-dpcht from openshift-monitoring started at 2020-05-26 20:04:36 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.818: INFO: 	Container prometheus-operator ready: true, restart count 0
May 26 21:47:34.818: INFO: cluster-samples-operator-759dd556bf-bgg5n from openshift-cluster-samples-operator started at 2020-05-26 19:59:44 +0000 UTC (2 container statuses recorded)
May 26 21:47:34.818: INFO: 	Container cluster-samples-operator ready: true, restart count 0
May 26 21:47:34.818: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
May 26 21:47:34.818: INFO: service-serving-cert-signer-69cddbb454-wjjcq from openshift-service-ca started at 2020-05-26 19:59:44 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.818: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
May 26 21:47:34.818: INFO: ibmcloud-block-storage-driver-zq7xq from kube-system started at 2020-05-26 19:57:36 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.818: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May 26 21:47:34.818: INFO: openshift-state-metrics-74997686f-cgqlx from openshift-monitoring started at 2020-05-26 19:59:35 +0000 UTC (3 container statuses recorded)
May 26 21:47:34.818: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 26 21:47:34.818: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 26 21:47:34.818: INFO: 	Container openshift-state-metrics ready: true, restart count 0
May 26 21:47:34.818: INFO: console-d8768d4f5-m8fb9 from openshift-console started at 2020-05-26 20:01:37 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.818: INFO: 	Container console ready: true, restart count 0
May 26 21:47:34.818: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-05-26 20:05:02 +0000 UTC (3 container statuses recorded)
May 26 21:47:34.818: INFO: 	Container alertmanager ready: true, restart count 0
May 26 21:47:34.818: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 26 21:47:34.818: INFO: 	Container config-reloader ready: true, restart count 0
May 26 21:47:34.818: INFO: multus-88m9q from openshift-multus started at 2020-05-26 19:57:58 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.818: INFO: 	Container kube-multus ready: true, restart count 0
May 26 21:47:34.818: INFO: configmap-cabundle-injector-795c74476d-6hzjj from openshift-service-ca started at 2020-05-26 19:59:45 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.818: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
May 26 21:47:34.818: INFO: 
Logging pods the kubelet thinks is on node 10.215.60.34 before test
May 26 21:47:34.869: INFO: node-exporter-7b5t7 from openshift-monitoring started at 2020-05-26 19:59:36 +0000 UTC (2 container statuses recorded)
May 26 21:47:34.869: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:47:34.869: INFO: 	Container node-exporter ready: true, restart count 0
May 26 21:47:34.870: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-05-26 20:04:50 +0000 UTC (3 container statuses recorded)
May 26 21:47:34.870: INFO: 	Container alertmanager ready: true, restart count 0
May 26 21:47:34.870: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 26 21:47:34.870: INFO: 	Container config-reloader ready: true, restart count 0
May 26 21:47:34.870: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-05-26 20:01:20 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.870: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
May 26 21:47:34.870: INFO: sonobuoy-e2e-job-543b6d87a6d14642 from sonobuoy started at 2020-05-26 21:25:53 +0000 UTC (2 container statuses recorded)
May 26 21:47:34.870: INFO: 	Container e2e ready: true, restart count 0
May 26 21:47:34.870: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 26 21:47:34.870: INFO: ibm-cloud-provider-ip-159-122-104-134-84fdcfb5c5-7ghc6 from ibm-system started at 2020-05-26 20:02:30 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.870: INFO: 	Container ibm-cloud-provider-ip-159-122-104-134 ready: true, restart count 0
May 26 21:47:34.870: INFO: openshift-kube-proxy-2sfd8 from openshift-kube-proxy started at 2020-05-26 19:58:23 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.870: INFO: 	Container kube-proxy ready: true, restart count 0
May 26 21:47:34.870: INFO: calico-node-9s8hb from calico-system started at 2020-05-26 19:58:46 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.870: INFO: 	Container calico-node ready: true, restart count 0
May 26 21:47:34.870: INFO: multus-j2db5 from openshift-multus started at 2020-05-26 19:58:23 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.870: INFO: 	Container kube-multus ready: true, restart count 0
May 26 21:47:34.870: INFO: grafana-5648b7fdd9-rlqss from openshift-monitoring started at 2020-05-26 20:04:53 +0000 UTC (2 container statuses recorded)
May 26 21:47:34.870: INFO: 	Container grafana ready: true, restart count 0
May 26 21:47:34.870: INFO: 	Container grafana-proxy ready: true, restart count 0
May 26 21:47:34.870: INFO: sonobuoy from sonobuoy started at 2020-05-26 21:25:44 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.870: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 26 21:47:34.870: INFO: sonobuoy-systemd-logs-daemon-set-e1fd62b7d6894c57-mptms from sonobuoy started at 2020-05-26 21:25:54 +0000 UTC (2 container statuses recorded)
May 26 21:47:34.870: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 26 21:47:34.870: INFO: 	Container systemd-logs ready: true, restart count 0
May 26 21:47:34.870: INFO: ibm-master-proxy-static-10.215.60.34 from kube-system started at 2020-05-26 19:58:20 +0000 UTC (2 container statuses recorded)
May 26 21:47:34.870: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May 26 21:47:34.870: INFO: 	Container pause ready: true, restart count 0
May 26 21:47:34.870: INFO: calico-typha-f4f4dbb8c-7f56v from calico-system started at 2020-05-26 19:58:45 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.870: INFO: 	Container calico-typha ready: true, restart count 0
May 26 21:47:34.870: INFO: ibmcloud-block-storage-driver-nwvlx from kube-system started at 2020-05-26 19:58:31 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.870: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May 26 21:47:34.870: INFO: dns-default-qpbzv from openshift-dns started at 2020-05-26 20:00:40 +0000 UTC (2 container statuses recorded)
May 26 21:47:34.870: INFO: 	Container dns ready: true, restart count 0
May 26 21:47:34.870: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 26 21:47:34.870: INFO: console-d8768d4f5-wxtmt from openshift-console started at 2020-05-26 20:01:27 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.870: INFO: 	Container console ready: true, restart count 0
May 26 21:47:34.870: INFO: router-default-6cfd8db4bf-6dl6d from openshift-ingress started at 2020-05-26 20:02:24 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.870: INFO: 	Container router ready: true, restart count 0
May 26 21:47:34.870: INFO: prometheus-adapter-5646d5d5dd-jw54m from openshift-monitoring started at 2020-05-26 20:05:35 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.870: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 26 21:47:34.870: INFO: telemeter-client-6745779989-724qv from openshift-monitoring started at 2020-05-26 20:04:46 +0000 UTC (3 container statuses recorded)
May 26 21:47:34.870: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:47:34.870: INFO: 	Container reload ready: true, restart count 0
May 26 21:47:34.870: INFO: 	Container telemeter-client ready: true, restart count 0
May 26 21:47:34.870: INFO: tuned-4sgbw from openshift-cluster-node-tuning-operator started at 2020-05-26 19:59:39 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.870: INFO: 	Container tuned ready: true, restart count 0
May 26 21:47:34.870: INFO: node-ca-6c8qk from openshift-image-registry started at 2020-05-26 20:00:40 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.870: INFO: 	Container node-ca ready: true, restart count 0
May 26 21:47:34.870: INFO: packageserver-7849f6fc4f-vlssd from openshift-operator-lifecycle-manager started at 2020-05-26 20:03:07 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.870: INFO: 	Container packageserver ready: true, restart count 0
May 26 21:47:34.870: INFO: ibm-keepalived-watcher-gtvct from kube-system started at 2020-05-26 19:58:23 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.870: INFO: 	Container keepalived-watcher ready: true, restart count 0
May 26 21:47:34.870: INFO: multus-admission-controller-grkq9 from openshift-multus started at 2020-05-26 19:59:24 +0000 UTC (1 container statuses recorded)
May 26 21:47:34.870: INFO: 	Container multus-admission-controller ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-3ad4d689-9986-4421-9fd8-76065d9c001d 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-3ad4d689-9986-4421-9fd8-76065d9c001d off the node 10.215.60.34
STEP: verifying the node doesn't have the label kubernetes.io/e2e-3ad4d689-9986-4421-9fd8-76065d9c001d
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:47:39.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7360" for this suite.
May 26 21:47:51.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:47:53.003: INFO: namespace sched-pred-7360 deletion completed in 13.88811793s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:18.526 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:47:53.003: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:47:55.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9637" for this suite.
May 26 21:48:43.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:48:45.159: INFO: namespace kubelet-test-9637 deletion completed in 49.903024355s

â€¢ [SLOW TEST:52.156 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:48:45.162: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 26 21:48:47.374: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:48:47.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9940" for this suite.
May 26 21:48:55.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:48:57.341: INFO: namespace container-runtime-9940 deletion completed in 9.899228827s

â€¢ [SLOW TEST:12.180 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:48:57.343: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 21:48:57.493: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-0f0ab0cb-67fa-44e5-ae9d-f72e6021a0a9
STEP: Creating secret with name s-test-opt-upd-47742ed0-1694-4acc-94ce-fab8f271ee37
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-0f0ab0cb-67fa-44e5-ae9d-f72e6021a0a9
STEP: Updating secret s-test-opt-upd-47742ed0-1694-4acc-94ce-fab8f271ee37
STEP: Creating secret with name s-test-opt-create-0bbdb71e-d9b3-42ec-894f-cfd05f316678
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:50:06.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-470" for this suite.
May 26 21:50:22.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:50:24.480: INFO: namespace secrets-470 deletion completed in 17.897767049s

â€¢ [SLOW TEST:87.138 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:50:24.483: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
May 26 21:50:24.602: INFO: Waiting up to 1m0s for all nodes to be ready
May 26 21:51:24.699: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 21:51:24.712: INFO: Starting informer...
STEP: Starting pod...
May 26 21:51:24.955: INFO: Pod is running on 10.215.60.34. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
May 26 21:51:24.995: INFO: Pod wasn't evicted. Proceeding
May 26 21:51:24.995: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
May 26 21:52:40.043: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:52:40.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-5201" for this suite.
May 26 21:52:54.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:52:55.953: INFO: namespace taint-single-pod-5201 deletion completed in 15.892286579s

â€¢ [SLOW TEST:151.470 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:52:55.953: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May 26 21:52:56.073: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 26 21:52:56.121: INFO: Waiting for terminating namespaces to be deleted...
May 26 21:52:56.132: INFO: 
Logging pods the kubelet thinks is on node 10.215.60.10 before test
May 26 21:52:56.223: INFO: registry-pvc-permissions-j5wnp from openshift-image-registry started at 2020-05-26 20:03:30 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container pvc-permissions ready: false, restart count 0
May 26 21:52:56.223: INFO: thanos-querier-7f4dfb8d6f-5m6ft from openshift-monitoring started at 2020-05-26 20:05:38 +0000 UTC (4 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:52:56.223: INFO: 	Container oauth-proxy ready: true, restart count 0
May 26 21:52:56.223: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 26 21:52:56.223: INFO: 	Container thanos-querier ready: true, restart count 0
May 26 21:52:56.223: INFO: multus-admission-controller-4kd9c from openshift-multus started at 2020-05-26 19:59:01 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container multus-admission-controller ready: true, restart count 0
May 26 21:52:56.223: INFO: catalog-operator-6956d96f67-5xvls from openshift-operator-lifecycle-manager started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container catalog-operator ready: true, restart count 0
May 26 21:52:56.223: INFO: console-operator-56c4d6445c-brk5r from openshift-console-operator started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container console-operator ready: true, restart count 1
May 26 21:52:56.223: INFO: ingress-operator-f6594bf4d-m74mj from openshift-ingress-operator started at 2020-05-26 19:59:03 +0000 UTC (2 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container ingress-operator ready: true, restart count 0
May 26 21:52:56.223: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:52:56.223: INFO: tuned-22gmz from openshift-cluster-node-tuning-operator started at 2020-05-26 19:59:39 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container tuned ready: true, restart count 0
May 26 21:52:56.223: INFO: prometheus-adapter-5646d5d5dd-ql9h7 from openshift-monitoring started at 2020-05-26 20:05:35 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 26 21:52:56.223: INFO: ibm-keepalived-watcher-bl295 from kube-system started at 2020-05-26 19:58:00 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container keepalived-watcher ready: true, restart count 0
May 26 21:52:56.223: INFO: downloads-8479fbbf57-qhklc from openshift-console started at 2020-05-26 19:59:01 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container download-server ready: true, restart count 0
May 26 21:52:56.223: INFO: calico-kube-controllers-599969f895-xfmxb from calico-system started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 26 21:52:56.223: INFO: dns-default-s9jv4 from openshift-dns started at 2020-05-26 20:00:40 +0000 UTC (2 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container dns ready: true, restart count 0
May 26 21:52:56.223: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 26 21:52:56.223: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-05-26 20:05:48 +0000 UTC (7 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:52:56.223: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 26 21:52:56.223: INFO: 	Container prometheus ready: true, restart count 1
May 26 21:52:56.223: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May 26 21:52:56.223: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 26 21:52:56.223: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May 26 21:52:56.223: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 26 21:52:56.223: INFO: ibmcloud-block-storage-driver-sncrt from kube-system started at 2020-05-26 19:58:07 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May 26 21:52:56.223: INFO: calico-node-p2dwh from calico-system started at 2020-05-26 19:58:46 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container calico-node ready: true, restart count 0
May 26 21:52:56.223: INFO: ibm-file-plugin-5999bd7d7d-mcpqg from kube-system started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
May 26 21:52:56.223: INFO: cluster-node-tuning-operator-58cb5999f5-dqqxl from openshift-cluster-node-tuning-operator started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
May 26 21:52:56.223: INFO: cluster-monitoring-operator-64b79969cc-g94mm from openshift-monitoring started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
May 26 21:52:56.223: INFO: marketplace-operator-6d86c46f6b-j7hh9 from openshift-marketplace started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container marketplace-operator ready: true, restart count 0
May 26 21:52:56.223: INFO: sonobuoy-systemd-logs-daemon-set-e1fd62b7d6894c57-wrl9n from sonobuoy started at 2020-05-26 21:25:54 +0000 UTC (2 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 26 21:52:56.223: INFO: 	Container systemd-logs ready: true, restart count 0
May 26 21:52:56.223: INFO: ibm-storage-watcher-5f6b5dbcd4-kn4xs from kube-system started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
May 26 21:52:56.223: INFO: ibmcloud-block-storage-plugin-9d877d7bc-kjrsl from kube-system started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
May 26 21:52:56.223: INFO: node-exporter-b6pqt from openshift-monitoring started at 2020-05-26 19:59:35 +0000 UTC (2 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:52:56.223: INFO: 	Container node-exporter ready: true, restart count 0
May 26 21:52:56.223: INFO: apiservice-cabundle-injector-5c88555f6d-r7p9b from openshift-service-ca started at 2020-05-26 19:59:45 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
May 26 21:52:56.223: INFO: redhat-operators-7b74fd75b7-8dvbx from openshift-marketplace started at 2020-05-26 20:01:34 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container redhat-operators ready: true, restart count 0
May 26 21:52:56.223: INFO: dns-operator-c68448f89-lfqlv from openshift-dns-operator started at 2020-05-26 19:59:02 +0000 UTC (2 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container dns-operator ready: true, restart count 0
May 26 21:52:56.223: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:52:56.223: INFO: openshift-service-catalog-controller-manager-operator-86b49fb6n from openshift-service-catalog-controller-manager-operator started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container operator ready: true, restart count 1
May 26 21:52:56.223: INFO: grafana-5648b7fdd9-rmjbk from openshift-monitoring started at 2020-05-26 21:51:25 +0000 UTC (2 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container grafana ready: true, restart count 0
May 26 21:52:56.223: INFO: 	Container grafana-proxy ready: true, restart count 0
May 26 21:52:56.223: INFO: router-default-6cfd8db4bf-7dstv from openshift-ingress started at 2020-05-26 21:51:25 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container router ready: true, restart count 0
May 26 21:52:56.223: INFO: packageserver-7d5f79c4db-89cw9 from openshift-operator-lifecycle-manager started at 2020-05-26 21:51:35 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container packageserver ready: true, restart count 0
May 26 21:52:56.223: INFO: downloads-8479fbbf57-z9pkl from openshift-console started at 2020-05-26 19:59:01 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container download-server ready: true, restart count 0
May 26 21:52:56.223: INFO: cluster-image-registry-operator-6f78cddbbc-qrdmf from openshift-image-registry started at 2020-05-26 19:59:03 +0000 UTC (2 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
May 26 21:52:56.223: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
May 26 21:52:56.223: INFO: service-ca-operator-7bb6cf7fbc-j7nvb from openshift-service-ca-operator started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container operator ready: true, restart count 0
May 26 21:52:56.223: INFO: openshift-service-catalog-apiserver-operator-5897998845-kpk9m from openshift-service-catalog-apiserver-operator started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container operator ready: true, restart count 1
May 26 21:52:56.223: INFO: calico-typha-f4f4dbb8c-s262v from calico-system started at 2020-05-26 20:00:43 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container calico-typha ready: true, restart count 0
May 26 21:52:56.223: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-05-26 20:05:06 +0000 UTC (3 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container alertmanager ready: true, restart count 0
May 26 21:52:56.223: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 26 21:52:56.223: INFO: 	Container config-reloader ready: true, restart count 0
May 26 21:52:56.223: INFO: multus-lh654 from openshift-multus started at 2020-05-26 19:58:00 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container kube-multus ready: true, restart count 0
May 26 21:52:56.223: INFO: cluster-storage-operator-8696454489-8wttw from openshift-cluster-storage-operator started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container cluster-storage-operator ready: true, restart count 0
May 26 21:52:56.223: INFO: olm-operator-9d9b9dc65-rg7n5 from openshift-operator-lifecycle-manager started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container olm-operator ready: true, restart count 0
May 26 21:52:56.223: INFO: node-ca-q2fgf from openshift-image-registry started at 2020-05-26 20:00:40 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container node-ca ready: true, restart count 0
May 26 21:52:56.223: INFO: certified-operators-7c7bd497cb-hjpfx from openshift-marketplace started at 2020-05-26 20:01:33 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container certified-operators ready: true, restart count 0
May 26 21:52:56.223: INFO: ibm-master-proxy-static-10.215.60.10 from kube-system started at 2020-05-26 19:57:58 +0000 UTC (2 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May 26 21:52:56.223: INFO: 	Container pause ready: true, restart count 0
May 26 21:52:56.223: INFO: openshift-kube-proxy-ns7pk from openshift-kube-proxy started at 2020-05-26 19:58:04 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container kube-proxy ready: true, restart count 0
May 26 21:52:56.223: INFO: ibm-cloud-provider-ip-159-122-104-134-84fdcfb5c5-cgj49 from ibm-system started at 2020-05-26 20:02:23 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.223: INFO: 	Container ibm-cloud-provider-ip-159-122-104-134 ready: true, restart count 0
May 26 21:52:56.223: INFO: 
Logging pods the kubelet thinks is on node 10.215.60.11 before test
May 26 21:52:56.310: INFO: network-operator-76d6fbdbb8-5hgl8 from openshift-network-operator started at 2020-05-26 19:57:36 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.310: INFO: 	Container network-operator ready: true, restart count 0
May 26 21:52:56.310: INFO: kube-state-metrics-7498bc479d-5pbn8 from openshift-monitoring started at 2020-05-26 19:59:34 +0000 UTC (3 container statuses recorded)
May 26 21:52:56.310: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 26 21:52:56.310: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 26 21:52:56.310: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 26 21:52:56.310: INFO: sonobuoy-systemd-logs-daemon-set-e1fd62b7d6894c57-gzfvj from sonobuoy started at 2020-05-26 21:25:54 +0000 UTC (2 container statuses recorded)
May 26 21:52:56.310: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 26 21:52:56.310: INFO: 	Container systemd-logs ready: true, restart count 0
May 26 21:52:56.310: INFO: vpn-6847db666c-dd6pq from kube-system started at 2020-05-26 20:03:07 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.310: INFO: 	Container vpn ready: true, restart count 0
May 26 21:52:56.310: INFO: thanos-querier-7f4dfb8d6f-w2z69 from openshift-monitoring started at 2020-05-26 20:05:34 +0000 UTC (4 container statuses recorded)
May 26 21:52:56.310: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:52:56.310: INFO: 	Container oauth-proxy ready: true, restart count 0
May 26 21:52:56.310: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 26 21:52:56.310: INFO: 	Container thanos-querier ready: true, restart count 0
May 26 21:52:56.310: INFO: ibm-cloud-provider-ip-159-122-104-134-84fdcfb5c5-l5b9t from ibm-system started at 2020-05-26 21:51:25 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.310: INFO: 	Container ibm-cloud-provider-ip-159-122-104-134 ready: true, restart count 0
May 26 21:52:56.310: INFO: ibm-master-proxy-static-10.215.60.11 from kube-system started at 2020-05-26 19:57:32 +0000 UTC (2 container statuses recorded)
May 26 21:52:56.310: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May 26 21:52:56.310: INFO: 	Container pause ready: true, restart count 0
May 26 21:52:56.310: INFO: tigera-operator-798cfbf7dd-q5frt from tigera-operator started at 2020-05-26 19:57:36 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.310: INFO: 	Container tigera-operator ready: true, restart count 2
May 26 21:52:56.310: INFO: tuned-88qws from openshift-cluster-node-tuning-operator started at 2020-05-26 19:59:39 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.310: INFO: 	Container tuned ready: true, restart count 0
May 26 21:52:56.310: INFO: node-ca-jh22q from openshift-image-registry started at 2020-05-26 20:00:40 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.310: INFO: 	Container node-ca ready: true, restart count 0
May 26 21:52:56.310: INFO: image-registry-766b5fd974-j2s4m from openshift-image-registry started at 2020-05-26 20:03:30 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.310: INFO: 	Container registry ready: true, restart count 0
May 26 21:52:56.310: INFO: prometheus-operator-9d5b5788b-dpcht from openshift-monitoring started at 2020-05-26 20:04:36 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.310: INFO: 	Container prometheus-operator ready: true, restart count 0
May 26 21:52:56.310: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-05-26 20:05:57 +0000 UTC (7 container statuses recorded)
May 26 21:52:56.310: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:52:56.310: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 26 21:52:56.310: INFO: 	Container prometheus ready: true, restart count 1
May 26 21:52:56.310: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May 26 21:52:56.310: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 26 21:52:56.310: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May 26 21:52:56.310: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 26 21:52:56.310: INFO: ibmcloud-block-storage-driver-zq7xq from kube-system started at 2020-05-26 19:57:36 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.310: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May 26 21:52:56.310: INFO: openshift-state-metrics-74997686f-cgqlx from openshift-monitoring started at 2020-05-26 19:59:35 +0000 UTC (3 container statuses recorded)
May 26 21:52:56.310: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 26 21:52:56.310: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 26 21:52:56.310: INFO: 	Container openshift-state-metrics ready: true, restart count 0
May 26 21:52:56.310: INFO: cluster-samples-operator-759dd556bf-bgg5n from openshift-cluster-samples-operator started at 2020-05-26 19:59:44 +0000 UTC (2 container statuses recorded)
May 26 21:52:56.310: INFO: 	Container cluster-samples-operator ready: true, restart count 0
May 26 21:52:56.310: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
May 26 21:52:56.311: INFO: service-serving-cert-signer-69cddbb454-wjjcq from openshift-service-ca started at 2020-05-26 19:59:44 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.311: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
May 26 21:52:56.311: INFO: multus-88m9q from openshift-multus started at 2020-05-26 19:57:58 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.311: INFO: 	Container kube-multus ready: true, restart count 0
May 26 21:52:56.311: INFO: configmap-cabundle-injector-795c74476d-6hzjj from openshift-service-ca started at 2020-05-26 19:59:45 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.311: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
May 26 21:52:56.311: INFO: console-d8768d4f5-m8fb9 from openshift-console started at 2020-05-26 20:01:37 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.311: INFO: 	Container console ready: true, restart count 0
May 26 21:52:56.311: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-05-26 20:05:02 +0000 UTC (3 container statuses recorded)
May 26 21:52:56.311: INFO: 	Container alertmanager ready: true, restart count 0
May 26 21:52:56.311: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 26 21:52:56.311: INFO: 	Container config-reloader ready: true, restart count 0
May 26 21:52:56.311: INFO: ibm-keepalived-watcher-w8tv5 from kube-system started at 2020-05-26 19:57:34 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.311: INFO: 	Container keepalived-watcher ready: true, restart count 0
May 26 21:52:56.311: INFO: openshift-kube-proxy-h4l9g from openshift-kube-proxy started at 2020-05-26 19:58:05 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.311: INFO: 	Container kube-proxy ready: true, restart count 0
May 26 21:52:56.311: INFO: community-operators-674864f5c4-bwwnn from openshift-marketplace started at 2020-05-26 20:01:32 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.311: INFO: 	Container community-operators ready: true, restart count 0
May 26 21:52:56.311: INFO: router-default-6cfd8db4bf-jv7v4 from openshift-ingress started at 2020-05-26 20:01:45 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.311: INFO: 	Container router ready: true, restart count 0
May 26 21:52:56.311: INFO: multus-admission-controller-mqrcn from openshift-multus started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.311: INFO: 	Container multus-admission-controller ready: true, restart count 0
May 26 21:52:56.311: INFO: node-exporter-spjk9 from openshift-monitoring started at 2020-05-26 19:59:36 +0000 UTC (2 container statuses recorded)
May 26 21:52:56.311: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:52:56.311: INFO: 	Container node-exporter ready: true, restart count 0
May 26 21:52:56.311: INFO: calico-typha-f4f4dbb8c-jg59b from calico-system started at 2020-05-26 20:00:43 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.311: INFO: 	Container calico-typha ready: true, restart count 0
May 26 21:52:56.311: INFO: calico-node-6tv2z from calico-system started at 2020-05-26 19:58:46 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.311: INFO: 	Container calico-node ready: true, restart count 0
May 26 21:52:56.311: INFO: dns-default-jlw4t from openshift-dns started at 2020-05-26 20:00:40 +0000 UTC (2 container statuses recorded)
May 26 21:52:56.311: INFO: 	Container dns ready: true, restart count 0
May 26 21:52:56.311: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 26 21:52:56.311: INFO: 
Logging pods the kubelet thinks is on node 10.215.60.34 before test
May 26 21:52:56.386: INFO: prometheus-adapter-5646d5d5dd-9cprr from openshift-monitoring started at 2020-05-26 21:51:25 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.386: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 26 21:52:56.386: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-05-26 21:51:30 +0000 UTC (3 container statuses recorded)
May 26 21:52:56.387: INFO: 	Container alertmanager ready: true, restart count 0
May 26 21:52:56.387: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 26 21:52:56.387: INFO: 	Container config-reloader ready: true, restart count 0
May 26 21:52:56.387: INFO: console-d8768d4f5-vpb4j from openshift-console started at 2020-05-26 21:51:25 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.387: INFO: 	Container console ready: true, restart count 0
May 26 21:52:56.387: INFO: ibm-keepalived-watcher-gtvct from kube-system started at 2020-05-26 19:58:23 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.387: INFO: 	Container keepalived-watcher ready: true, restart count 0
May 26 21:52:56.387: INFO: dns-default-ngb66 from openshift-dns started at 2020-05-26 21:51:30 +0000 UTC (2 container statuses recorded)
May 26 21:52:56.387: INFO: 	Container dns ready: true, restart count 0
May 26 21:52:56.387: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 26 21:52:56.387: INFO: tuned-4sgbw from openshift-cluster-node-tuning-operator started at 2020-05-26 19:59:39 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.387: INFO: 	Container tuned ready: true, restart count 0
May 26 21:52:56.387: INFO: node-ca-6c8qk from openshift-image-registry started at 2020-05-26 20:00:40 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.387: INFO: 	Container node-ca ready: true, restart count 0
May 26 21:52:56.387: INFO: node-exporter-7b5t7 from openshift-monitoring started at 2020-05-26 19:59:36 +0000 UTC (2 container statuses recorded)
May 26 21:52:56.387: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:52:56.387: INFO: 	Container node-exporter ready: true, restart count 0
May 26 21:52:56.387: INFO: telemeter-client-6745779989-mjhdd from openshift-monitoring started at 2020-05-26 21:51:25 +0000 UTC (3 container statuses recorded)
May 26 21:52:56.387: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:52:56.387: INFO: 	Container reload ready: true, restart count 0
May 26 21:52:56.387: INFO: 	Container telemeter-client ready: true, restart count 0
May 26 21:52:56.387: INFO: openshift-kube-proxy-2sfd8 from openshift-kube-proxy started at 2020-05-26 19:58:23 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.387: INFO: 	Container kube-proxy ready: true, restart count 0
May 26 21:52:56.387: INFO: calico-node-9s8hb from calico-system started at 2020-05-26 19:58:46 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.387: INFO: 	Container calico-node ready: true, restart count 0
May 26 21:52:56.387: INFO: sonobuoy-e2e-job-543b6d87a6d14642 from sonobuoy started at 2020-05-26 21:25:53 +0000 UTC (2 container statuses recorded)
May 26 21:52:56.387: INFO: 	Container e2e ready: true, restart count 0
May 26 21:52:56.387: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 26 21:52:56.387: INFO: multus-j2db5 from openshift-multus started at 2020-05-26 19:58:23 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.387: INFO: 	Container kube-multus ready: true, restart count 0
May 26 21:52:56.387: INFO: ibm-master-proxy-static-10.215.60.34 from kube-system started at 2020-05-26 19:58:20 +0000 UTC (2 container statuses recorded)
May 26 21:52:56.387: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May 26 21:52:56.387: INFO: 	Container pause ready: true, restart count 0
May 26 21:52:56.387: INFO: calico-typha-f4f4dbb8c-7f56v from calico-system started at 2020-05-26 19:58:45 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.387: INFO: 	Container calico-typha ready: true, restart count 0
May 26 21:52:56.387: INFO: sonobuoy from sonobuoy started at 2020-05-26 21:25:44 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.387: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 26 21:52:56.387: INFO: sonobuoy-systemd-logs-daemon-set-e1fd62b7d6894c57-mptms from sonobuoy started at 2020-05-26 21:25:54 +0000 UTC (2 container statuses recorded)
May 26 21:52:56.387: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 26 21:52:56.387: INFO: 	Container systemd-logs ready: true, restart count 0
May 26 21:52:56.387: INFO: multus-admission-controller-xvl7w from openshift-multus started at 2020-05-26 21:52:00 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.387: INFO: 	Container multus-admission-controller ready: true, restart count 0
May 26 21:52:56.387: INFO: ibmcloud-block-storage-driver-nwvlx from kube-system started at 2020-05-26 19:58:31 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.387: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May 26 21:52:56.387: INFO: packageserver-7d5f79c4db-dpjmm from openshift-operator-lifecycle-manager started at 2020-05-26 21:51:27 +0000 UTC (1 container statuses recorded)
May 26 21:52:56.387: INFO: 	Container packageserver ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1612b2962c2fba54], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:52:57.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2766" for this suite.
May 26 21:53:05.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:53:07.382: INFO: namespace sched-pred-2766 deletion completed in 9.887961689s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:11.429 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:53:07.382: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 26 21:53:07.584: INFO: Waiting up to 5m0s for pod "downwardapi-volume-68e936d6-90e9-4fc6-8645-9ce925c81d90" in namespace "projected-4656" to be "success or failure"
May 26 21:53:07.594: INFO: Pod "downwardapi-volume-68e936d6-90e9-4fc6-8645-9ce925c81d90": Phase="Pending", Reason="", readiness=false. Elapsed: 9.284873ms
May 26 21:53:09.604: INFO: Pod "downwardapi-volume-68e936d6-90e9-4fc6-8645-9ce925c81d90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019724028s
May 26 21:53:11.614: INFO: Pod "downwardapi-volume-68e936d6-90e9-4fc6-8645-9ce925c81d90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029546404s
STEP: Saw pod success
May 26 21:53:11.614: INFO: Pod "downwardapi-volume-68e936d6-90e9-4fc6-8645-9ce925c81d90" satisfied condition "success or failure"
May 26 21:53:11.623: INFO: Trying to get logs from node 10.215.60.34 pod downwardapi-volume-68e936d6-90e9-4fc6-8645-9ce925c81d90 container client-container: <nil>
STEP: delete the pod
May 26 21:53:11.679: INFO: Waiting for pod downwardapi-volume-68e936d6-90e9-4fc6-8645-9ce925c81d90 to disappear
May 26 21:53:11.688: INFO: Pod downwardapi-volume-68e936d6-90e9-4fc6-8645-9ce925c81d90 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:53:11.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4656" for this suite.
May 26 21:53:19.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:53:21.594: INFO: namespace projected-4656 deletion completed in 9.893644733s

â€¢ [SLOW TEST:14.212 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:53:21.594: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 26 21:53:21.770: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1329 /api/v1/namespaces/watch-1329/configmaps/e2e-watch-test-watch-closed 503d121b-003f-4b4d-9a66-58425c6bd9ad 51904 0 2020-05-26 21:53:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 26 21:53:21.770: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1329 /api/v1/namespaces/watch-1329/configmaps/e2e-watch-test-watch-closed 503d121b-003f-4b4d-9a66-58425c6bd9ad 51907 0 2020-05-26 21:53:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 26 21:53:21.841: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1329 /api/v1/namespaces/watch-1329/configmaps/e2e-watch-test-watch-closed 503d121b-003f-4b4d-9a66-58425c6bd9ad 51910 0 2020-05-26 21:53:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 26 21:53:21.841: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1329 /api/v1/namespaces/watch-1329/configmaps/e2e-watch-test-watch-closed 503d121b-003f-4b4d-9a66-58425c6bd9ad 51913 0 2020-05-26 21:53:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:53:21.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1329" for this suite.
May 26 21:53:29.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:53:31.743: INFO: namespace watch-1329 deletion completed in 9.889417831s

â€¢ [SLOW TEST:10.149 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:53:31.743: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May 26 21:53:31.857: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 26 21:53:31.904: INFO: Waiting for terminating namespaces to be deleted...
May 26 21:53:31.917: INFO: 
Logging pods the kubelet thinks is on node 10.215.60.10 before test
May 26 21:53:31.963: INFO: ibm-keepalived-watcher-bl295 from kube-system started at 2020-05-26 19:58:00 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container keepalived-watcher ready: true, restart count 0
May 26 21:53:31.963: INFO: downloads-8479fbbf57-qhklc from openshift-console started at 2020-05-26 19:59:01 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container download-server ready: true, restart count 0
May 26 21:53:31.963: INFO: calico-kube-controllers-599969f895-xfmxb from calico-system started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 26 21:53:31.963: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-05-26 20:05:48 +0000 UTC (7 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:53:31.963: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 26 21:53:31.963: INFO: 	Container prometheus ready: true, restart count 1
May 26 21:53:31.963: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May 26 21:53:31.963: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 26 21:53:31.963: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May 26 21:53:31.963: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 26 21:53:31.963: INFO: ibmcloud-block-storage-driver-sncrt from kube-system started at 2020-05-26 19:58:07 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May 26 21:53:31.963: INFO: calico-node-p2dwh from calico-system started at 2020-05-26 19:58:46 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container calico-node ready: true, restart count 0
May 26 21:53:31.963: INFO: ibm-file-plugin-5999bd7d7d-mcpqg from kube-system started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
May 26 21:53:31.963: INFO: cluster-node-tuning-operator-58cb5999f5-dqqxl from openshift-cluster-node-tuning-operator started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
May 26 21:53:31.963: INFO: cluster-monitoring-operator-64b79969cc-g94mm from openshift-monitoring started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
May 26 21:53:31.963: INFO: marketplace-operator-6d86c46f6b-j7hh9 from openshift-marketplace started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container marketplace-operator ready: true, restart count 0
May 26 21:53:31.963: INFO: dns-default-s9jv4 from openshift-dns started at 2020-05-26 20:00:40 +0000 UTC (2 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container dns ready: true, restart count 0
May 26 21:53:31.963: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 26 21:53:31.963: INFO: sonobuoy-systemd-logs-daemon-set-e1fd62b7d6894c57-wrl9n from sonobuoy started at 2020-05-26 21:25:54 +0000 UTC (2 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 26 21:53:31.963: INFO: 	Container systemd-logs ready: true, restart count 0
May 26 21:53:31.963: INFO: ibm-storage-watcher-5f6b5dbcd4-kn4xs from kube-system started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
May 26 21:53:31.963: INFO: ibmcloud-block-storage-plugin-9d877d7bc-kjrsl from kube-system started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
May 26 21:53:31.963: INFO: node-exporter-b6pqt from openshift-monitoring started at 2020-05-26 19:59:35 +0000 UTC (2 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:53:31.963: INFO: 	Container node-exporter ready: true, restart count 0
May 26 21:53:31.963: INFO: apiservice-cabundle-injector-5c88555f6d-r7p9b from openshift-service-ca started at 2020-05-26 19:59:45 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
May 26 21:53:31.963: INFO: redhat-operators-7b74fd75b7-8dvbx from openshift-marketplace started at 2020-05-26 20:01:34 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container redhat-operators ready: true, restart count 0
May 26 21:53:31.963: INFO: dns-operator-c68448f89-lfqlv from openshift-dns-operator started at 2020-05-26 19:59:02 +0000 UTC (2 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container dns-operator ready: true, restart count 0
May 26 21:53:31.963: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:53:31.963: INFO: openshift-service-catalog-controller-manager-operator-86b49fb6n from openshift-service-catalog-controller-manager-operator started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container operator ready: true, restart count 1
May 26 21:53:31.963: INFO: grafana-5648b7fdd9-rmjbk from openshift-monitoring started at 2020-05-26 21:51:25 +0000 UTC (2 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container grafana ready: true, restart count 0
May 26 21:53:31.963: INFO: 	Container grafana-proxy ready: true, restart count 0
May 26 21:53:31.963: INFO: packageserver-7d5f79c4db-89cw9 from openshift-operator-lifecycle-manager started at 2020-05-26 21:51:35 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container packageserver ready: true, restart count 0
May 26 21:53:31.963: INFO: downloads-8479fbbf57-z9pkl from openshift-console started at 2020-05-26 19:59:01 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container download-server ready: true, restart count 0
May 26 21:53:31.963: INFO: cluster-image-registry-operator-6f78cddbbc-qrdmf from openshift-image-registry started at 2020-05-26 19:59:03 +0000 UTC (2 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
May 26 21:53:31.963: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
May 26 21:53:31.963: INFO: service-ca-operator-7bb6cf7fbc-j7nvb from openshift-service-ca-operator started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container operator ready: true, restart count 0
May 26 21:53:31.963: INFO: openshift-service-catalog-apiserver-operator-5897998845-kpk9m from openshift-service-catalog-apiserver-operator started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container operator ready: true, restart count 1
May 26 21:53:31.963: INFO: calico-typha-f4f4dbb8c-s262v from calico-system started at 2020-05-26 20:00:43 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container calico-typha ready: true, restart count 0
May 26 21:53:31.963: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-05-26 20:05:06 +0000 UTC (3 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container alertmanager ready: true, restart count 0
May 26 21:53:31.963: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 26 21:53:31.963: INFO: 	Container config-reloader ready: true, restart count 0
May 26 21:53:31.963: INFO: router-default-6cfd8db4bf-7dstv from openshift-ingress started at 2020-05-26 21:51:25 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container router ready: true, restart count 0
May 26 21:53:31.963: INFO: multus-lh654 from openshift-multus started at 2020-05-26 19:58:00 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container kube-multus ready: true, restart count 0
May 26 21:53:31.963: INFO: cluster-storage-operator-8696454489-8wttw from openshift-cluster-storage-operator started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container cluster-storage-operator ready: true, restart count 0
May 26 21:53:31.963: INFO: olm-operator-9d9b9dc65-rg7n5 from openshift-operator-lifecycle-manager started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container olm-operator ready: true, restart count 0
May 26 21:53:31.963: INFO: node-ca-q2fgf from openshift-image-registry started at 2020-05-26 20:00:40 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container node-ca ready: true, restart count 0
May 26 21:53:31.963: INFO: certified-operators-7c7bd497cb-hjpfx from openshift-marketplace started at 2020-05-26 20:01:33 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.963: INFO: 	Container certified-operators ready: true, restart count 0
May 26 21:53:31.964: INFO: ibm-master-proxy-static-10.215.60.10 from kube-system started at 2020-05-26 19:57:58 +0000 UTC (2 container statuses recorded)
May 26 21:53:31.964: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May 26 21:53:31.964: INFO: 	Container pause ready: true, restart count 0
May 26 21:53:31.964: INFO: openshift-kube-proxy-ns7pk from openshift-kube-proxy started at 2020-05-26 19:58:04 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.964: INFO: 	Container kube-proxy ready: true, restart count 0
May 26 21:53:31.964: INFO: ibm-cloud-provider-ip-159-122-104-134-84fdcfb5c5-cgj49 from ibm-system started at 2020-05-26 20:02:23 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.964: INFO: 	Container ibm-cloud-provider-ip-159-122-104-134 ready: true, restart count 0
May 26 21:53:31.964: INFO: thanos-querier-7f4dfb8d6f-5m6ft from openshift-monitoring started at 2020-05-26 20:05:38 +0000 UTC (4 container statuses recorded)
May 26 21:53:31.964: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:53:31.964: INFO: 	Container oauth-proxy ready: true, restart count 0
May 26 21:53:31.964: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 26 21:53:31.964: INFO: 	Container thanos-querier ready: true, restart count 0
May 26 21:53:31.964: INFO: multus-admission-controller-4kd9c from openshift-multus started at 2020-05-26 19:59:01 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.964: INFO: 	Container multus-admission-controller ready: true, restart count 0
May 26 21:53:31.964: INFO: catalog-operator-6956d96f67-5xvls from openshift-operator-lifecycle-manager started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.964: INFO: 	Container catalog-operator ready: true, restart count 0
May 26 21:53:31.964: INFO: console-operator-56c4d6445c-brk5r from openshift-console-operator started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.964: INFO: 	Container console-operator ready: true, restart count 1
May 26 21:53:31.964: INFO: ingress-operator-f6594bf4d-m74mj from openshift-ingress-operator started at 2020-05-26 19:59:03 +0000 UTC (2 container statuses recorded)
May 26 21:53:31.964: INFO: 	Container ingress-operator ready: true, restart count 0
May 26 21:53:31.964: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:53:31.964: INFO: tuned-22gmz from openshift-cluster-node-tuning-operator started at 2020-05-26 19:59:39 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.964: INFO: 	Container tuned ready: true, restart count 0
May 26 21:53:31.964: INFO: prometheus-adapter-5646d5d5dd-ql9h7 from openshift-monitoring started at 2020-05-26 20:05:35 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.964: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 26 21:53:31.964: INFO: registry-pvc-permissions-j5wnp from openshift-image-registry started at 2020-05-26 20:03:30 +0000 UTC (1 container statuses recorded)
May 26 21:53:31.964: INFO: 	Container pvc-permissions ready: false, restart count 0
May 26 21:53:31.964: INFO: 
Logging pods the kubelet thinks is on node 10.215.60.11 before test
May 26 21:53:32.002: INFO: prometheus-operator-9d5b5788b-dpcht from openshift-monitoring started at 2020-05-26 20:04:36 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.002: INFO: 	Container prometheus-operator ready: true, restart count 0
May 26 21:53:32.002: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-05-26 20:05:57 +0000 UTC (7 container statuses recorded)
May 26 21:53:32.002: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:53:32.002: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 26 21:53:32.002: INFO: 	Container prometheus ready: true, restart count 1
May 26 21:53:32.003: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May 26 21:53:32.003: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 26 21:53:32.003: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May 26 21:53:32.003: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 26 21:53:32.003: INFO: image-registry-766b5fd974-j2s4m from openshift-image-registry started at 2020-05-26 20:03:30 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.003: INFO: 	Container registry ready: true, restart count 0
May 26 21:53:32.003: INFO: openshift-state-metrics-74997686f-cgqlx from openshift-monitoring started at 2020-05-26 19:59:35 +0000 UTC (3 container statuses recorded)
May 26 21:53:32.003: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 26 21:53:32.003: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 26 21:53:32.003: INFO: 	Container openshift-state-metrics ready: true, restart count 0
May 26 21:53:32.003: INFO: cluster-samples-operator-759dd556bf-bgg5n from openshift-cluster-samples-operator started at 2020-05-26 19:59:44 +0000 UTC (2 container statuses recorded)
May 26 21:53:32.003: INFO: 	Container cluster-samples-operator ready: true, restart count 0
May 26 21:53:32.003: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
May 26 21:53:32.003: INFO: service-serving-cert-signer-69cddbb454-wjjcq from openshift-service-ca started at 2020-05-26 19:59:44 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.004: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
May 26 21:53:32.004: INFO: ibmcloud-block-storage-driver-zq7xq from kube-system started at 2020-05-26 19:57:36 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.004: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May 26 21:53:32.004: INFO: configmap-cabundle-injector-795c74476d-6hzjj from openshift-service-ca started at 2020-05-26 19:59:45 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.004: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
May 26 21:53:32.004: INFO: console-d8768d4f5-m8fb9 from openshift-console started at 2020-05-26 20:01:37 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.004: INFO: 	Container console ready: true, restart count 0
May 26 21:53:32.004: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-05-26 20:05:02 +0000 UTC (3 container statuses recorded)
May 26 21:53:32.004: INFO: 	Container alertmanager ready: true, restart count 0
May 26 21:53:32.004: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 26 21:53:32.004: INFO: 	Container config-reloader ready: true, restart count 0
May 26 21:53:32.004: INFO: multus-88m9q from openshift-multus started at 2020-05-26 19:57:58 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.004: INFO: 	Container kube-multus ready: true, restart count 0
May 26 21:53:32.004: INFO: openshift-kube-proxy-h4l9g from openshift-kube-proxy started at 2020-05-26 19:58:05 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.004: INFO: 	Container kube-proxy ready: true, restart count 0
May 26 21:53:32.004: INFO: community-operators-674864f5c4-bwwnn from openshift-marketplace started at 2020-05-26 20:01:32 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.005: INFO: 	Container community-operators ready: true, restart count 0
May 26 21:53:32.005: INFO: router-default-6cfd8db4bf-jv7v4 from openshift-ingress started at 2020-05-26 20:01:45 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.005: INFO: 	Container router ready: true, restart count 0
May 26 21:53:32.005: INFO: ibm-keepalived-watcher-w8tv5 from kube-system started at 2020-05-26 19:57:34 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.005: INFO: 	Container keepalived-watcher ready: true, restart count 0
May 26 21:53:32.005: INFO: node-exporter-spjk9 from openshift-monitoring started at 2020-05-26 19:59:36 +0000 UTC (2 container statuses recorded)
May 26 21:53:32.005: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:53:32.005: INFO: 	Container node-exporter ready: true, restart count 0
May 26 21:53:32.005: INFO: calico-typha-f4f4dbb8c-jg59b from calico-system started at 2020-05-26 20:00:43 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.005: INFO: 	Container calico-typha ready: true, restart count 0
May 26 21:53:32.005: INFO: multus-admission-controller-mqrcn from openshift-multus started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.005: INFO: 	Container multus-admission-controller ready: true, restart count 0
May 26 21:53:32.005: INFO: dns-default-jlw4t from openshift-dns started at 2020-05-26 20:00:40 +0000 UTC (2 container statuses recorded)
May 26 21:53:32.005: INFO: 	Container dns ready: true, restart count 0
May 26 21:53:32.005: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 26 21:53:32.006: INFO: calico-node-6tv2z from calico-system started at 2020-05-26 19:58:46 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.006: INFO: 	Container calico-node ready: true, restart count 0
May 26 21:53:32.006: INFO: kube-state-metrics-7498bc479d-5pbn8 from openshift-monitoring started at 2020-05-26 19:59:34 +0000 UTC (3 container statuses recorded)
May 26 21:53:32.006: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 26 21:53:32.006: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 26 21:53:32.006: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 26 21:53:32.006: INFO: sonobuoy-systemd-logs-daemon-set-e1fd62b7d6894c57-gzfvj from sonobuoy started at 2020-05-26 21:25:54 +0000 UTC (2 container statuses recorded)
May 26 21:53:32.006: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 26 21:53:32.006: INFO: 	Container systemd-logs ready: true, restart count 0
May 26 21:53:32.006: INFO: network-operator-76d6fbdbb8-5hgl8 from openshift-network-operator started at 2020-05-26 19:57:36 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.006: INFO: 	Container network-operator ready: true, restart count 0
May 26 21:53:32.006: INFO: tigera-operator-798cfbf7dd-q5frt from tigera-operator started at 2020-05-26 19:57:36 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.006: INFO: 	Container tigera-operator ready: true, restart count 2
May 26 21:53:32.006: INFO: tuned-88qws from openshift-cluster-node-tuning-operator started at 2020-05-26 19:59:39 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.006: INFO: 	Container tuned ready: true, restart count 0
May 26 21:53:32.007: INFO: node-ca-jh22q from openshift-image-registry started at 2020-05-26 20:00:40 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.007: INFO: 	Container node-ca ready: true, restart count 0
May 26 21:53:32.007: INFO: vpn-6847db666c-dd6pq from kube-system started at 2020-05-26 20:03:07 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.007: INFO: 	Container vpn ready: true, restart count 0
May 26 21:53:32.007: INFO: thanos-querier-7f4dfb8d6f-w2z69 from openshift-monitoring started at 2020-05-26 20:05:34 +0000 UTC (4 container statuses recorded)
May 26 21:53:32.007: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:53:32.007: INFO: 	Container oauth-proxy ready: true, restart count 0
May 26 21:53:32.007: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 26 21:53:32.007: INFO: 	Container thanos-querier ready: true, restart count 0
May 26 21:53:32.007: INFO: ibm-cloud-provider-ip-159-122-104-134-84fdcfb5c5-l5b9t from ibm-system started at 2020-05-26 21:51:25 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.007: INFO: 	Container ibm-cloud-provider-ip-159-122-104-134 ready: true, restart count 0
May 26 21:53:32.007: INFO: ibm-master-proxy-static-10.215.60.11 from kube-system started at 2020-05-26 19:57:32 +0000 UTC (2 container statuses recorded)
May 26 21:53:32.007: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May 26 21:53:32.007: INFO: 	Container pause ready: true, restart count 0
May 26 21:53:32.007: INFO: 
Logging pods the kubelet thinks is on node 10.215.60.34 before test
May 26 21:53:32.041: INFO: console-d8768d4f5-vpb4j from openshift-console started at 2020-05-26 21:51:25 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.041: INFO: 	Container console ready: true, restart count 0
May 26 21:53:32.041: INFO: node-ca-6c8qk from openshift-image-registry started at 2020-05-26 20:00:40 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.041: INFO: 	Container node-ca ready: true, restart count 0
May 26 21:53:32.041: INFO: ibm-keepalived-watcher-gtvct from kube-system started at 2020-05-26 19:58:23 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.041: INFO: 	Container keepalived-watcher ready: true, restart count 0
May 26 21:53:32.041: INFO: dns-default-ngb66 from openshift-dns started at 2020-05-26 21:51:30 +0000 UTC (2 container statuses recorded)
May 26 21:53:32.041: INFO: 	Container dns ready: true, restart count 0
May 26 21:53:32.041: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 26 21:53:32.041: INFO: tuned-4sgbw from openshift-cluster-node-tuning-operator started at 2020-05-26 19:59:39 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.041: INFO: 	Container tuned ready: true, restart count 0
May 26 21:53:32.041: INFO: node-exporter-7b5t7 from openshift-monitoring started at 2020-05-26 19:59:36 +0000 UTC (2 container statuses recorded)
May 26 21:53:32.041: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:53:32.041: INFO: 	Container node-exporter ready: true, restart count 0
May 26 21:53:32.041: INFO: telemeter-client-6745779989-mjhdd from openshift-monitoring started at 2020-05-26 21:51:25 +0000 UTC (3 container statuses recorded)
May 26 21:53:32.041: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 21:53:32.041: INFO: 	Container reload ready: true, restart count 0
May 26 21:53:32.041: INFO: 	Container telemeter-client ready: true, restart count 0
May 26 21:53:32.041: INFO: sonobuoy-e2e-job-543b6d87a6d14642 from sonobuoy started at 2020-05-26 21:25:53 +0000 UTC (2 container statuses recorded)
May 26 21:53:32.041: INFO: 	Container e2e ready: true, restart count 0
May 26 21:53:32.041: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 26 21:53:32.041: INFO: openshift-kube-proxy-2sfd8 from openshift-kube-proxy started at 2020-05-26 19:58:23 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.041: INFO: 	Container kube-proxy ready: true, restart count 0
May 26 21:53:32.041: INFO: calico-node-9s8hb from calico-system started at 2020-05-26 19:58:46 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.041: INFO: 	Container calico-node ready: true, restart count 0
May 26 21:53:32.041: INFO: multus-j2db5 from openshift-multus started at 2020-05-26 19:58:23 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.041: INFO: 	Container kube-multus ready: true, restart count 0
May 26 21:53:32.041: INFO: multus-admission-controller-xvl7w from openshift-multus started at 2020-05-26 21:52:00 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.041: INFO: 	Container multus-admission-controller ready: true, restart count 0
May 26 21:53:32.041: INFO: ibm-master-proxy-static-10.215.60.34 from kube-system started at 2020-05-26 19:58:20 +0000 UTC (2 container statuses recorded)
May 26 21:53:32.041: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May 26 21:53:32.041: INFO: 	Container pause ready: true, restart count 0
May 26 21:53:32.041: INFO: calico-typha-f4f4dbb8c-7f56v from calico-system started at 2020-05-26 19:58:45 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.041: INFO: 	Container calico-typha ready: true, restart count 0
May 26 21:53:32.041: INFO: sonobuoy from sonobuoy started at 2020-05-26 21:25:44 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.041: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 26 21:53:32.041: INFO: sonobuoy-systemd-logs-daemon-set-e1fd62b7d6894c57-mptms from sonobuoy started at 2020-05-26 21:25:54 +0000 UTC (2 container statuses recorded)
May 26 21:53:32.041: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 26 21:53:32.041: INFO: 	Container systemd-logs ready: true, restart count 0
May 26 21:53:32.041: INFO: ibmcloud-block-storage-driver-nwvlx from kube-system started at 2020-05-26 19:58:31 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.041: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May 26 21:53:32.041: INFO: packageserver-7d5f79c4db-dpjmm from openshift-operator-lifecycle-manager started at 2020-05-26 21:51:27 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.041: INFO: 	Container packageserver ready: true, restart count 0
May 26 21:53:32.041: INFO: prometheus-adapter-5646d5d5dd-9cprr from openshift-monitoring started at 2020-05-26 21:51:25 +0000 UTC (1 container statuses recorded)
May 26 21:53:32.041: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 26 21:53:32.041: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-05-26 21:51:30 +0000 UTC (3 container statuses recorded)
May 26 21:53:32.041: INFO: 	Container alertmanager ready: true, restart count 0
May 26 21:53:32.041: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 26 21:53:32.041: INFO: 	Container config-reloader ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-07736154-4810-42c6-b90c-826b9453ba66 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-07736154-4810-42c6-b90c-826b9453ba66 off the node 10.215.60.34
STEP: verifying the node doesn't have the label kubernetes.io/e2e-07736154-4810-42c6-b90c-826b9453ba66
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:58:36.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7690" for this suite.
May 26 21:58:48.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:58:50.246: INFO: namespace sched-pred-7690 deletion completed in 13.88488442s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:318.503 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:58:50.249: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
May 26 21:58:50.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-606 -- logs-generator --log-lines-total 100 --run-duration 20s'
May 26 21:58:50.809: INFO: stderr: ""
May 26 21:58:50.809: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
May 26 21:58:50.809: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May 26 21:58:50.809: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-606" to be "running and ready, or succeeded"
May 26 21:58:50.826: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 16.536141ms
May 26 21:58:52.836: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.026825435s
May 26 21:58:52.836: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May 26 21:58:52.836: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
May 26 21:58:52.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 logs logs-generator logs-generator --namespace=kubectl-606'
May 26 21:58:53.051: INFO: stderr: ""
May 26 21:58:53.051: INFO: stdout: "I0526 21:58:52.038317       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/5dd 481\nI0526 21:58:52.238459       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/hnmj 427\nI0526 21:58:52.438483       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/nl9k 261\nI0526 21:58:52.638484       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/l9k8 448\nI0526 21:58:52.840656       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/zfbn 465\nI0526 21:58:53.038527       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/d274 540\n"
May 26 21:58:55.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 logs logs-generator logs-generator --namespace=kubectl-606'
May 26 21:58:55.246: INFO: stderr: ""
May 26 21:58:55.246: INFO: stdout: "I0526 21:58:52.038317       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/5dd 481\nI0526 21:58:52.238459       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/hnmj 427\nI0526 21:58:52.438483       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/nl9k 261\nI0526 21:58:52.638484       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/l9k8 448\nI0526 21:58:52.840656       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/zfbn 465\nI0526 21:58:53.038527       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/d274 540\nI0526 21:58:53.238538       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/qnz 323\nI0526 21:58:53.438519       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/crmg 492\nI0526 21:58:53.638539       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/rwwz 276\nI0526 21:58:53.838489       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/w5c4 488\nI0526 21:58:54.038479       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/6kg 320\nI0526 21:58:54.238459       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/69pw 505\nI0526 21:58:54.438468       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/xbd2 444\nI0526 21:58:54.638487       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/jsqg 463\nI0526 21:58:54.838506       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/v49z 582\nI0526 21:58:55.038569       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/c8qh 429\nI0526 21:58:55.238522       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/79k 553\n"
STEP: limiting log lines
May 26 21:58:55.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 logs logs-generator logs-generator --namespace=kubectl-606 --tail=1'
May 26 21:58:55.439: INFO: stderr: ""
May 26 21:58:55.439: INFO: stdout: "I0526 21:58:55.238522       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/79k 553\n"
STEP: limiting log bytes
May 26 21:58:55.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 logs logs-generator logs-generator --namespace=kubectl-606 --limit-bytes=1'
May 26 21:58:55.626: INFO: stderr: ""
May 26 21:58:55.626: INFO: stdout: "I"
STEP: exposing timestamps
May 26 21:58:55.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 logs logs-generator logs-generator --namespace=kubectl-606 --tail=1 --timestamps'
May 26 21:58:55.799: INFO: stderr: ""
May 26 21:58:55.799: INFO: stdout: "2020-05-26T16:58:55.638553824-05:00 I0526 21:58:55.638476       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/c97c 596\n"
STEP: restricting to a time range
May 26 21:58:58.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 logs logs-generator logs-generator --namespace=kubectl-606 --since=1s'
May 26 21:58:58.462: INFO: stderr: ""
May 26 21:58:58.462: INFO: stdout: "I0526 21:58:57.638482       1 logs_generator.go:76] 28 POST /api/v1/namespaces/default/pods/2qnz 201\nI0526 21:58:57.838492       1 logs_generator.go:76] 29 POST /api/v1/namespaces/default/pods/778v 519\nI0526 21:58:58.038506       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/default/pods/2qs 542\nI0526 21:58:58.238494       1 logs_generator.go:76] 31 PUT /api/v1/namespaces/ns/pods/2mqb 367\nI0526 21:58:58.438492       1 logs_generator.go:76] 32 PUT /api/v1/namespaces/kube-system/pods/mwr 264\n"
May 26 21:58:58.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 logs logs-generator logs-generator --namespace=kubectl-606 --since=24h'
May 26 21:58:58.621: INFO: stderr: ""
May 26 21:58:58.621: INFO: stdout: "I0526 21:58:52.038317       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/5dd 481\nI0526 21:58:52.238459       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/hnmj 427\nI0526 21:58:52.438483       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/nl9k 261\nI0526 21:58:52.638484       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/l9k8 448\nI0526 21:58:52.840656       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/zfbn 465\nI0526 21:58:53.038527       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/d274 540\nI0526 21:58:53.238538       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/qnz 323\nI0526 21:58:53.438519       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/crmg 492\nI0526 21:58:53.638539       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/rwwz 276\nI0526 21:58:53.838489       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/w5c4 488\nI0526 21:58:54.038479       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/6kg 320\nI0526 21:58:54.238459       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/69pw 505\nI0526 21:58:54.438468       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/xbd2 444\nI0526 21:58:54.638487       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/jsqg 463\nI0526 21:58:54.838506       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/v49z 582\nI0526 21:58:55.038569       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/c8qh 429\nI0526 21:58:55.238522       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/79k 553\nI0526 21:58:55.438525       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/prr 465\nI0526 21:58:55.638476       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/c97c 596\nI0526 21:58:55.838533       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/wnv4 575\nI0526 21:58:56.038554       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/h272 484\nI0526 21:58:56.238554       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/dvx 404\nI0526 21:58:56.438529       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/default/pods/4tj 304\nI0526 21:58:56.638552       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/mtq4 404\nI0526 21:58:56.838522       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/default/pods/sdqg 354\nI0526 21:58:57.038490       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/kube-system/pods/69v 428\nI0526 21:58:57.238466       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/vg77 321\nI0526 21:58:57.438443       1 logs_generator.go:76] 27 GET /api/v1/namespaces/default/pods/glzh 405\nI0526 21:58:57.638482       1 logs_generator.go:76] 28 POST /api/v1/namespaces/default/pods/2qnz 201\nI0526 21:58:57.838492       1 logs_generator.go:76] 29 POST /api/v1/namespaces/default/pods/778v 519\nI0526 21:58:58.038506       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/default/pods/2qs 542\nI0526 21:58:58.238494       1 logs_generator.go:76] 31 PUT /api/v1/namespaces/ns/pods/2mqb 367\nI0526 21:58:58.438492       1 logs_generator.go:76] 32 PUT /api/v1/namespaces/kube-system/pods/mwr 264\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
May 26 21:58:58.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 delete pod logs-generator --namespace=kubectl-606'
May 26 21:59:00.766: INFO: stderr: ""
May 26 21:59:00.766: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:59:00.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-606" for this suite.
May 26 21:59:08.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:59:10.736: INFO: namespace kubectl-606 deletion completed in 9.956838442s

â€¢ [SLOW TEST:20.487 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:59:10.739: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 26 21:59:10.934: INFO: Waiting up to 5m0s for pod "pod-8aef0a44-b83d-4754-b7d8-af5239df984b" in namespace "emptydir-987" to be "success or failure"
May 26 21:59:10.943: INFO: Pod "pod-8aef0a44-b83d-4754-b7d8-af5239df984b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.130056ms
May 26 21:59:12.953: INFO: Pod "pod-8aef0a44-b83d-4754-b7d8-af5239df984b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019709547s
STEP: Saw pod success
May 26 21:59:12.954: INFO: Pod "pod-8aef0a44-b83d-4754-b7d8-af5239df984b" satisfied condition "success or failure"
May 26 21:59:12.965: INFO: Trying to get logs from node 10.215.60.34 pod pod-8aef0a44-b83d-4754-b7d8-af5239df984b container test-container: <nil>
STEP: delete the pod
May 26 21:59:13.020: INFO: Waiting for pod pod-8aef0a44-b83d-4754-b7d8-af5239df984b to disappear
May 26 21:59:13.030: INFO: Pod pod-8aef0a44-b83d-4754-b7d8-af5239df984b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 21:59:13.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-987" for this suite.
May 26 21:59:21.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 21:59:23.013: INFO: namespace emptydir-987 deletion completed in 9.969847204s

â€¢ [SLOW TEST:12.275 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 21:59:23.014: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-46108789-b76b-4c50-9ee7-c74f76ea502a in namespace container-probe-3381
May 26 21:59:26.231: INFO: Started pod test-webserver-46108789-b76b-4c50-9ee7-c74f76ea502a in namespace container-probe-3381
STEP: checking the pod's current state and verifying that restartCount is present
May 26 21:59:26.243: INFO: Initial restart count of pod test-webserver-46108789-b76b-4c50-9ee7-c74f76ea502a is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:03:27.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3381" for this suite.
May 26 22:03:35.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:03:37.539: INFO: namespace container-probe-3381 deletion completed in 9.90949507s

â€¢ [SLOW TEST:254.525 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:03:37.539: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-6901
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
May 26 22:03:37.703: INFO: Found 0 stateful pods, waiting for 3
May 26 22:03:47.714: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 26 22:03:47.714: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 26 22:03:47.714: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 26 22:03:47.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-6901 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 26 22:03:48.079: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 26 22:03:48.079: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 26 22:03:48.079: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
May 26 22:03:58.148: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 26 22:04:08.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-6901 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 22:04:08.514: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 26 22:04:08.514: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 26 22:04:08.514: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 26 22:04:18.573: INFO: Waiting for StatefulSet statefulset-6901/ss2 to complete update
May 26 22:04:18.573: INFO: Waiting for Pod statefulset-6901/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 26 22:04:18.573: INFO: Waiting for Pod statefulset-6901/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 26 22:04:28.599: INFO: Waiting for StatefulSet statefulset-6901/ss2 to complete update
May 26 22:04:28.599: INFO: Waiting for Pod statefulset-6901/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 26 22:04:28.599: INFO: Waiting for Pod statefulset-6901/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 26 22:04:38.595: INFO: Waiting for StatefulSet statefulset-6901/ss2 to complete update
May 26 22:04:38.595: INFO: Waiting for Pod statefulset-6901/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 26 22:04:38.595: INFO: Waiting for Pod statefulset-6901/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 26 22:04:48.595: INFO: Waiting for StatefulSet statefulset-6901/ss2 to complete update
May 26 22:04:48.595: INFO: Waiting for Pod statefulset-6901/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 26 22:04:58.592: INFO: Waiting for StatefulSet statefulset-6901/ss2 to complete update
STEP: Rolling back to a previous revision
May 26 22:05:08.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-6901 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 26 22:05:08.951: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 26 22:05:08.951: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 26 22:05:08.951: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 26 22:05:19.019: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 26 22:05:29.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-6901 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 22:05:29.415: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 26 22:05:29.415: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 26 22:05:29.415: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 26 22:05:39.471: INFO: Waiting for StatefulSet statefulset-6901/ss2 to complete update
May 26 22:05:39.471: INFO: Waiting for Pod statefulset-6901/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 26 22:05:39.471: INFO: Waiting for Pod statefulset-6901/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 26 22:05:49.491: INFO: Waiting for StatefulSet statefulset-6901/ss2 to complete update
May 26 22:05:49.491: INFO: Waiting for Pod statefulset-6901/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May 26 22:05:59.498: INFO: Deleting all statefulset in ns statefulset-6901
May 26 22:05:59.507: INFO: Scaling statefulset ss2 to 0
May 26 22:06:19.550: INFO: Waiting for statefulset status.replicas updated to 0
May 26 22:06:19.558: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:06:19.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6901" for this suite.
May 26 22:06:27.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:06:29.498: INFO: namespace statefulset-6901 deletion completed in 9.8861151s

â€¢ [SLOW TEST:171.959 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:06:29.498: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
May 26 22:06:29.670: INFO: Waiting up to 5m0s for pod "pod-265e6bdb-43c5-444b-99a8-9fe98211a113" in namespace "emptydir-8497" to be "success or failure"
May 26 22:06:29.680: INFO: Pod "pod-265e6bdb-43c5-444b-99a8-9fe98211a113": Phase="Pending", Reason="", readiness=false. Elapsed: 9.400669ms
May 26 22:06:31.690: INFO: Pod "pod-265e6bdb-43c5-444b-99a8-9fe98211a113": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019541427s
STEP: Saw pod success
May 26 22:06:31.690: INFO: Pod "pod-265e6bdb-43c5-444b-99a8-9fe98211a113" satisfied condition "success or failure"
May 26 22:06:31.699: INFO: Trying to get logs from node 10.215.60.34 pod pod-265e6bdb-43c5-444b-99a8-9fe98211a113 container test-container: <nil>
STEP: delete the pod
May 26 22:06:31.781: INFO: Waiting for pod pod-265e6bdb-43c5-444b-99a8-9fe98211a113 to disappear
May 26 22:06:31.790: INFO: Pod pod-265e6bdb-43c5-444b-99a8-9fe98211a113 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:06:31.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8497" for this suite.
May 26 22:06:39.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:06:41.715: INFO: namespace emptydir-8497 deletion completed in 9.901060642s

â€¢ [SLOW TEST:12.217 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:06:41.716: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-1c607f07-675f-4ad1-af1c-400275ea305e
STEP: Creating a pod to test consume configMaps
May 26 22:06:41.906: INFO: Waiting up to 5m0s for pod "pod-configmaps-74458e12-46e5-4c33-831b-10302cecaf41" in namespace "configmap-4263" to be "success or failure"
May 26 22:06:41.914: INFO: Pod "pod-configmaps-74458e12-46e5-4c33-831b-10302cecaf41": Phase="Pending", Reason="", readiness=false. Elapsed: 8.441493ms
May 26 22:06:43.924: INFO: Pod "pod-configmaps-74458e12-46e5-4c33-831b-10302cecaf41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01820189s
May 26 22:06:45.934: INFO: Pod "pod-configmaps-74458e12-46e5-4c33-831b-10302cecaf41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028692195s
STEP: Saw pod success
May 26 22:06:45.935: INFO: Pod "pod-configmaps-74458e12-46e5-4c33-831b-10302cecaf41" satisfied condition "success or failure"
May 26 22:06:45.944: INFO: Trying to get logs from node 10.215.60.34 pod pod-configmaps-74458e12-46e5-4c33-831b-10302cecaf41 container configmap-volume-test: <nil>
STEP: delete the pod
May 26 22:06:45.995: INFO: Waiting for pod pod-configmaps-74458e12-46e5-4c33-831b-10302cecaf41 to disappear
May 26 22:06:46.006: INFO: Pod pod-configmaps-74458e12-46e5-4c33-831b-10302cecaf41 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:06:46.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4263" for this suite.
May 26 22:06:54.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:06:55.918: INFO: namespace configmap-4263 deletion completed in 9.896895561s

â€¢ [SLOW TEST:14.202 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:06:55.918: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 26 22:06:58.096: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 26 22:07:00.127: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726127618, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726127618, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726127618, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726127618, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 26 22:07:03.171: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:07:03.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7599" for this suite.
May 26 22:07:11.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:07:13.105: INFO: namespace webhook-7599 deletion completed in 9.89645584s
STEP: Destroying namespace "webhook-7599-markers" for this suite.
May 26 22:07:21.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:07:22.993: INFO: namespace webhook-7599-markers deletion completed in 9.888189106s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:27.121 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:07:23.039: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 22:07:23.181: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:07:24.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2499" for this suite.
May 26 22:07:32.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:07:34.311: INFO: namespace custom-resource-definition-2499 deletion completed in 10.055950071s

â€¢ [SLOW TEST:11.272 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:07:34.316: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
May 26 22:07:34.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 api-versions'
May 26 22:07:34.577: INFO: stderr: ""
May 26 22:07:34.577: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1beta1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.openshift.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:07:34.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7175" for this suite.
May 26 22:07:42.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:07:44.485: INFO: namespace kubectl-7175 deletion completed in 9.894449659s

â€¢ [SLOW TEST:10.170 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:07:44.486: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-892
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
May 26 22:07:44.672: INFO: Found 0 stateful pods, waiting for 3
May 26 22:07:54.683: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 26 22:07:54.683: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 26 22:07:54.683: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
May 26 22:07:54.743: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 26 22:08:04.810: INFO: Updating stateful set ss2
May 26 22:08:04.835: INFO: Waiting for Pod statefulset-892/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 26 22:08:14.856: INFO: Waiting for Pod statefulset-892/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
May 26 22:08:24.937: INFO: Found 2 stateful pods, waiting for 3
May 26 22:08:34.948: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 26 22:08:34.949: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 26 22:08:34.949: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 26 22:08:35.000: INFO: Updating stateful set ss2
May 26 22:08:35.021: INFO: Waiting for Pod statefulset-892/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 26 22:08:45.082: INFO: Updating stateful set ss2
May 26 22:08:45.100: INFO: Waiting for StatefulSet statefulset-892/ss2 to complete update
May 26 22:08:45.101: INFO: Waiting for Pod statefulset-892/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May 26 22:08:55.120: INFO: Deleting all statefulset in ns statefulset-892
May 26 22:08:55.129: INFO: Scaling statefulset ss2 to 0
May 26 22:09:15.177: INFO: Waiting for statefulset status.replicas updated to 0
May 26 22:09:15.187: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:09:15.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-892" for this suite.
May 26 22:09:23.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:09:25.140: INFO: namespace statefulset-892 deletion completed in 9.888113497s

â€¢ [SLOW TEST:100.654 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:09:25.140: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:09:25.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1448" for this suite.
May 26 22:09:33.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:09:35.275: INFO: namespace resourcequota-1448 deletion completed in 9.90164898s

â€¢ [SLOW TEST:10.135 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:09:35.276: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 22:09:35.401: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-cacecbac-8078-40d2-8344-9a8bf60220c9
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:09:39.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6258" for this suite.
May 26 22:09:53.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:09:55.461: INFO: namespace configmap-6258 deletion completed in 15.889971356s

â€¢ [SLOW TEST:20.186 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:09:55.462: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 26 22:09:58.686: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:09:58.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2334" for this suite.
May 26 22:10:06.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:10:08.638: INFO: namespace container-runtime-2334 deletion completed in 9.889261126s

â€¢ [SLOW TEST:13.176 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:10:08.639: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4793
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-4793
I0526 22:10:08.898094      23 runners.go:184] Created replication controller with name: externalname-service, namespace: services-4793, replica count: 2
I0526 22:10:11.948646      23 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 26 22:10:11.948: INFO: Creating new exec pod
May 26 22:10:15.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=services-4793 execpodxnkrw -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 26 22:10:15.665: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 26 22:10:15.665: INFO: stdout: ""
May 26 22:10:15.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=services-4793 execpodxnkrw -- /bin/sh -x -c nc -zv -t -w 2 172.21.40.183 80'
May 26 22:10:16.007: INFO: stderr: "+ nc -zv -t -w 2 172.21.40.183 80\nConnection to 172.21.40.183 80 port [tcp/http] succeeded!\n"
May 26 22:10:16.007: INFO: stdout: ""
May 26 22:10:16.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=services-4793 execpodxnkrw -- /bin/sh -x -c nc -zv -t -w 2 10.215.60.10 32025'
May 26 22:10:16.360: INFO: stderr: "+ nc -zv -t -w 2 10.215.60.10 32025\nConnection to 10.215.60.10 32025 port [tcp/32025] succeeded!\n"
May 26 22:10:16.360: INFO: stdout: ""
May 26 22:10:16.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=services-4793 execpodxnkrw -- /bin/sh -x -c nc -zv -t -w 2 10.215.60.11 32025'
May 26 22:10:16.722: INFO: stderr: "+ nc -zv -t -w 2 10.215.60.11 32025\nConnection to 10.215.60.11 32025 port [tcp/32025] succeeded!\n"
May 26 22:10:16.722: INFO: stdout: ""
May 26 22:10:16.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=services-4793 execpodxnkrw -- /bin/sh -x -c nc -zv -t -w 2 158.177.211.78 32025'
May 26 22:10:17.133: INFO: stderr: "+ nc -zv -t -w 2 158.177.211.78 32025\nConnection to 158.177.211.78 32025 port [tcp/32025] succeeded!\n"
May 26 22:10:17.133: INFO: stdout: ""
May 26 22:10:17.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=services-4793 execpodxnkrw -- /bin/sh -x -c nc -zv -t -w 2 158.177.211.69 32025'
May 26 22:10:17.502: INFO: stderr: "+ nc -zv -t -w 2 158.177.211.69 32025\nConnection to 158.177.211.69 32025 port [tcp/32025] succeeded!\n"
May 26 22:10:17.502: INFO: stdout: ""
May 26 22:10:17.502: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:10:17.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4793" for this suite.
May 26 22:10:25.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:10:27.491: INFO: namespace services-4793 deletion completed in 9.889612584s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:18.852 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:10:27.491: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
May 26 22:10:27.660: INFO: Waiting up to 5m0s for pod "var-expansion-6242ec83-efa4-40af-bd14-49b8af2a0fe9" in namespace "var-expansion-5721" to be "success or failure"
May 26 22:10:27.669: INFO: Pod "var-expansion-6242ec83-efa4-40af-bd14-49b8af2a0fe9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.189726ms
May 26 22:10:29.680: INFO: Pod "var-expansion-6242ec83-efa4-40af-bd14-49b8af2a0fe9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019630303s
May 26 22:10:31.692: INFO: Pod "var-expansion-6242ec83-efa4-40af-bd14-49b8af2a0fe9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031453971s
STEP: Saw pod success
May 26 22:10:31.692: INFO: Pod "var-expansion-6242ec83-efa4-40af-bd14-49b8af2a0fe9" satisfied condition "success or failure"
May 26 22:10:31.713: INFO: Trying to get logs from node 10.215.60.34 pod var-expansion-6242ec83-efa4-40af-bd14-49b8af2a0fe9 container dapi-container: <nil>
STEP: delete the pod
May 26 22:10:31.772: INFO: Waiting for pod var-expansion-6242ec83-efa4-40af-bd14-49b8af2a0fe9 to disappear
May 26 22:10:31.781: INFO: Pod var-expansion-6242ec83-efa4-40af-bd14-49b8af2a0fe9 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:10:31.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5721" for this suite.
May 26 22:10:39.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:10:41.687: INFO: namespace var-expansion-5721 deletion completed in 9.89041218s

â€¢ [SLOW TEST:14.196 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:10:41.688: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 26 22:10:42.890: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e420652-dfb6-4942-ac68-3e0f3cae4966" in namespace "downward-api-196" to be "success or failure"
May 26 22:10:42.900: INFO: Pod "downwardapi-volume-4e420652-dfb6-4942-ac68-3e0f3cae4966": Phase="Pending", Reason="", readiness=false. Elapsed: 10.211935ms
May 26 22:10:44.914: INFO: Pod "downwardapi-volume-4e420652-dfb6-4942-ac68-3e0f3cae4966": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023776515s
May 26 22:10:46.924: INFO: Pod "downwardapi-volume-4e420652-dfb6-4942-ac68-3e0f3cae4966": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034060399s
STEP: Saw pod success
May 26 22:10:46.924: INFO: Pod "downwardapi-volume-4e420652-dfb6-4942-ac68-3e0f3cae4966" satisfied condition "success or failure"
May 26 22:10:46.933: INFO: Trying to get logs from node 10.215.60.34 pod downwardapi-volume-4e420652-dfb6-4942-ac68-3e0f3cae4966 container client-container: <nil>
STEP: delete the pod
May 26 22:10:46.985: INFO: Waiting for pod downwardapi-volume-4e420652-dfb6-4942-ac68-3e0f3cae4966 to disappear
May 26 22:10:46.996: INFO: Pod downwardapi-volume-4e420652-dfb6-4942-ac68-3e0f3cae4966 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:10:46.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-196" for this suite.
May 26 22:10:55.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:10:56.900: INFO: namespace downward-api-196 deletion completed in 9.887548059s

â€¢ [SLOW TEST:15.213 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:10:56.901: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-a5e6d411-8682-4020-9f7a-04eb5da1d682
STEP: Creating a pod to test consume secrets
May 26 22:10:58.121: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b3459b86-86b5-4991-90f2-cec56f32cd3c" in namespace "projected-4513" to be "success or failure"
May 26 22:10:58.130: INFO: Pod "pod-projected-secrets-b3459b86-86b5-4991-90f2-cec56f32cd3c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.215383ms
May 26 22:11:00.143: INFO: Pod "pod-projected-secrets-b3459b86-86b5-4991-90f2-cec56f32cd3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022051279s
STEP: Saw pod success
May 26 22:11:00.143: INFO: Pod "pod-projected-secrets-b3459b86-86b5-4991-90f2-cec56f32cd3c" satisfied condition "success or failure"
May 26 22:11:00.155: INFO: Trying to get logs from node 10.215.60.34 pod pod-projected-secrets-b3459b86-86b5-4991-90f2-cec56f32cd3c container projected-secret-volume-test: <nil>
STEP: delete the pod
May 26 22:11:00.219: INFO: Waiting for pod pod-projected-secrets-b3459b86-86b5-4991-90f2-cec56f32cd3c to disappear
May 26 22:11:00.228: INFO: Pod pod-projected-secrets-b3459b86-86b5-4991-90f2-cec56f32cd3c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:11:00.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4513" for this suite.
May 26 22:11:08.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:11:10.160: INFO: namespace projected-4513 deletion completed in 9.912186141s

â€¢ [SLOW TEST:13.259 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:11:10.160: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 26 22:11:11.344: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 26 22:11:14.393: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:11:14.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-845" for this suite.
May 26 22:11:22.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:11:24.415: INFO: namespace webhook-845 deletion completed in 9.898038302s
STEP: Destroying namespace "webhook-845-markers" for this suite.
May 26 22:11:32.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:11:34.322: INFO: namespace webhook-845-markers deletion completed in 9.90719575s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:24.208 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:11:34.369: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 26 22:11:34.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-5168'
May 26 22:11:34.674: INFO: stderr: ""
May 26 22:11:34.674: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
May 26 22:11:39.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pod e2e-test-httpd-pod --namespace=kubectl-5168 -o json'
May 26 22:11:39.869: INFO: stderr: ""
May 26 22:11:39.869: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.30.182.125/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.182.125/32\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.182.125\\\"\\n    ],\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2020-05-26T22:11:34Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5168\",\n        \"resourceVersion\": \"58488\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5168/pods/e2e-test-httpd-pod\",\n        \"uid\": \"2e915733-1c17-46a6-a66e-a56f73910f00\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-g7n2v\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-kqxrk\"\n            }\n        ],\n        \"nodeName\": \"10.215.60.34\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c51,c25\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-g7n2v\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-g7n2v\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-05-26T22:11:34Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-05-26T22:11:36Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-05-26T22:11:36Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-05-26T22:11:34Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://95025f2573194bc3d09f74a95b0c156bd348014f599ea6ece4b4e41e426d7c6e\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-05-26T22:11:35Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.215.60.34\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.182.125\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.182.125\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-05-26T22:11:34Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 26 22:11:39.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 replace -f - --namespace=kubectl-5168'
May 26 22:11:40.485: INFO: stderr: ""
May 26 22:11:40.485: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
May 26 22:11:40.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 delete pods e2e-test-httpd-pod --namespace=kubectl-5168'
May 26 22:11:50.462: INFO: stderr: ""
May 26 22:11:50.462: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:11:50.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5168" for this suite.
May 26 22:11:58.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:12:00.383: INFO: namespace kubectl-5168 deletion completed in 9.900091585s

â€¢ [SLOW TEST:26.015 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:12:00.384: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 26 22:12:01.544: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 26 22:12:04.609: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:12:04.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7465" for this suite.
May 26 22:12:12.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:12:14.788: INFO: namespace webhook-7465 deletion completed in 9.899282704s
STEP: Destroying namespace "webhook-7465-markers" for this suite.
May 26 22:12:23.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:12:24.876: INFO: namespace webhook-7465-markers deletion completed in 10.08827527s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:24.537 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:12:24.921: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 26 22:12:26.097: INFO: Waiting up to 5m0s for pod "pod-0cd56304-12c3-4113-a3ea-288ac102261e" in namespace "emptydir-8512" to be "success or failure"
May 26 22:12:26.105: INFO: Pod "pod-0cd56304-12c3-4113-a3ea-288ac102261e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.832473ms
May 26 22:12:28.114: INFO: Pod "pod-0cd56304-12c3-4113-a3ea-288ac102261e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017395911s
May 26 22:12:30.125: INFO: Pod "pod-0cd56304-12c3-4113-a3ea-288ac102261e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02840209s
STEP: Saw pod success
May 26 22:12:30.126: INFO: Pod "pod-0cd56304-12c3-4113-a3ea-288ac102261e" satisfied condition "success or failure"
May 26 22:12:30.134: INFO: Trying to get logs from node 10.215.60.34 pod pod-0cd56304-12c3-4113-a3ea-288ac102261e container test-container: <nil>
STEP: delete the pod
May 26 22:12:30.196: INFO: Waiting for pod pod-0cd56304-12c3-4113-a3ea-288ac102261e to disappear
May 26 22:12:30.204: INFO: Pod pod-0cd56304-12c3-4113-a3ea-288ac102261e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:12:30.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8512" for this suite.
May 26 22:12:38.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:12:40.112: INFO: namespace emptydir-8512 deletion completed in 9.883838275s

â€¢ [SLOW TEST:15.191 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:12:40.115: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 26 22:12:41.014: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 26 22:12:43.052: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726127961, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726127961, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726127961, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726127961, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 26 22:12:46.091: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 22:12:46.101: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8412-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:12:47.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5976" for this suite.
May 26 22:12:55.351: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:12:57.207: INFO: namespace webhook-5976 deletion completed in 9.887639855s
STEP: Destroying namespace "webhook-5976-markers" for this suite.
May 26 22:13:05.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:13:07.095: INFO: namespace webhook-5976-markers deletion completed in 9.888115784s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:27.027 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:13:07.142: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-986
I0526 22:13:07.329069      23 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-986, replica count: 1
I0526 22:13:08.380987      23 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0526 22:13:09.381282      23 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 26 22:13:09.530: INFO: Created: latency-svc-tllsr
May 26 22:13:09.541: INFO: Got endpoints: latency-svc-tllsr [59.857119ms]
May 26 22:13:09.570: INFO: Created: latency-svc-9drls
May 26 22:13:09.582: INFO: Got endpoints: latency-svc-9drls [40.195087ms]
May 26 22:13:09.585: INFO: Created: latency-svc-ls5z5
May 26 22:13:09.596: INFO: Got endpoints: latency-svc-ls5z5 [53.556246ms]
May 26 22:13:09.601: INFO: Created: latency-svc-k5qqk
May 26 22:13:09.613: INFO: Got endpoints: latency-svc-k5qqk [70.177721ms]
May 26 22:13:09.616: INFO: Created: latency-svc-8b2fn
May 26 22:13:09.626: INFO: Got endpoints: latency-svc-8b2fn [83.131972ms]
May 26 22:13:09.630: INFO: Created: latency-svc-5wlzs
May 26 22:13:09.638: INFO: Got endpoints: latency-svc-5wlzs [95.240921ms]
May 26 22:13:09.640: INFO: Created: latency-svc-d9w88
May 26 22:13:09.650: INFO: Got endpoints: latency-svc-d9w88 [107.522899ms]
May 26 22:13:09.652: INFO: Created: latency-svc-p6k4c
May 26 22:13:09.662: INFO: Got endpoints: latency-svc-p6k4c [118.908015ms]
May 26 22:13:09.667: INFO: Created: latency-svc-lc5lx
May 26 22:13:09.685: INFO: Got endpoints: latency-svc-lc5lx [142.238035ms]
May 26 22:13:09.690: INFO: Created: latency-svc-sg5f7
May 26 22:13:09.699: INFO: Got endpoints: latency-svc-sg5f7 [156.52706ms]
May 26 22:13:09.703: INFO: Created: latency-svc-cpgl6
May 26 22:13:09.712: INFO: Got endpoints: latency-svc-cpgl6 [169.048768ms]
May 26 22:13:09.717: INFO: Created: latency-svc-kvz87
May 26 22:13:09.726: INFO: Got endpoints: latency-svc-kvz87 [183.062974ms]
May 26 22:13:09.731: INFO: Created: latency-svc-mcbhn
May 26 22:13:09.740: INFO: Got endpoints: latency-svc-mcbhn [197.224437ms]
May 26 22:13:09.746: INFO: Created: latency-svc-ncnzz
May 26 22:13:09.755: INFO: Got endpoints: latency-svc-ncnzz [212.196534ms]
May 26 22:13:09.757: INFO: Created: latency-svc-v8nmd
May 26 22:13:09.766: INFO: Got endpoints: latency-svc-v8nmd [223.289815ms]
May 26 22:13:09.771: INFO: Created: latency-svc-wx6nr
May 26 22:13:09.780: INFO: Got endpoints: latency-svc-wx6nr [236.637507ms]
May 26 22:13:09.785: INFO: Created: latency-svc-jktvr
May 26 22:13:09.796: INFO: Got endpoints: latency-svc-jktvr [214.456089ms]
May 26 22:13:09.800: INFO: Created: latency-svc-qqbzj
May 26 22:13:09.808: INFO: Got endpoints: latency-svc-qqbzj [212.490497ms]
May 26 22:13:09.814: INFO: Created: latency-svc-lcqt8
May 26 22:13:09.825: INFO: Created: latency-svc-l7zgc
May 26 22:13:09.825: INFO: Got endpoints: latency-svc-lcqt8 [211.701602ms]
May 26 22:13:09.836: INFO: Got endpoints: latency-svc-l7zgc [209.692601ms]
May 26 22:13:09.838: INFO: Created: latency-svc-tpmhh
May 26 22:13:09.849: INFO: Got endpoints: latency-svc-tpmhh [210.033889ms]
May 26 22:13:09.851: INFO: Created: latency-svc-nnwpk
May 26 22:13:09.862: INFO: Got endpoints: latency-svc-nnwpk [212.060218ms]
May 26 22:13:09.868: INFO: Created: latency-svc-ttmzz
May 26 22:13:09.877: INFO: Got endpoints: latency-svc-ttmzz [215.028865ms]
May 26 22:13:09.886: INFO: Created: latency-svc-bff46
May 26 22:13:09.894: INFO: Got endpoints: latency-svc-bff46 [208.909124ms]
May 26 22:13:09.895: INFO: Created: latency-svc-mbxp9
May 26 22:13:09.905: INFO: Got endpoints: latency-svc-mbxp9 [205.28579ms]
May 26 22:13:09.910: INFO: Created: latency-svc-66g8t
May 26 22:13:09.925: INFO: Got endpoints: latency-svc-66g8t [213.202602ms]
May 26 22:13:09.933: INFO: Created: latency-svc-7dgbn
May 26 22:13:09.942: INFO: Got endpoints: latency-svc-7dgbn [215.991921ms]
May 26 22:13:09.947: INFO: Created: latency-svc-nckhm
May 26 22:13:09.958: INFO: Got endpoints: latency-svc-nckhm [217.593389ms]
May 26 22:13:09.965: INFO: Created: latency-svc-hpqv6
May 26 22:13:09.976: INFO: Got endpoints: latency-svc-hpqv6 [220.295364ms]
May 26 22:13:09.983: INFO: Created: latency-svc-4bbjr
May 26 22:13:09.996: INFO: Got endpoints: latency-svc-4bbjr [229.81508ms]
May 26 22:13:10.001: INFO: Created: latency-svc-5k9t4
May 26 22:13:10.017: INFO: Created: latency-svc-pqxfx
May 26 22:13:10.018: INFO: Got endpoints: latency-svc-5k9t4 [238.015909ms]
May 26 22:13:10.028: INFO: Got endpoints: latency-svc-pqxfx [230.996157ms]
May 26 22:13:10.033: INFO: Created: latency-svc-f7kvb
May 26 22:13:10.044: INFO: Got endpoints: latency-svc-f7kvb [235.262286ms]
May 26 22:13:10.054: INFO: Created: latency-svc-ktwfx
May 26 22:13:10.062: INFO: Got endpoints: latency-svc-ktwfx [237.164876ms]
May 26 22:13:10.072: INFO: Created: latency-svc-bqxjb
May 26 22:13:10.079: INFO: Got endpoints: latency-svc-bqxjb [243.447468ms]
May 26 22:13:10.090: INFO: Created: latency-svc-m7bjj
May 26 22:13:10.099: INFO: Got endpoints: latency-svc-m7bjj [250.693168ms]
May 26 22:13:10.103: INFO: Created: latency-svc-6m67l
May 26 22:13:10.123: INFO: Got endpoints: latency-svc-6m67l [43.296623ms]
May 26 22:13:10.131: INFO: Created: latency-svc-qmxp4
May 26 22:13:10.144: INFO: Got endpoints: latency-svc-qmxp4 [281.483854ms]
May 26 22:13:10.151: INFO: Created: latency-svc-7f7g4
May 26 22:13:10.161: INFO: Got endpoints: latency-svc-7f7g4 [283.243086ms]
May 26 22:13:10.168: INFO: Created: latency-svc-m48th
May 26 22:13:10.176: INFO: Got endpoints: latency-svc-m48th [282.019336ms]
May 26 22:13:10.182: INFO: Created: latency-svc-7bg8h
May 26 22:13:10.192: INFO: Got endpoints: latency-svc-7bg8h [287.049574ms]
May 26 22:13:10.196: INFO: Created: latency-svc-lzdqk
May 26 22:13:10.206: INFO: Got endpoints: latency-svc-lzdqk [280.581851ms]
May 26 22:13:10.213: INFO: Created: latency-svc-v5ggw
May 26 22:13:10.227: INFO: Got endpoints: latency-svc-v5ggw [285.594434ms]
May 26 22:13:10.234: INFO: Created: latency-svc-6mxm8
May 26 22:13:10.244: INFO: Got endpoints: latency-svc-6mxm8 [285.609862ms]
May 26 22:13:10.250: INFO: Created: latency-svc-pb876
May 26 22:13:10.261: INFO: Got endpoints: latency-svc-pb876 [284.611884ms]
May 26 22:13:10.267: INFO: Created: latency-svc-pv8qd
May 26 22:13:10.280: INFO: Got endpoints: latency-svc-pv8qd [283.977784ms]
May 26 22:13:10.281: INFO: Created: latency-svc-dr2hd
May 26 22:13:10.290: INFO: Got endpoints: latency-svc-dr2hd [272.058111ms]
May 26 22:13:10.297: INFO: Created: latency-svc-rwkz5
May 26 22:13:10.307: INFO: Got endpoints: latency-svc-rwkz5 [279.384877ms]
May 26 22:13:10.314: INFO: Created: latency-svc-fvlx7
May 26 22:13:10.326: INFO: Got endpoints: latency-svc-fvlx7 [281.935341ms]
May 26 22:13:10.331: INFO: Created: latency-svc-wdx7h
May 26 22:13:10.341: INFO: Got endpoints: latency-svc-wdx7h [279.208838ms]
May 26 22:13:10.345: INFO: Created: latency-svc-m8lz9
May 26 22:13:10.364: INFO: Got endpoints: latency-svc-m8lz9 [264.610006ms]
May 26 22:13:10.370: INFO: Created: latency-svc-bfng8
May 26 22:13:10.382: INFO: Got endpoints: latency-svc-bfng8 [258.928197ms]
May 26 22:13:10.392: INFO: Created: latency-svc-p66zf
May 26 22:13:10.402: INFO: Got endpoints: latency-svc-p66zf [258.623406ms]
May 26 22:13:10.407: INFO: Created: latency-svc-f2bwb
May 26 22:13:10.416: INFO: Got endpoints: latency-svc-f2bwb [255.420285ms]
May 26 22:13:10.424: INFO: Created: latency-svc-jwzzn
May 26 22:13:10.439: INFO: Got endpoints: latency-svc-jwzzn [262.105327ms]
May 26 22:13:10.443: INFO: Created: latency-svc-tpl2r
May 26 22:13:10.455: INFO: Got endpoints: latency-svc-tpl2r [263.111316ms]
May 26 22:13:10.473: INFO: Created: latency-svc-4bvnx
May 26 22:13:10.488: INFO: Got endpoints: latency-svc-4bvnx [281.743402ms]
May 26 22:13:10.490: INFO: Created: latency-svc-lkjzr
May 26 22:13:10.513: INFO: Created: latency-svc-vk587
May 26 22:13:10.513: INFO: Got endpoints: latency-svc-lkjzr [285.433584ms]
May 26 22:13:10.530: INFO: Got endpoints: latency-svc-vk587 [286.306588ms]
May 26 22:13:10.533: INFO: Created: latency-svc-wc9sz
May 26 22:13:10.564: INFO: Created: latency-svc-n7njz
May 26 22:13:10.565: INFO: Got endpoints: latency-svc-wc9sz [304.001211ms]
May 26 22:13:10.577: INFO: Got endpoints: latency-svc-n7njz [296.920193ms]
May 26 22:13:10.583: INFO: Created: latency-svc-9sl6p
May 26 22:13:10.598: INFO: Got endpoints: latency-svc-9sl6p [307.797037ms]
May 26 22:13:10.609: INFO: Created: latency-svc-dq8l9
May 26 22:13:10.617: INFO: Got endpoints: latency-svc-dq8l9 [309.418169ms]
May 26 22:13:10.622: INFO: Created: latency-svc-ptqhc
May 26 22:13:10.630: INFO: Got endpoints: latency-svc-ptqhc [303.273614ms]
May 26 22:13:10.637: INFO: Created: latency-svc-bd9wk
May 26 22:13:10.657: INFO: Got endpoints: latency-svc-bd9wk [315.38509ms]
May 26 22:13:10.658: INFO: Created: latency-svc-sjvjm
May 26 22:13:10.669: INFO: Got endpoints: latency-svc-sjvjm [302.376698ms]
May 26 22:13:10.686: INFO: Created: latency-svc-84bfd
May 26 22:13:10.694: INFO: Got endpoints: latency-svc-84bfd [311.981252ms]
May 26 22:13:10.699: INFO: Created: latency-svc-zfvq8
May 26 22:13:10.716: INFO: Got endpoints: latency-svc-zfvq8 [313.692703ms]
May 26 22:13:10.728: INFO: Created: latency-svc-8k9gr
May 26 22:13:10.746: INFO: Got endpoints: latency-svc-8k9gr [329.417104ms]
May 26 22:13:10.751: INFO: Created: latency-svc-ljfr9
May 26 22:13:10.768: INFO: Got endpoints: latency-svc-ljfr9 [329.402669ms]
May 26 22:13:10.773: INFO: Created: latency-svc-fc4k4
May 26 22:13:10.793: INFO: Got endpoints: latency-svc-fc4k4 [337.279837ms]
May 26 22:13:10.799: INFO: Created: latency-svc-wbk84
May 26 22:13:10.812: INFO: Got endpoints: latency-svc-wbk84 [323.845816ms]
May 26 22:13:10.818: INFO: Created: latency-svc-xs6t7
May 26 22:13:10.828: INFO: Got endpoints: latency-svc-xs6t7 [314.411204ms]
May 26 22:13:10.834: INFO: Created: latency-svc-r4p8t
May 26 22:13:10.843: INFO: Got endpoints: latency-svc-r4p8t [312.825053ms]
May 26 22:13:10.853: INFO: Created: latency-svc-stsgc
May 26 22:13:10.869: INFO: Got endpoints: latency-svc-stsgc [303.645012ms]
May 26 22:13:10.870: INFO: Created: latency-svc-ssfnh
May 26 22:13:10.891: INFO: Got endpoints: latency-svc-ssfnh [313.606508ms]
May 26 22:13:10.895: INFO: Created: latency-svc-67ngw
May 26 22:13:10.907: INFO: Got endpoints: latency-svc-67ngw [308.629554ms]
May 26 22:13:10.908: INFO: Created: latency-svc-m7fts
May 26 22:13:10.919: INFO: Got endpoints: latency-svc-m7fts [302.158281ms]
May 26 22:13:10.930: INFO: Created: latency-svc-qtk8d
May 26 22:13:10.935: INFO: Got endpoints: latency-svc-qtk8d [304.577061ms]
May 26 22:13:10.940: INFO: Created: latency-svc-9h27j
May 26 22:13:10.951: INFO: Got endpoints: latency-svc-9h27j [293.362484ms]
May 26 22:13:10.954: INFO: Created: latency-svc-gswfp
May 26 22:13:10.964: INFO: Got endpoints: latency-svc-gswfp [294.686671ms]
May 26 22:13:10.968: INFO: Created: latency-svc-9hvt2
May 26 22:13:10.978: INFO: Got endpoints: latency-svc-9hvt2 [283.610492ms]
May 26 22:13:10.988: INFO: Created: latency-svc-dt226
May 26 22:13:10.999: INFO: Got endpoints: latency-svc-dt226 [282.524778ms]
May 26 22:13:11.006: INFO: Created: latency-svc-vkjdc
May 26 22:13:11.017: INFO: Got endpoints: latency-svc-vkjdc [270.363822ms]
May 26 22:13:11.023: INFO: Created: latency-svc-d7f7c
May 26 22:13:11.034: INFO: Got endpoints: latency-svc-d7f7c [265.438057ms]
May 26 22:13:11.037: INFO: Created: latency-svc-8v42n
May 26 22:13:11.047: INFO: Got endpoints: latency-svc-8v42n [254.510799ms]
May 26 22:13:11.054: INFO: Created: latency-svc-k87dj
May 26 22:13:11.064: INFO: Got endpoints: latency-svc-k87dj [251.66048ms]
May 26 22:13:11.069: INFO: Created: latency-svc-qgsbp
May 26 22:13:11.079: INFO: Got endpoints: latency-svc-qgsbp [250.643717ms]
May 26 22:13:11.085: INFO: Created: latency-svc-lnfvz
May 26 22:13:11.096: INFO: Got endpoints: latency-svc-lnfvz [252.527928ms]
May 26 22:13:11.098: INFO: Created: latency-svc-qtmrt
May 26 22:13:11.107: INFO: Got endpoints: latency-svc-qtmrt [238.041675ms]
May 26 22:13:11.113: INFO: Created: latency-svc-nfdrc
May 26 22:13:11.122: INFO: Got endpoints: latency-svc-nfdrc [230.880815ms]
May 26 22:13:11.127: INFO: Created: latency-svc-cn6jr
May 26 22:13:11.137: INFO: Got endpoints: latency-svc-cn6jr [229.817335ms]
May 26 22:13:11.141: INFO: Created: latency-svc-bgr8r
May 26 22:13:11.151: INFO: Got endpoints: latency-svc-bgr8r [231.810458ms]
May 26 22:13:11.160: INFO: Created: latency-svc-k9k4k
May 26 22:13:11.172: INFO: Got endpoints: latency-svc-k9k4k [236.401219ms]
May 26 22:13:11.182: INFO: Created: latency-svc-xz5zr
May 26 22:13:11.192: INFO: Got endpoints: latency-svc-xz5zr [240.740133ms]
May 26 22:13:11.200: INFO: Created: latency-svc-qw744
May 26 22:13:11.209: INFO: Got endpoints: latency-svc-qw744 [244.677515ms]
May 26 22:13:11.213: INFO: Created: latency-svc-n24zg
May 26 22:13:11.224: INFO: Got endpoints: latency-svc-n24zg [245.790637ms]
May 26 22:13:11.231: INFO: Created: latency-svc-5mhts
May 26 22:13:11.245: INFO: Got endpoints: latency-svc-5mhts [245.525005ms]
May 26 22:13:11.247: INFO: Created: latency-svc-59798
May 26 22:13:11.262: INFO: Got endpoints: latency-svc-59798 [245.016347ms]
May 26 22:13:11.270: INFO: Created: latency-svc-j9tct
May 26 22:13:11.279: INFO: Got endpoints: latency-svc-j9tct [245.10011ms]
May 26 22:13:11.285: INFO: Created: latency-svc-dvc9v
May 26 22:13:11.296: INFO: Got endpoints: latency-svc-dvc9v [248.32442ms]
May 26 22:13:11.301: INFO: Created: latency-svc-vkkdk
May 26 22:13:11.308: INFO: Got endpoints: latency-svc-vkkdk [244.098878ms]
May 26 22:13:11.317: INFO: Created: latency-svc-rpzj2
May 26 22:13:11.325: INFO: Got endpoints: latency-svc-rpzj2 [246.093824ms]
May 26 22:13:11.329: INFO: Created: latency-svc-n9q57
May 26 22:13:11.340: INFO: Got endpoints: latency-svc-n9q57 [243.578674ms]
May 26 22:13:11.348: INFO: Created: latency-svc-lck8m
May 26 22:13:11.358: INFO: Got endpoints: latency-svc-lck8m [251.332618ms]
May 26 22:13:11.367: INFO: Created: latency-svc-mlwh9
May 26 22:13:11.378: INFO: Got endpoints: latency-svc-mlwh9 [256.013708ms]
May 26 22:13:11.385: INFO: Created: latency-svc-s72bq
May 26 22:13:11.394: INFO: Got endpoints: latency-svc-s72bq [256.872834ms]
May 26 22:13:11.399: INFO: Created: latency-svc-qfw9n
May 26 22:13:11.413: INFO: Got endpoints: latency-svc-qfw9n [261.64827ms]
May 26 22:13:11.418: INFO: Created: latency-svc-6v8kl
May 26 22:13:11.429: INFO: Got endpoints: latency-svc-6v8kl [256.783789ms]
May 26 22:13:11.434: INFO: Created: latency-svc-xtmts
May 26 22:13:11.448: INFO: Got endpoints: latency-svc-xtmts [256.439525ms]
May 26 22:13:11.449: INFO: Created: latency-svc-9qglt
May 26 22:13:11.462: INFO: Created: latency-svc-q952j
May 26 22:13:11.463: INFO: Got endpoints: latency-svc-9qglt [252.957421ms]
May 26 22:13:11.473: INFO: Got endpoints: latency-svc-q952j [249.473608ms]
May 26 22:13:11.482: INFO: Created: latency-svc-tgzw2
May 26 22:13:11.491: INFO: Got endpoints: latency-svc-tgzw2 [245.644641ms]
May 26 22:13:11.493: INFO: Created: latency-svc-wlwpm
May 26 22:13:11.502: INFO: Got endpoints: latency-svc-wlwpm [239.36885ms]
May 26 22:13:11.507: INFO: Created: latency-svc-tbss5
May 26 22:13:11.518: INFO: Got endpoints: latency-svc-tbss5 [238.800268ms]
May 26 22:13:11.526: INFO: Created: latency-svc-gzzjz
May 26 22:13:11.532: INFO: Got endpoints: latency-svc-gzzjz [236.018284ms]
May 26 22:13:11.540: INFO: Created: latency-svc-xqpml
May 26 22:13:11.554: INFO: Got endpoints: latency-svc-xqpml [245.285579ms]
May 26 22:13:11.557: INFO: Created: latency-svc-25284
May 26 22:13:11.568: INFO: Got endpoints: latency-svc-25284 [242.561892ms]
May 26 22:13:11.573: INFO: Created: latency-svc-b8j58
May 26 22:13:11.585: INFO: Got endpoints: latency-svc-b8j58 [243.351649ms]
May 26 22:13:11.597: INFO: Created: latency-svc-x7pxt
May 26 22:13:11.603: INFO: Got endpoints: latency-svc-x7pxt [244.751014ms]
May 26 22:13:11.614: INFO: Created: latency-svc-kl6g2
May 26 22:13:11.623: INFO: Created: latency-svc-ckg84
May 26 22:13:11.629: INFO: Got endpoints: latency-svc-kl6g2 [251.248486ms]
May 26 22:13:11.632: INFO: Got endpoints: latency-svc-ckg84 [238.128305ms]
May 26 22:13:11.639: INFO: Created: latency-svc-csghp
May 26 22:13:11.653: INFO: Got endpoints: latency-svc-csghp [239.81084ms]
May 26 22:13:11.656: INFO: Created: latency-svc-8t2lx
May 26 22:13:11.664: INFO: Got endpoints: latency-svc-8t2lx [235.310777ms]
May 26 22:13:11.672: INFO: Created: latency-svc-g6999
May 26 22:13:11.683: INFO: Got endpoints: latency-svc-g6999 [234.50247ms]
May 26 22:13:11.688: INFO: Created: latency-svc-8vsls
May 26 22:13:11.697: INFO: Got endpoints: latency-svc-8vsls [234.20901ms]
May 26 22:13:11.702: INFO: Created: latency-svc-2dfv6
May 26 22:13:11.714: INFO: Got endpoints: latency-svc-2dfv6 [240.617212ms]
May 26 22:13:11.721: INFO: Created: latency-svc-2dsp5
May 26 22:13:11.732: INFO: Got endpoints: latency-svc-2dsp5 [241.065857ms]
May 26 22:13:11.736: INFO: Created: latency-svc-md7nn
May 26 22:13:11.746: INFO: Got endpoints: latency-svc-md7nn [244.092255ms]
May 26 22:13:11.752: INFO: Created: latency-svc-f59vw
May 26 22:13:11.762: INFO: Got endpoints: latency-svc-f59vw [244.042404ms]
May 26 22:13:11.767: INFO: Created: latency-svc-wbrjp
May 26 22:13:11.777: INFO: Got endpoints: latency-svc-wbrjp [244.933332ms]
May 26 22:13:11.784: INFO: Created: latency-svc-xgwk9
May 26 22:13:11.793: INFO: Got endpoints: latency-svc-xgwk9 [239.168275ms]
May 26 22:13:11.798: INFO: Created: latency-svc-5rmc8
May 26 22:13:11.809: INFO: Got endpoints: latency-svc-5rmc8 [241.521997ms]
May 26 22:13:11.814: INFO: Created: latency-svc-qpsjf
May 26 22:13:11.824: INFO: Got endpoints: latency-svc-qpsjf [238.59169ms]
May 26 22:13:11.829: INFO: Created: latency-svc-5xb4k
May 26 22:13:11.840: INFO: Got endpoints: latency-svc-5xb4k [236.224307ms]
May 26 22:13:11.843: INFO: Created: latency-svc-5gx2m
May 26 22:13:11.854: INFO: Got endpoints: latency-svc-5gx2m [224.09284ms]
May 26 22:13:11.857: INFO: Created: latency-svc-cbb9z
May 26 22:13:11.867: INFO: Got endpoints: latency-svc-cbb9z [234.526385ms]
May 26 22:13:11.871: INFO: Created: latency-svc-789t6
May 26 22:13:11.884: INFO: Got endpoints: latency-svc-789t6 [231.32447ms]
May 26 22:13:11.891: INFO: Created: latency-svc-zb2k4
May 26 22:13:11.901: INFO: Got endpoints: latency-svc-zb2k4 [236.918676ms]
May 26 22:13:11.914: INFO: Created: latency-svc-wh9gd
May 26 22:13:11.931: INFO: Got endpoints: latency-svc-wh9gd [248.031399ms]
May 26 22:13:11.934: INFO: Created: latency-svc-zdpxg
May 26 22:13:11.944: INFO: Got endpoints: latency-svc-zdpxg [246.59634ms]
May 26 22:13:11.951: INFO: Created: latency-svc-9knxm
May 26 22:13:11.963: INFO: Got endpoints: latency-svc-9knxm [248.596455ms]
May 26 22:13:11.970: INFO: Created: latency-svc-kqvtd
May 26 22:13:11.980: INFO: Got endpoints: latency-svc-kqvtd [248.613445ms]
May 26 22:13:11.987: INFO: Created: latency-svc-ffbxp
May 26 22:13:11.998: INFO: Got endpoints: latency-svc-ffbxp [251.679842ms]
May 26 22:13:12.003: INFO: Created: latency-svc-pwmnv
May 26 22:13:12.013: INFO: Got endpoints: latency-svc-pwmnv [251.127099ms]
May 26 22:13:12.018: INFO: Created: latency-svc-d48s8
May 26 22:13:12.028: INFO: Got endpoints: latency-svc-d48s8 [250.888339ms]
May 26 22:13:12.034: INFO: Created: latency-svc-4rwpd
May 26 22:13:12.045: INFO: Got endpoints: latency-svc-4rwpd [251.739486ms]
May 26 22:13:12.049: INFO: Created: latency-svc-t7hnm
May 26 22:13:12.062: INFO: Got endpoints: latency-svc-t7hnm [252.312411ms]
May 26 22:13:12.066: INFO: Created: latency-svc-fsj9t
May 26 22:13:12.076: INFO: Got endpoints: latency-svc-fsj9t [251.504477ms]
May 26 22:13:12.082: INFO: Created: latency-svc-sgjt4
May 26 22:13:12.093: INFO: Got endpoints: latency-svc-sgjt4 [253.513307ms]
May 26 22:13:12.097: INFO: Created: latency-svc-t8djv
May 26 22:13:12.106: INFO: Got endpoints: latency-svc-t8djv [252.104605ms]
May 26 22:13:12.112: INFO: Created: latency-svc-68zpv
May 26 22:13:12.124: INFO: Got endpoints: latency-svc-68zpv [256.974859ms]
May 26 22:13:12.132: INFO: Created: latency-svc-jb5qv
May 26 22:13:12.143: INFO: Got endpoints: latency-svc-jb5qv [258.350577ms]
May 26 22:13:12.150: INFO: Created: latency-svc-krb7c
May 26 22:13:12.165: INFO: Created: latency-svc-24ztf
May 26 22:13:12.166: INFO: Got endpoints: latency-svc-krb7c [264.396573ms]
May 26 22:13:12.179: INFO: Got endpoints: latency-svc-24ztf [247.045032ms]
May 26 22:13:12.189: INFO: Created: latency-svc-z4gt9
May 26 22:13:12.200: INFO: Got endpoints: latency-svc-z4gt9 [256.279181ms]
May 26 22:13:12.209: INFO: Created: latency-svc-xvdnk
May 26 22:13:12.222: INFO: Got endpoints: latency-svc-xvdnk [259.565177ms]
May 26 22:13:12.243: INFO: Created: latency-svc-m9tgc
May 26 22:13:12.249: INFO: Created: latency-svc-h787w
May 26 22:13:12.259: INFO: Got endpoints: latency-svc-m9tgc [277.939379ms]
May 26 22:13:12.263: INFO: Got endpoints: latency-svc-h787w [264.974636ms]
May 26 22:13:12.270: INFO: Created: latency-svc-8z2vx
May 26 22:13:12.276: INFO: Got endpoints: latency-svc-8z2vx [262.903257ms]
May 26 22:13:12.286: INFO: Created: latency-svc-m64db
May 26 22:13:12.295: INFO: Got endpoints: latency-svc-m64db [267.110411ms]
May 26 22:13:12.304: INFO: Created: latency-svc-jw9dj
May 26 22:13:12.314: INFO: Got endpoints: latency-svc-jw9dj [269.603211ms]
May 26 22:13:12.318: INFO: Created: latency-svc-qzxgl
May 26 22:13:12.329: INFO: Got endpoints: latency-svc-qzxgl [267.076594ms]
May 26 22:13:12.331: INFO: Created: latency-svc-tpkmk
May 26 22:13:12.351: INFO: Created: latency-svc-h6r4j
May 26 22:13:12.351: INFO: Got endpoints: latency-svc-tpkmk [275.540924ms]
May 26 22:13:12.355: INFO: Got endpoints: latency-svc-h6r4j [261.906478ms]
May 26 22:13:12.362: INFO: Created: latency-svc-wsb27
May 26 22:13:12.370: INFO: Got endpoints: latency-svc-wsb27 [264.221711ms]
May 26 22:13:12.378: INFO: Created: latency-svc-jpgvz
May 26 22:13:12.389: INFO: Got endpoints: latency-svc-jpgvz [265.077498ms]
May 26 22:13:12.396: INFO: Created: latency-svc-dh86h
May 26 22:13:12.408: INFO: Got endpoints: latency-svc-dh86h [264.01426ms]
May 26 22:13:12.415: INFO: Created: latency-svc-zsv7n
May 26 22:13:12.424: INFO: Got endpoints: latency-svc-zsv7n [258.564837ms]
May 26 22:13:12.433: INFO: Created: latency-svc-4kp8k
May 26 22:13:12.440: INFO: Got endpoints: latency-svc-4kp8k [261.156794ms]
May 26 22:13:12.449: INFO: Created: latency-svc-gnfww
May 26 22:13:12.457: INFO: Got endpoints: latency-svc-gnfww [256.647717ms]
May 26 22:13:12.463: INFO: Created: latency-svc-rpwsl
May 26 22:13:12.472: INFO: Got endpoints: latency-svc-rpwsl [249.427796ms]
May 26 22:13:12.480: INFO: Created: latency-svc-vxwvj
May 26 22:13:12.488: INFO: Got endpoints: latency-svc-vxwvj [229.829118ms]
May 26 22:13:12.494: INFO: Created: latency-svc-t42lk
May 26 22:13:12.506: INFO: Got endpoints: latency-svc-t42lk [243.100775ms]
May 26 22:13:12.519: INFO: Created: latency-svc-rv9j6
May 26 22:13:12.526: INFO: Got endpoints: latency-svc-rv9j6 [249.918256ms]
May 26 22:13:12.535: INFO: Created: latency-svc-zhsfj
May 26 22:13:12.547: INFO: Got endpoints: latency-svc-zhsfj [251.66124ms]
May 26 22:13:12.553: INFO: Created: latency-svc-snf85
May 26 22:13:12.565: INFO: Got endpoints: latency-svc-snf85 [250.48932ms]
May 26 22:13:12.569: INFO: Created: latency-svc-jx4rv
May 26 22:13:12.578: INFO: Got endpoints: latency-svc-jx4rv [248.75802ms]
May 26 22:13:12.585: INFO: Created: latency-svc-6m2bw
May 26 22:13:12.595: INFO: Got endpoints: latency-svc-6m2bw [243.499645ms]
May 26 22:13:12.601: INFO: Created: latency-svc-vlptb
May 26 22:13:12.615: INFO: Got endpoints: latency-svc-vlptb [259.762997ms]
May 26 22:13:12.622: INFO: Created: latency-svc-g7t55
May 26 22:13:12.633: INFO: Got endpoints: latency-svc-g7t55 [262.860852ms]
May 26 22:13:12.640: INFO: Created: latency-svc-lflwm
May 26 22:13:12.653: INFO: Got endpoints: latency-svc-lflwm [263.665862ms]
May 26 22:13:12.658: INFO: Created: latency-svc-4grd6
May 26 22:13:12.670: INFO: Got endpoints: latency-svc-4grd6 [262.19366ms]
May 26 22:13:12.675: INFO: Created: latency-svc-s42f6
May 26 22:13:12.687: INFO: Got endpoints: latency-svc-s42f6 [262.140955ms]
May 26 22:13:12.693: INFO: Created: latency-svc-n58x7
May 26 22:13:12.706: INFO: Got endpoints: latency-svc-n58x7 [265.513705ms]
May 26 22:13:12.712: INFO: Created: latency-svc-bpm87
May 26 22:13:12.730: INFO: Got endpoints: latency-svc-bpm87 [273.139274ms]
May 26 22:13:12.736: INFO: Created: latency-svc-8l7c6
May 26 22:13:12.746: INFO: Got endpoints: latency-svc-8l7c6 [273.725262ms]
May 26 22:13:12.751: INFO: Created: latency-svc-75sfb
May 26 22:13:12.764: INFO: Got endpoints: latency-svc-75sfb [275.58762ms]
May 26 22:13:12.772: INFO: Created: latency-svc-wr8jq
May 26 22:13:12.790: INFO: Got endpoints: latency-svc-wr8jq [283.567279ms]
May 26 22:13:12.795: INFO: Created: latency-svc-b7gj5
May 26 22:13:12.814: INFO: Got endpoints: latency-svc-b7gj5 [288.124093ms]
May 26 22:13:12.819: INFO: Created: latency-svc-6lmvn
May 26 22:13:12.836: INFO: Got endpoints: latency-svc-6lmvn [288.437667ms]
May 26 22:13:12.842: INFO: Created: latency-svc-xg46n
May 26 22:13:12.853: INFO: Got endpoints: latency-svc-xg46n [288.305769ms]
May 26 22:13:12.856: INFO: Created: latency-svc-vgvd5
May 26 22:13:12.876: INFO: Got endpoints: latency-svc-vgvd5 [297.861611ms]
May 26 22:13:12.897: INFO: Created: latency-svc-5r7j4
May 26 22:13:12.917: INFO: Got endpoints: latency-svc-5r7j4 [322.257209ms]
May 26 22:13:12.925: INFO: Created: latency-svc-rm5t9
May 26 22:13:12.936: INFO: Got endpoints: latency-svc-rm5t9 [320.66595ms]
May 26 22:13:12.951: INFO: Created: latency-svc-hzptw
May 26 22:13:12.965: INFO: Got endpoints: latency-svc-hzptw [331.208817ms]
May 26 22:13:12.969: INFO: Created: latency-svc-w46l5
May 26 22:13:12.980: INFO: Got endpoints: latency-svc-w46l5 [327.458333ms]
May 26 22:13:12.986: INFO: Created: latency-svc-qknxd
May 26 22:13:12.995: INFO: Got endpoints: latency-svc-qknxd [324.982991ms]
May 26 22:13:13.001: INFO: Created: latency-svc-g59ql
May 26 22:13:13.013: INFO: Got endpoints: latency-svc-g59ql [326.306908ms]
May 26 22:13:13.017: INFO: Created: latency-svc-kvtnk
May 26 22:13:13.025: INFO: Got endpoints: latency-svc-kvtnk [319.154476ms]
May 26 22:13:13.032: INFO: Created: latency-svc-4q5zk
May 26 22:13:13.041: INFO: Got endpoints: latency-svc-4q5zk [311.174657ms]
May 26 22:13:13.042: INFO: Latencies: [40.195087ms 43.296623ms 53.556246ms 70.177721ms 83.131972ms 95.240921ms 107.522899ms 118.908015ms 142.238035ms 156.52706ms 169.048768ms 183.062974ms 197.224437ms 205.28579ms 208.909124ms 209.692601ms 210.033889ms 211.701602ms 212.060218ms 212.196534ms 212.490497ms 213.202602ms 214.456089ms 215.028865ms 215.991921ms 217.593389ms 220.295364ms 223.289815ms 224.09284ms 229.81508ms 229.817335ms 229.829118ms 230.880815ms 230.996157ms 231.32447ms 231.810458ms 234.20901ms 234.50247ms 234.526385ms 235.262286ms 235.310777ms 236.018284ms 236.224307ms 236.401219ms 236.637507ms 236.918676ms 237.164876ms 238.015909ms 238.041675ms 238.128305ms 238.59169ms 238.800268ms 239.168275ms 239.36885ms 239.81084ms 240.617212ms 240.740133ms 241.065857ms 241.521997ms 242.561892ms 243.100775ms 243.351649ms 243.447468ms 243.499645ms 243.578674ms 244.042404ms 244.092255ms 244.098878ms 244.677515ms 244.751014ms 244.933332ms 245.016347ms 245.10011ms 245.285579ms 245.525005ms 245.644641ms 245.790637ms 246.093824ms 246.59634ms 247.045032ms 248.031399ms 248.32442ms 248.596455ms 248.613445ms 248.75802ms 249.427796ms 249.473608ms 249.918256ms 250.48932ms 250.643717ms 250.693168ms 250.888339ms 251.127099ms 251.248486ms 251.332618ms 251.504477ms 251.66048ms 251.66124ms 251.679842ms 251.739486ms 252.104605ms 252.312411ms 252.527928ms 252.957421ms 253.513307ms 254.510799ms 255.420285ms 256.013708ms 256.279181ms 256.439525ms 256.647717ms 256.783789ms 256.872834ms 256.974859ms 258.350577ms 258.564837ms 258.623406ms 258.928197ms 259.565177ms 259.762997ms 261.156794ms 261.64827ms 261.906478ms 262.105327ms 262.140955ms 262.19366ms 262.860852ms 262.903257ms 263.111316ms 263.665862ms 264.01426ms 264.221711ms 264.396573ms 264.610006ms 264.974636ms 265.077498ms 265.438057ms 265.513705ms 267.076594ms 267.110411ms 269.603211ms 270.363822ms 272.058111ms 273.139274ms 273.725262ms 275.540924ms 275.58762ms 277.939379ms 279.208838ms 279.384877ms 280.581851ms 281.483854ms 281.743402ms 281.935341ms 282.019336ms 282.524778ms 283.243086ms 283.567279ms 283.610492ms 283.977784ms 284.611884ms 285.433584ms 285.594434ms 285.609862ms 286.306588ms 287.049574ms 288.124093ms 288.305769ms 288.437667ms 293.362484ms 294.686671ms 296.920193ms 297.861611ms 302.158281ms 302.376698ms 303.273614ms 303.645012ms 304.001211ms 304.577061ms 307.797037ms 308.629554ms 309.418169ms 311.174657ms 311.981252ms 312.825053ms 313.606508ms 313.692703ms 314.411204ms 315.38509ms 319.154476ms 320.66595ms 322.257209ms 323.845816ms 324.982991ms 326.306908ms 327.458333ms 329.402669ms 329.417104ms 331.208817ms 337.279837ms]
May 26 22:13:13.042: INFO: 50 %ile: 252.104605ms
May 26 22:13:13.042: INFO: 90 %ile: 308.629554ms
May 26 22:13:13.042: INFO: 99 %ile: 331.208817ms
May 26 22:13:13.042: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:13:13.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-986" for this suite.
May 26 22:13:33.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:13:34.951: INFO: namespace svc-latency-986 deletion completed in 21.889737704s

â€¢ [SLOW TEST:27.809 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:13:34.953: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 22:13:35.073: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
May 26 22:13:42.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 --namespace=crd-publish-openapi-7036 create -f -'
May 26 22:13:43.544: INFO: stderr: ""
May 26 22:13:43.544: INFO: stdout: "e2e-test-crd-publish-openapi-4822-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 26 22:13:43.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 --namespace=crd-publish-openapi-7036 delete e2e-test-crd-publish-openapi-4822-crds test-foo'
May 26 22:13:43.778: INFO: stderr: ""
May 26 22:13:43.778: INFO: stdout: "e2e-test-crd-publish-openapi-4822-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May 26 22:13:43.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 --namespace=crd-publish-openapi-7036 apply -f -'
May 26 22:13:44.155: INFO: stderr: ""
May 26 22:13:44.155: INFO: stdout: "e2e-test-crd-publish-openapi-4822-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 26 22:13:44.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 --namespace=crd-publish-openapi-7036 delete e2e-test-crd-publish-openapi-4822-crds test-foo'
May 26 22:13:44.316: INFO: stderr: ""
May 26 22:13:44.316: INFO: stdout: "e2e-test-crd-publish-openapi-4822-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
May 26 22:13:44.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 --namespace=crd-publish-openapi-7036 create -f -'
May 26 22:13:44.876: INFO: rc: 1
May 26 22:13:44.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 --namespace=crd-publish-openapi-7036 apply -f -'
May 26 22:13:45.402: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
May 26 22:13:45.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 --namespace=crd-publish-openapi-7036 create -f -'
May 26 22:13:45.951: INFO: rc: 1
May 26 22:13:45.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 --namespace=crd-publish-openapi-7036 apply -f -'
May 26 22:13:46.474: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
May 26 22:13:46.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 explain e2e-test-crd-publish-openapi-4822-crds'
May 26 22:13:46.812: INFO: stderr: ""
May 26 22:13:46.812: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4822-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
May 26 22:13:46.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 explain e2e-test-crd-publish-openapi-4822-crds.metadata'
May 26 22:13:47.379: INFO: stderr: ""
May 26 22:13:47.379: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4822-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May 26 22:13:47.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 explain e2e-test-crd-publish-openapi-4822-crds.spec'
May 26 22:13:47.731: INFO: stderr: ""
May 26 22:13:47.731: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4822-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May 26 22:13:47.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 explain e2e-test-crd-publish-openapi-4822-crds.spec.bars'
May 26 22:13:48.179: INFO: stderr: ""
May 26 22:13:48.179: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4822-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
May 26 22:13:48.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 explain e2e-test-crd-publish-openapi-4822-crds.spec.bars2'
May 26 22:13:48.729: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:13:56.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7036" for this suite.
May 26 22:14:04.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:14:06.330: INFO: namespace crd-publish-openapi-7036 deletion completed in 9.8882092s

â€¢ [SLOW TEST:31.377 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:14:06.330: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:14:06.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1436" for this suite.
May 26 22:14:14.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:14:16.371: INFO: namespace custom-resource-definition-1436 deletion completed in 9.88545723s

â€¢ [SLOW TEST:10.041 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:14:16.371: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 26 22:14:16.593: INFO: Number of nodes with available pods: 0
May 26 22:14:16.594: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 22:14:17.619: INFO: Number of nodes with available pods: 0
May 26 22:14:17.619: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 22:14:18.618: INFO: Number of nodes with available pods: 3
May 26 22:14:18.618: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 26 22:14:18.673: INFO: Number of nodes with available pods: 3
May 26 22:14:18.674: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5292, will wait for the garbage collector to delete the pods
May 26 22:14:19.786: INFO: Deleting DaemonSet.extensions daemon-set took: 23.38137ms
May 26 22:14:20.087: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.542195ms
May 26 22:14:32.297: INFO: Number of nodes with available pods: 0
May 26 22:14:32.297: INFO: Number of running nodes: 0, number of available pods: 0
May 26 22:14:32.306: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5292/daemonsets","resourceVersion":"61137"},"items":null}

May 26 22:14:32.315: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5292/pods","resourceVersion":"61137"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:14:32.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5292" for this suite.
May 26 22:14:40.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:14:42.296: INFO: namespace daemonsets-5292 deletion completed in 9.892628096s

â€¢ [SLOW TEST:25.925 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:14:42.297: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-601036fa-c2c2-43fb-a509-ff57f16c57f4
STEP: Creating a pod to test consume secrets
May 26 22:14:42.728: INFO: Waiting up to 5m0s for pod "pod-secrets-ddbd502d-70a3-44f6-ae78-ce285ed44ad4" in namespace "secrets-1596" to be "success or failure"
May 26 22:14:42.744: INFO: Pod "pod-secrets-ddbd502d-70a3-44f6-ae78-ce285ed44ad4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.569691ms
May 26 22:14:44.754: INFO: Pod "pod-secrets-ddbd502d-70a3-44f6-ae78-ce285ed44ad4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026186118s
STEP: Saw pod success
May 26 22:14:44.754: INFO: Pod "pod-secrets-ddbd502d-70a3-44f6-ae78-ce285ed44ad4" satisfied condition "success or failure"
May 26 22:14:44.764: INFO: Trying to get logs from node 10.215.60.34 pod pod-secrets-ddbd502d-70a3-44f6-ae78-ce285ed44ad4 container secret-volume-test: <nil>
STEP: delete the pod
May 26 22:14:44.860: INFO: Waiting for pod pod-secrets-ddbd502d-70a3-44f6-ae78-ce285ed44ad4 to disappear
May 26 22:14:44.869: INFO: Pod pod-secrets-ddbd502d-70a3-44f6-ae78-ce285ed44ad4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:14:44.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1596" for this suite.
May 26 22:14:52.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:14:54.771: INFO: namespace secrets-1596 deletion completed in 9.888388909s

â€¢ [SLOW TEST:12.474 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:14:54.772: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-936bc928-b2d9-4098-a580-3f93e4cda53b
STEP: Creating a pod to test consume secrets
May 26 22:14:54.929: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1c452146-9326-46f0-949f-28f0684373c5" in namespace "projected-9239" to be "success or failure"
May 26 22:14:54.938: INFO: Pod "pod-projected-secrets-1c452146-9326-46f0-949f-28f0684373c5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.697966ms
May 26 22:14:56.949: INFO: Pod "pod-projected-secrets-1c452146-9326-46f0-949f-28f0684373c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018900233s
STEP: Saw pod success
May 26 22:14:56.949: INFO: Pod "pod-projected-secrets-1c452146-9326-46f0-949f-28f0684373c5" satisfied condition "success or failure"
May 26 22:14:56.957: INFO: Trying to get logs from node 10.215.60.34 pod pod-projected-secrets-1c452146-9326-46f0-949f-28f0684373c5 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 26 22:14:57.007: INFO: Waiting for pod pod-projected-secrets-1c452146-9326-46f0-949f-28f0684373c5 to disappear
May 26 22:14:57.015: INFO: Pod pod-projected-secrets-1c452146-9326-46f0-949f-28f0684373c5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:14:57.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9239" for this suite.
May 26 22:15:05.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:15:06.929: INFO: namespace projected-9239 deletion completed in 9.899916809s

â€¢ [SLOW TEST:12.157 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:15:06.929: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
May 26 22:15:07.086: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
May 26 22:15:37.244: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 22:15:44.957: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:16:15.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1193" for this suite.
May 26 22:16:23.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:16:25.210: INFO: namespace crd-publish-openapi-1193 deletion completed in 9.918086935s

â€¢ [SLOW TEST:78.281 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:16:25.212: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:16:36.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4587" for this suite.
May 26 22:16:44.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:16:46.381: INFO: namespace resourcequota-4587 deletion completed in 9.914910236s

â€¢ [SLOW TEST:21.169 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:16:46.382: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
May 26 22:16:49.110: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7626 pod-service-account-49004a49-4b54-44fc-be97-d47a03931fda -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 26 22:16:49.421: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7626 pod-service-account-49004a49-4b54-44fc-be97-d47a03931fda -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 26 22:16:49.775: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7626 pod-service-account-49004a49-4b54-44fc-be97-d47a03931fda -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:16:50.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7626" for this suite.
May 26 22:16:58.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:17:00.059: INFO: namespace svcaccounts-7626 deletion completed in 9.885946606s

â€¢ [SLOW TEST:13.677 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:17:00.059: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-1169/secret-test-7c9f7f2c-3927-400a-b91e-67dba7c41fdb
STEP: Creating a pod to test consume secrets
May 26 22:17:00.238: INFO: Waiting up to 5m0s for pod "pod-configmaps-cbf1a7b3-062f-4a4b-95cb-53eed1b71f49" in namespace "secrets-1169" to be "success or failure"
May 26 22:17:00.247: INFO: Pod "pod-configmaps-cbf1a7b3-062f-4a4b-95cb-53eed1b71f49": Phase="Pending", Reason="", readiness=false. Elapsed: 8.57226ms
May 26 22:17:02.258: INFO: Pod "pod-configmaps-cbf1a7b3-062f-4a4b-95cb-53eed1b71f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019650869s
STEP: Saw pod success
May 26 22:17:02.258: INFO: Pod "pod-configmaps-cbf1a7b3-062f-4a4b-95cb-53eed1b71f49" satisfied condition "success or failure"
May 26 22:17:02.276: INFO: Trying to get logs from node 10.215.60.34 pod pod-configmaps-cbf1a7b3-062f-4a4b-95cb-53eed1b71f49 container env-test: <nil>
STEP: delete the pod
May 26 22:17:02.377: INFO: Waiting for pod pod-configmaps-cbf1a7b3-062f-4a4b-95cb-53eed1b71f49 to disappear
May 26 22:17:02.388: INFO: Pod pod-configmaps-cbf1a7b3-062f-4a4b-95cb-53eed1b71f49 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:17:02.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1169" for this suite.
May 26 22:17:10.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:17:12.300: INFO: namespace secrets-1169 deletion completed in 9.899162414s

â€¢ [SLOW TEST:12.241 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:17:12.303: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 22:17:12.413: INFO: Creating deployment "webserver-deployment"
May 26 22:17:12.430: INFO: Waiting for observed generation 1
May 26 22:17:14.446: INFO: Waiting for all required pods to come up
May 26 22:17:14.456: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
May 26 22:17:16.482: INFO: Waiting for deployment "webserver-deployment" to complete
May 26 22:17:16.504: INFO: Updating deployment "webserver-deployment" with a non-existent image
May 26 22:17:16.574: INFO: Updating deployment webserver-deployment
May 26 22:17:16.574: INFO: Waiting for observed generation 2
May 26 22:17:18.592: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 26 22:17:18.600: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 26 22:17:18.608: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 26 22:17:18.634: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 26 22:17:18.634: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 26 22:17:18.642: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 26 22:17:18.658: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May 26 22:17:18.658: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May 26 22:17:18.676: INFO: Updating deployment webserver-deployment
May 26 22:17:18.676: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May 26 22:17:18.697: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 26 22:17:20.725: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May 26 22:17:20.746: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5616 /apis/apps/v1/namespaces/deployment-5616/deployments/webserver-deployment c99fe614-686c-41a3-9079-909892e6998b 62701 3 2020-05-26 22:17:12 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006861b18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-05-26 22:17:18 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-05-26 22:17:19 +0000 UTC,LastTransitionTime:2020-05-26 22:17:12 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May 26 22:17:20.756: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-5616 /apis/apps/v1/namespaces/deployment-5616/replicasets/webserver-deployment-c7997dcc8 9afcb895-dd7a-482a-ba4e-9d7e903ad3c2 62700 3 2020-05-26 22:17:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment c99fe614-686c-41a3-9079-909892e6998b 0xc0068ab007 0xc0068ab008}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0068ab088 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 26 22:17:20.756: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May 26 22:17:20.756: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-5616 /apis/apps/v1/namespaces/deployment-5616/replicasets/webserver-deployment-595b5b9587 2eae217f-f611-4f91-ab26-8d23ab359f16 62645 3 2020-05-26 22:17:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment c99fe614-686c-41a3-9079-909892e6998b 0xc0068aaee7 0xc0068aaee8}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0068aaf88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May 26 22:17:20.788: INFO: Pod "webserver-deployment-595b5b9587-2kgrd" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2kgrd webserver-deployment-595b5b9587- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-595b5b9587-2kgrd cff14f24-3d12-439e-a134-95932ae9ba83 62659 0 2020-05-26 22:17:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2eae217f-f611-4f91-ab26-8d23ab359f16 0xc0068f80f7 0xc0068f80f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.34,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.34,PodIP:,StartTime:2020-05-26 22:17:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.789: INFO: Pod "webserver-deployment-595b5b9587-48qcv" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-48qcv webserver-deployment-595b5b9587- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-595b5b9587-48qcv 6b65c0e1-209a-4de4-b160-7f5ffc2fa00a 62450 0 2020-05-26 22:17:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.182.67/32 cni.projectcalico.org/podIPs:172.30.182.67/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.182.67"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2eae217f-f611-4f91-ab26-8d23ab359f16 0xc0068f83f7 0xc0068f83f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.34,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.34,PodIP:172.30.182.67,StartTime:2020-05-26 22:17:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-26 22:17:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://9c04c3e92b8959f1fd15a9ac02e15567669e4d7fb6b770fa9c69170994ae94bf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.182.67,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.789: INFO: Pod "webserver-deployment-595b5b9587-4d4vr" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4d4vr webserver-deployment-595b5b9587- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-595b5b9587-4d4vr 59a9ab85-1923-4523-a77e-b8c637ee6384 62478 0 2020-05-26 22:17:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.99.155/32 cni.projectcalico.org/podIPs:172.30.99.155/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.99.155"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2eae217f-f611-4f91-ab26-8d23ab359f16 0xc0068f86f7 0xc0068f86f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.11,PodIP:172.30.99.155,StartTime:2020-05-26 22:17:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-26 22:17:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://780563ee671928c80e08ef68710ae37c0220b16897ecd3d9a76012eced33d2c6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.99.155,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.789: INFO: Pod "webserver-deployment-595b5b9587-6w7tr" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-6w7tr webserver-deployment-595b5b9587- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-595b5b9587-6w7tr 0f900b29-705c-4fbd-998f-6416cbfe94e4 62476 0 2020-05-26 22:17:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.99.154/32 cni.projectcalico.org/podIPs:172.30.99.154/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.99.154"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2eae217f-f611-4f91-ab26-8d23ab359f16 0xc0068f89e7 0xc0068f89e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.11,PodIP:172.30.99.154,StartTime:2020-05-26 22:17:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-26 22:17:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://2083238465f2916e5a6a9d5655d10d65e7aeba6ccf2204a08172255c22400149,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.99.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.789: INFO: Pod "webserver-deployment-595b5b9587-99djw" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-99djw webserver-deployment-595b5b9587- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-595b5b9587-99djw 41975e7b-81b3-43e2-9438-4227b54ee405 62784 0 2020-05-26 22:17:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.141.183/32 cni.projectcalico.org/podIPs:172.30.141.183/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.141.183"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2eae217f-f611-4f91-ab26-8d23ab359f16 0xc0068f8c47 0xc0068f8c48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.10,PodIP:,StartTime:2020-05-26 22:17:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.789: INFO: Pod "webserver-deployment-595b5b9587-bkgmq" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bkgmq webserver-deployment-595b5b9587- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-595b5b9587-bkgmq 378bf34c-31f6-4b32-be5e-a9280bf9cde4 62486 0 2020-05-26 22:17:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.182.74/32 cni.projectcalico.org/podIPs:172.30.182.74/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.182.74"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2eae217f-f611-4f91-ab26-8d23ab359f16 0xc0068f8e47 0xc0068f8e48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.34,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.34,PodIP:172.30.182.74,StartTime:2020-05-26 22:17:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-26 22:17:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://68f3f74ff8a1fe4719c1c6160ec59f451f77a938a3f5e4687515bdeff637c6fa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.182.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.794: INFO: Pod "webserver-deployment-595b5b9587-bnvl2" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bnvl2 webserver-deployment-595b5b9587- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-595b5b9587-bnvl2 a9ffbfc6-0246-469f-924f-6242cb59f560 62797 0 2020-05-26 22:17:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.182.92/32 cni.projectcalico.org/podIPs:172.30.182.92/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.182.92"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2eae217f-f611-4f91-ab26-8d23ab359f16 0xc0068f90c7 0xc0068f90c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.34,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.34,PodIP:,StartTime:2020-05-26 22:17:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.794: INFO: Pod "webserver-deployment-595b5b9587-dj5mw" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-dj5mw webserver-deployment-595b5b9587- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-595b5b9587-dj5mw 8d4ddfa5-f17c-44c9-a67a-c9d3639eb8ed 62745 0 2020-05-26 22:17:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.99.160/32 cni.projectcalico.org/podIPs:172.30.99.160/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.99.160"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2eae217f-f611-4f91-ab26-8d23ab359f16 0xc0068f9397 0xc0068f9398}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.11,PodIP:,StartTime:2020-05-26 22:17:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.794: INFO: Pod "webserver-deployment-595b5b9587-dt62h" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-dt62h webserver-deployment-595b5b9587- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-595b5b9587-dt62h 4faf6e0a-7baa-4c02-a90c-f9d3c0190317 62464 0 2020-05-26 22:17:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.141.178/32 cni.projectcalico.org/podIPs:172.30.141.178/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.141.178"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2eae217f-f611-4f91-ab26-8d23ab359f16 0xc0068f95a7 0xc0068f95a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.10,PodIP:172.30.141.178,StartTime:2020-05-26 22:17:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-26 22:17:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://38dbd81df3d5544c6bbe0c5f552c010896260e310111d950f81826360e2b711d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.141.178,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.794: INFO: Pod "webserver-deployment-595b5b9587-gtdsk" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-gtdsk webserver-deployment-595b5b9587- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-595b5b9587-gtdsk d4b3ab5f-94aa-4318-8563-bdf87dd8e18b 62773 0 2020-05-26 22:17:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.141.181/32 cni.projectcalico.org/podIPs:172.30.141.181/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.141.181"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2eae217f-f611-4f91-ab26-8d23ab359f16 0xc0068f9807 0xc0068f9808}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.10,PodIP:,StartTime:2020-05-26 22:17:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.799: INFO: Pod "webserver-deployment-595b5b9587-lr6mq" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lr6mq webserver-deployment-595b5b9587- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-595b5b9587-lr6mq 3a694e73-91ae-4192-85e9-acfbd3f62897 62832 0 2020-05-26 22:17:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.182.78/32 cni.projectcalico.org/podIPs:172.30.182.78/32 openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2eae217f-f611-4f91-ab26-8d23ab359f16 0xc0068f9a37 0xc0068f9a38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.34,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.34,PodIP:,StartTime:2020-05-26 22:17:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.799: INFO: Pod "webserver-deployment-595b5b9587-mb92v" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mb92v webserver-deployment-595b5b9587- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-595b5b9587-mb92v 783b7a7e-cc5d-4aa5-8f01-b21abdcb28a1 62782 0 2020-05-26 22:17:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.99.162/32 cni.projectcalico.org/podIPs:172.30.99.162/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.99.162"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2eae217f-f611-4f91-ab26-8d23ab359f16 0xc0068f9c07 0xc0068f9c08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.11,PodIP:,StartTime:2020-05-26 22:17:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.800: INFO: Pod "webserver-deployment-595b5b9587-q25gf" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-q25gf webserver-deployment-595b5b9587- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-595b5b9587-q25gf 83a67e04-d59d-4ff9-af45-58817554a337 62808 0 2020-05-26 22:17:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.182.77/32 cni.projectcalico.org/podIPs:172.30.182.77/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.182.77"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2eae217f-f611-4f91-ab26-8d23ab359f16 0xc0068f9e07 0xc0068f9e08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.34,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.34,PodIP:,StartTime:2020-05-26 22:17:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.800: INFO: Pod "webserver-deployment-595b5b9587-q6v2q" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-q6v2q webserver-deployment-595b5b9587- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-595b5b9587-q6v2q 3479e317-fd3c-48d0-b81e-9f5842065fcc 62839 0 2020-05-26 22:17:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.182.82/32 cni.projectcalico.org/podIPs:172.30.182.82/32 openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2eae217f-f611-4f91-ab26-8d23ab359f16 0xc006920247 0xc006920248}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.34,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.34,PodIP:,StartTime:2020-05-26 22:17:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.801: INFO: Pod "webserver-deployment-595b5b9587-qq7mk" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qq7mk webserver-deployment-595b5b9587- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-595b5b9587-qq7mk b096f873-3478-467d-9507-86cd5c93d353 62825 0 2020-05-26 22:17:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.182.68/32 cni.projectcalico.org/podIPs:172.30.182.68/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.182.68"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2eae217f-f611-4f91-ab26-8d23ab359f16 0xc006920507 0xc006920508}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.34,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.34,PodIP:,StartTime:2020-05-26 22:17:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.801: INFO: Pod "webserver-deployment-595b5b9587-r4dcx" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-r4dcx webserver-deployment-595b5b9587- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-595b5b9587-r4dcx aa1db2f3-4c00-4959-9fbe-4ce07dc4b195 62438 0 2020-05-26 22:17:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.99.153/32 cni.projectcalico.org/podIPs:172.30.99.153/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.99.153"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2eae217f-f611-4f91-ab26-8d23ab359f16 0xc0069207c7 0xc0069207c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.11,PodIP:172.30.99.153,StartTime:2020-05-26 22:17:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-26 22:17:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://3fa8978cd7cd6e4589a2451b991c59a55072647fab2241a7573d6b87c05aed70,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.99.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.801: INFO: Pod "webserver-deployment-595b5b9587-rb2dv" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rb2dv webserver-deployment-595b5b9587- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-595b5b9587-rb2dv b3cbb319-c5e2-454d-9271-f65f243f1261 62724 0 2020-05-26 22:17:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.99.158/32 cni.projectcalico.org/podIPs:172.30.99.158/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.99.158"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2eae217f-f611-4f91-ab26-8d23ab359f16 0xc0069209d7 0xc0069209d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.11,PodIP:,StartTime:2020-05-26 22:17:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.802: INFO: Pod "webserver-deployment-595b5b9587-skr4h" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-skr4h webserver-deployment-595b5b9587- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-595b5b9587-skr4h fbbada88-1502-4988-a5f6-33b30a115df7 62467 0 2020-05-26 22:17:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.141.179/32 cni.projectcalico.org/podIPs:172.30.141.179/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.141.179"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2eae217f-f611-4f91-ab26-8d23ab359f16 0xc006920ba7 0xc006920ba8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.10,PodIP:172.30.141.179,StartTime:2020-05-26 22:17:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-26 22:17:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://a1dd5906ad9407c15ac6d9e6874623d6a199b3be8562dc5e1aa9fc34d7367012,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.141.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.802: INFO: Pod "webserver-deployment-595b5b9587-vvgd9" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-vvgd9 webserver-deployment-595b5b9587- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-595b5b9587-vvgd9 9f1a9741-058f-48d0-aaa2-852670d2784f 62795 0 2020-05-26 22:17:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.141.185/32 cni.projectcalico.org/podIPs:172.30.141.185/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.141.185"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2eae217f-f611-4f91-ab26-8d23ab359f16 0xc006920e27 0xc006920e28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.10,PodIP:,StartTime:2020-05-26 22:17:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.803: INFO: Pod "webserver-deployment-595b5b9587-z9w42" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-z9w42 webserver-deployment-595b5b9587- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-595b5b9587-z9w42 661cc29c-be8b-499a-95da-af8d424f160a 62469 0 2020-05-26 22:17:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.141.177/32 cni.projectcalico.org/podIPs:172.30.141.177/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.141.177"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2eae217f-f611-4f91-ab26-8d23ab359f16 0xc006920fe7 0xc006920fe8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.10,PodIP:172.30.141.177,StartTime:2020-05-26 22:17:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-26 22:17:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://dafe1bfa5d21995764ff52af8c9551ad3e41baf92f10669703695b7d5f06742d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.141.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.804: INFO: Pod "webserver-deployment-c7997dcc8-27cjw" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-27cjw webserver-deployment-c7997dcc8- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-c7997dcc8-27cjw 022e2382-b4c8-470f-a1ee-6e656827cda6 62691 0 2020-05-26 22:17:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9afcb895-dd7a-482a-ba4e-9d7e903ad3c2 0xc006921307 0xc006921308}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.34,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.34,PodIP:,StartTime:2020-05-26 22:17:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.805: INFO: Pod "webserver-deployment-c7997dcc8-84962" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-84962 webserver-deployment-c7997dcc8- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-c7997dcc8-84962 1dbe14a4-7fb6-4e30-83bb-176547a00f37 62736 0 2020-05-26 22:17:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.99.159/32 cni.projectcalico.org/podIPs:172.30.99.159/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.99.159"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9afcb895-dd7a-482a-ba4e-9d7e903ad3c2 0xc006921607 0xc006921608}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.11,PodIP:,StartTime:2020-05-26 22:17:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.805: INFO: Pod "webserver-deployment-c7997dcc8-84g8q" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-84g8q webserver-deployment-c7997dcc8- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-c7997dcc8-84g8q 4473deab-7475-45a9-a797-05310db85873 62750 0 2020-05-26 22:17:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.182.93/32 cni.projectcalico.org/podIPs:172.30.182.93/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.182.93"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9afcb895-dd7a-482a-ba4e-9d7e903ad3c2 0xc006921897 0xc006921898}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.34,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.34,PodIP:172.30.182.93,StartTime:2020-05-26 22:17:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.182.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.805: INFO: Pod "webserver-deployment-c7997dcc8-c2wx6" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-c2wx6 webserver-deployment-c7997dcc8- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-c7997dcc8-c2wx6 c396a5e1-ae9d-4179-a1ef-504569c5b33e 62757 0 2020-05-26 22:17:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.141.176/32 cni.projectcalico.org/podIPs:172.30.141.176/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.141.176"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9afcb895-dd7a-482a-ba4e-9d7e903ad3c2 0xc006921ac7 0xc006921ac8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.10,PodIP:,StartTime:2020-05-26 22:17:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.806: INFO: Pod "webserver-deployment-c7997dcc8-pg7jf" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-pg7jf webserver-deployment-c7997dcc8- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-c7997dcc8-pg7jf 2e0c1807-e686-479f-a035-89433a7816ee 62666 0 2020-05-26 22:17:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9afcb895-dd7a-482a-ba4e-9d7e903ad3c2 0xc006921d77 0xc006921d78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.34,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.34,PodIP:,StartTime:2020-05-26 22:17:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.808: INFO: Pod "webserver-deployment-c7997dcc8-qzq8w" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qzq8w webserver-deployment-c7997dcc8- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-c7997dcc8-qzq8w 52ce67a8-9b3d-4bcb-a947-bd745f04aba6 62788 0 2020-05-26 22:17:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.182.89/32 cni.projectcalico.org/podIPs:172.30.182.89/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.182.89"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9afcb895-dd7a-482a-ba4e-9d7e903ad3c2 0xc006921fa7 0xc006921fa8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.34,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.34,PodIP:,StartTime:2020-05-26 22:17:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.811: INFO: Pod "webserver-deployment-c7997dcc8-rxwq6" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-rxwq6 webserver-deployment-c7997dcc8- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-c7997dcc8-rxwq6 d56b4322-291f-4138-98a7-48fa52ce6112 62576 0 2020-05-26 22:17:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.99.157/32 cni.projectcalico.org/podIPs:172.30.99.157/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.99.157"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9afcb895-dd7a-482a-ba4e-9d7e903ad3c2 0xc00694e217 0xc00694e218}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.11,PodIP:,StartTime:2020-05-26 22:17:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.811: INFO: Pod "webserver-deployment-c7997dcc8-sfrdc" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-sfrdc webserver-deployment-c7997dcc8- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-c7997dcc8-sfrdc 956c9a68-39ed-4e51-9ffd-3109f1f0cbb4 62810 0 2020-05-26 22:17:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.141.184/32 cni.projectcalico.org/podIPs:172.30.141.184/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.141.184"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9afcb895-dd7a-482a-ba4e-9d7e903ad3c2 0xc00694e3c7 0xc00694e3c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.10,PodIP:,StartTime:2020-05-26 22:17:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.811: INFO: Pod "webserver-deployment-c7997dcc8-tq6mm" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-tq6mm webserver-deployment-c7997dcc8- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-c7997dcc8-tq6mm 9b0048d6-d0aa-4140-821f-b40076d3d495 62761 0 2020-05-26 22:17:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.99.161/32 cni.projectcalico.org/podIPs:172.30.99.161/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.99.161"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9afcb895-dd7a-482a-ba4e-9d7e903ad3c2 0xc00694e5f7 0xc00694e5f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.11,PodIP:,StartTime:2020-05-26 22:17:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.813: INFO: Pod "webserver-deployment-c7997dcc8-tz56m" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-tz56m webserver-deployment-c7997dcc8- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-c7997dcc8-tz56m 99849eb2-9afa-4b4c-a44b-92f1e4c1fc19 62730 0 2020-05-26 22:17:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.99.156/32 cni.projectcalico.org/podIPs:172.30.99.156/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.99.156"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9afcb895-dd7a-482a-ba4e-9d7e903ad3c2 0xc00694e7c7 0xc00694e7c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.11,PodIP:172.30.99.156,StartTime:2020-05-26 22:17:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.99.156,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.813: INFO: Pod "webserver-deployment-c7997dcc8-w6j5b" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-w6j5b webserver-deployment-c7997dcc8- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-c7997dcc8-w6j5b 9192d6da-26bb-4102-9af8-a8b381102a2c 62693 0 2020-05-26 22:17:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9afcb895-dd7a-482a-ba4e-9d7e903ad3c2 0xc00694e997 0xc00694e998}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.34,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.34,PodIP:,StartTime:2020-05-26 22:17:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.815: INFO: Pod "webserver-deployment-c7997dcc8-w8mlw" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-w8mlw webserver-deployment-c7997dcc8- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-c7997dcc8-w8mlw 33ed3ca7-a176-4eb8-8c3a-0d062efcee2b 62834 0 2020-05-26 22:17:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.141.186/32 cni.projectcalico.org/podIPs:172.30.141.186/32 openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9afcb895-dd7a-482a-ba4e-9d7e903ad3c2 0xc00694ebb7 0xc00694ebb8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.10,PodIP:,StartTime:2020-05-26 22:17:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 26 22:17:20.815: INFO: Pod "webserver-deployment-c7997dcc8-wfllw" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-wfllw webserver-deployment-c7997dcc8- deployment-5616 /api/v1/namespaces/deployment-5616/pods/webserver-deployment-c7997dcc8-wfllw 8c3d1640-6e4b-4b9e-a3ee-a577f34af31b 62710 0 2020-05-26 22:17:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.141.180/32 cni.projectcalico.org/podIPs:172.30.141.180/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.141.180"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9afcb895-dd7a-482a-ba4e-9d7e903ad3c2 0xc00694ed87 0xc00694ed88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qjsf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qjsf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qjsf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94l4g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:17:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.10,PodIP:172.30.141.180,StartTime:2020-05-26 22:17:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.141.180,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:17:20.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5616" for this suite.
May 26 22:17:30.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:17:33.095: INFO: namespace deployment-5616 deletion completed in 12.260350311s

â€¢ [SLOW TEST:20.792 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:17:33.095: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-0147413c-737a-4c61-a587-1e078f348630
STEP: Creating a pod to test consume configMaps
May 26 22:17:34.283: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-117794e4-33ed-454d-ac21-85235a8cecf7" in namespace "projected-2641" to be "success or failure"
May 26 22:17:34.292: INFO: Pod "pod-projected-configmaps-117794e4-33ed-454d-ac21-85235a8cecf7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.556695ms
May 26 22:17:36.301: INFO: Pod "pod-projected-configmaps-117794e4-33ed-454d-ac21-85235a8cecf7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018223739s
May 26 22:17:38.316: INFO: Pod "pod-projected-configmaps-117794e4-33ed-454d-ac21-85235a8cecf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033102475s
STEP: Saw pod success
May 26 22:17:38.317: INFO: Pod "pod-projected-configmaps-117794e4-33ed-454d-ac21-85235a8cecf7" satisfied condition "success or failure"
May 26 22:17:38.326: INFO: Trying to get logs from node 10.215.60.34 pod pod-projected-configmaps-117794e4-33ed-454d-ac21-85235a8cecf7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 26 22:17:38.381: INFO: Waiting for pod pod-projected-configmaps-117794e4-33ed-454d-ac21-85235a8cecf7 to disappear
May 26 22:17:38.392: INFO: Pod pod-projected-configmaps-117794e4-33ed-454d-ac21-85235a8cecf7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:17:38.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2641" for this suite.
May 26 22:17:46.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:17:48.295: INFO: namespace projected-2641 deletion completed in 9.890529315s

â€¢ [SLOW TEST:15.200 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:17:48.295: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
May 26 22:17:48.481: INFO: Waiting up to 5m0s for pod "pod-3bf58e26-4028-4f7a-a352-7cd48b980262" in namespace "emptydir-6065" to be "success or failure"
May 26 22:17:48.490: INFO: Pod "pod-3bf58e26-4028-4f7a-a352-7cd48b980262": Phase="Pending", Reason="", readiness=false. Elapsed: 8.798189ms
May 26 22:17:50.499: INFO: Pod "pod-3bf58e26-4028-4f7a-a352-7cd48b980262": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018361539s
STEP: Saw pod success
May 26 22:17:50.499: INFO: Pod "pod-3bf58e26-4028-4f7a-a352-7cd48b980262" satisfied condition "success or failure"
May 26 22:17:50.508: INFO: Trying to get logs from node 10.215.60.34 pod pod-3bf58e26-4028-4f7a-a352-7cd48b980262 container test-container: <nil>
STEP: delete the pod
May 26 22:17:50.561: INFO: Waiting for pod pod-3bf58e26-4028-4f7a-a352-7cd48b980262 to disappear
May 26 22:17:50.573: INFO: Pod pod-3bf58e26-4028-4f7a-a352-7cd48b980262 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:17:50.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6065" for this suite.
May 26 22:17:58.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:18:00.482: INFO: namespace emptydir-6065 deletion completed in 9.896581017s

â€¢ [SLOW TEST:12.186 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:18:00.482: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-cb4daba1-75d0-43f8-9bbc-373ffa639976
STEP: Creating a pod to test consume configMaps
May 26 22:18:01.698: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a20b1c31-6392-4c79-b6e9-f83cb6613001" in namespace "projected-1650" to be "success or failure"
May 26 22:18:01.708: INFO: Pod "pod-projected-configmaps-a20b1c31-6392-4c79-b6e9-f83cb6613001": Phase="Pending", Reason="", readiness=false. Elapsed: 9.891249ms
May 26 22:18:03.718: INFO: Pod "pod-projected-configmaps-a20b1c31-6392-4c79-b6e9-f83cb6613001": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01984705s
STEP: Saw pod success
May 26 22:18:03.718: INFO: Pod "pod-projected-configmaps-a20b1c31-6392-4c79-b6e9-f83cb6613001" satisfied condition "success or failure"
May 26 22:18:03.727: INFO: Trying to get logs from node 10.215.60.34 pod pod-projected-configmaps-a20b1c31-6392-4c79-b6e9-f83cb6613001 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 26 22:18:03.779: INFO: Waiting for pod pod-projected-configmaps-a20b1c31-6392-4c79-b6e9-f83cb6613001 to disappear
May 26 22:18:03.789: INFO: Pod pod-projected-configmaps-a20b1c31-6392-4c79-b6e9-f83cb6613001 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:18:03.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1650" for this suite.
May 26 22:18:11.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:18:13.699: INFO: namespace projected-1650 deletion completed in 9.89417892s

â€¢ [SLOW TEST:13.218 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:18:13.700: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:18:30.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-448" for this suite.
May 26 22:18:38.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:18:40.867: INFO: namespace resourcequota-448 deletion completed in 9.903088681s

â€¢ [SLOW TEST:27.167 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:18:40.867: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 26 22:18:41.045: INFO: Waiting up to 5m0s for pod "downwardapi-volume-033bb129-4778-4463-b44a-bbafd3d873b1" in namespace "projected-2640" to be "success or failure"
May 26 22:18:41.055: INFO: Pod "downwardapi-volume-033bb129-4778-4463-b44a-bbafd3d873b1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.942127ms
May 26 22:18:43.064: INFO: Pod "downwardapi-volume-033bb129-4778-4463-b44a-bbafd3d873b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019440742s
STEP: Saw pod success
May 26 22:18:43.064: INFO: Pod "downwardapi-volume-033bb129-4778-4463-b44a-bbafd3d873b1" satisfied condition "success or failure"
May 26 22:18:43.073: INFO: Trying to get logs from node 10.215.60.34 pod downwardapi-volume-033bb129-4778-4463-b44a-bbafd3d873b1 container client-container: <nil>
STEP: delete the pod
May 26 22:18:43.123: INFO: Waiting for pod downwardapi-volume-033bb129-4778-4463-b44a-bbafd3d873b1 to disappear
May 26 22:18:43.132: INFO: Pod downwardapi-volume-033bb129-4778-4463-b44a-bbafd3d873b1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:18:43.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2640" for this suite.
May 26 22:18:51.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:18:53.041: INFO: namespace projected-2640 deletion completed in 9.892752512s

â€¢ [SLOW TEST:12.174 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:18:53.041: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1903.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1903.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1903.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1903.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1903.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1903.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1903.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1903.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1903.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1903.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1903.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1903.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1903.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 95.63.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.63.95_udp@PTR;check="$$(dig +tcp +noall +answer +search 95.63.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.63.95_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1903.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1903.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1903.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1903.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1903.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1903.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1903.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1903.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1903.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1903.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1903.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1903.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1903.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 95.63.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.63.95_udp@PTR;check="$$(dig +tcp +noall +answer +search 95.63.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.63.95_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 26 22:19:09.320: INFO: Unable to read wheezy_udp@dns-test-service.dns-1903.svc.cluster.local from pod dns-1903/dns-test-51896b56-6ec0-43fe-998f-4e5f497dc215: the server could not find the requested resource (get pods dns-test-51896b56-6ec0-43fe-998f-4e5f497dc215)
May 26 22:19:09.348: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1903.svc.cluster.local from pod dns-1903/dns-test-51896b56-6ec0-43fe-998f-4e5f497dc215: the server could not find the requested resource (get pods dns-test-51896b56-6ec0-43fe-998f-4e5f497dc215)
May 26 22:19:09.362: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1903.svc.cluster.local from pod dns-1903/dns-test-51896b56-6ec0-43fe-998f-4e5f497dc215: the server could not find the requested resource (get pods dns-test-51896b56-6ec0-43fe-998f-4e5f497dc215)
May 26 22:19:09.377: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1903.svc.cluster.local from pod dns-1903/dns-test-51896b56-6ec0-43fe-998f-4e5f497dc215: the server could not find the requested resource (get pods dns-test-51896b56-6ec0-43fe-998f-4e5f497dc215)
May 26 22:19:09.497: INFO: Unable to read jessie_udp@dns-test-service.dns-1903.svc.cluster.local from pod dns-1903/dns-test-51896b56-6ec0-43fe-998f-4e5f497dc215: the server could not find the requested resource (get pods dns-test-51896b56-6ec0-43fe-998f-4e5f497dc215)
May 26 22:19:09.513: INFO: Unable to read jessie_tcp@dns-test-service.dns-1903.svc.cluster.local from pod dns-1903/dns-test-51896b56-6ec0-43fe-998f-4e5f497dc215: the server could not find the requested resource (get pods dns-test-51896b56-6ec0-43fe-998f-4e5f497dc215)
May 26 22:19:09.528: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1903.svc.cluster.local from pod dns-1903/dns-test-51896b56-6ec0-43fe-998f-4e5f497dc215: the server could not find the requested resource (get pods dns-test-51896b56-6ec0-43fe-998f-4e5f497dc215)
May 26 22:19:09.543: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1903.svc.cluster.local from pod dns-1903/dns-test-51896b56-6ec0-43fe-998f-4e5f497dc215: the server could not find the requested resource (get pods dns-test-51896b56-6ec0-43fe-998f-4e5f497dc215)
May 26 22:19:09.631: INFO: Lookups using dns-1903/dns-test-51896b56-6ec0-43fe-998f-4e5f497dc215 failed for: [wheezy_udp@dns-test-service.dns-1903.svc.cluster.local wheezy_tcp@dns-test-service.dns-1903.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1903.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1903.svc.cluster.local jessie_udp@dns-test-service.dns-1903.svc.cluster.local jessie_tcp@dns-test-service.dns-1903.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1903.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1903.svc.cluster.local]

May 26 22:19:14.942: INFO: DNS probes using dns-1903/dns-test-51896b56-6ec0-43fe-998f-4e5f497dc215 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:19:15.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1903" for this suite.
May 26 22:19:23.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:19:25.015: INFO: namespace dns-1903 deletion completed in 9.909709613s

â€¢ [SLOW TEST:31.973 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:19:25.015: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
May 26 22:19:25.128: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

May 26 22:19:25.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 create -f - --namespace=kubectl-9717'
May 26 22:19:25.628: INFO: stderr: ""
May 26 22:19:25.628: INFO: stdout: "service/redis-slave created\n"
May 26 22:19:25.629: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

May 26 22:19:25.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 create -f - --namespace=kubectl-9717'
May 26 22:19:26.193: INFO: stderr: ""
May 26 22:19:26.194: INFO: stdout: "service/redis-master created\n"
May 26 22:19:26.194: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 26 22:19:26.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 create -f - --namespace=kubectl-9717'
May 26 22:19:26.562: INFO: stderr: ""
May 26 22:19:26.562: INFO: stdout: "service/frontend created\n"
May 26 22:19:26.562: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

May 26 22:19:26.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 create -f - --namespace=kubectl-9717'
May 26 22:19:27.146: INFO: stderr: ""
May 26 22:19:27.146: INFO: stdout: "deployment.apps/frontend created\n"
May 26 22:19:27.147: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 26 22:19:27.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 create -f - --namespace=kubectl-9717'
May 26 22:19:27.639: INFO: stderr: ""
May 26 22:19:27.639: INFO: stdout: "deployment.apps/redis-master created\n"
May 26 22:19:27.640: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

May 26 22:19:27.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 create -f - --namespace=kubectl-9717'
May 26 22:19:28.191: INFO: stderr: ""
May 26 22:19:28.191: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
May 26 22:19:28.191: INFO: Waiting for all frontend pods to be Running.
May 26 22:19:48.242: INFO: Waiting for frontend to serve content.
May 26 22:19:48.278: INFO: Trying to add a new entry to the guestbook.
May 26 22:19:53.320: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-master:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-mas...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Str in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

May 26 22:19:58.376: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May 26 22:19:58.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 delete --grace-period=0 --force -f - --namespace=kubectl-9717'
May 26 22:19:58.579: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 26 22:19:58.579: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
May 26 22:19:58.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 delete --grace-period=0 --force -f - --namespace=kubectl-9717'
May 26 22:19:58.828: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 26 22:19:58.828: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 26 22:19:58.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 delete --grace-period=0 --force -f - --namespace=kubectl-9717'
May 26 22:19:59.011: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 26 22:19:59.011: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 26 22:19:59.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 delete --grace-period=0 --force -f - --namespace=kubectl-9717'
May 26 22:19:59.175: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 26 22:19:59.175: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 26 22:19:59.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 delete --grace-period=0 --force -f - --namespace=kubectl-9717'
May 26 22:19:59.326: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 26 22:19:59.326: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 26 22:19:59.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 delete --grace-period=0 --force -f - --namespace=kubectl-9717'
May 26 22:19:59.480: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 26 22:19:59.480: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:19:59.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9717" for this suite.
May 26 22:20:21.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:20:23.420: INFO: namespace kubectl-9717 deletion completed in 23.918996072s

â€¢ [SLOW TEST:58.405 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:20:23.420: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:20:34.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3409" for this suite.
May 26 22:20:42.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:20:44.591: INFO: namespace resourcequota-3409 deletion completed in 9.888488144s

â€¢ [SLOW TEST:21.171 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:20:44.591: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6378, will wait for the garbage collector to delete the pods
May 26 22:20:48.831: INFO: Deleting Job.batch foo took: 20.386941ms
May 26 22:20:49.131: INFO: Terminating Job.batch foo pods took: 300.275462ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:21:30.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6378" for this suite.
May 26 22:21:38.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:21:40.440: INFO: namespace job-6378 deletion completed in 9.885134818s

â€¢ [SLOW TEST:55.849 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:21:40.440: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 26 22:21:40.685: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8392 /api/v1/namespaces/watch-8392/configmaps/e2e-watch-test-resource-version 487c399a-e948-4ecc-b3de-7eacddf287cf 65243 0 2020-05-26 22:21:40 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 26 22:21:40.685: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8392 /api/v1/namespaces/watch-8392/configmaps/e2e-watch-test-resource-version 487c399a-e948-4ecc-b3de-7eacddf287cf 65244 0 2020-05-26 22:21:40 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:21:40.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8392" for this suite.
May 26 22:21:48.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:21:50.608: INFO: namespace watch-8392 deletion completed in 9.90752649s

â€¢ [SLOW TEST:10.168 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:21:50.609: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 26 22:21:50.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-3645'
May 26 22:21:50.948: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 26 22:21:50.948: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
May 26 22:21:54.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 delete deployment e2e-test-httpd-deployment --namespace=kubectl-3645'
May 26 22:21:55.136: INFO: stderr: ""
May 26 22:21:55.136: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:21:55.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3645" for this suite.
May 26 22:22:03.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:22:05.039: INFO: namespace kubectl-3645 deletion completed in 9.886640822s

â€¢ [SLOW TEST:14.430 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:22:05.040: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
May 26 22:22:05.211: INFO: Waiting up to 5m0s for pod "client-containers-1f5f1f82-c853-4751-a1d0-68f1dfbe9a8a" in namespace "containers-7350" to be "success or failure"
May 26 22:22:05.222: INFO: Pod "client-containers-1f5f1f82-c853-4751-a1d0-68f1dfbe9a8a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.332338ms
May 26 22:22:07.231: INFO: Pod "client-containers-1f5f1f82-c853-4751-a1d0-68f1dfbe9a8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019916537s
May 26 22:22:09.241: INFO: Pod "client-containers-1f5f1f82-c853-4751-a1d0-68f1dfbe9a8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029721405s
STEP: Saw pod success
May 26 22:22:09.241: INFO: Pod "client-containers-1f5f1f82-c853-4751-a1d0-68f1dfbe9a8a" satisfied condition "success or failure"
May 26 22:22:09.250: INFO: Trying to get logs from node 10.215.60.34 pod client-containers-1f5f1f82-c853-4751-a1d0-68f1dfbe9a8a container test-container: <nil>
STEP: delete the pod
May 26 22:22:09.326: INFO: Waiting for pod client-containers-1f5f1f82-c853-4751-a1d0-68f1dfbe9a8a to disappear
May 26 22:22:09.334: INFO: Pod client-containers-1f5f1f82-c853-4751-a1d0-68f1dfbe9a8a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:22:09.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7350" for this suite.
May 26 22:22:17.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:22:19.269: INFO: namespace containers-7350 deletion completed in 9.903188643s

â€¢ [SLOW TEST:14.229 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:22:19.269: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May 26 22:22:19.413: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 26 22:22:19.479: INFO: Waiting for terminating namespaces to be deleted...
May 26 22:22:19.494: INFO: 
Logging pods the kubelet thinks is on node 10.215.60.10 before test
May 26 22:22:19.608: INFO: cluster-image-registry-operator-6f78cddbbc-qrdmf from openshift-image-registry started at 2020-05-26 19:59:03 +0000 UTC (2 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
May 26 22:22:19.608: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
May 26 22:22:19.608: INFO: openshift-service-catalog-apiserver-operator-5897998845-kpk9m from openshift-service-catalog-apiserver-operator started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container operator ready: true, restart count 1
May 26 22:22:19.608: INFO: calico-typha-f4f4dbb8c-s262v from calico-system started at 2020-05-26 20:00:43 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container calico-typha ready: true, restart count 0
May 26 22:22:19.608: INFO: multus-lh654 from openshift-multus started at 2020-05-26 19:58:00 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container kube-multus ready: true, restart count 0
May 26 22:22:19.608: INFO: olm-operator-9d9b9dc65-rg7n5 from openshift-operator-lifecycle-manager started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container olm-operator ready: true, restart count 0
May 26 22:22:19.608: INFO: node-ca-q2fgf from openshift-image-registry started at 2020-05-26 20:00:40 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container node-ca ready: true, restart count 0
May 26 22:22:19.608: INFO: certified-operators-7c7bd497cb-hjpfx from openshift-marketplace started at 2020-05-26 20:01:33 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container certified-operators ready: true, restart count 0
May 26 22:22:19.608: INFO: catalog-operator-6956d96f67-5xvls from openshift-operator-lifecycle-manager started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container catalog-operator ready: true, restart count 0
May 26 22:22:19.608: INFO: ingress-operator-f6594bf4d-m74mj from openshift-ingress-operator started at 2020-05-26 19:59:03 +0000 UTC (2 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container ingress-operator ready: true, restart count 0
May 26 22:22:19.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 22:22:19.608: INFO: thanos-querier-7f4dfb8d6f-5m6ft from openshift-monitoring started at 2020-05-26 20:05:38 +0000 UTC (4 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 22:22:19.608: INFO: 	Container oauth-proxy ready: true, restart count 0
May 26 22:22:19.608: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 26 22:22:19.608: INFO: 	Container thanos-querier ready: true, restart count 0
May 26 22:22:19.608: INFO: calico-kube-controllers-599969f895-xfmxb from calico-system started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 26 22:22:19.608: INFO: ibmcloud-block-storage-driver-sncrt from kube-system started at 2020-05-26 19:58:07 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May 26 22:22:19.608: INFO: cluster-node-tuning-operator-58cb5999f5-dqqxl from openshift-cluster-node-tuning-operator started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
May 26 22:22:19.608: INFO: marketplace-operator-6d86c46f6b-j7hh9 from openshift-marketplace started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container marketplace-operator ready: true, restart count 0
May 26 22:22:19.608: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-05-26 20:05:48 +0000 UTC (7 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 22:22:19.608: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 26 22:22:19.608: INFO: 	Container prometheus ready: true, restart count 1
May 26 22:22:19.608: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May 26 22:22:19.608: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 26 22:22:19.608: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May 26 22:22:19.608: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 26 22:22:19.608: INFO: ibm-storage-watcher-5f6b5dbcd4-kn4xs from kube-system started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
May 26 22:22:19.608: INFO: ibmcloud-block-storage-plugin-9d877d7bc-kjrsl from kube-system started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
May 26 22:22:19.608: INFO: dns-operator-c68448f89-lfqlv from openshift-dns-operator started at 2020-05-26 19:59:02 +0000 UTC (2 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container dns-operator ready: true, restart count 0
May 26 22:22:19.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 22:22:19.608: INFO: openshift-service-catalog-controller-manager-operator-86b49fb6n from openshift-service-catalog-controller-manager-operator started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container operator ready: true, restart count 1
May 26 22:22:19.608: INFO: grafana-5648b7fdd9-rmjbk from openshift-monitoring started at 2020-05-26 21:51:25 +0000 UTC (2 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container grafana ready: true, restart count 0
May 26 22:22:19.608: INFO: 	Container grafana-proxy ready: true, restart count 0
May 26 22:22:19.608: INFO: packageserver-7d5f79c4db-89cw9 from openshift-operator-lifecycle-manager started at 2020-05-26 21:51:35 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container packageserver ready: true, restart count 0
May 26 22:22:19.608: INFO: downloads-8479fbbf57-z9pkl from openshift-console started at 2020-05-26 19:59:01 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container download-server ready: true, restart count 0
May 26 22:22:19.608: INFO: service-ca-operator-7bb6cf7fbc-j7nvb from openshift-service-ca-operator started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container operator ready: true, restart count 0
May 26 22:22:19.608: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-05-26 20:05:06 +0000 UTC (3 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container alertmanager ready: true, restart count 0
May 26 22:22:19.608: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 26 22:22:19.608: INFO: 	Container config-reloader ready: true, restart count 0
May 26 22:22:19.608: INFO: router-default-6cfd8db4bf-7dstv from openshift-ingress started at 2020-05-26 21:51:25 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container router ready: true, restart count 0
May 26 22:22:19.608: INFO: cluster-storage-operator-8696454489-8wttw from openshift-cluster-storage-operator started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container cluster-storage-operator ready: true, restart count 0
May 26 22:22:19.608: INFO: ibm-master-proxy-static-10.215.60.10 from kube-system started at 2020-05-26 19:57:58 +0000 UTC (2 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May 26 22:22:19.608: INFO: 	Container pause ready: true, restart count 0
May 26 22:22:19.608: INFO: openshift-kube-proxy-ns7pk from openshift-kube-proxy started at 2020-05-26 19:58:04 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container kube-proxy ready: true, restart count 0
May 26 22:22:19.608: INFO: ibm-cloud-provider-ip-159-122-104-134-84fdcfb5c5-cgj49 from ibm-system started at 2020-05-26 20:02:23 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container ibm-cloud-provider-ip-159-122-104-134 ready: true, restart count 0
May 26 22:22:19.608: INFO: registry-pvc-permissions-j5wnp from openshift-image-registry started at 2020-05-26 20:03:30 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container pvc-permissions ready: false, restart count 0
May 26 22:22:19.608: INFO: multus-admission-controller-4kd9c from openshift-multus started at 2020-05-26 19:59:01 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container multus-admission-controller ready: true, restart count 0
May 26 22:22:19.608: INFO: console-operator-56c4d6445c-brk5r from openshift-console-operator started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container console-operator ready: true, restart count 1
May 26 22:22:19.608: INFO: tuned-22gmz from openshift-cluster-node-tuning-operator started at 2020-05-26 19:59:39 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container tuned ready: true, restart count 0
May 26 22:22:19.608: INFO: prometheus-adapter-5646d5d5dd-ql9h7 from openshift-monitoring started at 2020-05-26 20:05:35 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 26 22:22:19.608: INFO: ibm-keepalived-watcher-bl295 from kube-system started at 2020-05-26 19:58:00 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container keepalived-watcher ready: true, restart count 0
May 26 22:22:19.608: INFO: downloads-8479fbbf57-qhklc from openshift-console started at 2020-05-26 19:59:01 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container download-server ready: true, restart count 0
May 26 22:22:19.608: INFO: sonobuoy-systemd-logs-daemon-set-e1fd62b7d6894c57-wrl9n from sonobuoy started at 2020-05-26 21:25:54 +0000 UTC (2 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 26 22:22:19.608: INFO: 	Container systemd-logs ready: true, restart count 0
May 26 22:22:19.608: INFO: calico-node-p2dwh from calico-system started at 2020-05-26 19:58:46 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container calico-node ready: true, restart count 0
May 26 22:22:19.608: INFO: ibm-file-plugin-5999bd7d7d-mcpqg from kube-system started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
May 26 22:22:19.608: INFO: cluster-monitoring-operator-64b79969cc-g94mm from openshift-monitoring started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
May 26 22:22:19.608: INFO: dns-default-s9jv4 from openshift-dns started at 2020-05-26 20:00:40 +0000 UTC (2 container statuses recorded)
May 26 22:22:19.608: INFO: 	Container dns ready: true, restart count 0
May 26 22:22:19.609: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 26 22:22:19.609: INFO: node-exporter-b6pqt from openshift-monitoring started at 2020-05-26 19:59:35 +0000 UTC (2 container statuses recorded)
May 26 22:22:19.609: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 22:22:19.609: INFO: 	Container node-exporter ready: true, restart count 0
May 26 22:22:19.609: INFO: apiservice-cabundle-injector-5c88555f6d-r7p9b from openshift-service-ca started at 2020-05-26 19:59:45 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.609: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
May 26 22:22:19.609: INFO: redhat-operators-7b74fd75b7-8dvbx from openshift-marketplace started at 2020-05-26 20:01:34 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.609: INFO: 	Container redhat-operators ready: true, restart count 0
May 26 22:22:19.609: INFO: 
Logging pods the kubelet thinks is on node 10.215.60.11 before test
May 26 22:22:19.690: INFO: service-serving-cert-signer-69cddbb454-wjjcq from openshift-service-ca started at 2020-05-26 19:59:44 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.690: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
May 26 22:22:19.690: INFO: ibmcloud-block-storage-driver-zq7xq from kube-system started at 2020-05-26 19:57:36 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.690: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May 26 22:22:19.690: INFO: openshift-state-metrics-74997686f-cgqlx from openshift-monitoring started at 2020-05-26 19:59:35 +0000 UTC (3 container statuses recorded)
May 26 22:22:19.690: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 26 22:22:19.690: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 26 22:22:19.690: INFO: 	Container openshift-state-metrics ready: true, restart count 0
May 26 22:22:19.690: INFO: cluster-samples-operator-759dd556bf-bgg5n from openshift-cluster-samples-operator started at 2020-05-26 19:59:44 +0000 UTC (2 container statuses recorded)
May 26 22:22:19.690: INFO: 	Container cluster-samples-operator ready: true, restart count 0
May 26 22:22:19.690: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
May 26 22:22:19.690: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-05-26 20:05:02 +0000 UTC (3 container statuses recorded)
May 26 22:22:19.690: INFO: 	Container alertmanager ready: true, restart count 0
May 26 22:22:19.690: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 26 22:22:19.690: INFO: 	Container config-reloader ready: true, restart count 0
May 26 22:22:19.690: INFO: multus-88m9q from openshift-multus started at 2020-05-26 19:57:58 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.690: INFO: 	Container kube-multus ready: true, restart count 0
May 26 22:22:19.690: INFO: configmap-cabundle-injector-795c74476d-6hzjj from openshift-service-ca started at 2020-05-26 19:59:45 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.690: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
May 26 22:22:19.690: INFO: console-d8768d4f5-m8fb9 from openshift-console started at 2020-05-26 20:01:37 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.690: INFO: 	Container console ready: true, restart count 0
May 26 22:22:19.690: INFO: router-default-6cfd8db4bf-jv7v4 from openshift-ingress started at 2020-05-26 20:01:45 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.690: INFO: 	Container router ready: true, restart count 0
May 26 22:22:19.690: INFO: ibm-keepalived-watcher-w8tv5 from kube-system started at 2020-05-26 19:57:34 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.690: INFO: 	Container keepalived-watcher ready: true, restart count 0
May 26 22:22:19.690: INFO: openshift-kube-proxy-h4l9g from openshift-kube-proxy started at 2020-05-26 19:58:05 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.690: INFO: 	Container kube-proxy ready: true, restart count 0
May 26 22:22:19.690: INFO: community-operators-674864f5c4-bwwnn from openshift-marketplace started at 2020-05-26 20:01:32 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.691: INFO: 	Container community-operators ready: true, restart count 0
May 26 22:22:19.691: INFO: multus-admission-controller-mqrcn from openshift-multus started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.691: INFO: 	Container multus-admission-controller ready: true, restart count 0
May 26 22:22:19.691: INFO: node-exporter-spjk9 from openshift-monitoring started at 2020-05-26 19:59:36 +0000 UTC (2 container statuses recorded)
May 26 22:22:19.691: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 22:22:19.691: INFO: 	Container node-exporter ready: true, restart count 0
May 26 22:22:19.691: INFO: calico-typha-f4f4dbb8c-jg59b from calico-system started at 2020-05-26 20:00:43 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.691: INFO: 	Container calico-typha ready: true, restart count 0
May 26 22:22:19.691: INFO: calico-node-6tv2z from calico-system started at 2020-05-26 19:58:46 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.691: INFO: 	Container calico-node ready: true, restart count 0
May 26 22:22:19.691: INFO: dns-default-jlw4t from openshift-dns started at 2020-05-26 20:00:40 +0000 UTC (2 container statuses recorded)
May 26 22:22:19.691: INFO: 	Container dns ready: true, restart count 0
May 26 22:22:19.691: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 26 22:22:19.691: INFO: network-operator-76d6fbdbb8-5hgl8 from openshift-network-operator started at 2020-05-26 19:57:36 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.691: INFO: 	Container network-operator ready: true, restart count 0
May 26 22:22:19.691: INFO: kube-state-metrics-7498bc479d-5pbn8 from openshift-monitoring started at 2020-05-26 19:59:34 +0000 UTC (3 container statuses recorded)
May 26 22:22:19.691: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 26 22:22:19.691: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 26 22:22:19.691: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 26 22:22:19.691: INFO: sonobuoy-systemd-logs-daemon-set-e1fd62b7d6894c57-gzfvj from sonobuoy started at 2020-05-26 21:25:54 +0000 UTC (2 container statuses recorded)
May 26 22:22:19.691: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 26 22:22:19.691: INFO: 	Container systemd-logs ready: true, restart count 0
May 26 22:22:19.691: INFO: node-ca-jh22q from openshift-image-registry started at 2020-05-26 20:00:40 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.691: INFO: 	Container node-ca ready: true, restart count 0
May 26 22:22:19.691: INFO: vpn-6847db666c-dd6pq from kube-system started at 2020-05-26 20:03:07 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.691: INFO: 	Container vpn ready: true, restart count 0
May 26 22:22:19.691: INFO: thanos-querier-7f4dfb8d6f-w2z69 from openshift-monitoring started at 2020-05-26 20:05:34 +0000 UTC (4 container statuses recorded)
May 26 22:22:19.691: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 22:22:19.691: INFO: 	Container oauth-proxy ready: true, restart count 0
May 26 22:22:19.691: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 26 22:22:19.691: INFO: 	Container thanos-querier ready: true, restart count 0
May 26 22:22:19.691: INFO: ibm-cloud-provider-ip-159-122-104-134-84fdcfb5c5-l5b9t from ibm-system started at 2020-05-26 21:51:25 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.691: INFO: 	Container ibm-cloud-provider-ip-159-122-104-134 ready: true, restart count 0
May 26 22:22:19.691: INFO: ibm-master-proxy-static-10.215.60.11 from kube-system started at 2020-05-26 19:57:32 +0000 UTC (2 container statuses recorded)
May 26 22:22:19.691: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May 26 22:22:19.691: INFO: 	Container pause ready: true, restart count 0
May 26 22:22:19.691: INFO: tigera-operator-798cfbf7dd-q5frt from tigera-operator started at 2020-05-26 19:57:36 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.691: INFO: 	Container tigera-operator ready: true, restart count 2
May 26 22:22:19.691: INFO: tuned-88qws from openshift-cluster-node-tuning-operator started at 2020-05-26 19:59:39 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.691: INFO: 	Container tuned ready: true, restart count 0
May 26 22:22:19.691: INFO: image-registry-766b5fd974-j2s4m from openshift-image-registry started at 2020-05-26 20:03:30 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.691: INFO: 	Container registry ready: true, restart count 0
May 26 22:22:19.691: INFO: prometheus-operator-9d5b5788b-dpcht from openshift-monitoring started at 2020-05-26 20:04:36 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.691: INFO: 	Container prometheus-operator ready: true, restart count 0
May 26 22:22:19.691: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-05-26 20:05:57 +0000 UTC (7 container statuses recorded)
May 26 22:22:19.691: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 22:22:19.691: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 26 22:22:19.691: INFO: 	Container prometheus ready: true, restart count 1
May 26 22:22:19.691: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May 26 22:22:19.691: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 26 22:22:19.691: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May 26 22:22:19.691: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 26 22:22:19.691: INFO: 
Logging pods the kubelet thinks is on node 10.215.60.34 before test
May 26 22:22:19.736: INFO: packageserver-7d5f79c4db-dpjmm from openshift-operator-lifecycle-manager started at 2020-05-26 21:51:27 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.736: INFO: 	Container packageserver ready: true, restart count 0
May 26 22:22:19.736: INFO: ibmcloud-block-storage-driver-nwvlx from kube-system started at 2020-05-26 19:58:31 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.736: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May 26 22:22:19.736: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-05-26 21:51:30 +0000 UTC (3 container statuses recorded)
May 26 22:22:19.736: INFO: 	Container alertmanager ready: true, restart count 0
May 26 22:22:19.736: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 26 22:22:19.736: INFO: 	Container config-reloader ready: true, restart count 0
May 26 22:22:19.736: INFO: prometheus-adapter-5646d5d5dd-9cprr from openshift-monitoring started at 2020-05-26 21:51:25 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.736: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 26 22:22:19.736: INFO: console-d8768d4f5-vpb4j from openshift-console started at 2020-05-26 21:51:25 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.736: INFO: 	Container console ready: true, restart count 0
May 26 22:22:19.736: INFO: dns-default-ngb66 from openshift-dns started at 2020-05-26 21:51:30 +0000 UTC (2 container statuses recorded)
May 26 22:22:19.736: INFO: 	Container dns ready: true, restart count 0
May 26 22:22:19.736: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 26 22:22:19.736: INFO: tuned-4sgbw from openshift-cluster-node-tuning-operator started at 2020-05-26 19:59:39 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.736: INFO: 	Container tuned ready: true, restart count 0
May 26 22:22:19.736: INFO: node-ca-6c8qk from openshift-image-registry started at 2020-05-26 20:00:40 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.736: INFO: 	Container node-ca ready: true, restart count 0
May 26 22:22:19.736: INFO: ibm-keepalived-watcher-gtvct from kube-system started at 2020-05-26 19:58:23 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.736: INFO: 	Container keepalived-watcher ready: true, restart count 0
May 26 22:22:19.736: INFO: node-exporter-7b5t7 from openshift-monitoring started at 2020-05-26 19:59:36 +0000 UTC (2 container statuses recorded)
May 26 22:22:19.736: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 22:22:19.736: INFO: 	Container node-exporter ready: true, restart count 0
May 26 22:22:19.736: INFO: telemeter-client-6745779989-mjhdd from openshift-monitoring started at 2020-05-26 21:51:25 +0000 UTC (3 container statuses recorded)
May 26 22:22:19.736: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 22:22:19.736: INFO: 	Container reload ready: true, restart count 0
May 26 22:22:19.736: INFO: 	Container telemeter-client ready: true, restart count 0
May 26 22:22:19.736: INFO: sonobuoy-e2e-job-543b6d87a6d14642 from sonobuoy started at 2020-05-26 21:25:53 +0000 UTC (2 container statuses recorded)
May 26 22:22:19.736: INFO: 	Container e2e ready: true, restart count 0
May 26 22:22:19.736: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 26 22:22:19.736: INFO: openshift-kube-proxy-2sfd8 from openshift-kube-proxy started at 2020-05-26 19:58:23 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.736: INFO: 	Container kube-proxy ready: true, restart count 0
May 26 22:22:19.736: INFO: calico-node-9s8hb from calico-system started at 2020-05-26 19:58:46 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.736: INFO: 	Container calico-node ready: true, restart count 0
May 26 22:22:19.736: INFO: multus-j2db5 from openshift-multus started at 2020-05-26 19:58:23 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.736: INFO: 	Container kube-multus ready: true, restart count 0
May 26 22:22:19.736: INFO: sonobuoy from sonobuoy started at 2020-05-26 21:25:44 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.736: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 26 22:22:19.736: INFO: sonobuoy-systemd-logs-daemon-set-e1fd62b7d6894c57-mptms from sonobuoy started at 2020-05-26 21:25:54 +0000 UTC (2 container statuses recorded)
May 26 22:22:19.736: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 26 22:22:19.736: INFO: 	Container systemd-logs ready: true, restart count 0
May 26 22:22:19.736: INFO: multus-admission-controller-xvl7w from openshift-multus started at 2020-05-26 21:52:00 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.736: INFO: 	Container multus-admission-controller ready: true, restart count 0
May 26 22:22:19.736: INFO: ibm-master-proxy-static-10.215.60.34 from kube-system started at 2020-05-26 19:58:20 +0000 UTC (2 container statuses recorded)
May 26 22:22:19.736: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May 26 22:22:19.736: INFO: 	Container pause ready: true, restart count 0
May 26 22:22:19.736: INFO: calico-typha-f4f4dbb8c-7f56v from calico-system started at 2020-05-26 19:58:45 +0000 UTC (1 container statuses recorded)
May 26 22:22:19.736: INFO: 	Container calico-typha ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-05461072-2573-4fd6-9f7f-ebbaa675bab3 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-05461072-2573-4fd6-9f7f-ebbaa675bab3 off the node 10.215.60.34
STEP: verifying the node doesn't have the label kubernetes.io/e2e-05461072-2573-4fd6-9f7f-ebbaa675bab3
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:22:36.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5199" for this suite.
May 26 22:22:52.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:22:53.970: INFO: namespace sched-pred-5199 deletion completed in 17.913839397s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:34.701 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:22:53.970: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 26 22:22:55.084: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 26 22:22:57.110: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726128575, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726128575, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726128575, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726128575, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 26 22:23:00.145: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:23:00.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5787" for this suite.
May 26 22:23:14.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:23:16.233: INFO: namespace webhook-5787 deletion completed in 15.889319397s
STEP: Destroying namespace "webhook-5787-markers" for this suite.
May 26 22:23:24.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:23:26.120: INFO: namespace webhook-5787-markers deletion completed in 9.887496235s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:32.191 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:23:26.165: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-nn5n
STEP: Creating a pod to test atomic-volume-subpath
May 26 22:23:26.366: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-nn5n" in namespace "subpath-4829" to be "success or failure"
May 26 22:23:26.376: INFO: Pod "pod-subpath-test-configmap-nn5n": Phase="Pending", Reason="", readiness=false. Elapsed: 9.750273ms
May 26 22:23:28.386: INFO: Pod "pod-subpath-test-configmap-nn5n": Phase="Running", Reason="", readiness=true. Elapsed: 2.020012332s
May 26 22:23:30.396: INFO: Pod "pod-subpath-test-configmap-nn5n": Phase="Running", Reason="", readiness=true. Elapsed: 4.029741689s
May 26 22:23:32.406: INFO: Pod "pod-subpath-test-configmap-nn5n": Phase="Running", Reason="", readiness=true. Elapsed: 6.040124928s
May 26 22:23:34.416: INFO: Pod "pod-subpath-test-configmap-nn5n": Phase="Running", Reason="", readiness=true. Elapsed: 8.050134266s
May 26 22:23:36.426: INFO: Pod "pod-subpath-test-configmap-nn5n": Phase="Running", Reason="", readiness=true. Elapsed: 10.059531385s
May 26 22:23:38.436: INFO: Pod "pod-subpath-test-configmap-nn5n": Phase="Running", Reason="", readiness=true. Elapsed: 12.070248818s
May 26 22:23:40.446: INFO: Pod "pod-subpath-test-configmap-nn5n": Phase="Running", Reason="", readiness=true. Elapsed: 14.080246279s
May 26 22:23:42.457: INFO: Pod "pod-subpath-test-configmap-nn5n": Phase="Running", Reason="", readiness=true. Elapsed: 16.090432939s
May 26 22:23:44.467: INFO: Pod "pod-subpath-test-configmap-nn5n": Phase="Running", Reason="", readiness=true. Elapsed: 18.100534159s
May 26 22:23:46.477: INFO: Pod "pod-subpath-test-configmap-nn5n": Phase="Running", Reason="", readiness=true. Elapsed: 20.110608666s
May 26 22:23:48.497: INFO: Pod "pod-subpath-test-configmap-nn5n": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.130987989s
STEP: Saw pod success
May 26 22:23:48.497: INFO: Pod "pod-subpath-test-configmap-nn5n" satisfied condition "success or failure"
May 26 22:23:48.507: INFO: Trying to get logs from node 10.215.60.34 pod pod-subpath-test-configmap-nn5n container test-container-subpath-configmap-nn5n: <nil>
STEP: delete the pod
May 26 22:23:48.565: INFO: Waiting for pod pod-subpath-test-configmap-nn5n to disappear
May 26 22:23:48.581: INFO: Pod pod-subpath-test-configmap-nn5n no longer exists
STEP: Deleting pod pod-subpath-test-configmap-nn5n
May 26 22:23:48.581: INFO: Deleting pod "pod-subpath-test-configmap-nn5n" in namespace "subpath-4829"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:23:48.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4829" for this suite.
May 26 22:23:56.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:23:58.492: INFO: namespace subpath-4829 deletion completed in 9.886053172s

â€¢ [SLOW TEST:32.327 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:23:58.493: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0526 22:24:04.713250      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 26 22:24:04.713: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:24:04.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8220" for this suite.
May 26 22:24:12.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:24:14.616: INFO: namespace gc-8220 deletion completed in 9.88686751s

â€¢ [SLOW TEST:16.123 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:24:14.616: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 22:24:14.883: INFO: Create a RollingUpdate DaemonSet
May 26 22:24:14.903: INFO: Check that daemon pods launch on every node of the cluster
May 26 22:24:14.934: INFO: Number of nodes with available pods: 0
May 26 22:24:14.934: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 22:24:15.966: INFO: Number of nodes with available pods: 0
May 26 22:24:15.966: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 22:24:16.959: INFO: Number of nodes with available pods: 2
May 26 22:24:16.959: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 22:24:17.958: INFO: Number of nodes with available pods: 3
May 26 22:24:17.958: INFO: Number of running nodes: 3, number of available pods: 3
May 26 22:24:17.958: INFO: Update the DaemonSet to trigger a rollout
May 26 22:24:18.024: INFO: Updating DaemonSet daemon-set
May 26 22:24:33.073: INFO: Roll back the DaemonSet before rollout is complete
May 26 22:24:33.103: INFO: Updating DaemonSet daemon-set
May 26 22:24:33.103: INFO: Make sure DaemonSet rollback is complete
May 26 22:24:33.117: INFO: Wrong image for pod: daemon-set-dxkqt. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 26 22:24:33.117: INFO: Pod daemon-set-dxkqt is not available
May 26 22:24:34.146: INFO: Wrong image for pod: daemon-set-dxkqt. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 26 22:24:34.146: INFO: Pod daemon-set-dxkqt is not available
May 26 22:24:35.148: INFO: Pod daemon-set-pnkdw is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-38, will wait for the garbage collector to delete the pods
May 26 22:24:35.274: INFO: Deleting DaemonSet.extensions daemon-set took: 21.015206ms
May 26 22:24:35.674: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.275444ms
May 26 22:26:08.684: INFO: Number of nodes with available pods: 0
May 26 22:26:08.684: INFO: Number of running nodes: 0, number of available pods: 0
May 26 22:26:08.694: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-38/daemonsets","resourceVersion":"67345"},"items":null}

May 26 22:26:08.703: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-38/pods","resourceVersion":"67345"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:26:08.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-38" for this suite.
May 26 22:26:16.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:26:18.650: INFO: namespace daemonsets-38 deletion completed in 9.885715037s

â€¢ [SLOW TEST:124.035 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:26:18.651: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
May 26 22:26:18.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 --namespace=kubectl-7955 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
May 26 22:26:21.137: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
May 26 22:26:21.137: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:26:23.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7955" for this suite.
May 26 22:26:31.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:26:33.059: INFO: namespace kubectl-7955 deletion completed in 9.888651474s

â€¢ [SLOW TEST:14.408 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:26:33.060: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
May 26 22:26:33.248: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-297629347 proxy --unix-socket=/tmp/kubectl-proxy-unix058487258/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:26:33.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6872" for this suite.
May 26 22:26:41.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:26:43.243: INFO: namespace kubectl-6872 deletion completed in 9.890167608s

â€¢ [SLOW TEST:10.183 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:26:43.243: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 26 22:26:44.232: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 26 22:26:47.284: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 22:26:47.294: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1282-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:26:48.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-222" for this suite.
May 26 22:26:56.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:26:58.604: INFO: namespace webhook-222 deletion completed in 9.891529543s
STEP: Destroying namespace "webhook-222-markers" for this suite.
May 26 22:27:06.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:27:08.494: INFO: namespace webhook-222-markers deletion completed in 9.890330842s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:25.307 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:27:08.551: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 22:27:08.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 create -f - --namespace=kubectl-2365'
May 26 22:27:09.241: INFO: stderr: ""
May 26 22:27:09.242: INFO: stdout: "replicationcontroller/redis-master created\n"
May 26 22:27:09.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 create -f - --namespace=kubectl-2365'
May 26 22:27:09.818: INFO: stderr: ""
May 26 22:27:09.818: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
May 26 22:27:10.829: INFO: Selector matched 1 pods for map[app:redis]
May 26 22:27:10.829: INFO: Found 0 / 1
May 26 22:27:11.828: INFO: Selector matched 1 pods for map[app:redis]
May 26 22:27:11.828: INFO: Found 1 / 1
May 26 22:27:11.828: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 26 22:27:11.838: INFO: Selector matched 1 pods for map[app:redis]
May 26 22:27:11.838: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 26 22:27:11.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 describe pod redis-master-rhh5p --namespace=kubectl-2365'
May 26 22:27:12.011: INFO: stderr: ""
May 26 22:27:12.011: INFO: stdout: "Name:         redis-master-rhh5p\nNamespace:    kubectl-2365\nPriority:     0\nNode:         10.215.60.34/10.215.60.34\nStart Time:   Tue, 26 May 2020 22:27:09 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 172.30.182.125/32\n              cni.projectcalico.org/podIPs: 172.30.182.125/32\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"k8s-pod-network\",\n                    \"ips\": [\n                        \"172.30.182.125\"\n                    ],\n                    \"dns\": {}\n                }]\n              openshift.io/scc: privileged\nStatus:       Running\nIP:           172.30.182.125\nIPs:\n  IP:           172.30.182.125\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   cri-o://397398cc89564539366e5eaed70ca7609ba9721c13e12c96c515aae2bdd6bc1d\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 26 May 2020 22:27:10 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-ndb84 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-ndb84:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-ndb84\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                   Message\n  ----    ------     ----       ----                   -------\n  Normal  Scheduled  <unknown>  default-scheduler      Successfully assigned kubectl-2365/redis-master-rhh5p to 10.215.60.34\n  Normal  Pulled     2s         kubelet, 10.215.60.34  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    2s         kubelet, 10.215.60.34  Created container redis-master\n  Normal  Started    2s         kubelet, 10.215.60.34  Started container redis-master\n"
May 26 22:27:12.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 describe rc redis-master --namespace=kubectl-2365'
May 26 22:27:12.203: INFO: stderr: ""
May 26 22:27:12.203: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-2365\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-rhh5p\n"
May 26 22:27:12.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 describe service redis-master --namespace=kubectl-2365'
May 26 22:27:12.370: INFO: stderr: ""
May 26 22:27:12.370: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-2365\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.21.117.152\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.30.182.125:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 26 22:27:12.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 describe node 10.215.60.10'
May 26 22:27:12.701: INFO: stderr: ""
May 26 22:27:12.701: INFO: stdout: "Name:               10.215.60.10\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-de\n                    failure-domain.beta.kubernetes.io/zone=fra02\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=158.177.211.78\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.215.60.10\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_7_64\n                    ibm-cloud.kubernetes.io/region=eu-de\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-br6n2mvf0gkcffrcj9u0-kubee2epvgd-default-0000014a\n                    ibm-cloud.kubernetes.io/worker-pool-id=br6n2mvf0gkcffrcj9u0-1147f09\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.3.21_1523_openshift\n                    ibm-cloud.kubernetes.io/zone=fra02\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.215.60.10\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2722990\n                    publicVLAN=2722976\n                    topology.kubernetes.io/region=eu-de\n                    topology.kubernetes.io/zone=fra02\nAnnotations:        projectcalico.org/IPv4Address: 10.215.60.10/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.141.128\nCreationTimestamp:  Tue, 26 May 2020 19:58:00 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 26 May 2020 19:59:00 +0000   Tue, 26 May 2020 19:59:00 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 26 May 2020 22:27:01 +0000   Tue, 26 May 2020 19:58:00 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 26 May 2020 22:27:01 +0000   Tue, 26 May 2020 19:58:00 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 26 May 2020 22:27:01 +0000   Tue, 26 May 2020 19:58:00 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 26 May 2020 22:27:01 +0000   Tue, 26 May 2020 19:59:01 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.215.60.10\n  ExternalIP:  158.177.211.78\n  Hostname:    10.215.60.10\nCapacity:\n cpu:                4\n ephemeral-storage:  103078840Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16260876Ki\n pods:               110\nAllocatable:\n cpu:                3910m\n ephemeral-storage:  100275095474\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             13484812Ki\n pods:               110\nSystem Info:\n Machine ID:                                             fcf65adc6d144360a86f5d1b762b93c6\n System UUID:                                            55514CF2-4F39-7F45-B801-ACC8895C3E3A\n Boot ID:                                                06c76418-40f7-46cf-8fba-ac33b0453345\n Kernel Version:                                         3.10.0-1127.8.2.el7.x86_64\n OS Image:                                               Red Hat\n Operating System:                                       linux\n Architecture:                                           amd64\n Container Runtime Version:                              cri-o://1.16.6-14.dev.rhaos4.3.git24e5f4e.el7\n Kubelet Version:                                        v1.16.2\n Kube-Proxy Version:                                     v1.16.2\nProviderID:                                              ibm://fee034388aa6435883a1f720010ab3a2///br6n2mvf0gkcffrcj9u0/kube-br6n2mvf0gkcffrcj9u0-kubee2epvgd-default-0000014a\nNon-terminated Pods:                                     (43 in total)\n  Namespace                                              Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                                              ----                                                               ------------  ----------  ---------------  -------------  ---\n  calico-system                                          calico-kube-controllers-599969f895-xfmxb                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         148m\n  calico-system                                          calico-node-p2dwh                                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         148m\n  calico-system                                          calico-typha-f4f4dbb8c-s262v                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         146m\n  ibm-system                                             ibm-cloud-provider-ip-159-122-104-134-84fdcfb5c5-cgj49             5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         144m\n  kube-system                                            ibm-file-plugin-5999bd7d7d-mcpqg                                   50m (1%)      200m (5%)   100Mi (0%)       0 (0%)         154m\n  kube-system                                            ibm-keepalived-watcher-bl295                                       5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         149m\n  kube-system                                            ibm-master-proxy-static-10.215.60.10                               25m (0%)      300m (7%)   32M (0%)         512M (3%)      147m\n  kube-system                                            ibm-storage-watcher-5f6b5dbcd4-kn4xs                               50m (1%)      200m (5%)   100Mi (0%)       0 (0%)         154m\n  kube-system                                            ibmcloud-block-storage-driver-sncrt                                25m (0%)      100m (2%)   50Mi (0%)        200Mi (1%)     149m\n  kube-system                                            ibmcloud-block-storage-plugin-9d877d7bc-kjrsl                      50m (1%)      200m (5%)   100Mi (0%)       0 (0%)         154m\n  openshift-cluster-node-tuning-operator                 cluster-node-tuning-operator-58cb5999f5-dqqxl                      10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         156m\n  openshift-cluster-node-tuning-operator                 tuned-22gmz                                                        10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         147m\n  openshift-cluster-storage-operator                     cluster-storage-operator-8696454489-8wttw                          10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         156m\n  openshift-console-operator                             console-operator-56c4d6445c-brk5r                                  10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         156m\n  openshift-console                                      downloads-8479fbbf57-qhklc                                         10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         156m\n  openshift-console                                      downloads-8479fbbf57-z9pkl                                         10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         156m\n  openshift-dns-operator                                 dns-operator-c68448f89-lfqlv                                       20m (0%)      0 (0%)      40Mi (0%)        0 (0%)         156m\n  openshift-dns                                          dns-default-s9jv4                                                  110m (2%)     0 (0%)      70Mi (0%)        512Mi (3%)     146m\n  openshift-image-registry                               cluster-image-registry-operator-6f78cddbbc-qrdmf                   20m (0%)      0 (0%)      0 (0%)           0 (0%)         156m\n  openshift-image-registry                               node-ca-q2fgf                                                      10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         146m\n  openshift-ingress-operator                             ingress-operator-f6594bf4d-m74mj                                   20m (0%)      0 (0%)      40Mi (0%)        0 (0%)         156m\n  openshift-ingress                                      router-default-6cfd8db4bf-7dstv                                    100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         35m\n  openshift-kube-proxy                                   openshift-kube-proxy-ns7pk                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         149m\n  openshift-marketplace                                  certified-operators-7c7bd497cb-hjpfx                               10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         145m\n  openshift-marketplace                                  marketplace-operator-6d86c46f6b-j7hh9                              10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         156m\n  openshift-marketplace                                  redhat-operators-7b74fd75b7-8dvbx                                  10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         145m\n  openshift-monitoring                                   alertmanager-main-0                                                110m (2%)     100m (2%)   245Mi (1%)       25Mi (0%)      142m\n  openshift-monitoring                                   cluster-monitoring-operator-64b79969cc-g94mm                       10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         156m\n  openshift-monitoring                                   grafana-5648b7fdd9-rmjbk                                           110m (2%)     0 (0%)      120Mi (0%)       0 (0%)         35m\n  openshift-monitoring                                   node-exporter-b6pqt                                                112m (2%)     0 (0%)      200Mi (1%)       0 (0%)         147m\n  openshift-monitoring                                   prometheus-adapter-5646d5d5dd-ql9h7                                10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         141m\n  openshift-monitoring                                   prometheus-k8s-1                                                   480m (12%)    200m (5%)   1234Mi (9%)      50Mi (0%)      141m\n  openshift-monitoring                                   thanos-querier-7f4dfb8d6f-5m6ft                                    40m (1%)      0 (0%)      72Mi (0%)        0 (0%)         141m\n  openshift-multus                                       multus-admission-controller-4kd9c                                  10m (0%)      0 (0%)      0 (0%)           0 (0%)         148m\n  openshift-multus                                       multus-lh654                                                       10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         149m\n  openshift-operator-lifecycle-manager                   catalog-operator-6956d96f67-5xvls                                  10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         156m\n  openshift-operator-lifecycle-manager                   olm-operator-9d9b9dc65-rg7n5                                       10m (0%)      0 (0%)      160Mi (1%)       0 (0%)         156m\n  openshift-operator-lifecycle-manager                   packageserver-7d5f79c4db-89cw9                                     10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         35m\n  openshift-service-ca-operator                          service-ca-operator-7bb6cf7fbc-j7nvb                               10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         156m\n  openshift-service-ca                                   apiservice-cabundle-injector-5c88555f6d-r7p9b                      10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         147m\n  openshift-service-catalog-apiserver-operator           openshift-service-catalog-apiserver-operator-5897998845-kpk9m      0 (0%)        0 (0%)      50Mi (0%)        0 (0%)         156m\n  openshift-service-catalog-controller-manager-operator  openshift-service-catalog-controller-manager-operator-86b49fb6n    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         156m\n  sonobuoy                                               sonobuoy-systemd-logs-daemon-set-e1fd62b7d6894c57-wrl9n            0 (0%)        0 (0%)      0 (0%)           0 (0%)         61m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                1532m (39%)      1300m (33%)\n  memory             4062738Ki (30%)  1337229312 (9%)\n  ephemeral-storage  0 (0%)           0 (0%)\nEvents:\n  Type    Reason                   Age                  From                      Message\n  ----    ------                   ----                 ----                      -------\n  Normal  Starting                 149m                 kubelet, 10.215.60.10     Starting kubelet.\n  Normal  NodeAllocatableEnforced  149m                 kubelet, 10.215.60.10     Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientMemory  149m (x8 over 149m)  kubelet, 10.215.60.10     Node 10.215.60.10 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    149m (x8 over 149m)  kubelet, 10.215.60.10     Node 10.215.60.10 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     149m (x7 over 149m)  kubelet, 10.215.60.10     Node 10.215.60.10 status is now: NodeHasSufficientPID\n  Normal  Starting                 148m                 kube-proxy, 10.215.60.10  Starting kube-proxy.\n"
May 26 22:27:12.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 describe namespace kubectl-2365'
May 26 22:27:12.883: INFO: stderr: ""
May 26 22:27:12.883: INFO: stdout: "Name:         kubectl-2365\nLabels:       e2e-framework=kubectl\n              e2e-run=0c282475-98ec-4464-a834-167e881d9705\nAnnotations:  openshift.io/sa.scc.mcs: s0:c55,c10\n              openshift.io/sa.scc.supplemental-groups: 1002990000/10000\n              openshift.io/sa.scc.uid-range: 1002990000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:27:12.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2365" for this suite.
May 26 22:27:44.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:27:46.799: INFO: namespace kubectl-2365 deletion completed in 33.901454852s

â€¢ [SLOW TEST:38.249 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:27:46.802: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 26 22:27:55.106: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 26 22:27:55.115: INFO: Pod pod-with-prestop-exec-hook still exists
May 26 22:27:57.115: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 26 22:27:57.126: INFO: Pod pod-with-prestop-exec-hook still exists
May 26 22:27:59.115: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 26 22:27:59.125: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:27:59.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8398" for this suite.
May 26 22:28:13.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:28:15.113: INFO: namespace container-lifecycle-hook-8398 deletion completed in 15.913627563s

â€¢ [SLOW TEST:28.311 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:28:15.115: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-0576b16f-8aba-4fdf-be39-d04fd81219de
STEP: Creating a pod to test consume secrets
May 26 22:28:15.310: INFO: Waiting up to 5m0s for pod "pod-secrets-d39fb1fd-e667-46f9-822c-74c93eeddd33" in namespace "secrets-9539" to be "success or failure"
May 26 22:28:15.320: INFO: Pod "pod-secrets-d39fb1fd-e667-46f9-822c-74c93eeddd33": Phase="Pending", Reason="", readiness=false. Elapsed: 9.372159ms
May 26 22:28:17.331: INFO: Pod "pod-secrets-d39fb1fd-e667-46f9-822c-74c93eeddd33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020584608s
May 26 22:28:19.341: INFO: Pod "pod-secrets-d39fb1fd-e667-46f9-822c-74c93eeddd33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03075122s
STEP: Saw pod success
May 26 22:28:19.341: INFO: Pod "pod-secrets-d39fb1fd-e667-46f9-822c-74c93eeddd33" satisfied condition "success or failure"
May 26 22:28:19.350: INFO: Trying to get logs from node 10.215.60.34 pod pod-secrets-d39fb1fd-e667-46f9-822c-74c93eeddd33 container secret-volume-test: <nil>
STEP: delete the pod
May 26 22:28:19.420: INFO: Waiting for pod pod-secrets-d39fb1fd-e667-46f9-822c-74c93eeddd33 to disappear
May 26 22:28:19.429: INFO: Pod pod-secrets-d39fb1fd-e667-46f9-822c-74c93eeddd33 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:28:19.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9539" for this suite.
May 26 22:28:27.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:28:29.403: INFO: namespace secrets-9539 deletion completed in 9.96094517s

â€¢ [SLOW TEST:14.288 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:28:29.403: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 22:28:29.555: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 26 22:28:37.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 --namespace=crd-publish-openapi-9008 create -f -'
May 26 22:28:38.250: INFO: stderr: ""
May 26 22:28:38.250: INFO: stdout: "e2e-test-crd-publish-openapi-9238-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 26 22:28:38.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 --namespace=crd-publish-openapi-9008 delete e2e-test-crd-publish-openapi-9238-crds test-cr'
May 26 22:28:38.480: INFO: stderr: ""
May 26 22:28:38.480: INFO: stdout: "e2e-test-crd-publish-openapi-9238-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May 26 22:28:38.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 --namespace=crd-publish-openapi-9008 apply -f -'
May 26 22:28:38.984: INFO: stderr: ""
May 26 22:28:38.984: INFO: stdout: "e2e-test-crd-publish-openapi-9238-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 26 22:28:38.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 --namespace=crd-publish-openapi-9008 delete e2e-test-crd-publish-openapi-9238-crds test-cr'
May 26 22:28:39.205: INFO: stderr: ""
May 26 22:28:39.206: INFO: stdout: "e2e-test-crd-publish-openapi-9238-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 26 22:28:39.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 explain e2e-test-crd-publish-openapi-9238-crds'
May 26 22:28:39.818: INFO: stderr: ""
May 26 22:28:39.819: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9238-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:28:47.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9008" for this suite.
May 26 22:28:55.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:28:57.443: INFO: namespace crd-publish-openapi-9008 deletion completed in 9.890050203s

â€¢ [SLOW TEST:28.040 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:28:57.443: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:29:08.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5495" for this suite.
May 26 22:29:16.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:29:18.891: INFO: namespace resourcequota-5495 deletion completed in 10.108346002s

â€¢ [SLOW TEST:21.448 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:29:18.893: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May 26 22:29:19.077: INFO: Waiting up to 5m0s for pod "downward-api-713ba5cd-1785-4ef5-a2f8-1e25bf183008" in namespace "downward-api-5487" to be "success or failure"
May 26 22:29:19.086: INFO: Pod "downward-api-713ba5cd-1785-4ef5-a2f8-1e25bf183008": Phase="Pending", Reason="", readiness=false. Elapsed: 9.235444ms
May 26 22:29:21.096: INFO: Pod "downward-api-713ba5cd-1785-4ef5-a2f8-1e25bf183008": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01881323s
STEP: Saw pod success
May 26 22:29:21.096: INFO: Pod "downward-api-713ba5cd-1785-4ef5-a2f8-1e25bf183008" satisfied condition "success or failure"
May 26 22:29:21.104: INFO: Trying to get logs from node 10.215.60.34 pod downward-api-713ba5cd-1785-4ef5-a2f8-1e25bf183008 container dapi-container: <nil>
STEP: delete the pod
May 26 22:29:21.170: INFO: Waiting for pod downward-api-713ba5cd-1785-4ef5-a2f8-1e25bf183008 to disappear
May 26 22:29:21.179: INFO: Pod downward-api-713ba5cd-1785-4ef5-a2f8-1e25bf183008 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:29:21.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5487" for this suite.
May 26 22:29:29.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:29:31.100: INFO: namespace downward-api-5487 deletion completed in 9.901855318s

â€¢ [SLOW TEST:12.207 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:29:31.100: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-5184/configmap-test-4180c7b8-0f67-47ab-9773-98025b17b757
STEP: Creating a pod to test consume configMaps
May 26 22:29:31.305: INFO: Waiting up to 5m0s for pod "pod-configmaps-9bf7338f-b939-451d-ba18-4c160e2c7741" in namespace "configmap-5184" to be "success or failure"
May 26 22:29:31.315: INFO: Pod "pod-configmaps-9bf7338f-b939-451d-ba18-4c160e2c7741": Phase="Pending", Reason="", readiness=false. Elapsed: 10.358965ms
May 26 22:29:33.326: INFO: Pod "pod-configmaps-9bf7338f-b939-451d-ba18-4c160e2c7741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02096513s
STEP: Saw pod success
May 26 22:29:33.326: INFO: Pod "pod-configmaps-9bf7338f-b939-451d-ba18-4c160e2c7741" satisfied condition "success or failure"
May 26 22:29:33.336: INFO: Trying to get logs from node 10.215.60.34 pod pod-configmaps-9bf7338f-b939-451d-ba18-4c160e2c7741 container env-test: <nil>
STEP: delete the pod
May 26 22:29:33.395: INFO: Waiting for pod pod-configmaps-9bf7338f-b939-451d-ba18-4c160e2c7741 to disappear
May 26 22:29:33.404: INFO: Pod pod-configmaps-9bf7338f-b939-451d-ba18-4c160e2c7741 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:29:33.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5184" for this suite.
May 26 22:29:41.453: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:29:43.320: INFO: namespace configmap-5184 deletion completed in 9.901270276s

â€¢ [SLOW TEST:12.220 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:29:43.321: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
May 26 22:29:43.599: INFO: Waiting up to 5m0s for pod "client-containers-7252ddf1-9feb-4ef5-b441-75a020cbd738" in namespace "containers-2098" to be "success or failure"
May 26 22:29:43.614: INFO: Pod "client-containers-7252ddf1-9feb-4ef5-b441-75a020cbd738": Phase="Pending", Reason="", readiness=false. Elapsed: 15.50695ms
May 26 22:29:45.625: INFO: Pod "client-containers-7252ddf1-9feb-4ef5-b441-75a020cbd738": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026253017s
May 26 22:29:47.635: INFO: Pod "client-containers-7252ddf1-9feb-4ef5-b441-75a020cbd738": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036330702s
STEP: Saw pod success
May 26 22:29:47.635: INFO: Pod "client-containers-7252ddf1-9feb-4ef5-b441-75a020cbd738" satisfied condition "success or failure"
May 26 22:29:47.644: INFO: Trying to get logs from node 10.215.60.34 pod client-containers-7252ddf1-9feb-4ef5-b441-75a020cbd738 container test-container: <nil>
STEP: delete the pod
May 26 22:29:47.697: INFO: Waiting for pod client-containers-7252ddf1-9feb-4ef5-b441-75a020cbd738 to disappear
May 26 22:29:47.706: INFO: Pod client-containers-7252ddf1-9feb-4ef5-b441-75a020cbd738 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:29:47.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2098" for this suite.
May 26 22:29:55.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:29:57.608: INFO: namespace containers-2098 deletion completed in 9.887906897s

â€¢ [SLOW TEST:14.287 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:29:57.608: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-8788
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-8788
STEP: Deleting pre-stop pod
May 26 22:30:07.933: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:30:07.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-8788" for this suite.
May 26 22:30:46.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:30:47.919: INFO: namespace prestop-8788 deletion completed in 39.949839727s

â€¢ [SLOW TEST:50.311 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:30:47.921: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 22:30:50.292: INFO: Waiting up to 5m0s for pod "client-envvars-9ca691fb-e3a6-4e80-9a2a-f2f1193c0ef0" in namespace "pods-93" to be "success or failure"
May 26 22:30:50.304: INFO: Pod "client-envvars-9ca691fb-e3a6-4e80-9a2a-f2f1193c0ef0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.650758ms
May 26 22:30:52.326: INFO: Pod "client-envvars-9ca691fb-e3a6-4e80-9a2a-f2f1193c0ef0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033838901s
STEP: Saw pod success
May 26 22:30:52.326: INFO: Pod "client-envvars-9ca691fb-e3a6-4e80-9a2a-f2f1193c0ef0" satisfied condition "success or failure"
May 26 22:30:52.335: INFO: Trying to get logs from node 10.215.60.34 pod client-envvars-9ca691fb-e3a6-4e80-9a2a-f2f1193c0ef0 container env3cont: <nil>
STEP: delete the pod
May 26 22:30:52.401: INFO: Waiting for pod client-envvars-9ca691fb-e3a6-4e80-9a2a-f2f1193c0ef0 to disappear
May 26 22:30:52.414: INFO: Pod client-envvars-9ca691fb-e3a6-4e80-9a2a-f2f1193c0ef0 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:30:52.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-93" for this suite.
May 26 22:31:24.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:31:26.348: INFO: namespace pods-93 deletion completed in 33.917506993s

â€¢ [SLOW TEST:38.428 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:31:26.350: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 26 22:31:27.280: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
May 26 22:31:29.311: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726129087, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726129087, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726129087, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726129087, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 26 22:31:32.348: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 22:31:32.357: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:31:33.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8778" for this suite.
May 26 22:31:41.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:31:43.556: INFO: namespace crd-webhook-8778 deletion completed in 9.906363355s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:17.252 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:31:43.603: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 26 22:31:43.769: INFO: Waiting up to 5m0s for pod "pod-aed5c38b-7097-4d37-b8bc-6d6583dcd47f" in namespace "emptydir-4862" to be "success or failure"
May 26 22:31:43.782: INFO: Pod "pod-aed5c38b-7097-4d37-b8bc-6d6583dcd47f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.032583ms
May 26 22:31:45.792: INFO: Pod "pod-aed5c38b-7097-4d37-b8bc-6d6583dcd47f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023339888s
STEP: Saw pod success
May 26 22:31:45.792: INFO: Pod "pod-aed5c38b-7097-4d37-b8bc-6d6583dcd47f" satisfied condition "success or failure"
May 26 22:31:45.802: INFO: Trying to get logs from node 10.215.60.34 pod pod-aed5c38b-7097-4d37-b8bc-6d6583dcd47f container test-container: <nil>
STEP: delete the pod
May 26 22:31:45.864: INFO: Waiting for pod pod-aed5c38b-7097-4d37-b8bc-6d6583dcd47f to disappear
May 26 22:31:45.875: INFO: Pod pod-aed5c38b-7097-4d37-b8bc-6d6583dcd47f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:31:45.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4862" for this suite.
May 26 22:31:53.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:31:55.782: INFO: namespace emptydir-4862 deletion completed in 9.886625876s

â€¢ [SLOW TEST:12.179 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:31:55.782: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0526 22:32:35.997692      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 26 22:32:35.997: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:32:35.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-157" for this suite.
May 26 22:32:44.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:32:45.913: INFO: namespace gc-157 deletion completed in 9.902056492s

â€¢ [SLOW TEST:50.132 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:32:45.916: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
May 26 22:32:46.092: INFO: Waiting up to 5m0s for pod "var-expansion-aa52e900-490b-4805-9482-562c5695865f" in namespace "var-expansion-6473" to be "success or failure"
May 26 22:32:46.109: INFO: Pod "var-expansion-aa52e900-490b-4805-9482-562c5695865f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.965125ms
May 26 22:32:48.120: INFO: Pod "var-expansion-aa52e900-490b-4805-9482-562c5695865f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028392572s
May 26 22:32:50.131: INFO: Pod "var-expansion-aa52e900-490b-4805-9482-562c5695865f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038788488s
STEP: Saw pod success
May 26 22:32:50.131: INFO: Pod "var-expansion-aa52e900-490b-4805-9482-562c5695865f" satisfied condition "success or failure"
May 26 22:32:50.140: INFO: Trying to get logs from node 10.215.60.34 pod var-expansion-aa52e900-490b-4805-9482-562c5695865f container dapi-container: <nil>
STEP: delete the pod
May 26 22:32:50.196: INFO: Waiting for pod var-expansion-aa52e900-490b-4805-9482-562c5695865f to disappear
May 26 22:32:50.206: INFO: Pod var-expansion-aa52e900-490b-4805-9482-562c5695865f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:32:50.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6473" for this suite.
May 26 22:32:58.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:33:00.123: INFO: namespace var-expansion-6473 deletion completed in 9.901135368s

â€¢ [SLOW TEST:14.208 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:33:00.124: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-7cc4b81a-d8ce-4730-9af5-b966dee23caa
May 26 22:33:00.306: INFO: Pod name my-hostname-basic-7cc4b81a-d8ce-4730-9af5-b966dee23caa: Found 0 pods out of 1
May 26 22:33:05.317: INFO: Pod name my-hostname-basic-7cc4b81a-d8ce-4730-9af5-b966dee23caa: Found 1 pods out of 1
May 26 22:33:05.317: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-7cc4b81a-d8ce-4730-9af5-b966dee23caa" are running
May 26 22:33:05.327: INFO: Pod "my-hostname-basic-7cc4b81a-d8ce-4730-9af5-b966dee23caa-jgcss" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-26 22:33:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-26 22:33:03 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-26 22:33:03 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-26 22:33:01 +0000 UTC Reason: Message:}])
May 26 22:33:05.327: INFO: Trying to dial the pod
May 26 22:33:10.381: INFO: Controller my-hostname-basic-7cc4b81a-d8ce-4730-9af5-b966dee23caa: Got expected result from replica 1 [my-hostname-basic-7cc4b81a-d8ce-4730-9af5-b966dee23caa-jgcss]: "my-hostname-basic-7cc4b81a-d8ce-4730-9af5-b966dee23caa-jgcss", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:33:10.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2673" for this suite.
May 26 22:33:18.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:33:20.298: INFO: namespace replication-controller-2673 deletion completed in 9.899733735s

â€¢ [SLOW TEST:20.174 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:33:20.298: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
May 26 22:33:20.411: INFO: namespace kubectl-4590
May 26 22:33:20.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 create -f - --namespace=kubectl-4590'
May 26 22:33:21.017: INFO: stderr: ""
May 26 22:33:21.017: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 26 22:33:22.028: INFO: Selector matched 1 pods for map[app:redis]
May 26 22:33:22.028: INFO: Found 0 / 1
May 26 22:33:23.027: INFO: Selector matched 1 pods for map[app:redis]
May 26 22:33:23.027: INFO: Found 1 / 1
May 26 22:33:23.028: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 26 22:33:23.036: INFO: Selector matched 1 pods for map[app:redis]
May 26 22:33:23.036: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 26 22:33:23.036: INFO: wait on redis-master startup in kubectl-4590 
May 26 22:33:23.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 logs redis-master-6qpt4 redis-master --namespace=kubectl-4590'
May 26 22:33:23.230: INFO: stderr: ""
May 26 22:33:23.230: INFO: stdout: "1:C 26 May 2020 22:33:22.287 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 26 May 2020 22:33:22.287 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 26 May 2020 22:33:22.287 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 26 May 2020 22:33:22.290 * Running mode=standalone, port=6379.\n1:M 26 May 2020 22:33:22.291 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 26 May 2020 22:33:22.291 # Server initialized\n1:M 26 May 2020 22:33:22.291 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 26 May 2020 22:33:22.291 * Ready to accept connections\n"
STEP: exposing RC
May 26 22:33:23.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-4590'
May 26 22:33:23.415: INFO: stderr: ""
May 26 22:33:23.415: INFO: stdout: "service/rm2 exposed\n"
May 26 22:33:23.422: INFO: Service rm2 in namespace kubectl-4590 found.
STEP: exposing service
May 26 22:33:25.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-4590'
May 26 22:33:25.637: INFO: stderr: ""
May 26 22:33:25.637: INFO: stdout: "service/rm3 exposed\n"
May 26 22:33:25.645: INFO: Service rm3 in namespace kubectl-4590 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:33:27.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4590" for this suite.
May 26 22:33:59.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:34:01.573: INFO: namespace kubectl-4590 deletion completed in 33.896049931s

â€¢ [SLOW TEST:41.275 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:34:01.573: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
May 26 22:34:01.781: INFO: Waiting up to 5m0s for pod "pod-78d1bd56-8959-4f0b-b3e7-bf9d5155fe68" in namespace "emptydir-7329" to be "success or failure"
May 26 22:34:01.795: INFO: Pod "pod-78d1bd56-8959-4f0b-b3e7-bf9d5155fe68": Phase="Pending", Reason="", readiness=false. Elapsed: 13.599076ms
May 26 22:34:03.804: INFO: Pod "pod-78d1bd56-8959-4f0b-b3e7-bf9d5155fe68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022796116s
STEP: Saw pod success
May 26 22:34:03.804: INFO: Pod "pod-78d1bd56-8959-4f0b-b3e7-bf9d5155fe68" satisfied condition "success or failure"
May 26 22:34:03.813: INFO: Trying to get logs from node 10.215.60.34 pod pod-78d1bd56-8959-4f0b-b3e7-bf9d5155fe68 container test-container: <nil>
STEP: delete the pod
May 26 22:34:03.874: INFO: Waiting for pod pod-78d1bd56-8959-4f0b-b3e7-bf9d5155fe68 to disappear
May 26 22:34:03.883: INFO: Pod pod-78d1bd56-8959-4f0b-b3e7-bf9d5155fe68 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:34:03.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7329" for this suite.
May 26 22:34:11.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:34:13.783: INFO: namespace emptydir-7329 deletion completed in 9.886665748s

â€¢ [SLOW TEST:12.210 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:34:13.783: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 26 22:34:14.581: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 26 22:34:16.609: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726129254, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726129254, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726129254, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726129254, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 26 22:34:19.647: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 22:34:19.656: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:34:21.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6068" for this suite.
May 26 22:34:29.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:34:30.935: INFO: namespace webhook-6068 deletion completed in 9.886315564s
STEP: Destroying namespace "webhook-6068-markers" for this suite.
May 26 22:34:38.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:34:40.820: INFO: namespace webhook-6068-markers deletion completed in 9.885409258s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:27.084 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:34:40.868: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 26 22:34:41.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-4747'
May 26 22:34:41.177: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 26 22:34:41.177: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
May 26 22:34:41.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 delete jobs e2e-test-httpd-job --namespace=kubectl-4747'
May 26 22:34:41.353: INFO: stderr: ""
May 26 22:34:41.353: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:34:41.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4747" for this suite.
May 26 22:34:49.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:34:51.262: INFO: namespace kubectl-4747 deletion completed in 9.890326123s

â€¢ [SLOW TEST:10.394 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:34:51.262: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 26 22:34:51.449: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9f0554aa-140a-45b3-a4f3-4a65f37e5a59" in namespace "downward-api-1166" to be "success or failure"
May 26 22:34:51.459: INFO: Pod "downwardapi-volume-9f0554aa-140a-45b3-a4f3-4a65f37e5a59": Phase="Pending", Reason="", readiness=false. Elapsed: 10.241544ms
May 26 22:34:53.469: INFO: Pod "downwardapi-volume-9f0554aa-140a-45b3-a4f3-4a65f37e5a59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020086862s
STEP: Saw pod success
May 26 22:34:53.469: INFO: Pod "downwardapi-volume-9f0554aa-140a-45b3-a4f3-4a65f37e5a59" satisfied condition "success or failure"
May 26 22:34:53.477: INFO: Trying to get logs from node 10.215.60.34 pod downwardapi-volume-9f0554aa-140a-45b3-a4f3-4a65f37e5a59 container client-container: <nil>
STEP: delete the pod
May 26 22:34:53.542: INFO: Waiting for pod downwardapi-volume-9f0554aa-140a-45b3-a4f3-4a65f37e5a59 to disappear
May 26 22:34:53.553: INFO: Pod downwardapi-volume-9f0554aa-140a-45b3-a4f3-4a65f37e5a59 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:34:53.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1166" for this suite.
May 26 22:35:01.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:35:03.482: INFO: namespace downward-api-1166 deletion completed in 9.911543059s

â€¢ [SLOW TEST:12.220 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:35:03.482: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:36:03.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2596" for this suite.
May 26 22:36:33.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:36:35.649: INFO: namespace container-probe-2596 deletion completed in 31.887581033s

â€¢ [SLOW TEST:92.167 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:36:35.649: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 26 22:36:35.797: INFO: Waiting up to 5m0s for pod "downwardapi-volume-17440d84-3227-448a-a0d6-15ba6925b14b" in namespace "projected-9508" to be "success or failure"
May 26 22:36:35.807: INFO: Pod "downwardapi-volume-17440d84-3227-448a-a0d6-15ba6925b14b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.364081ms
May 26 22:36:37.817: INFO: Pod "downwardapi-volume-17440d84-3227-448a-a0d6-15ba6925b14b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019063519s
STEP: Saw pod success
May 26 22:36:37.817: INFO: Pod "downwardapi-volume-17440d84-3227-448a-a0d6-15ba6925b14b" satisfied condition "success or failure"
May 26 22:36:37.826: INFO: Trying to get logs from node 10.215.60.34 pod downwardapi-volume-17440d84-3227-448a-a0d6-15ba6925b14b container client-container: <nil>
STEP: delete the pod
May 26 22:36:37.912: INFO: Waiting for pod downwardapi-volume-17440d84-3227-448a-a0d6-15ba6925b14b to disappear
May 26 22:36:37.920: INFO: Pod downwardapi-volume-17440d84-3227-448a-a0d6-15ba6925b14b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:36:37.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9508" for this suite.
May 26 22:36:45.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:36:47.822: INFO: namespace projected-9508 deletion completed in 9.884772731s

â€¢ [SLOW TEST:12.173 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:36:47.824: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 22:36:47.959: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 26 22:36:55.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 --namespace=crd-publish-openapi-6650 create -f -'
May 26 22:36:56.466: INFO: stderr: ""
May 26 22:36:56.466: INFO: stdout: "e2e-test-crd-publish-openapi-7011-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 26 22:36:56.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 --namespace=crd-publish-openapi-6650 delete e2e-test-crd-publish-openapi-7011-crds test-cr'
May 26 22:36:56.632: INFO: stderr: ""
May 26 22:36:56.632: INFO: stdout: "e2e-test-crd-publish-openapi-7011-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May 26 22:36:56.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 --namespace=crd-publish-openapi-6650 apply -f -'
May 26 22:36:57.169: INFO: stderr: ""
May 26 22:36:57.169: INFO: stdout: "e2e-test-crd-publish-openapi-7011-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 26 22:36:57.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 --namespace=crd-publish-openapi-6650 delete e2e-test-crd-publish-openapi-7011-crds test-cr'
May 26 22:36:57.341: INFO: stderr: ""
May 26 22:36:57.341: INFO: stdout: "e2e-test-crd-publish-openapi-7011-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 26 22:36:57.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 explain e2e-test-crd-publish-openapi-7011-crds'
May 26 22:36:57.914: INFO: stderr: ""
May 26 22:36:57.914: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7011-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:37:05.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6650" for this suite.
May 26 22:37:13.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:37:15.529: INFO: namespace crd-publish-openapi-6650 deletion completed in 9.899016761s

â€¢ [SLOW TEST:27.705 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:37:15.529: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 22:37:15.695: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-5a2441d1-cf30-4051-b4b4-00e9ddf2deaf" in namespace "security-context-test-5525" to be "success or failure"
May 26 22:37:15.706: INFO: Pod "alpine-nnp-false-5a2441d1-cf30-4051-b4b4-00e9ddf2deaf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.894192ms
May 26 22:37:17.716: INFO: Pod "alpine-nnp-false-5a2441d1-cf30-4051-b4b4-00e9ddf2deaf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021068464s
May 26 22:37:19.725: INFO: Pod "alpine-nnp-false-5a2441d1-cf30-4051-b4b4-00e9ddf2deaf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030390479s
May 26 22:37:21.735: INFO: Pod "alpine-nnp-false-5a2441d1-cf30-4051-b4b4-00e9ddf2deaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039884487s
May 26 22:37:21.735: INFO: Pod "alpine-nnp-false-5a2441d1-cf30-4051-b4b4-00e9ddf2deaf" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:37:21.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5525" for this suite.
May 26 22:37:29.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:37:31.660: INFO: namespace security-context-test-5525 deletion completed in 9.886804957s

â€¢ [SLOW TEST:16.131 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:37:31.663: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 26 22:37:31.823: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2e5df546-284f-4d1a-a67d-7364234c92c8" in namespace "projected-7314" to be "success or failure"
May 26 22:37:31.836: INFO: Pod "downwardapi-volume-2e5df546-284f-4d1a-a67d-7364234c92c8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.93308ms
May 26 22:37:33.847: INFO: Pod "downwardapi-volume-2e5df546-284f-4d1a-a67d-7364234c92c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023985344s
STEP: Saw pod success
May 26 22:37:33.847: INFO: Pod "downwardapi-volume-2e5df546-284f-4d1a-a67d-7364234c92c8" satisfied condition "success or failure"
May 26 22:37:33.856: INFO: Trying to get logs from node 10.215.60.34 pod downwardapi-volume-2e5df546-284f-4d1a-a67d-7364234c92c8 container client-container: <nil>
STEP: delete the pod
May 26 22:37:33.911: INFO: Waiting for pod downwardapi-volume-2e5df546-284f-4d1a-a67d-7364234c92c8 to disappear
May 26 22:37:33.921: INFO: Pod downwardapi-volume-2e5df546-284f-4d1a-a67d-7364234c92c8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:37:33.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7314" for this suite.
May 26 22:37:41.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:37:43.831: INFO: namespace projected-7314 deletion completed in 9.897525917s

â€¢ [SLOW TEST:12.169 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:37:43.835: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
May 26 22:37:44.015: INFO: Waiting up to 5m0s for pod "client-containers-1b53610d-878d-4b5f-9887-9957698e5ab8" in namespace "containers-2163" to be "success or failure"
May 26 22:37:44.025: INFO: Pod "client-containers-1b53610d-878d-4b5f-9887-9957698e5ab8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.055178ms
May 26 22:37:46.034: INFO: Pod "client-containers-1b53610d-878d-4b5f-9887-9957698e5ab8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01967286s
May 26 22:37:48.044: INFO: Pod "client-containers-1b53610d-878d-4b5f-9887-9957698e5ab8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029146296s
STEP: Saw pod success
May 26 22:37:48.044: INFO: Pod "client-containers-1b53610d-878d-4b5f-9887-9957698e5ab8" satisfied condition "success or failure"
May 26 22:37:48.054: INFO: Trying to get logs from node 10.215.60.34 pod client-containers-1b53610d-878d-4b5f-9887-9957698e5ab8 container test-container: <nil>
STEP: delete the pod
May 26 22:37:48.103: INFO: Waiting for pod client-containers-1b53610d-878d-4b5f-9887-9957698e5ab8 to disappear
May 26 22:37:48.113: INFO: Pod client-containers-1b53610d-878d-4b5f-9887-9957698e5ab8 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:37:48.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2163" for this suite.
May 26 22:37:56.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:37:58.160: INFO: namespace containers-2163 deletion completed in 10.033343079s

â€¢ [SLOW TEST:14.326 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:37:58.160: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 26 22:38:00.430: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:38:00.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8008" for this suite.
May 26 22:38:08.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:38:10.460: INFO: namespace container-runtime-8008 deletion completed in 9.92277952s

â€¢ [SLOW TEST:12.300 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:38:10.460: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-307
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-307
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-307
May 26 22:38:10.691: INFO: Found 0 stateful pods, waiting for 1
May 26 22:38:20.701: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 26 22:38:20.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-307 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 26 22:38:21.072: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 26 22:38:21.072: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 26 22:38:21.072: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 26 22:38:21.090: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 26 22:38:31.101: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 26 22:38:31.101: INFO: Waiting for statefulset status.replicas updated to 0
May 26 22:38:31.145: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998004s
May 26 22:38:32.170: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.9878072s
May 26 22:38:33.181: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.962124835s
May 26 22:38:34.192: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.951870075s
May 26 22:38:35.204: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.940298672s
May 26 22:38:36.215: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.928298367s
May 26 22:38:37.226: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.91729649s
May 26 22:38:38.237: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.906394748s
May 26 22:38:39.249: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.895492167s
May 26 22:38:40.260: INFO: Verifying statefulset ss doesn't scale past 1 for another 883.683074ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-307
May 26 22:38:41.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-307 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 22:38:41.646: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 26 22:38:41.646: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 26 22:38:41.646: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 26 22:38:41.656: INFO: Found 1 stateful pods, waiting for 3
May 26 22:38:51.667: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 26 22:38:51.667: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 26 22:38:51.667: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 26 22:38:51.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-307 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 26 22:38:51.997: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 26 22:38:51.997: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 26 22:38:51.997: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 26 22:38:51.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-307 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 26 22:38:52.324: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 26 22:38:52.324: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 26 22:38:52.324: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 26 22:38:52.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-307 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 26 22:38:52.680: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 26 22:38:52.680: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 26 22:38:52.680: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 26 22:38:52.680: INFO: Waiting for statefulset status.replicas updated to 0
May 26 22:38:52.689: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
May 26 22:39:02.708: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 26 22:39:02.708: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 26 22:39:02.708: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 26 22:39:02.745: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999997888s
May 26 22:39:03.755: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.982750478s
May 26 22:39:04.767: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.97220783s
May 26 22:39:05.779: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.960848389s
May 26 22:39:06.795: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.94381758s
May 26 22:39:07.805: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.932687594s
May 26 22:39:08.816: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.922781427s
May 26 22:39:09.827: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.911477926s
May 26 22:39:10.837: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.900876018s
May 26 22:39:11.848: INFO: Verifying statefulset ss doesn't scale past 3 for another 889.839051ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-307
May 26 22:39:12.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-307 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 22:39:13.199: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 26 22:39:13.199: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 26 22:39:13.199: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 26 22:39:13.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-307 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 22:39:13.529: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 26 22:39:13.529: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 26 22:39:13.529: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 26 22:39:13.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-307 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 22:39:13.878: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 26 22:39:13.878: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 26 22:39:13.878: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 26 22:39:13.878: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May 26 22:39:33.923: INFO: Deleting all statefulset in ns statefulset-307
May 26 22:39:33.931: INFO: Scaling statefulset ss to 0
May 26 22:39:33.963: INFO: Waiting for statefulset status.replicas updated to 0
May 26 22:39:33.974: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:39:34.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-307" for this suite.
May 26 22:39:42.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:39:43.933: INFO: namespace statefulset-307 deletion completed in 9.905313645s

â€¢ [SLOW TEST:93.473 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:39:43.933: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 26 22:39:44.141: INFO: Waiting up to 5m0s for pod "downwardapi-volume-59822fe2-3eb7-4ee8-92c2-a5812fef28a7" in namespace "projected-8398" to be "success or failure"
May 26 22:39:44.153: INFO: Pod "downwardapi-volume-59822fe2-3eb7-4ee8-92c2-a5812fef28a7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.431844ms
May 26 22:39:46.163: INFO: Pod "downwardapi-volume-59822fe2-3eb7-4ee8-92c2-a5812fef28a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020936115s
STEP: Saw pod success
May 26 22:39:46.163: INFO: Pod "downwardapi-volume-59822fe2-3eb7-4ee8-92c2-a5812fef28a7" satisfied condition "success or failure"
May 26 22:39:46.173: INFO: Trying to get logs from node 10.215.60.34 pod downwardapi-volume-59822fe2-3eb7-4ee8-92c2-a5812fef28a7 container client-container: <nil>
STEP: delete the pod
May 26 22:39:46.252: INFO: Waiting for pod downwardapi-volume-59822fe2-3eb7-4ee8-92c2-a5812fef28a7 to disappear
May 26 22:39:46.261: INFO: Pod downwardapi-volume-59822fe2-3eb7-4ee8-92c2-a5812fef28a7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:39:46.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8398" for this suite.
May 26 22:39:54.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:39:56.162: INFO: namespace projected-8398 deletion completed in 9.887145455s

â€¢ [SLOW TEST:12.229 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:39:56.162: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 22:39:56.284: INFO: Creating deployment "test-recreate-deployment"
May 26 22:39:56.303: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 26 22:39:56.325: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 26 22:39:58.344: INFO: Waiting deployment "test-recreate-deployment" to complete
May 26 22:39:58.352: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 26 22:39:58.372: INFO: Updating deployment test-recreate-deployment
May 26 22:39:58.373: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May 26 22:39:58.552: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-8266 /apis/apps/v1/namespaces/deployment-8266/deployments/test-recreate-deployment 8256931e-5f63-431f-a0d0-cd89c183d01e 73780 2 2020-05-26 22:39:56 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004ddf1c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-05-26 22:39:58 +0000 UTC,LastTransitionTime:2020-05-26 22:39:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-05-26 22:39:58 +0000 UTC,LastTransitionTime:2020-05-26 22:39:56 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May 26 22:39:58.564: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-8266 /apis/apps/v1/namespaces/deployment-8266/replicasets/test-recreate-deployment-5f94c574ff 62c12e43-0c9a-4bf3-ba47-42a5d1406cae 73778 1 2020-05-26 22:39:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 8256931e-5f63-431f-a0d0-cd89c183d01e 0xc0083836a7 0xc0083836a8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc008383708 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 26 22:39:58.564: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 26 22:39:58.565: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-8266 /apis/apps/v1/namespaces/deployment-8266/replicasets/test-recreate-deployment-68fc85c7bb ba32b178-ae06-4c6b-8157-9333d448d357 73769 2 2020-05-26 22:39:56 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 8256931e-5f63-431f-a0d0-cd89c183d01e 0xc008383777 0xc008383778}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0083837d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 26 22:39:58.576: INFO: Pod "test-recreate-deployment-5f94c574ff-skl88" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-skl88 test-recreate-deployment-5f94c574ff- deployment-8266 /api/v1/namespaces/deployment-8266/pods/test-recreate-deployment-5f94c574ff-skl88 bdab7769-b40d-4e9a-99d3-882761e87956 73781 0 2020-05-26 22:39:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 62c12e43-0c9a-4bf3-ba47-42a5d1406cae 0xc008383c57 0xc008383c58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pdhmb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pdhmb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pdhmb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.34,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-mnwqs,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:39:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:39:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:39:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:39:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.34,PodIP:,StartTime:2020-05-26 22:39:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:39:58.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8266" for this suite.
May 26 22:40:06.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:40:08.525: INFO: namespace deployment-8266 deletion completed in 9.930692371s

â€¢ [SLOW TEST:12.363 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:40:08.526: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May 26 22:40:08.707: INFO: Waiting up to 5m0s for pod "downward-api-ec2fb222-605f-4bbf-a23d-faedb409dab5" in namespace "downward-api-6059" to be "success or failure"
May 26 22:40:08.719: INFO: Pod "downward-api-ec2fb222-605f-4bbf-a23d-faedb409dab5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.225487ms
May 26 22:40:10.746: INFO: Pod "downward-api-ec2fb222-605f-4bbf-a23d-faedb409dab5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039212324s
May 26 22:40:12.756: INFO: Pod "downward-api-ec2fb222-605f-4bbf-a23d-faedb409dab5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04979075s
STEP: Saw pod success
May 26 22:40:12.756: INFO: Pod "downward-api-ec2fb222-605f-4bbf-a23d-faedb409dab5" satisfied condition "success or failure"
May 26 22:40:12.767: INFO: Trying to get logs from node 10.215.60.34 pod downward-api-ec2fb222-605f-4bbf-a23d-faedb409dab5 container dapi-container: <nil>
STEP: delete the pod
May 26 22:40:12.830: INFO: Waiting for pod downward-api-ec2fb222-605f-4bbf-a23d-faedb409dab5 to disappear
May 26 22:40:12.840: INFO: Pod downward-api-ec2fb222-605f-4bbf-a23d-faedb409dab5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:40:12.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6059" for this suite.
May 26 22:40:20.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:40:22.753: INFO: namespace downward-api-6059 deletion completed in 9.886671315s

â€¢ [SLOW TEST:14.227 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:40:22.754: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:40:23.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2363" for this suite.
May 26 22:40:31.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:40:32.943: INFO: namespace kubelet-test-2363 deletion completed in 9.911919781s

â€¢ [SLOW TEST:10.189 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:40:32.945: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 22:40:33.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 version'
May 26 22:40:33.238: INFO: stderr: ""
May 26 22:40:33.238: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.2\", GitCommit:\"c97fe5036ef3df2967d086711e6c0c405941e14b\", GitTreeState:\"clean\", BuildDate:\"2019-10-15T19:18:23Z\", GoVersion:\"go1.12.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16+\", GitVersion:\"v1.16.2\", GitCommit:\"a3ec9df\", GitTreeState:\"clean\", BuildDate:\"2020-05-04T12:54:43Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:40:33.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9221" for this suite.
May 26 22:40:41.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:40:43.160: INFO: namespace kubectl-9221 deletion completed in 9.902426323s

â€¢ [SLOW TEST:10.215 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:40:43.161: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-2139
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 26 22:40:43.294: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 26 22:41:05.636: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.99.175 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2139 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 26 22:41:05.636: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 22:41:06.839: INFO: Found all expected endpoints: [netserver-0]
May 26 22:41:06.848: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.141.152 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2139 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 26 22:41:06.848: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 22:41:08.043: INFO: Found all expected endpoints: [netserver-1]
May 26 22:41:08.052: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.182.103 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2139 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 26 22:41:08.052: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 22:41:09.248: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:41:09.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2139" for this suite.
May 26 22:41:17.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:41:19.171: INFO: namespace pod-network-test-2139 deletion completed in 9.906726856s

â€¢ [SLOW TEST:36.010 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:41:19.171: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May 26 22:41:19.320: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 26 22:41:19.372: INFO: Waiting for terminating namespaces to be deleted...
May 26 22:41:19.387: INFO: 
Logging pods the kubelet thinks is on node 10.215.60.10 before test
May 26 22:41:19.492: INFO: multus-admission-controller-4kd9c from openshift-multus started at 2020-05-26 19:59:01 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container multus-admission-controller ready: true, restart count 0
May 26 22:41:19.492: INFO: console-operator-56c4d6445c-brk5r from openshift-console-operator started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container console-operator ready: true, restart count 1
May 26 22:41:19.492: INFO: tuned-22gmz from openshift-cluster-node-tuning-operator started at 2020-05-26 19:59:39 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container tuned ready: true, restart count 0
May 26 22:41:19.492: INFO: prometheus-adapter-5646d5d5dd-ql9h7 from openshift-monitoring started at 2020-05-26 20:05:35 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 26 22:41:19.492: INFO: registry-pvc-permissions-j5wnp from openshift-image-registry started at 2020-05-26 20:03:30 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container pvc-permissions ready: false, restart count 0
May 26 22:41:19.492: INFO: ibm-keepalived-watcher-bl295 from kube-system started at 2020-05-26 19:58:00 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container keepalived-watcher ready: true, restart count 0
May 26 22:41:19.492: INFO: downloads-8479fbbf57-qhklc from openshift-console started at 2020-05-26 19:59:01 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container download-server ready: true, restart count 0
May 26 22:41:19.492: INFO: calico-node-p2dwh from calico-system started at 2020-05-26 19:58:46 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container calico-node ready: true, restart count 0
May 26 22:41:19.492: INFO: ibm-file-plugin-5999bd7d7d-mcpqg from kube-system started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
May 26 22:41:19.492: INFO: cluster-monitoring-operator-64b79969cc-g94mm from openshift-monitoring started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
May 26 22:41:19.492: INFO: dns-default-s9jv4 from openshift-dns started at 2020-05-26 20:00:40 +0000 UTC (2 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container dns ready: true, restart count 0
May 26 22:41:19.492: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 26 22:41:19.492: INFO: sonobuoy-systemd-logs-daemon-set-e1fd62b7d6894c57-wrl9n from sonobuoy started at 2020-05-26 21:25:54 +0000 UTC (2 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 26 22:41:19.492: INFO: 	Container systemd-logs ready: true, restart count 0
May 26 22:41:19.492: INFO: node-exporter-b6pqt from openshift-monitoring started at 2020-05-26 19:59:35 +0000 UTC (2 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 22:41:19.492: INFO: 	Container node-exporter ready: true, restart count 0
May 26 22:41:19.492: INFO: apiservice-cabundle-injector-5c88555f6d-r7p9b from openshift-service-ca started at 2020-05-26 19:59:45 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
May 26 22:41:19.492: INFO: redhat-operators-7b74fd75b7-8dvbx from openshift-marketplace started at 2020-05-26 20:01:34 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container redhat-operators ready: true, restart count 0
May 26 22:41:19.492: INFO: cluster-image-registry-operator-6f78cddbbc-qrdmf from openshift-image-registry started at 2020-05-26 19:59:03 +0000 UTC (2 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
May 26 22:41:19.492: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
May 26 22:41:19.492: INFO: openshift-service-catalog-apiserver-operator-5897998845-kpk9m from openshift-service-catalog-apiserver-operator started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container operator ready: true, restart count 1
May 26 22:41:19.492: INFO: calico-typha-f4f4dbb8c-s262v from calico-system started at 2020-05-26 20:00:43 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container calico-typha ready: true, restart count 0
May 26 22:41:19.492: INFO: multus-lh654 from openshift-multus started at 2020-05-26 19:58:00 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container kube-multus ready: true, restart count 0
May 26 22:41:19.492: INFO: olm-operator-9d9b9dc65-rg7n5 from openshift-operator-lifecycle-manager started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container olm-operator ready: true, restart count 0
May 26 22:41:19.492: INFO: node-ca-q2fgf from openshift-image-registry started at 2020-05-26 20:00:40 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container node-ca ready: true, restart count 0
May 26 22:41:19.492: INFO: certified-operators-7c7bd497cb-hjpfx from openshift-marketplace started at 2020-05-26 20:01:33 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container certified-operators ready: true, restart count 0
May 26 22:41:19.492: INFO: catalog-operator-6956d96f67-5xvls from openshift-operator-lifecycle-manager started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container catalog-operator ready: true, restart count 0
May 26 22:41:19.492: INFO: ingress-operator-f6594bf4d-m74mj from openshift-ingress-operator started at 2020-05-26 19:59:03 +0000 UTC (2 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container ingress-operator ready: true, restart count 0
May 26 22:41:19.492: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 22:41:19.492: INFO: thanos-querier-7f4dfb8d6f-5m6ft from openshift-monitoring started at 2020-05-26 20:05:38 +0000 UTC (4 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 22:41:19.492: INFO: 	Container oauth-proxy ready: true, restart count 0
May 26 22:41:19.492: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 26 22:41:19.492: INFO: 	Container thanos-querier ready: true, restart count 0
May 26 22:41:19.492: INFO: calico-kube-controllers-599969f895-xfmxb from calico-system started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 26 22:41:19.492: INFO: ibmcloud-block-storage-driver-sncrt from kube-system started at 2020-05-26 19:58:07 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May 26 22:41:19.492: INFO: cluster-node-tuning-operator-58cb5999f5-dqqxl from openshift-cluster-node-tuning-operator started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
May 26 22:41:19.492: INFO: marketplace-operator-6d86c46f6b-j7hh9 from openshift-marketplace started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container marketplace-operator ready: true, restart count 0
May 26 22:41:19.492: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-05-26 20:05:48 +0000 UTC (7 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 22:41:19.492: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 26 22:41:19.492: INFO: 	Container prometheus ready: true, restart count 1
May 26 22:41:19.492: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May 26 22:41:19.492: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 26 22:41:19.492: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May 26 22:41:19.492: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 26 22:41:19.492: INFO: ibm-storage-watcher-5f6b5dbcd4-kn4xs from kube-system started at 2020-05-26 19:59:02 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
May 26 22:41:19.492: INFO: ibmcloud-block-storage-plugin-9d877d7bc-kjrsl from kube-system started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
May 26 22:41:19.492: INFO: dns-operator-c68448f89-lfqlv from openshift-dns-operator started at 2020-05-26 19:59:02 +0000 UTC (2 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container dns-operator ready: true, restart count 0
May 26 22:41:19.492: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 22:41:19.492: INFO: openshift-service-catalog-controller-manager-operator-86b49fb6n from openshift-service-catalog-controller-manager-operator started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container operator ready: true, restart count 1
May 26 22:41:19.492: INFO: grafana-5648b7fdd9-rmjbk from openshift-monitoring started at 2020-05-26 21:51:25 +0000 UTC (2 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container grafana ready: true, restart count 0
May 26 22:41:19.492: INFO: 	Container grafana-proxy ready: true, restart count 0
May 26 22:41:19.492: INFO: downloads-8479fbbf57-z9pkl from openshift-console started at 2020-05-26 19:59:01 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container download-server ready: true, restart count 0
May 26 22:41:19.492: INFO: service-ca-operator-7bb6cf7fbc-j7nvb from openshift-service-ca-operator started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container operator ready: true, restart count 0
May 26 22:41:19.492: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-05-26 20:05:06 +0000 UTC (3 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container alertmanager ready: true, restart count 0
May 26 22:41:19.492: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 26 22:41:19.492: INFO: 	Container config-reloader ready: true, restart count 0
May 26 22:41:19.492: INFO: router-default-6cfd8db4bf-7dstv from openshift-ingress started at 2020-05-26 21:51:25 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container router ready: true, restart count 0
May 26 22:41:19.492: INFO: packageserver-7d5f79c4db-89cw9 from openshift-operator-lifecycle-manager started at 2020-05-26 21:51:35 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container packageserver ready: true, restart count 0
May 26 22:41:19.492: INFO: cluster-storage-operator-8696454489-8wttw from openshift-cluster-storage-operator started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container cluster-storage-operator ready: true, restart count 0
May 26 22:41:19.492: INFO: ibm-master-proxy-static-10.215.60.10 from kube-system started at 2020-05-26 19:57:58 +0000 UTC (2 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May 26 22:41:19.492: INFO: 	Container pause ready: true, restart count 0
May 26 22:41:19.492: INFO: openshift-kube-proxy-ns7pk from openshift-kube-proxy started at 2020-05-26 19:58:04 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container kube-proxy ready: true, restart count 0
May 26 22:41:19.492: INFO: ibm-cloud-provider-ip-159-122-104-134-84fdcfb5c5-cgj49 from ibm-system started at 2020-05-26 20:02:23 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.492: INFO: 	Container ibm-cloud-provider-ip-159-122-104-134 ready: true, restart count 0
May 26 22:41:19.492: INFO: 
Logging pods the kubelet thinks is on node 10.215.60.11 before test
May 26 22:41:19.578: INFO: ibm-keepalived-watcher-w8tv5 from kube-system started at 2020-05-26 19:57:34 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container keepalived-watcher ready: true, restart count 0
May 26 22:41:19.578: INFO: openshift-kube-proxy-h4l9g from openshift-kube-proxy started at 2020-05-26 19:58:05 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container kube-proxy ready: true, restart count 0
May 26 22:41:19.578: INFO: community-operators-674864f5c4-bwwnn from openshift-marketplace started at 2020-05-26 20:01:32 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container community-operators ready: true, restart count 0
May 26 22:41:19.578: INFO: router-default-6cfd8db4bf-jv7v4 from openshift-ingress started at 2020-05-26 20:01:45 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container router ready: true, restart count 0
May 26 22:41:19.578: INFO: multus-admission-controller-mqrcn from openshift-multus started at 2020-05-26 19:59:03 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container multus-admission-controller ready: true, restart count 0
May 26 22:41:19.578: INFO: node-exporter-spjk9 from openshift-monitoring started at 2020-05-26 19:59:36 +0000 UTC (2 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 22:41:19.578: INFO: 	Container node-exporter ready: true, restart count 0
May 26 22:41:19.578: INFO: calico-typha-f4f4dbb8c-jg59b from calico-system started at 2020-05-26 20:00:43 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container calico-typha ready: true, restart count 0
May 26 22:41:19.578: INFO: calico-node-6tv2z from calico-system started at 2020-05-26 19:58:46 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container calico-node ready: true, restart count 0
May 26 22:41:19.578: INFO: dns-default-jlw4t from openshift-dns started at 2020-05-26 20:00:40 +0000 UTC (2 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container dns ready: true, restart count 0
May 26 22:41:19.578: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 26 22:41:19.578: INFO: network-operator-76d6fbdbb8-5hgl8 from openshift-network-operator started at 2020-05-26 19:57:36 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container network-operator ready: true, restart count 0
May 26 22:41:19.578: INFO: kube-state-metrics-7498bc479d-5pbn8 from openshift-monitoring started at 2020-05-26 19:59:34 +0000 UTC (3 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 26 22:41:19.578: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 26 22:41:19.578: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 26 22:41:19.578: INFO: sonobuoy-systemd-logs-daemon-set-e1fd62b7d6894c57-gzfvj from sonobuoy started at 2020-05-26 21:25:54 +0000 UTC (2 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 26 22:41:19.578: INFO: 	Container systemd-logs ready: true, restart count 0
May 26 22:41:19.578: INFO: ibm-cloud-provider-ip-159-122-104-134-84fdcfb5c5-l5b9t from ibm-system started at 2020-05-26 21:51:25 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container ibm-cloud-provider-ip-159-122-104-134 ready: true, restart count 0
May 26 22:41:19.578: INFO: ibm-master-proxy-static-10.215.60.11 from kube-system started at 2020-05-26 19:57:32 +0000 UTC (2 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May 26 22:41:19.578: INFO: 	Container pause ready: true, restart count 0
May 26 22:41:19.578: INFO: tigera-operator-798cfbf7dd-q5frt from tigera-operator started at 2020-05-26 19:57:36 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container tigera-operator ready: true, restart count 2
May 26 22:41:19.578: INFO: tuned-88qws from openshift-cluster-node-tuning-operator started at 2020-05-26 19:59:39 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container tuned ready: true, restart count 0
May 26 22:41:19.578: INFO: node-ca-jh22q from openshift-image-registry started at 2020-05-26 20:00:40 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container node-ca ready: true, restart count 0
May 26 22:41:19.578: INFO: vpn-6847db666c-dd6pq from kube-system started at 2020-05-26 20:03:07 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container vpn ready: true, restart count 0
May 26 22:41:19.578: INFO: thanos-querier-7f4dfb8d6f-w2z69 from openshift-monitoring started at 2020-05-26 20:05:34 +0000 UTC (4 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 22:41:19.578: INFO: 	Container oauth-proxy ready: true, restart count 0
May 26 22:41:19.578: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 26 22:41:19.578: INFO: 	Container thanos-querier ready: true, restart count 0
May 26 22:41:19.578: INFO: image-registry-766b5fd974-j2s4m from openshift-image-registry started at 2020-05-26 20:03:30 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container registry ready: true, restart count 0
May 26 22:41:19.578: INFO: prometheus-operator-9d5b5788b-dpcht from openshift-monitoring started at 2020-05-26 20:04:36 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container prometheus-operator ready: true, restart count 0
May 26 22:41:19.578: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-05-26 20:05:57 +0000 UTC (7 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 22:41:19.578: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 26 22:41:19.578: INFO: 	Container prometheus ready: true, restart count 1
May 26 22:41:19.578: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
May 26 22:41:19.578: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 26 22:41:19.578: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
May 26 22:41:19.578: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 26 22:41:19.578: INFO: ibmcloud-block-storage-driver-zq7xq from kube-system started at 2020-05-26 19:57:36 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May 26 22:41:19.578: INFO: openshift-state-metrics-74997686f-cgqlx from openshift-monitoring started at 2020-05-26 19:59:35 +0000 UTC (3 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 26 22:41:19.578: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 26 22:41:19.578: INFO: 	Container openshift-state-metrics ready: true, restart count 0
May 26 22:41:19.578: INFO: cluster-samples-operator-759dd556bf-bgg5n from openshift-cluster-samples-operator started at 2020-05-26 19:59:44 +0000 UTC (2 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container cluster-samples-operator ready: true, restart count 0
May 26 22:41:19.578: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
May 26 22:41:19.578: INFO: service-serving-cert-signer-69cddbb454-wjjcq from openshift-service-ca started at 2020-05-26 19:59:44 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
May 26 22:41:19.578: INFO: multus-88m9q from openshift-multus started at 2020-05-26 19:57:58 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container kube-multus ready: true, restart count 0
May 26 22:41:19.578: INFO: configmap-cabundle-injector-795c74476d-6hzjj from openshift-service-ca started at 2020-05-26 19:59:45 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
May 26 22:41:19.578: INFO: console-d8768d4f5-m8fb9 from openshift-console started at 2020-05-26 20:01:37 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container console ready: true, restart count 0
May 26 22:41:19.578: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-05-26 20:05:02 +0000 UTC (3 container statuses recorded)
May 26 22:41:19.578: INFO: 	Container alertmanager ready: true, restart count 0
May 26 22:41:19.578: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 26 22:41:19.578: INFO: 	Container config-reloader ready: true, restart count 0
May 26 22:41:19.578: INFO: 
Logging pods the kubelet thinks is on node 10.215.60.34 before test
May 26 22:41:19.639: INFO: node-exporter-7b5t7 from openshift-monitoring started at 2020-05-26 19:59:36 +0000 UTC (2 container statuses recorded)
May 26 22:41:19.640: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 22:41:19.640: INFO: 	Container node-exporter ready: true, restart count 0
May 26 22:41:19.640: INFO: telemeter-client-6745779989-mjhdd from openshift-monitoring started at 2020-05-26 21:51:25 +0000 UTC (3 container statuses recorded)
May 26 22:41:19.640: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 26 22:41:19.640: INFO: 	Container reload ready: true, restart count 0
May 26 22:41:19.640: INFO: 	Container telemeter-client ready: true, restart count 0
May 26 22:41:19.640: INFO: sonobuoy-e2e-job-543b6d87a6d14642 from sonobuoy started at 2020-05-26 21:25:53 +0000 UTC (2 container statuses recorded)
May 26 22:41:19.640: INFO: 	Container e2e ready: true, restart count 0
May 26 22:41:19.640: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 26 22:41:19.640: INFO: openshift-kube-proxy-2sfd8 from openshift-kube-proxy started at 2020-05-26 19:58:23 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.640: INFO: 	Container kube-proxy ready: true, restart count 0
May 26 22:41:19.640: INFO: calico-node-9s8hb from calico-system started at 2020-05-26 19:58:46 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.640: INFO: 	Container calico-node ready: true, restart count 0
May 26 22:41:19.640: INFO: multus-j2db5 from openshift-multus started at 2020-05-26 19:58:23 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.640: INFO: 	Container kube-multus ready: true, restart count 0
May 26 22:41:19.640: INFO: sonobuoy-systemd-logs-daemon-set-e1fd62b7d6894c57-mptms from sonobuoy started at 2020-05-26 21:25:54 +0000 UTC (2 container statuses recorded)
May 26 22:41:19.640: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 26 22:41:19.640: INFO: 	Container systemd-logs ready: true, restart count 0
May 26 22:41:19.640: INFO: multus-admission-controller-xvl7w from openshift-multus started at 2020-05-26 21:52:00 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.640: INFO: 	Container multus-admission-controller ready: true, restart count 0
May 26 22:41:19.640: INFO: ibm-master-proxy-static-10.215.60.34 from kube-system started at 2020-05-26 19:58:20 +0000 UTC (2 container statuses recorded)
May 26 22:41:19.640: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May 26 22:41:19.640: INFO: 	Container pause ready: true, restart count 0
May 26 22:41:19.640: INFO: calico-typha-f4f4dbb8c-7f56v from calico-system started at 2020-05-26 19:58:45 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.640: INFO: 	Container calico-typha ready: true, restart count 0
May 26 22:41:19.640: INFO: sonobuoy from sonobuoy started at 2020-05-26 21:25:44 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.640: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 26 22:41:19.640: INFO: ibmcloud-block-storage-driver-nwvlx from kube-system started at 2020-05-26 19:58:31 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.640: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May 26 22:41:19.640: INFO: packageserver-7d5f79c4db-dpjmm from openshift-operator-lifecycle-manager started at 2020-05-26 21:51:27 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.640: INFO: 	Container packageserver ready: true, restart count 0
May 26 22:41:19.640: INFO: prometheus-adapter-5646d5d5dd-9cprr from openshift-monitoring started at 2020-05-26 21:51:25 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.640: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 26 22:41:19.640: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-05-26 21:51:30 +0000 UTC (3 container statuses recorded)
May 26 22:41:19.640: INFO: 	Container alertmanager ready: true, restart count 0
May 26 22:41:19.640: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 26 22:41:19.640: INFO: 	Container config-reloader ready: true, restart count 0
May 26 22:41:19.640: INFO: console-d8768d4f5-vpb4j from openshift-console started at 2020-05-26 21:51:25 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.640: INFO: 	Container console ready: true, restart count 0
May 26 22:41:19.640: INFO: tuned-4sgbw from openshift-cluster-node-tuning-operator started at 2020-05-26 19:59:39 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.640: INFO: 	Container tuned ready: true, restart count 0
May 26 22:41:19.640: INFO: node-ca-6c8qk from openshift-image-registry started at 2020-05-26 20:00:40 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.640: INFO: 	Container node-ca ready: true, restart count 0
May 26 22:41:19.640: INFO: ibm-keepalived-watcher-gtvct from kube-system started at 2020-05-26 19:58:23 +0000 UTC (1 container statuses recorded)
May 26 22:41:19.640: INFO: 	Container keepalived-watcher ready: true, restart count 0
May 26 22:41:19.640: INFO: dns-default-ngb66 from openshift-dns started at 2020-05-26 21:51:30 +0000 UTC (2 container statuses recorded)
May 26 22:41:19.640: INFO: 	Container dns ready: true, restart count 0
May 26 22:41:19.640: INFO: 	Container dns-node-resolver ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node 10.215.60.10
STEP: verifying the node has the label node 10.215.60.11
STEP: verifying the node has the label node 10.215.60.34
May 26 22:41:19.819: INFO: Pod calico-kube-controllers-599969f895-xfmxb requesting resource cpu=0m on Node 10.215.60.10
May 26 22:41:19.819: INFO: Pod calico-node-6tv2z requesting resource cpu=0m on Node 10.215.60.11
May 26 22:41:19.819: INFO: Pod calico-node-9s8hb requesting resource cpu=0m on Node 10.215.60.34
May 26 22:41:19.819: INFO: Pod calico-node-p2dwh requesting resource cpu=0m on Node 10.215.60.10
May 26 22:41:19.820: INFO: Pod calico-typha-f4f4dbb8c-7f56v requesting resource cpu=0m on Node 10.215.60.34
May 26 22:41:19.820: INFO: Pod calico-typha-f4f4dbb8c-jg59b requesting resource cpu=0m on Node 10.215.60.11
May 26 22:41:19.820: INFO: Pod calico-typha-f4f4dbb8c-s262v requesting resource cpu=0m on Node 10.215.60.10
May 26 22:41:19.820: INFO: Pod ibm-cloud-provider-ip-159-122-104-134-84fdcfb5c5-cgj49 requesting resource cpu=5m on Node 10.215.60.10
May 26 22:41:19.820: INFO: Pod ibm-cloud-provider-ip-159-122-104-134-84fdcfb5c5-l5b9t requesting resource cpu=5m on Node 10.215.60.11
May 26 22:41:19.820: INFO: Pod ibm-file-plugin-5999bd7d7d-mcpqg requesting resource cpu=50m on Node 10.215.60.10
May 26 22:41:19.820: INFO: Pod ibm-keepalived-watcher-bl295 requesting resource cpu=5m on Node 10.215.60.10
May 26 22:41:19.820: INFO: Pod ibm-keepalived-watcher-gtvct requesting resource cpu=5m on Node 10.215.60.34
May 26 22:41:19.820: INFO: Pod ibm-keepalived-watcher-w8tv5 requesting resource cpu=5m on Node 10.215.60.11
May 26 22:41:19.820: INFO: Pod ibm-master-proxy-static-10.215.60.10 requesting resource cpu=25m on Node 10.215.60.10
May 26 22:41:19.820: INFO: Pod ibm-master-proxy-static-10.215.60.11 requesting resource cpu=25m on Node 10.215.60.11
May 26 22:41:19.820: INFO: Pod ibm-master-proxy-static-10.215.60.34 requesting resource cpu=25m on Node 10.215.60.34
May 26 22:41:19.820: INFO: Pod ibm-storage-watcher-5f6b5dbcd4-kn4xs requesting resource cpu=50m on Node 10.215.60.10
May 26 22:41:19.820: INFO: Pod ibmcloud-block-storage-driver-nwvlx requesting resource cpu=25m on Node 10.215.60.34
May 26 22:41:19.821: INFO: Pod ibmcloud-block-storage-driver-sncrt requesting resource cpu=25m on Node 10.215.60.10
May 26 22:41:19.821: INFO: Pod ibmcloud-block-storage-driver-zq7xq requesting resource cpu=25m on Node 10.215.60.11
May 26 22:41:19.821: INFO: Pod ibmcloud-block-storage-plugin-9d877d7bc-kjrsl requesting resource cpu=50m on Node 10.215.60.10
May 26 22:41:19.821: INFO: Pod vpn-6847db666c-dd6pq requesting resource cpu=5m on Node 10.215.60.11
May 26 22:41:19.821: INFO: Pod cluster-node-tuning-operator-58cb5999f5-dqqxl requesting resource cpu=10m on Node 10.215.60.10
May 26 22:41:19.821: INFO: Pod tuned-22gmz requesting resource cpu=10m on Node 10.215.60.10
May 26 22:41:19.821: INFO: Pod tuned-4sgbw requesting resource cpu=10m on Node 10.215.60.34
May 26 22:41:19.821: INFO: Pod tuned-88qws requesting resource cpu=10m on Node 10.215.60.11
May 26 22:41:19.821: INFO: Pod cluster-samples-operator-759dd556bf-bgg5n requesting resource cpu=20m on Node 10.215.60.11
May 26 22:41:19.821: INFO: Pod cluster-storage-operator-8696454489-8wttw requesting resource cpu=10m on Node 10.215.60.10
May 26 22:41:19.821: INFO: Pod console-operator-56c4d6445c-brk5r requesting resource cpu=10m on Node 10.215.60.10
May 26 22:41:19.821: INFO: Pod console-d8768d4f5-m8fb9 requesting resource cpu=10m on Node 10.215.60.11
May 26 22:41:19.821: INFO: Pod console-d8768d4f5-vpb4j requesting resource cpu=10m on Node 10.215.60.34
May 26 22:41:19.822: INFO: Pod downloads-8479fbbf57-qhklc requesting resource cpu=10m on Node 10.215.60.10
May 26 22:41:19.822: INFO: Pod downloads-8479fbbf57-z9pkl requesting resource cpu=10m on Node 10.215.60.10
May 26 22:41:19.822: INFO: Pod dns-operator-c68448f89-lfqlv requesting resource cpu=20m on Node 10.215.60.10
May 26 22:41:19.822: INFO: Pod dns-default-jlw4t requesting resource cpu=110m on Node 10.215.60.11
May 26 22:41:19.822: INFO: Pod dns-default-ngb66 requesting resource cpu=110m on Node 10.215.60.34
May 26 22:41:19.822: INFO: Pod dns-default-s9jv4 requesting resource cpu=110m on Node 10.215.60.10
May 26 22:41:19.822: INFO: Pod cluster-image-registry-operator-6f78cddbbc-qrdmf requesting resource cpu=20m on Node 10.215.60.10
May 26 22:41:19.822: INFO: Pod image-registry-766b5fd974-j2s4m requesting resource cpu=100m on Node 10.215.60.11
May 26 22:41:19.822: INFO: Pod node-ca-6c8qk requesting resource cpu=10m on Node 10.215.60.34
May 26 22:41:19.822: INFO: Pod node-ca-jh22q requesting resource cpu=10m on Node 10.215.60.11
May 26 22:41:19.822: INFO: Pod node-ca-q2fgf requesting resource cpu=10m on Node 10.215.60.10
May 26 22:41:19.823: INFO: Pod ingress-operator-f6594bf4d-m74mj requesting resource cpu=20m on Node 10.215.60.10
May 26 22:41:19.823: INFO: Pod router-default-6cfd8db4bf-7dstv requesting resource cpu=100m on Node 10.215.60.10
May 26 22:41:19.823: INFO: Pod router-default-6cfd8db4bf-jv7v4 requesting resource cpu=100m on Node 10.215.60.11
May 26 22:41:19.823: INFO: Pod openshift-kube-proxy-2sfd8 requesting resource cpu=0m on Node 10.215.60.34
May 26 22:41:19.823: INFO: Pod openshift-kube-proxy-h4l9g requesting resource cpu=0m on Node 10.215.60.11
May 26 22:41:19.823: INFO: Pod openshift-kube-proxy-ns7pk requesting resource cpu=0m on Node 10.215.60.10
May 26 22:41:19.823: INFO: Pod certified-operators-7c7bd497cb-hjpfx requesting resource cpu=10m on Node 10.215.60.10
May 26 22:41:19.823: INFO: Pod community-operators-674864f5c4-bwwnn requesting resource cpu=10m on Node 10.215.60.11
May 26 22:41:19.823: INFO: Pod marketplace-operator-6d86c46f6b-j7hh9 requesting resource cpu=10m on Node 10.215.60.10
May 26 22:41:19.823: INFO: Pod redhat-operators-7b74fd75b7-8dvbx requesting resource cpu=10m on Node 10.215.60.10
May 26 22:41:19.823: INFO: Pod alertmanager-main-0 requesting resource cpu=110m on Node 10.215.60.10
May 26 22:41:19.823: INFO: Pod alertmanager-main-1 requesting resource cpu=110m on Node 10.215.60.11
May 26 22:41:19.823: INFO: Pod alertmanager-main-2 requesting resource cpu=110m on Node 10.215.60.34
May 26 22:41:19.823: INFO: Pod cluster-monitoring-operator-64b79969cc-g94mm requesting resource cpu=10m on Node 10.215.60.10
May 26 22:41:19.823: INFO: Pod grafana-5648b7fdd9-rmjbk requesting resource cpu=110m on Node 10.215.60.10
May 26 22:41:19.824: INFO: Pod kube-state-metrics-7498bc479d-5pbn8 requesting resource cpu=30m on Node 10.215.60.11
May 26 22:41:19.824: INFO: Pod node-exporter-7b5t7 requesting resource cpu=112m on Node 10.215.60.34
May 26 22:41:19.824: INFO: Pod node-exporter-b6pqt requesting resource cpu=112m on Node 10.215.60.10
May 26 22:41:19.824: INFO: Pod node-exporter-spjk9 requesting resource cpu=112m on Node 10.215.60.11
May 26 22:41:19.824: INFO: Pod openshift-state-metrics-74997686f-cgqlx requesting resource cpu=120m on Node 10.215.60.11
May 26 22:41:19.824: INFO: Pod prometheus-adapter-5646d5d5dd-9cprr requesting resource cpu=10m on Node 10.215.60.34
May 26 22:41:19.824: INFO: Pod prometheus-adapter-5646d5d5dd-ql9h7 requesting resource cpu=10m on Node 10.215.60.10
May 26 22:41:19.824: INFO: Pod prometheus-k8s-0 requesting resource cpu=480m on Node 10.215.60.11
May 26 22:41:19.824: INFO: Pod prometheus-k8s-1 requesting resource cpu=480m on Node 10.215.60.10
May 26 22:41:19.824: INFO: Pod prometheus-operator-9d5b5788b-dpcht requesting resource cpu=10m on Node 10.215.60.11
May 26 22:41:19.824: INFO: Pod telemeter-client-6745779989-mjhdd requesting resource cpu=10m on Node 10.215.60.34
May 26 22:41:19.824: INFO: Pod thanos-querier-7f4dfb8d6f-5m6ft requesting resource cpu=40m on Node 10.215.60.10
May 26 22:41:19.824: INFO: Pod thanos-querier-7f4dfb8d6f-w2z69 requesting resource cpu=40m on Node 10.215.60.11
May 26 22:41:19.825: INFO: Pod multus-88m9q requesting resource cpu=10m on Node 10.215.60.11
May 26 22:41:19.825: INFO: Pod multus-admission-controller-4kd9c requesting resource cpu=10m on Node 10.215.60.10
May 26 22:41:19.825: INFO: Pod multus-admission-controller-mqrcn requesting resource cpu=10m on Node 10.215.60.11
May 26 22:41:19.825: INFO: Pod multus-admission-controller-xvl7w requesting resource cpu=10m on Node 10.215.60.34
May 26 22:41:19.825: INFO: Pod multus-j2db5 requesting resource cpu=10m on Node 10.215.60.34
May 26 22:41:19.825: INFO: Pod multus-lh654 requesting resource cpu=10m on Node 10.215.60.10
May 26 22:41:19.825: INFO: Pod network-operator-76d6fbdbb8-5hgl8 requesting resource cpu=10m on Node 10.215.60.11
May 26 22:41:19.825: INFO: Pod catalog-operator-6956d96f67-5xvls requesting resource cpu=10m on Node 10.215.60.10
May 26 22:41:19.825: INFO: Pod olm-operator-9d9b9dc65-rg7n5 requesting resource cpu=10m on Node 10.215.60.10
May 26 22:41:19.825: INFO: Pod packageserver-7d5f79c4db-89cw9 requesting resource cpu=10m on Node 10.215.60.10
May 26 22:41:19.825: INFO: Pod packageserver-7d5f79c4db-dpjmm requesting resource cpu=10m on Node 10.215.60.34
May 26 22:41:19.825: INFO: Pod service-ca-operator-7bb6cf7fbc-j7nvb requesting resource cpu=10m on Node 10.215.60.10
May 26 22:41:19.825: INFO: Pod apiservice-cabundle-injector-5c88555f6d-r7p9b requesting resource cpu=10m on Node 10.215.60.10
May 26 22:41:19.825: INFO: Pod configmap-cabundle-injector-795c74476d-6hzjj requesting resource cpu=10m on Node 10.215.60.11
May 26 22:41:19.825: INFO: Pod service-serving-cert-signer-69cddbb454-wjjcq requesting resource cpu=10m on Node 10.215.60.11
May 26 22:41:19.826: INFO: Pod openshift-service-catalog-apiserver-operator-5897998845-kpk9m requesting resource cpu=0m on Node 10.215.60.10
May 26 22:41:19.826: INFO: Pod openshift-service-catalog-controller-manager-operator-86b49fb6n requesting resource cpu=10m on Node 10.215.60.10
May 26 22:41:19.826: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.215.60.34
May 26 22:41:19.826: INFO: Pod sonobuoy-e2e-job-543b6d87a6d14642 requesting resource cpu=0m on Node 10.215.60.34
May 26 22:41:19.826: INFO: Pod sonobuoy-systemd-logs-daemon-set-e1fd62b7d6894c57-gzfvj requesting resource cpu=0m on Node 10.215.60.11
May 26 22:41:19.826: INFO: Pod sonobuoy-systemd-logs-daemon-set-e1fd62b7d6894c57-mptms requesting resource cpu=0m on Node 10.215.60.34
May 26 22:41:19.826: INFO: Pod sonobuoy-systemd-logs-daemon-set-e1fd62b7d6894c57-wrl9n requesting resource cpu=0m on Node 10.215.60.10
May 26 22:41:19.826: INFO: Pod tigera-operator-798cfbf7dd-q5frt requesting resource cpu=100m on Node 10.215.60.11
STEP: Starting Pods to consume most of the cluster CPU.
May 26 22:41:19.826: INFO: Creating a pod which consumes cpu=1664m on Node 10.215.60.10
May 26 22:41:19.871: INFO: Creating a pod which consumes cpu=1696m on Node 10.215.60.11
May 26 22:41:19.898: INFO: Creating a pod which consumes cpu=2410m on Node 10.215.60.34
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-56ef02f2-2cc2-4d7c-a83e-09e52834c975.1612b53a2f2b1011], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5117/filler-pod-56ef02f2-2cc2-4d7c-a83e-09e52834c975 to 10.215.60.11]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-56ef02f2-2cc2-4d7c-a83e-09e52834c975.1612b53a65c942b0], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-56ef02f2-2cc2-4d7c-a83e-09e52834c975.1612b53a70c4a40f], Reason = [Created], Message = [Created container filler-pod-56ef02f2-2cc2-4d7c-a83e-09e52834c975]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-56ef02f2-2cc2-4d7c-a83e-09e52834c975.1612b53a734c31f8], Reason = [Started], Message = [Started container filler-pod-56ef02f2-2cc2-4d7c-a83e-09e52834c975]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6a9d2bb5-dd05-4113-be3a-803bc378f739.1612b53a30c017f3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5117/filler-pod-6a9d2bb5-dd05-4113-be3a-803bc378f739 to 10.215.60.34]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6a9d2bb5-dd05-4113-be3a-803bc378f739.1612b53a6dce98e0], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6a9d2bb5-dd05-4113-be3a-803bc378f739.1612b53a7b97ecf2], Reason = [Created], Message = [Created container filler-pod-6a9d2bb5-dd05-4113-be3a-803bc378f739]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6a9d2bb5-dd05-4113-be3a-803bc378f739.1612b53a7e2b2951], Reason = [Started], Message = [Started container filler-pod-6a9d2bb5-dd05-4113-be3a-803bc378f739]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a6efcc48-32dc-45e9-8c16-7bd70736837d.1612b53a2d5993ca], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5117/filler-pod-a6efcc48-32dc-45e9-8c16-7bd70736837d to 10.215.60.10]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a6efcc48-32dc-45e9-8c16-7bd70736837d.1612b53a68ba93b6], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a6efcc48-32dc-45e9-8c16-7bd70736837d.1612b53a755c9d98], Reason = [Created], Message = [Created container filler-pod-a6efcc48-32dc-45e9-8c16-7bd70736837d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a6efcc48-32dc-45e9-8c16-7bd70736837d.1612b53a78181ed1], Reason = [Started], Message = [Started container filler-pod-a6efcc48-32dc-45e9-8c16-7bd70736837d]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1612b53b238b5745], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1612b53b2526a5ec], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.215.60.10
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.215.60.11
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.215.60.34
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:41:25.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5117" for this suite.
May 26 22:41:33.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:41:35.069: INFO: namespace sched-pred-5117 deletion completed in 9.898357188s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:15.899 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:41:35.070: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 26 22:41:35.293: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d09dfa16-79c1-4f06-b30d-7c7c4f9122a4" in namespace "downward-api-1744" to be "success or failure"
May 26 22:41:35.305: INFO: Pod "downwardapi-volume-d09dfa16-79c1-4f06-b30d-7c7c4f9122a4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.18246ms
May 26 22:41:37.320: INFO: Pod "downwardapi-volume-d09dfa16-79c1-4f06-b30d-7c7c4f9122a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026407266s
STEP: Saw pod success
May 26 22:41:37.320: INFO: Pod "downwardapi-volume-d09dfa16-79c1-4f06-b30d-7c7c4f9122a4" satisfied condition "success or failure"
May 26 22:41:37.338: INFO: Trying to get logs from node 10.215.60.34 pod downwardapi-volume-d09dfa16-79c1-4f06-b30d-7c7c4f9122a4 container client-container: <nil>
STEP: delete the pod
May 26 22:41:37.404: INFO: Waiting for pod downwardapi-volume-d09dfa16-79c1-4f06-b30d-7c7c4f9122a4 to disappear
May 26 22:41:37.414: INFO: Pod downwardapi-volume-d09dfa16-79c1-4f06-b30d-7c7c4f9122a4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:41:37.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1744" for this suite.
May 26 22:41:45.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:41:47.320: INFO: namespace downward-api-1744 deletion completed in 9.890657069s

â€¢ [SLOW TEST:12.250 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:41:47.329: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 26 22:41:51.611: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 26 22:41:51.620: INFO: Pod pod-with-poststart-exec-hook still exists
May 26 22:41:53.620: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 26 22:41:53.630: INFO: Pod pod-with-poststart-exec-hook still exists
May 26 22:41:55.620: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 26 22:41:55.630: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:41:55.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7896" for this suite.
May 26 22:42:11.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:42:13.539: INFO: namespace container-lifecycle-hook-7896 deletion completed in 17.894835973s

â€¢ [SLOW TEST:26.211 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:42:13.541: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 26 22:42:13.710: INFO: Waiting up to 5m0s for pod "pod-e6510e09-d738-4c37-b2be-55075b66eff2" in namespace "emptydir-1701" to be "success or failure"
May 26 22:42:13.719: INFO: Pod "pod-e6510e09-d738-4c37-b2be-55075b66eff2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.742532ms
May 26 22:42:15.729: INFO: Pod "pod-e6510e09-d738-4c37-b2be-55075b66eff2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018491884s
STEP: Saw pod success
May 26 22:42:15.729: INFO: Pod "pod-e6510e09-d738-4c37-b2be-55075b66eff2" satisfied condition "success or failure"
May 26 22:42:15.738: INFO: Trying to get logs from node 10.215.60.34 pod pod-e6510e09-d738-4c37-b2be-55075b66eff2 container test-container: <nil>
STEP: delete the pod
May 26 22:42:15.801: INFO: Waiting for pod pod-e6510e09-d738-4c37-b2be-55075b66eff2 to disappear
May 26 22:42:15.815: INFO: Pod pod-e6510e09-d738-4c37-b2be-55075b66eff2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:42:15.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1701" for this suite.
May 26 22:42:23.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:42:25.748: INFO: namespace emptydir-1701 deletion completed in 9.906051361s

â€¢ [SLOW TEST:12.207 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:42:25.749: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:42:42.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4193" for this suite.
May 26 22:42:50.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:42:52.242: INFO: namespace resourcequota-4193 deletion completed in 10.0862828s

â€¢ [SLOW TEST:26.493 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:42:52.242: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 26 22:42:52.992: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 26 22:42:55.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726129773, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726129773, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726129773, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726129772, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 26 22:42:58.062: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
May 26 22:42:58.131: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:42:58.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5968" for this suite.
May 26 22:43:06.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:43:08.107: INFO: namespace webhook-5968 deletion completed in 9.904564456s
STEP: Destroying namespace "webhook-5968-markers" for this suite.
May 26 22:43:16.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:43:17.997: INFO: namespace webhook-5968-markers deletion completed in 9.889574431s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:25.804 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:43:18.046: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:43:18.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7883" for this suite.
May 26 22:43:26.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:43:28.118: INFO: namespace tables-7883 deletion completed in 9.885483489s

â€¢ [SLOW TEST:10.072 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:43:28.124: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-8229
STEP: creating replication controller nodeport-test in namespace services-8229
I0526 22:43:28.358609      23 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-8229, replica count: 2
May 26 22:43:31.409: INFO: Creating new exec pod
I0526 22:43:31.409607      23 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 26 22:43:34.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=services-8229 execpod6lfwd -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
May 26 22:43:34.830: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 26 22:43:34.830: INFO: stdout: ""
May 26 22:43:34.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=services-8229 execpod6lfwd -- /bin/sh -x -c nc -zv -t -w 2 172.21.177.36 80'
May 26 22:43:35.169: INFO: stderr: "+ nc -zv -t -w 2 172.21.177.36 80\nConnection to 172.21.177.36 80 port [tcp/http] succeeded!\n"
May 26 22:43:35.169: INFO: stdout: ""
May 26 22:43:35.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=services-8229 execpod6lfwd -- /bin/sh -x -c nc -zv -t -w 2 10.215.60.10 32268'
May 26 22:43:35.487: INFO: stderr: "+ nc -zv -t -w 2 10.215.60.10 32268\nConnection to 10.215.60.10 32268 port [tcp/32268] succeeded!\n"
May 26 22:43:35.487: INFO: stdout: ""
May 26 22:43:35.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=services-8229 execpod6lfwd -- /bin/sh -x -c nc -zv -t -w 2 10.215.60.11 32268'
May 26 22:43:35.843: INFO: stderr: "+ nc -zv -t -w 2 10.215.60.11 32268\nConnection to 10.215.60.11 32268 port [tcp/32268] succeeded!\n"
May 26 22:43:35.843: INFO: stdout: ""
May 26 22:43:35.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=services-8229 execpod6lfwd -- /bin/sh -x -c nc -zv -t -w 2 158.177.211.78 32268'
May 26 22:43:36.171: INFO: stderr: "+ nc -zv -t -w 2 158.177.211.78 32268\nConnection to 158.177.211.78 32268 port [tcp/32268] succeeded!\n"
May 26 22:43:36.171: INFO: stdout: ""
May 26 22:43:36.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=services-8229 execpod6lfwd -- /bin/sh -x -c nc -zv -t -w 2 158.177.211.69 32268'
May 26 22:43:36.549: INFO: stderr: "+ nc -zv -t -w 2 158.177.211.69 32268\nConnection to 158.177.211.69 32268 port [tcp/32268] succeeded!\n"
May 26 22:43:36.549: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:43:36.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8229" for this suite.
May 26 22:43:44.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:43:46.456: INFO: namespace services-8229 deletion completed in 9.887565271s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:18.333 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:43:46.458: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 22:43:46.645: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-12e8e7f9-cc7c-4879-aa59-e294e4ce9e33
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-12e8e7f9-cc7c-4879-aa59-e294e4ce9e33
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:43:50.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1603" for this suite.
May 26 22:44:22.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:44:24.873: INFO: namespace projected-1603 deletion completed in 33.960519374s

â€¢ [SLOW TEST:38.415 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:44:24.873: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:44:27.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2504" for this suite.
May 26 22:45:17.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:45:19.074: INFO: namespace kubelet-test-2504 deletion completed in 51.908237302s

â€¢ [SLOW TEST:54.201 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:45:19.074: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
May 26 22:45:23.786: INFO: Successfully updated pod "adopt-release-8nd4s"
STEP: Checking that the Job readopts the Pod
May 26 22:45:23.786: INFO: Waiting up to 15m0s for pod "adopt-release-8nd4s" in namespace "job-1392" to be "adopted"
May 26 22:45:23.799: INFO: Pod "adopt-release-8nd4s": Phase="Running", Reason="", readiness=true. Elapsed: 12.848765ms
May 26 22:45:25.809: INFO: Pod "adopt-release-8nd4s": Phase="Running", Reason="", readiness=true. Elapsed: 2.022767967s
May 26 22:45:25.809: INFO: Pod "adopt-release-8nd4s" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
May 26 22:45:26.351: INFO: Successfully updated pod "adopt-release-8nd4s"
STEP: Checking that the Job releases the Pod
May 26 22:45:26.351: INFO: Waiting up to 15m0s for pod "adopt-release-8nd4s" in namespace "job-1392" to be "released"
May 26 22:45:26.363: INFO: Pod "adopt-release-8nd4s": Phase="Running", Reason="", readiness=true. Elapsed: 11.323691ms
May 26 22:45:28.372: INFO: Pod "adopt-release-8nd4s": Phase="Running", Reason="", readiness=true. Elapsed: 2.020690424s
May 26 22:45:28.372: INFO: Pod "adopt-release-8nd4s" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:45:28.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1392" for this suite.
May 26 22:46:14.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:46:16.286: INFO: namespace job-1392 deletion completed in 47.89671704s

â€¢ [SLOW TEST:57.211 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:46:16.286: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 26 22:46:22.547: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5836 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 26 22:46:22.547: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 22:46:22.730: INFO: Exec stderr: ""
May 26 22:46:22.730: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5836 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 26 22:46:22.730: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 22:46:22.937: INFO: Exec stderr: ""
May 26 22:46:22.937: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5836 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 26 22:46:22.937: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 22:46:23.136: INFO: Exec stderr: ""
May 26 22:46:23.136: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5836 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 26 22:46:23.136: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 22:46:23.349: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 26 22:46:23.349: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5836 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 26 22:46:23.349: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 22:46:23.552: INFO: Exec stderr: ""
May 26 22:46:23.552: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5836 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 26 22:46:23.552: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 22:46:23.749: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 26 22:46:23.749: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5836 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 26 22:46:23.749: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 22:46:23.937: INFO: Exec stderr: ""
May 26 22:46:23.937: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5836 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 26 22:46:23.938: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 22:46:24.104: INFO: Exec stderr: ""
May 26 22:46:24.104: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5836 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 26 22:46:24.104: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 22:46:24.268: INFO: Exec stderr: ""
May 26 22:46:24.268: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5836 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 26 22:46:24.268: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 22:46:24.453: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:46:24.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5836" for this suite.
May 26 22:47:14.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:47:16.370: INFO: namespace e2e-kubelet-etc-hosts-5836 deletion completed in 51.899687151s

â€¢ [SLOW TEST:60.084 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:47:16.370: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: Gathering metrics
May 26 22:47:17.652: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0526 22:47:17.652776      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 26 22:47:17.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8418" for this suite.
May 26 22:47:25.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:47:27.556: INFO: namespace gc-8418 deletion completed in 9.888334629s

â€¢ [SLOW TEST:11.186 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:47:27.557: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 22:47:27.704: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 26 22:47:27.954: INFO: Pod name sample-pod: Found 0 pods out of 1
May 26 22:47:32.974: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 26 22:47:32.974: INFO: Creating deployment "test-rolling-update-deployment"
May 26 22:47:32.988: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 26 22:47:33.016: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 26 22:47:35.038: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 26 22:47:35.047: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726130053, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726130053, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726130053, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726130053, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 26 22:47:37.056: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May 26 22:47:37.084: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2650 /apis/apps/v1/namespaces/deployment-2650/deployments/test-rolling-update-deployment 5003e6fc-ee01-41a9-b609-660f907dfeb7 77317 1 2020-05-26 22:47:32 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0065491e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-05-26 22:47:33 +0000 UTC,LastTransitionTime:2020-05-26 22:47:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2020-05-26 22:47:35 +0000 UTC,LastTransitionTime:2020-05-26 22:47:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 26 22:47:37.094: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-2650 /apis/apps/v1/namespaces/deployment-2650/replicasets/test-rolling-update-deployment-55d946486 81cc2278-94ec-4fcd-901b-18d2af4d53ba 77306 1 2020-05-26 22:47:33 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 5003e6fc-ee01-41a9-b609-660f907dfeb7 0xc0065496f0 0xc0065496f1}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006549758 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 26 22:47:37.094: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 26 22:47:37.094: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2650 /apis/apps/v1/namespaces/deployment-2650/replicasets/test-rolling-update-controller 8c5aaa97-c213-4fee-b633-7f7b9e5307f1 77316 2 2020-05-26 22:47:27 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 5003e6fc-ee01-41a9-b609-660f907dfeb7 0xc006549627 0xc006549628}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006549688 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 26 22:47:37.103: INFO: Pod "test-rolling-update-deployment-55d946486-szzwj" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-szzwj test-rolling-update-deployment-55d946486- deployment-2650 /api/v1/namespaces/deployment-2650/pods/test-rolling-update-deployment-55d946486-szzwj 22343e4a-a30e-435d-960d-aa1d8fb4199a 77305 0 2020-05-26 22:47:33 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[cni.projectcalico.org/podIP:172.30.182.127/32 cni.projectcalico.org/podIPs:172.30.182.127/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.182.127"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 81cc2278-94ec-4fcd-901b-18d2af4d53ba 0xc006549bf0 0xc006549bf1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ch66g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ch66g,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ch66g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.34,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rfq77,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:47:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:47:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:47:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 22:47:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.34,PodIP:172.30.182.127,StartTime:2020-05-26 22:47:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-26 22:47:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://7d6049ad1e5c77e5d44c8a27e0afd2326141c778e0820af4a603c0b1ba6dc520,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.182.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:47:37.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2650" for this suite.
May 26 22:47:45.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:47:47.010: INFO: namespace deployment-2650 deletion completed in 9.88685768s

â€¢ [SLOW TEST:19.453 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:47:47.010: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-5332
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-5332
May 26 22:47:47.265: INFO: Found 0 stateful pods, waiting for 1
May 26 22:47:57.276: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May 26 22:47:57.334: INFO: Deleting all statefulset in ns statefulset-5332
May 26 22:47:57.343: INFO: Scaling statefulset ss to 0
May 26 22:48:27.382: INFO: Waiting for statefulset status.replicas updated to 0
May 26 22:48:27.391: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:48:27.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5332" for this suite.
May 26 22:48:35.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:48:37.328: INFO: namespace statefulset-5332 deletion completed in 9.888205514s

â€¢ [SLOW TEST:50.318 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:48:37.328: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:48:50.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2737" for this suite.
May 26 22:48:58.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:49:00.584: INFO: namespace resourcequota-2737 deletion completed in 9.88626294s

â€¢ [SLOW TEST:23.256 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:49:00.584: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 26 22:49:01.716: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 26 22:49:03.743: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726130141, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726130141, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726130141, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726130141, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 26 22:49:06.777: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
May 26 22:49:08.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 attach --namespace=webhook-2914 to-be-attached-pod -i -c=container1'
May 26 22:49:09.394: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:49:09.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2914" for this suite.
May 26 22:49:41.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:49:43.326: INFO: namespace webhook-2914 deletion completed in 33.888624365s
STEP: Destroying namespace "webhook-2914-markers" for this suite.
May 26 22:49:51.357: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:49:53.213: INFO: namespace webhook-2914-markers deletion completed in 9.886912655s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:52.672 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:49:53.258: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 26 22:49:53.482: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2250 /api/v1/namespaces/watch-2250/configmaps/e2e-watch-test-label-changed 10294b7c-2a14-46bc-a4f0-96a29cc26de0 78298 0 2020-05-26 22:49:53 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 26 22:49:53.483: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2250 /api/v1/namespaces/watch-2250/configmaps/e2e-watch-test-label-changed 10294b7c-2a14-46bc-a4f0-96a29cc26de0 78302 0 2020-05-26 22:49:53 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 26 22:49:53.483: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2250 /api/v1/namespaces/watch-2250/configmaps/e2e-watch-test-label-changed 10294b7c-2a14-46bc-a4f0-96a29cc26de0 78305 0 2020-05-26 22:49:53 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 26 22:50:03.568: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2250 /api/v1/namespaces/watch-2250/configmaps/e2e-watch-test-label-changed 10294b7c-2a14-46bc-a4f0-96a29cc26de0 78353 0 2020-05-26 22:49:53 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 26 22:50:03.568: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2250 /api/v1/namespaces/watch-2250/configmaps/e2e-watch-test-label-changed 10294b7c-2a14-46bc-a4f0-96a29cc26de0 78354 0 2020-05-26 22:49:53 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
May 26 22:50:03.568: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2250 /api/v1/namespaces/watch-2250/configmaps/e2e-watch-test-label-changed 10294b7c-2a14-46bc-a4f0-96a29cc26de0 78355 0 2020-05-26 22:49:53 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:50:03.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2250" for this suite.
May 26 22:50:11.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:50:13.474: INFO: namespace watch-2250 deletion completed in 9.891942568s

â€¢ [SLOW TEST:20.216 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:50:13.474: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 26 22:50:13.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-6467'
May 26 22:50:13.758: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 26 22:50:13.758: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
May 26 22:50:15.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 delete deployment e2e-test-httpd-deployment --namespace=kubectl-6467'
May 26 22:50:15.955: INFO: stderr: ""
May 26 22:50:15.955: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:50:15.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6467" for this suite.
May 26 22:50:48.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:50:49.855: INFO: namespace kubectl-6467 deletion completed in 33.885700368s

â€¢ [SLOW TEST:36.381 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:50:49.855: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-8cc9f in namespace proxy-7755
I0526 22:50:50.035694      23 runners.go:184] Created replication controller with name: proxy-service-8cc9f, namespace: proxy-7755, replica count: 1
I0526 22:50:51.088430      23 runners.go:184] proxy-service-8cc9f Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0526 22:50:52.089057      23 runners.go:184] proxy-service-8cc9f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0526 22:50:53.089331      23 runners.go:184] proxy-service-8cc9f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0526 22:50:54.089690      23 runners.go:184] proxy-service-8cc9f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0526 22:50:55.090014      23 runners.go:184] proxy-service-8cc9f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0526 22:50:56.090295      23 runners.go:184] proxy-service-8cc9f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0526 22:50:57.090551      23 runners.go:184] proxy-service-8cc9f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0526 22:50:58.090843      23 runners.go:184] proxy-service-8cc9f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0526 22:50:59.091286      23 runners.go:184] proxy-service-8cc9f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0526 22:51:00.091660      23 runners.go:184] proxy-service-8cc9f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0526 22:51:01.092279      23 runners.go:184] proxy-service-8cc9f Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 26 22:51:01.102: INFO: setup took 11.112665854s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 26 22:51:01.133: INFO: (0) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">... (200; 29.319474ms)
May 26 22:51:01.134: INFO: (0) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/rewriteme">test</a> (200; 29.997405ms)
May 26 22:51:01.135: INFO: (0) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">test<... (200; 30.747527ms)
May 26 22:51:01.143: INFO: (0) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 39.319353ms)
May 26 22:51:01.144: INFO: (0) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 39.976363ms)
May 26 22:51:01.144: INFO: (0) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname2/proxy/: bar (200; 41.324002ms)
May 26 22:51:01.144: INFO: (0) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname2/proxy/: bar (200; 40.197608ms)
May 26 22:51:01.147: INFO: (0) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 43.349599ms)
May 26 22:51:01.147: INFO: (0) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 43.464117ms)
May 26 22:51:01.148: INFO: (0) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname1/proxy/: foo (200; 44.27856ms)
May 26 22:51:01.148: INFO: (0) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname1/proxy/: foo (200; 45.49063ms)
May 26 22:51:01.158: INFO: (0) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/tlsrewritem... (200; 55.718359ms)
May 26 22:51:01.167: INFO: (0) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname2/proxy/: tls qux (200; 63.091193ms)
May 26 22:51:01.167: INFO: (0) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:460/proxy/: tls baz (200; 64.180713ms)
May 26 22:51:01.170: INFO: (0) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname1/proxy/: tls baz (200; 66.009635ms)
May 26 22:51:01.177: INFO: (0) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:462/proxy/: tls qux (200; 74.650861ms)
May 26 22:51:01.194: INFO: (1) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 16.611099ms)
May 26 22:51:01.198: INFO: (1) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 20.792659ms)
May 26 22:51:01.202: INFO: (1) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">... (200; 23.184094ms)
May 26 22:51:01.202: INFO: (1) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 23.569061ms)
May 26 22:51:01.202: INFO: (1) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:462/proxy/: tls qux (200; 22.896692ms)
May 26 22:51:01.202: INFO: (1) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:460/proxy/: tls baz (200; 22.748573ms)
May 26 22:51:01.202: INFO: (1) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 22.894299ms)
May 26 22:51:01.202: INFO: (1) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/tlsrewritem... (200; 22.973327ms)
May 26 22:51:01.202: INFO: (1) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">test<... (200; 23.562892ms)
May 26 22:51:01.203: INFO: (1) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/rewriteme">test</a> (200; 23.158276ms)
May 26 22:51:01.206: INFO: (1) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname2/proxy/: bar (200; 27.790798ms)
May 26 22:51:01.207: INFO: (1) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname1/proxy/: tls baz (200; 27.827607ms)
May 26 22:51:01.207: INFO: (1) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname1/proxy/: foo (200; 27.822474ms)
May 26 22:51:01.207: INFO: (1) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname2/proxy/: tls qux (200; 27.958986ms)
May 26 22:51:01.209: INFO: (1) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname1/proxy/: foo (200; 29.855609ms)
May 26 22:51:01.209: INFO: (1) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname2/proxy/: bar (200; 31.518017ms)
May 26 22:51:01.226: INFO: (2) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 16.620215ms)
May 26 22:51:01.231: INFO: (2) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 20.404317ms)
May 26 22:51:01.231: INFO: (2) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">test<... (200; 19.734662ms)
May 26 22:51:01.231: INFO: (2) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:460/proxy/: tls baz (200; 19.665828ms)
May 26 22:51:01.232: INFO: (2) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/tlsrewritem... (200; 20.217829ms)
May 26 22:51:01.232: INFO: (2) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 21.815542ms)
May 26 22:51:01.232: INFO: (2) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:462/proxy/: tls qux (200; 20.361322ms)
May 26 22:51:01.232: INFO: (2) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 21.457023ms)
May 26 22:51:01.241: INFO: (2) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/rewriteme">test</a> (200; 29.940358ms)
May 26 22:51:01.241: INFO: (2) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">... (200; 30.965639ms)
May 26 22:51:01.243: INFO: (2) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname1/proxy/: tls baz (200; 31.752597ms)
May 26 22:51:01.243: INFO: (2) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname1/proxy/: foo (200; 31.03062ms)
May 26 22:51:01.243: INFO: (2) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname1/proxy/: foo (200; 31.048629ms)
May 26 22:51:01.247: INFO: (2) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname2/proxy/: tls qux (200; 35.521453ms)
May 26 22:51:01.250: INFO: (2) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname2/proxy/: bar (200; 39.863162ms)
May 26 22:51:01.445: INFO: (2) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname2/proxy/: bar (200; 232.975129ms)
May 26 22:51:01.462: INFO: (3) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 17.319754ms)
May 26 22:51:01.467: INFO: (3) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">test<... (200; 21.445482ms)
May 26 22:51:01.467: INFO: (3) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">... (200; 21.457572ms)
May 26 22:51:01.467: INFO: (3) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 21.799281ms)
May 26 22:51:01.467: INFO: (3) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:462/proxy/: tls qux (200; 21.710973ms)
May 26 22:51:01.467: INFO: (3) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 21.64197ms)
May 26 22:51:01.467: INFO: (3) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/rewriteme">test</a> (200; 21.787412ms)
May 26 22:51:01.467: INFO: (3) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/tlsrewritem... (200; 22.231264ms)
May 26 22:51:01.467: INFO: (3) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:460/proxy/: tls baz (200; 22.204232ms)
May 26 22:51:01.467: INFO: (3) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 21.939189ms)
May 26 22:51:01.475: INFO: (3) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname1/proxy/: foo (200; 29.599485ms)
May 26 22:51:01.475: INFO: (3) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname2/proxy/: bar (200; 29.687099ms)
May 26 22:51:01.475: INFO: (3) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname1/proxy/: foo (200; 29.816008ms)
May 26 22:51:01.475: INFO: (3) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname2/proxy/: tls qux (200; 29.90893ms)
May 26 22:51:01.475: INFO: (3) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname2/proxy/: bar (200; 30.137151ms)
May 26 22:51:01.475: INFO: (3) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname1/proxy/: tls baz (200; 30.228245ms)
May 26 22:51:01.495: INFO: (4) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">test<... (200; 19.368272ms)
May 26 22:51:01.499: INFO: (4) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:462/proxy/: tls qux (200; 22.422365ms)
May 26 22:51:01.499: INFO: (4) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 23.051289ms)
May 26 22:51:01.500: INFO: (4) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:460/proxy/: tls baz (200; 23.152044ms)
May 26 22:51:01.500: INFO: (4) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">... (200; 23.902899ms)
May 26 22:51:01.500: INFO: (4) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 23.802418ms)
May 26 22:51:01.500: INFO: (4) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 22.736303ms)
May 26 22:51:01.500: INFO: (4) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/tlsrewritem... (200; 23.387087ms)
May 26 22:51:01.500: INFO: (4) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 24.430134ms)
May 26 22:51:01.501: INFO: (4) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/rewriteme">test</a> (200; 24.702136ms)
May 26 22:51:01.510: INFO: (4) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname2/proxy/: bar (200; 33.108625ms)
May 26 22:51:01.510: INFO: (4) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname1/proxy/: tls baz (200; 34.066633ms)
May 26 22:51:01.511: INFO: (4) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname2/proxy/: tls qux (200; 33.95437ms)
May 26 22:51:01.511: INFO: (4) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname2/proxy/: bar (200; 33.476292ms)
May 26 22:51:01.511: INFO: (4) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname1/proxy/: foo (200; 33.923476ms)
May 26 22:51:01.511: INFO: (4) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname1/proxy/: foo (200; 33.741986ms)
May 26 22:51:01.531: INFO: (5) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/tlsrewritem... (200; 19.932045ms)
May 26 22:51:01.536: INFO: (5) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 24.379959ms)
May 26 22:51:01.536: INFO: (5) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">test<... (200; 24.533087ms)
May 26 22:51:01.536: INFO: (5) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:460/proxy/: tls baz (200; 24.958492ms)
May 26 22:51:01.536: INFO: (5) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 25.125592ms)
May 26 22:51:01.536: INFO: (5) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/rewriteme">test</a> (200; 24.7094ms)
May 26 22:51:01.536: INFO: (5) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">... (200; 24.672213ms)
May 26 22:51:01.537: INFO: (5) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:462/proxy/: tls qux (200; 24.542294ms)
May 26 22:51:01.537: INFO: (5) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 24.500421ms)
May 26 22:51:01.538: INFO: (5) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname1/proxy/: tls baz (200; 27.537274ms)
May 26 22:51:01.539: INFO: (5) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 26.695639ms)
May 26 22:51:01.546: INFO: (5) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname2/proxy/: bar (200; 34.263289ms)
May 26 22:51:01.546: INFO: (5) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname2/proxy/: tls qux (200; 34.189258ms)
May 26 22:51:01.546: INFO: (5) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname2/proxy/: bar (200; 34.660169ms)
May 26 22:51:01.546: INFO: (5) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname1/proxy/: foo (200; 33.866315ms)
May 26 22:51:01.550: INFO: (5) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname1/proxy/: foo (200; 38.567438ms)
May 26 22:51:01.577: INFO: (6) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/tlsrewritem... (200; 23.731051ms)
May 26 22:51:01.577: INFO: (6) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/rewriteme">test</a> (200; 24.822727ms)
May 26 22:51:01.578: INFO: (6) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 26.011104ms)
May 26 22:51:01.578: INFO: (6) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 26.299494ms)
May 26 22:51:01.578: INFO: (6) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 26.878097ms)
May 26 22:51:01.578: INFO: (6) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">test<... (200; 25.525288ms)
May 26 22:51:01.578: INFO: (6) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 26.343301ms)
May 26 22:51:01.578: INFO: (6) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:460/proxy/: tls baz (200; 24.887072ms)
May 26 22:51:01.578: INFO: (6) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">... (200; 25.747486ms)
May 26 22:51:01.579: INFO: (6) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname1/proxy/: tls baz (200; 26.819132ms)
May 26 22:51:01.587: INFO: (6) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname2/proxy/: bar (200; 33.640783ms)
May 26 22:51:01.587: INFO: (6) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname2/proxy/: bar (200; 35.56549ms)
May 26 22:51:01.587: INFO: (6) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname1/proxy/: foo (200; 36.177236ms)
May 26 22:51:01.587: INFO: (6) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname1/proxy/: foo (200; 34.348078ms)
May 26 22:51:01.598: INFO: (6) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname2/proxy/: tls qux (200; 45.41087ms)
May 26 22:51:01.598: INFO: (6) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:462/proxy/: tls qux (200; 47.221282ms)
May 26 22:51:01.616: INFO: (7) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:462/proxy/: tls qux (200; 18.395582ms)
May 26 22:51:01.622: INFO: (7) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/tlsrewritem... (200; 23.115729ms)
May 26 22:51:01.623: INFO: (7) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 24.180756ms)
May 26 22:51:01.623: INFO: (7) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 24.894605ms)
May 26 22:51:01.623: INFO: (7) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:460/proxy/: tls baz (200; 24.410215ms)
May 26 22:51:01.624: INFO: (7) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 25.150083ms)
May 26 22:51:01.624: INFO: (7) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">test<... (200; 24.697672ms)
May 26 22:51:01.624: INFO: (7) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">... (200; 25.059667ms)
May 26 22:51:01.624: INFO: (7) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 24.874973ms)
May 26 22:51:01.624: INFO: (7) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/rewriteme">test</a> (200; 24.842873ms)
May 26 22:51:01.630: INFO: (7) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname2/proxy/: bar (200; 31.273846ms)
May 26 22:51:01.633: INFO: (7) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname1/proxy/: tls baz (200; 33.928326ms)
May 26 22:51:01.633: INFO: (7) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname1/proxy/: foo (200; 33.972364ms)
May 26 22:51:01.633: INFO: (7) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname2/proxy/: bar (200; 34.629218ms)
May 26 22:51:01.633: INFO: (7) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname1/proxy/: foo (200; 34.949489ms)
May 26 22:51:01.633: INFO: (7) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname2/proxy/: tls qux (200; 34.296108ms)
May 26 22:51:01.650: INFO: (8) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 16.310913ms)
May 26 22:51:01.655: INFO: (8) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 21.108186ms)
May 26 22:51:01.656: INFO: (8) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/rewriteme">test</a> (200; 21.800821ms)
May 26 22:51:01.656: INFO: (8) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 21.355796ms)
May 26 22:51:01.656: INFO: (8) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">... (200; 21.695128ms)
May 26 22:51:01.656: INFO: (8) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">test<... (200; 21.974396ms)
May 26 22:51:01.656: INFO: (8) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/tlsrewritem... (200; 21.08358ms)
May 26 22:51:01.656: INFO: (8) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:462/proxy/: tls qux (200; 20.990727ms)
May 26 22:51:01.657: INFO: (8) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:460/proxy/: tls baz (200; 21.101781ms)
May 26 22:51:01.657: INFO: (8) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 21.969568ms)
May 26 22:51:01.659: INFO: (8) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname2/proxy/: bar (200; 25.311375ms)
May 26 22:51:01.663: INFO: (8) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname2/proxy/: bar (200; 27.559207ms)
May 26 22:51:01.663: INFO: (8) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname1/proxy/: foo (200; 27.730845ms)
May 26 22:51:01.663: INFO: (8) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname1/proxy/: foo (200; 28.024318ms)
May 26 22:51:01.664: INFO: (8) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname1/proxy/: tls baz (200; 28.857764ms)
May 26 22:51:01.664: INFO: (8) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname2/proxy/: tls qux (200; 28.834318ms)
May 26 22:51:01.687: INFO: (9) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">... (200; 21.972042ms)
May 26 22:51:01.687: INFO: (9) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">test<... (200; 22.207662ms)
May 26 22:51:01.687: INFO: (9) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 21.505964ms)
May 26 22:51:01.687: INFO: (9) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:460/proxy/: tls baz (200; 22.221822ms)
May 26 22:51:01.687: INFO: (9) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 22.274876ms)
May 26 22:51:01.687: INFO: (9) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:462/proxy/: tls qux (200; 22.266787ms)
May 26 22:51:01.688: INFO: (9) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 22.883722ms)
May 26 22:51:01.688: INFO: (9) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 23.624395ms)
May 26 22:51:01.688: INFO: (9) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/tlsrewritem... (200; 24.03243ms)
May 26 22:51:01.689: INFO: (9) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/rewriteme">test</a> (200; 23.867058ms)
May 26 22:51:01.692: INFO: (9) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname1/proxy/: tls baz (200; 27.197584ms)
May 26 22:51:01.693: INFO: (9) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname2/proxy/: tls qux (200; 28.568381ms)
May 26 22:51:01.693: INFO: (9) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname1/proxy/: foo (200; 27.808319ms)
May 26 22:51:01.694: INFO: (9) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname2/proxy/: bar (200; 29.445519ms)
May 26 22:51:01.695: INFO: (9) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname2/proxy/: bar (200; 29.519128ms)
May 26 22:51:01.695: INFO: (9) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname1/proxy/: foo (200; 29.607863ms)
May 26 22:51:01.709: INFO: (10) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 13.568848ms)
May 26 22:51:01.715: INFO: (10) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">test<... (200; 19.118916ms)
May 26 22:51:01.715: INFO: (10) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:460/proxy/: tls baz (200; 19.529269ms)
May 26 22:51:01.715: INFO: (10) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/tlsrewritem... (200; 19.505254ms)
May 26 22:51:01.715: INFO: (10) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 19.60355ms)
May 26 22:51:01.716: INFO: (10) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 20.141086ms)
May 26 22:51:01.716: INFO: (10) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 20.2559ms)
May 26 22:51:01.716: INFO: (10) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">... (200; 20.505601ms)
May 26 22:51:01.717: INFO: (10) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/rewriteme">test</a> (200; 21.331727ms)
May 26 22:51:01.717: INFO: (10) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:462/proxy/: tls qux (200; 21.069747ms)
May 26 22:51:01.718: INFO: (10) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname2/proxy/: bar (200; 22.395494ms)
May 26 22:51:01.722: INFO: (10) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname2/proxy/: tls qux (200; 25.533272ms)
May 26 22:51:01.722: INFO: (10) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname1/proxy/: foo (200; 25.822267ms)
May 26 22:51:01.722: INFO: (10) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname1/proxy/: foo (200; 26.080898ms)
May 26 22:51:01.722: INFO: (10) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname2/proxy/: bar (200; 26.568248ms)
May 26 22:51:01.723: INFO: (10) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname1/proxy/: tls baz (200; 26.585645ms)
May 26 22:51:01.739: INFO: (11) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 16.362979ms)
May 26 22:51:01.744: INFO: (11) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">... (200; 19.914462ms)
May 26 22:51:01.744: INFO: (11) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 19.638987ms)
May 26 22:51:01.744: INFO: (11) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">test<... (200; 20.491818ms)
May 26 22:51:01.745: INFO: (11) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:460/proxy/: tls baz (200; 20.072166ms)
May 26 22:51:01.745: INFO: (11) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/rewriteme">test</a> (200; 21.118446ms)
May 26 22:51:01.745: INFO: (11) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 20.524554ms)
May 26 22:51:01.745: INFO: (11) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 19.933909ms)
May 26 22:51:01.745: INFO: (11) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/tlsrewritem... (200; 19.997796ms)
May 26 22:51:01.745: INFO: (11) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:462/proxy/: tls qux (200; 20.038412ms)
May 26 22:51:01.749: INFO: (11) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname2/proxy/: tls qux (200; 25.685394ms)
May 26 22:51:01.752: INFO: (11) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname2/proxy/: bar (200; 27.602282ms)
May 26 22:51:01.753: INFO: (11) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname1/proxy/: foo (200; 27.999547ms)
May 26 22:51:01.754: INFO: (11) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname1/proxy/: tls baz (200; 31.50356ms)
May 26 22:51:01.755: INFO: (11) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname1/proxy/: foo (200; 29.797012ms)
May 26 22:51:01.755: INFO: (11) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname2/proxy/: bar (200; 30.182372ms)
May 26 22:51:01.771: INFO: (12) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 15.680194ms)
May 26 22:51:01.781: INFO: (12) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 24.381954ms)
May 26 22:51:01.782: INFO: (12) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/rewriteme">test</a> (200; 24.269748ms)
May 26 22:51:01.782: INFO: (12) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/tlsrewritem... (200; 26.25232ms)
May 26 22:51:01.782: INFO: (12) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:462/proxy/: tls qux (200; 26.016152ms)
May 26 22:51:01.782: INFO: (12) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 25.812112ms)
May 26 22:51:01.782: INFO: (12) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 24.388187ms)
May 26 22:51:01.782: INFO: (12) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:460/proxy/: tls baz (200; 26.530107ms)
May 26 22:51:01.783: INFO: (12) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">test<... (200; 25.071679ms)
May 26 22:51:01.783: INFO: (12) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">... (200; 24.976248ms)
May 26 22:51:01.787: INFO: (12) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname2/proxy/: tls qux (200; 29.605506ms)
May 26 22:51:01.787: INFO: (12) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname2/proxy/: bar (200; 31.40095ms)
May 26 22:51:01.787: INFO: (12) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname1/proxy/: tls baz (200; 30.512704ms)
May 26 22:51:01.789: INFO: (12) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname1/proxy/: foo (200; 32.417965ms)
May 26 22:51:01.789: INFO: (12) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname2/proxy/: bar (200; 32.181722ms)
May 26 22:51:01.789: INFO: (12) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname1/proxy/: foo (200; 33.719316ms)
May 26 22:51:01.812: INFO: (13) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/tlsrewritem... (200; 22.191ms)
May 26 22:51:01.813: INFO: (13) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:462/proxy/: tls qux (200; 23.219357ms)
May 26 22:51:01.813: INFO: (13) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 22.388021ms)
May 26 22:51:01.813: INFO: (13) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">test<... (200; 24.0021ms)
May 26 22:51:01.813: INFO: (13) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">... (200; 22.252631ms)
May 26 22:51:01.813: INFO: (13) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:460/proxy/: tls baz (200; 23.2549ms)
May 26 22:51:01.814: INFO: (13) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 22.373842ms)
May 26 22:51:01.814: INFO: (13) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/rewriteme">test</a> (200; 21.377619ms)
May 26 22:51:01.814: INFO: (13) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 22.531945ms)
May 26 22:51:01.816: INFO: (13) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 24.872093ms)
May 26 22:51:01.817: INFO: (13) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname2/proxy/: tls qux (200; 25.154699ms)
May 26 22:51:01.817: INFO: (13) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname1/proxy/: tls baz (200; 25.60229ms)
May 26 22:51:01.818: INFO: (13) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname2/proxy/: bar (200; 27.22417ms)
May 26 22:51:01.818: INFO: (13) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname1/proxy/: foo (200; 28.449026ms)
May 26 22:51:01.818: INFO: (13) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname2/proxy/: bar (200; 28.385145ms)
May 26 22:51:01.818: INFO: (13) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname1/proxy/: foo (200; 28.068265ms)
May 26 22:51:01.836: INFO: (14) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 17.427566ms)
May 26 22:51:01.840: INFO: (14) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/rewriteme">test</a> (200; 20.358158ms)
May 26 22:51:01.845: INFO: (14) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">... (200; 25.439314ms)
May 26 22:51:01.845: INFO: (14) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 25.849456ms)
May 26 22:51:01.845: INFO: (14) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 25.873553ms)
May 26 22:51:01.847: INFO: (14) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/tlsrewritem... (200; 27.653858ms)
May 26 22:51:01.847: INFO: (14) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 27.680661ms)
May 26 22:51:01.847: INFO: (14) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:462/proxy/: tls qux (200; 27.663021ms)
May 26 22:51:01.847: INFO: (14) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:460/proxy/: tls baz (200; 27.723191ms)
May 26 22:51:01.847: INFO: (14) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">test<... (200; 27.666231ms)
May 26 22:51:01.848: INFO: (14) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname1/proxy/: foo (200; 28.773915ms)
May 26 22:51:01.854: INFO: (14) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname1/proxy/: tls baz (200; 35.083892ms)
May 26 22:51:01.854: INFO: (14) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname2/proxy/: bar (200; 35.371292ms)
May 26 22:51:01.854: INFO: (14) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname2/proxy/: bar (200; 35.266546ms)
May 26 22:51:01.854: INFO: (14) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname2/proxy/: tls qux (200; 35.426625ms)
May 26 22:51:01.855: INFO: (14) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname1/proxy/: foo (200; 35.497392ms)
May 26 22:51:01.873: INFO: (15) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 17.947818ms)
May 26 22:51:01.881: INFO: (15) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:460/proxy/: tls baz (200; 25.456672ms)
May 26 22:51:01.881: INFO: (15) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">... (200; 25.170852ms)
May 26 22:51:01.881: INFO: (15) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/tlsrewritem... (200; 25.674152ms)
May 26 22:51:01.882: INFO: (15) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/rewriteme">test</a> (200; 25.414841ms)
May 26 22:51:01.882: INFO: (15) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:462/proxy/: tls qux (200; 25.59275ms)
May 26 22:51:01.882: INFO: (15) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">test<... (200; 25.295825ms)
May 26 22:51:01.882: INFO: (15) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 25.573526ms)
May 26 22:51:01.882: INFO: (15) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 25.308193ms)
May 26 22:51:01.882: INFO: (15) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 26.513343ms)
May 26 22:51:01.891: INFO: (15) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname1/proxy/: foo (200; 34.754794ms)
May 26 22:51:01.891: INFO: (15) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname2/proxy/: tls qux (200; 34.595241ms)
May 26 22:51:01.891: INFO: (15) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname1/proxy/: foo (200; 35.136044ms)
May 26 22:51:01.891: INFO: (15) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname1/proxy/: tls baz (200; 34.700061ms)
May 26 22:51:01.891: INFO: (15) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname2/proxy/: bar (200; 34.894166ms)
May 26 22:51:01.895: INFO: (15) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname2/proxy/: bar (200; 39.110691ms)
May 26 22:51:01.918: INFO: (16) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/tlsrewritem... (200; 22.64311ms)
May 26 22:51:01.920: INFO: (16) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 24.487887ms)
May 26 22:51:01.920: INFO: (16) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:462/proxy/: tls qux (200; 24.707115ms)
May 26 22:51:01.921: INFO: (16) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:460/proxy/: tls baz (200; 25.014036ms)
May 26 22:51:01.921: INFO: (16) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">... (200; 24.849432ms)
May 26 22:51:01.921: INFO: (16) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 25.415511ms)
May 26 22:51:01.922: INFO: (16) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/rewriteme">test</a> (200; 25.588226ms)
May 26 22:51:01.922: INFO: (16) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 26.895162ms)
May 26 22:51:01.922: INFO: (16) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">test<... (200; 25.951533ms)
May 26 22:51:01.922: INFO: (16) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 26.491126ms)
May 26 22:51:01.927: INFO: (16) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname1/proxy/: tls baz (200; 30.865813ms)
May 26 22:51:01.927: INFO: (16) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname2/proxy/: tls qux (200; 30.97653ms)
May 26 22:51:01.929: INFO: (16) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname1/proxy/: foo (200; 32.765503ms)
May 26 22:51:01.929: INFO: (16) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname1/proxy/: foo (200; 33.555718ms)
May 26 22:51:01.930: INFO: (16) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname2/proxy/: bar (200; 33.993329ms)
May 26 22:51:01.931: INFO: (16) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname2/proxy/: bar (200; 35.037543ms)
May 26 22:51:01.950: INFO: (17) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 18.939655ms)
May 26 22:51:01.954: INFO: (17) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">test<... (200; 22.929608ms)
May 26 22:51:01.954: INFO: (17) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">... (200; 23.014961ms)
May 26 22:51:01.955: INFO: (17) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 23.445237ms)
May 26 22:51:01.955: INFO: (17) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 23.194116ms)
May 26 22:51:01.955: INFO: (17) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/tlsrewritem... (200; 23.297952ms)
May 26 22:51:01.955: INFO: (17) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/rewriteme">test</a> (200; 23.077758ms)
May 26 22:51:01.956: INFO: (17) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 24.110523ms)
May 26 22:51:01.956: INFO: (17) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:462/proxy/: tls qux (200; 24.324615ms)
May 26 22:51:01.956: INFO: (17) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:460/proxy/: tls baz (200; 24.388283ms)
May 26 22:51:01.963: INFO: (17) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname2/proxy/: bar (200; 31.542143ms)
May 26 22:51:01.963: INFO: (17) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname2/proxy/: tls qux (200; 31.849044ms)
May 26 22:51:01.963: INFO: (17) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname1/proxy/: tls baz (200; 31.998663ms)
May 26 22:51:01.964: INFO: (17) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname1/proxy/: foo (200; 32.550592ms)
May 26 22:51:01.964: INFO: (17) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname2/proxy/: bar (200; 32.814451ms)
May 26 22:51:01.966: INFO: (17) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname1/proxy/: foo (200; 34.434776ms)
May 26 22:51:01.991: INFO: (18) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">... (200; 23.471913ms)
May 26 22:51:01.991: INFO: (18) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 24.00165ms)
May 26 22:51:01.991: INFO: (18) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 22.83634ms)
May 26 22:51:01.992: INFO: (18) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:462/proxy/: tls qux (200; 23.149357ms)
May 26 22:51:01.992: INFO: (18) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/rewriteme">test</a> (200; 24.969857ms)
May 26 22:51:01.992: INFO: (18) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 24.311033ms)
May 26 22:51:01.992: INFO: (18) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/tlsrewritem... (200; 23.898445ms)
May 26 22:51:01.992: INFO: (18) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">test<... (200; 25.159896ms)
May 26 22:51:01.992: INFO: (18) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 26.186199ms)
May 26 22:51:01.992: INFO: (18) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:460/proxy/: tls baz (200; 24.071193ms)
May 26 22:51:01.993: INFO: (18) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname2/proxy/: tls qux (200; 26.372947ms)
May 26 22:51:02.012: INFO: (18) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname2/proxy/: bar (200; 45.939905ms)
May 26 22:51:02.012: INFO: (18) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname2/proxy/: bar (200; 44.479573ms)
May 26 22:51:02.013: INFO: (18) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname1/proxy/: tls baz (200; 44.865795ms)
May 26 22:51:02.013: INFO: (18) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname1/proxy/: foo (200; 44.217817ms)
May 26 22:51:02.013: INFO: (18) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname1/proxy/: foo (200; 47.101599ms)
May 26 22:51:02.037: INFO: (19) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">test<... (200; 23.763135ms)
May 26 22:51:02.037: INFO: (19) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 23.715281ms)
May 26 22:51:02.037: INFO: (19) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 24.221733ms)
May 26 22:51:02.037: INFO: (19) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:462/proxy/: tls qux (200; 23.610631ms)
May 26 22:51:02.038: INFO: (19) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:160/proxy/: foo (200; 24.272346ms)
May 26 22:51:02.038: INFO: (19) /api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/http:proxy-service-8cc9f-zftdw:1080/proxy/rewriteme">... (200; 24.690394ms)
May 26 22:51:02.038: INFO: (19) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw/proxy/rewriteme">test</a> (200; 24.992359ms)
May 26 22:51:02.038: INFO: (19) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:460/proxy/: tls baz (200; 24.705115ms)
May 26 22:51:02.038: INFO: (19) /api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/: <a href="/api/v1/namespaces/proxy-7755/pods/https:proxy-service-8cc9f-zftdw:443/proxy/tlsrewritem... (200; 24.623001ms)
May 26 22:51:02.038: INFO: (19) /api/v1/namespaces/proxy-7755/pods/proxy-service-8cc9f-zftdw:162/proxy/: bar (200; 24.845529ms)
May 26 22:51:02.048: INFO: (19) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname2/proxy/: bar (200; 34.345621ms)
May 26 22:51:02.048: INFO: (19) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname1/proxy/: foo (200; 34.425324ms)
May 26 22:51:02.048: INFO: (19) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname2/proxy/: tls qux (200; 34.506946ms)
May 26 22:51:02.048: INFO: (19) /api/v1/namespaces/proxy-7755/services/https:proxy-service-8cc9f:tlsportname1/proxy/: tls baz (200; 34.587825ms)
May 26 22:51:02.048: INFO: (19) /api/v1/namespaces/proxy-7755/services/http:proxy-service-8cc9f:portname1/proxy/: foo (200; 34.564443ms)
May 26 22:51:02.049: INFO: (19) /api/v1/namespaces/proxy-7755/services/proxy-service-8cc9f:portname2/proxy/: bar (200; 35.190144ms)
STEP: deleting ReplicationController proxy-service-8cc9f in namespace proxy-7755, will wait for the garbage collector to delete the pods
May 26 22:51:02.134: INFO: Deleting ReplicationController proxy-service-8cc9f took: 26.013261ms
May 26 22:51:02.535: INFO: Terminating ReplicationController proxy-service-8cc9f pods took: 400.458873ms
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:51:05.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7755" for this suite.
May 26 22:51:13.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:51:13.837: INFO: namespace proxy-7755 deletion completed in 8.779195904s

â€¢ [SLOW TEST:23.982 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:51:13.838: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-5822
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5822 to expose endpoints map[]
May 26 22:51:13.997: INFO: successfully validated that service multi-endpoint-test in namespace services-5822 exposes endpoints map[] (8.486667ms elapsed)
STEP: Creating pod pod1 in namespace services-5822
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5822 to expose endpoints map[pod1:[100]]
May 26 22:51:16.096: INFO: successfully validated that service multi-endpoint-test in namespace services-5822 exposes endpoints map[pod1:[100]] (2.068964693s elapsed)
STEP: Creating pod pod2 in namespace services-5822
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5822 to expose endpoints map[pod1:[100] pod2:[101]]
May 26 22:51:18.209: INFO: successfully validated that service multi-endpoint-test in namespace services-5822 exposes endpoints map[pod1:[100] pod2:[101]] (2.086089192s elapsed)
STEP: Deleting pod pod1 in namespace services-5822
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5822 to expose endpoints map[pod2:[101]]
May 26 22:51:19.272: INFO: successfully validated that service multi-endpoint-test in namespace services-5822 exposes endpoints map[pod2:[101]] (1.044474857s elapsed)
STEP: Deleting pod pod2 in namespace services-5822
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5822 to expose endpoints map[]
May 26 22:51:20.312: INFO: successfully validated that service multi-endpoint-test in namespace services-5822 exposes endpoints map[] (1.018683443s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:51:20.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5822" for this suite.
May 26 22:51:34.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:51:36.260: INFO: namespace services-5822 deletion completed in 15.884892046s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:22.422 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:51:36.260: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-f58c
STEP: Creating a pod to test atomic-volume-subpath
May 26 22:51:36.457: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-f58c" in namespace "subpath-4053" to be "success or failure"
May 26 22:51:36.465: INFO: Pod "pod-subpath-test-configmap-f58c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.981577ms
May 26 22:51:38.475: INFO: Pod "pod-subpath-test-configmap-f58c": Phase="Running", Reason="", readiness=true. Elapsed: 2.017519237s
May 26 22:51:40.485: INFO: Pod "pod-subpath-test-configmap-f58c": Phase="Running", Reason="", readiness=true. Elapsed: 4.02772005s
May 26 22:51:42.496: INFO: Pod "pod-subpath-test-configmap-f58c": Phase="Running", Reason="", readiness=true. Elapsed: 6.038358938s
May 26 22:51:44.506: INFO: Pod "pod-subpath-test-configmap-f58c": Phase="Running", Reason="", readiness=true. Elapsed: 8.048227932s
May 26 22:51:46.518: INFO: Pod "pod-subpath-test-configmap-f58c": Phase="Running", Reason="", readiness=true. Elapsed: 10.060039553s
May 26 22:51:48.527: INFO: Pod "pod-subpath-test-configmap-f58c": Phase="Running", Reason="", readiness=true. Elapsed: 12.069877285s
May 26 22:51:50.538: INFO: Pod "pod-subpath-test-configmap-f58c": Phase="Running", Reason="", readiness=true. Elapsed: 14.080105631s
May 26 22:51:52.548: INFO: Pod "pod-subpath-test-configmap-f58c": Phase="Running", Reason="", readiness=true. Elapsed: 16.090645238s
May 26 22:51:54.559: INFO: Pod "pod-subpath-test-configmap-f58c": Phase="Running", Reason="", readiness=true. Elapsed: 18.101884985s
May 26 22:51:56.569: INFO: Pod "pod-subpath-test-configmap-f58c": Phase="Running", Reason="", readiness=true. Elapsed: 20.1112932s
May 26 22:51:58.580: INFO: Pod "pod-subpath-test-configmap-f58c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.122060523s
STEP: Saw pod success
May 26 22:51:58.580: INFO: Pod "pod-subpath-test-configmap-f58c" satisfied condition "success or failure"
May 26 22:51:58.593: INFO: Trying to get logs from node 10.215.60.34 pod pod-subpath-test-configmap-f58c container test-container-subpath-configmap-f58c: <nil>
STEP: delete the pod
May 26 22:51:58.692: INFO: Waiting for pod pod-subpath-test-configmap-f58c to disappear
May 26 22:51:58.701: INFO: Pod pod-subpath-test-configmap-f58c no longer exists
STEP: Deleting pod pod-subpath-test-configmap-f58c
May 26 22:51:58.701: INFO: Deleting pod "pod-subpath-test-configmap-f58c" in namespace "subpath-4053"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:51:58.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4053" for this suite.
May 26 22:52:06.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:52:08.638: INFO: namespace subpath-4053 deletion completed in 9.889777866s

â€¢ [SLOW TEST:32.377 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:52:08.638: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 22:52:08.761: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
May 26 22:52:10.878: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:52:11.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3729" for this suite.
May 26 22:52:19.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:52:21.805: INFO: namespace replication-controller-3729 deletion completed in 9.884181291s

â€¢ [SLOW TEST:13.167 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:52:21.806: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3737
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-3737
I0526 22:52:21.997394      23 runners.go:184] Created replication controller with name: externalname-service, namespace: services-3737, replica count: 2
I0526 22:52:25.048164      23 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 26 22:52:25.048: INFO: Creating new exec pod
May 26 22:52:30.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=services-3737 execpod55t4t -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 26 22:52:30.483: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 26 22:52:30.483: INFO: stdout: ""
May 26 22:52:30.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=services-3737 execpod55t4t -- /bin/sh -x -c nc -zv -t -w 2 172.21.176.209 80'
May 26 22:52:30.823: INFO: stderr: "+ nc -zv -t -w 2 172.21.176.209 80\nConnection to 172.21.176.209 80 port [tcp/http] succeeded!\n"
May 26 22:52:30.823: INFO: stdout: ""
May 26 22:52:30.823: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:52:30.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3737" for this suite.
May 26 22:52:38.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:52:40.800: INFO: namespace services-3737 deletion completed in 9.900051708s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:18.994 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:52:40.802: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-ac287d22-b11e-4c96-bd19-5c8cd9275dd7 in namespace container-probe-6507
May 26 22:52:42.985: INFO: Started pod liveness-ac287d22-b11e-4c96-bd19-5c8cd9275dd7 in namespace container-probe-6507
STEP: checking the pod's current state and verifying that restartCount is present
May 26 22:52:42.994: INFO: Initial restart count of pod liveness-ac287d22-b11e-4c96-bd19-5c8cd9275dd7 is 0
May 26 22:53:07.143: INFO: Restart count of pod container-probe-6507/liveness-ac287d22-b11e-4c96-bd19-5c8cd9275dd7 is now 1 (24.148838244s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:53:07.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6507" for this suite.
May 26 22:53:15.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:53:17.088: INFO: namespace container-probe-6507 deletion completed in 9.897564345s

â€¢ [SLOW TEST:36.286 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:53:17.088: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-ee2241e5-9471-4163-983d-0f6a287a1f06 in namespace container-probe-5646
May 26 22:53:21.315: INFO: Started pod busybox-ee2241e5-9471-4163-983d-0f6a287a1f06 in namespace container-probe-5646
STEP: checking the pod's current state and verifying that restartCount is present
May 26 22:53:21.326: INFO: Initial restart count of pod busybox-ee2241e5-9471-4163-983d-0f6a287a1f06 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:57:23.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5646" for this suite.
May 26 22:57:31.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:57:33.037: INFO: namespace container-probe-5646 deletion completed in 9.904127652s

â€¢ [SLOW TEST:255.949 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:57:33.037: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-7572
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 26 22:57:33.196: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 26 22:57:53.510: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.182.89:8080/dial?request=hostName&protocol=udp&host=172.30.141.131&port=8081&tries=1'] Namespace:pod-network-test-7572 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 26 22:57:53.510: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 22:57:53.717: INFO: Waiting for endpoints: map[]
May 26 22:57:53.727: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.182.89:8080/dial?request=hostName&protocol=udp&host=172.30.182.79&port=8081&tries=1'] Namespace:pod-network-test-7572 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 26 22:57:53.727: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 22:57:53.947: INFO: Waiting for endpoints: map[]
May 26 22:57:53.957: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.182.89:8080/dial?request=hostName&protocol=udp&host=172.30.99.183&port=8081&tries=1'] Namespace:pod-network-test-7572 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 26 22:57:53.957: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 22:57:54.136: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:57:54.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7572" for this suite.
May 26 22:58:02.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:58:04.039: INFO: namespace pod-network-test-7572 deletion completed in 9.887431239s

â€¢ [SLOW TEST:31.002 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:58:04.044: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9986.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9986.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 26 22:58:08.421: INFO: DNS probes using dns-9986/dns-test-0c4eef56-911f-4d2b-8bbc-d67f9d015e3c succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:58:08.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9986" for this suite.
May 26 22:58:16.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:58:18.372: INFO: namespace dns-9986 deletion completed in 9.890847762s

â€¢ [SLOW TEST:14.328 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:58:18.374: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-016889a2-04a2-4cb1-9163-348494e036d9
STEP: Creating a pod to test consume secrets
May 26 22:58:18.574: INFO: Waiting up to 5m0s for pod "pod-secrets-15370b7f-7797-4bb1-8669-47eedb765b75" in namespace "secrets-5264" to be "success or failure"
May 26 22:58:18.584: INFO: Pod "pod-secrets-15370b7f-7797-4bb1-8669-47eedb765b75": Phase="Pending", Reason="", readiness=false. Elapsed: 10.182701ms
May 26 22:58:20.607: INFO: Pod "pod-secrets-15370b7f-7797-4bb1-8669-47eedb765b75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032794201s
STEP: Saw pod success
May 26 22:58:20.607: INFO: Pod "pod-secrets-15370b7f-7797-4bb1-8669-47eedb765b75" satisfied condition "success or failure"
May 26 22:58:20.637: INFO: Trying to get logs from node 10.215.60.34 pod pod-secrets-15370b7f-7797-4bb1-8669-47eedb765b75 container secret-volume-test: <nil>
STEP: delete the pod
May 26 22:58:20.721: INFO: Waiting for pod pod-secrets-15370b7f-7797-4bb1-8669-47eedb765b75 to disappear
May 26 22:58:20.735: INFO: Pod pod-secrets-15370b7f-7797-4bb1-8669-47eedb765b75 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:58:20.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5264" for this suite.
May 26 22:58:28.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:58:30.639: INFO: namespace secrets-5264 deletion completed in 9.886020348s

â€¢ [SLOW TEST:12.265 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:58:30.639: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1241
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1241
STEP: creating replication controller externalsvc in namespace services-1241
I0526 22:58:30.860755      23 runners.go:184] Created replication controller with name: externalsvc, namespace: services-1241, replica count: 2
I0526 22:58:33.911331      23 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
May 26 22:58:33.958: INFO: Creating new exec pod
May 26 22:58:36.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=services-1241 execpodb95zl -- /bin/sh -x -c nslookup clusterip-service'
May 26 22:58:36.398: INFO: stderr: "+ nslookup clusterip-service\n"
May 26 22:58:36.398: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-1241.svc.cluster.local\tcanonical name = externalsvc.services-1241.svc.cluster.local.\nName:\texternalsvc.services-1241.svc.cluster.local\nAddress: 172.21.236.228\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1241, will wait for the garbage collector to delete the pods
May 26 22:58:36.485: INFO: Deleting ReplicationController externalsvc took: 27.722073ms
May 26 22:58:36.886: INFO: Terminating ReplicationController externalsvc pods took: 400.495527ms
May 26 22:58:52.339: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:58:52.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1241" for this suite.
May 26 22:59:00.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:59:02.281: INFO: namespace services-1241 deletion completed in 9.888536481s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:31.642 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:59:02.281: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
May 26 22:59:02.497: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:59:05.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1782" for this suite.
May 26 22:59:13.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:59:15.482: INFO: namespace init-container-1782 deletion completed in 9.897086016s

â€¢ [SLOW TEST:13.200 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:59:15.482: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-2d1c6abf-e897-4e18-80a4-8535092cd367
STEP: Creating a pod to test consume secrets
May 26 22:59:15.826: INFO: Waiting up to 5m0s for pod "pod-secrets-de2a5cdd-6a80-46fa-a8b7-e7e81faebda2" in namespace "secrets-6636" to be "success or failure"
May 26 22:59:15.835: INFO: Pod "pod-secrets-de2a5cdd-6a80-46fa-a8b7-e7e81faebda2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.052675ms
May 26 22:59:17.855: INFO: Pod "pod-secrets-de2a5cdd-6a80-46fa-a8b7-e7e81faebda2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029000677s
STEP: Saw pod success
May 26 22:59:17.856: INFO: Pod "pod-secrets-de2a5cdd-6a80-46fa-a8b7-e7e81faebda2" satisfied condition "success or failure"
May 26 22:59:17.866: INFO: Trying to get logs from node 10.215.60.34 pod pod-secrets-de2a5cdd-6a80-46fa-a8b7-e7e81faebda2 container secret-volume-test: <nil>
STEP: delete the pod
May 26 22:59:17.946: INFO: Waiting for pod pod-secrets-de2a5cdd-6a80-46fa-a8b7-e7e81faebda2 to disappear
May 26 22:59:17.954: INFO: Pod pod-secrets-de2a5cdd-6a80-46fa-a8b7-e7e81faebda2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:59:17.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6636" for this suite.
May 26 22:59:26.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:59:27.881: INFO: namespace secrets-6636 deletion completed in 9.905612186s
STEP: Destroying namespace "secret-namespace-5071" for this suite.
May 26 22:59:35.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:59:37.790: INFO: namespace secret-namespace-5071 deletion completed in 9.908479377s

â€¢ [SLOW TEST:22.308 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:59:37.790: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-b5de0d0f-8602-4df0-8f67-28f7bf29f662
STEP: Creating a pod to test consume secrets
May 26 22:59:37.980: INFO: Waiting up to 5m0s for pod "pod-secrets-32b74d8a-55ef-4859-850a-e622be872f9b" in namespace "secrets-5321" to be "success or failure"
May 26 22:59:37.989: INFO: Pod "pod-secrets-32b74d8a-55ef-4859-850a-e622be872f9b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.178335ms
May 26 22:59:40.001: INFO: Pod "pod-secrets-32b74d8a-55ef-4859-850a-e622be872f9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02076051s
STEP: Saw pod success
May 26 22:59:40.001: INFO: Pod "pod-secrets-32b74d8a-55ef-4859-850a-e622be872f9b" satisfied condition "success or failure"
May 26 22:59:40.012: INFO: Trying to get logs from node 10.215.60.34 pod pod-secrets-32b74d8a-55ef-4859-850a-e622be872f9b container secret-volume-test: <nil>
STEP: delete the pod
May 26 22:59:40.075: INFO: Waiting for pod pod-secrets-32b74d8a-55ef-4859-850a-e622be872f9b to disappear
May 26 22:59:40.086: INFO: Pod pod-secrets-32b74d8a-55ef-4859-850a-e622be872f9b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 22:59:40.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5321" for this suite.
May 26 22:59:48.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 22:59:49.994: INFO: namespace secrets-5321 deletion completed in 9.887076773s

â€¢ [SLOW TEST:12.204 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 22:59:49.994: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
May 26 22:59:50.151: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:00:34.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7900" for this suite.
May 26 23:00:42.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:00:44.840: INFO: namespace crd-publish-openapi-7900 deletion completed in 9.88770897s

â€¢ [SLOW TEST:54.846 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:00:44.840: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4934
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4934
STEP: Creating statefulset with conflicting port in namespace statefulset-4934
STEP: Waiting until pod test-pod will start running in namespace statefulset-4934
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4934
May 26 23:00:47.090: INFO: Observed stateful pod in namespace: statefulset-4934, name: ss-0, uid: 8bae3fe5-4b7c-43de-966f-8f0a744fa37a, status phase: Pending. Waiting for statefulset controller to delete.
May 26 23:00:47.100: INFO: Observed stateful pod in namespace: statefulset-4934, name: ss-0, uid: 8bae3fe5-4b7c-43de-966f-8f0a744fa37a, status phase: Failed. Waiting for statefulset controller to delete.
May 26 23:00:47.124: INFO: Observed stateful pod in namespace: statefulset-4934, name: ss-0, uid: 8bae3fe5-4b7c-43de-966f-8f0a744fa37a, status phase: Failed. Waiting for statefulset controller to delete.
May 26 23:00:47.142: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4934
STEP: Removing pod with conflicting port in namespace statefulset-4934
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4934 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May 26 23:00:49.206: INFO: Deleting all statefulset in ns statefulset-4934
May 26 23:00:49.216: INFO: Scaling statefulset ss to 0
May 26 23:00:59.254: INFO: Waiting for statefulset status.replicas updated to 0
May 26 23:00:59.263: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:00:59.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4934" for this suite.
May 26 23:01:07.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:01:09.222: INFO: namespace statefulset-4934 deletion completed in 9.900449883s

â€¢ [SLOW TEST:24.382 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:01:09.224: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
May 26 23:01:09.389: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:01:12.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4778" for this suite.
May 26 23:01:21.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:01:22.882: INFO: namespace init-container-4778 deletion completed in 9.890225492s

â€¢ [SLOW TEST:13.658 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:01:22.883: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-7678a687-a374-42bd-baf6-24b6cac76a2c
STEP: Creating a pod to test consume configMaps
May 26 23:01:23.100: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3118253c-20c7-47eb-9944-3e0c5996df07" in namespace "projected-4349" to be "success or failure"
May 26 23:01:23.109: INFO: Pod "pod-projected-configmaps-3118253c-20c7-47eb-9944-3e0c5996df07": Phase="Pending", Reason="", readiness=false. Elapsed: 8.656101ms
May 26 23:01:25.118: INFO: Pod "pod-projected-configmaps-3118253c-20c7-47eb-9944-3e0c5996df07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018184307s
STEP: Saw pod success
May 26 23:01:25.118: INFO: Pod "pod-projected-configmaps-3118253c-20c7-47eb-9944-3e0c5996df07" satisfied condition "success or failure"
May 26 23:01:25.130: INFO: Trying to get logs from node 10.215.60.34 pod pod-projected-configmaps-3118253c-20c7-47eb-9944-3e0c5996df07 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 26 23:01:25.206: INFO: Waiting for pod pod-projected-configmaps-3118253c-20c7-47eb-9944-3e0c5996df07 to disappear
May 26 23:01:25.215: INFO: Pod pod-projected-configmaps-3118253c-20c7-47eb-9944-3e0c5996df07 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:01:25.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4349" for this suite.
May 26 23:01:33.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:01:35.114: INFO: namespace projected-4349 deletion completed in 9.886116382s

â€¢ [SLOW TEST:12.232 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:01:35.120: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May 26 23:01:35.325: INFO: Waiting up to 5m0s for pod "downward-api-eb668929-f082-4842-a8e0-2d8f00a6bb34" in namespace "downward-api-6670" to be "success or failure"
May 26 23:01:35.335: INFO: Pod "downward-api-eb668929-f082-4842-a8e0-2d8f00a6bb34": Phase="Pending", Reason="", readiness=false. Elapsed: 9.857114ms
May 26 23:01:37.347: INFO: Pod "downward-api-eb668929-f082-4842-a8e0-2d8f00a6bb34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021298756s
STEP: Saw pod success
May 26 23:01:37.347: INFO: Pod "downward-api-eb668929-f082-4842-a8e0-2d8f00a6bb34" satisfied condition "success or failure"
May 26 23:01:37.356: INFO: Trying to get logs from node 10.215.60.34 pod downward-api-eb668929-f082-4842-a8e0-2d8f00a6bb34 container dapi-container: <nil>
STEP: delete the pod
May 26 23:01:37.407: INFO: Waiting for pod downward-api-eb668929-f082-4842-a8e0-2d8f00a6bb34 to disappear
May 26 23:01:37.416: INFO: Pod downward-api-eb668929-f082-4842-a8e0-2d8f00a6bb34 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:01:37.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6670" for this suite.
May 26 23:01:45.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:01:47.328: INFO: namespace downward-api-6670 deletion completed in 9.895034304s

â€¢ [SLOW TEST:12.209 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:01:47.329: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 26 23:01:48.059: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 26 23:01:50.086: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726130908, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726130908, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726130908, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726130908, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 26 23:01:53.120: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:01:53.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3585" for this suite.
May 26 23:02:01.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:02:03.631: INFO: namespace webhook-3585 deletion completed in 10.027535446s
STEP: Destroying namespace "webhook-3585-markers" for this suite.
May 26 23:02:11.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:02:13.536: INFO: namespace webhook-3585-markers deletion completed in 9.905054684s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:26.253 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:02:13.583: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 26 23:02:13.733: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cab17a76-2f31-4fe5-9d1d-c9d077160a6c" in namespace "downward-api-1606" to be "success or failure"
May 26 23:02:13.742: INFO: Pod "downwardapi-volume-cab17a76-2f31-4fe5-9d1d-c9d077160a6c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.85347ms
May 26 23:02:15.753: INFO: Pod "downwardapi-volume-cab17a76-2f31-4fe5-9d1d-c9d077160a6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019782397s
STEP: Saw pod success
May 26 23:02:15.753: INFO: Pod "downwardapi-volume-cab17a76-2f31-4fe5-9d1d-c9d077160a6c" satisfied condition "success or failure"
May 26 23:02:15.763: INFO: Trying to get logs from node 10.215.60.34 pod downwardapi-volume-cab17a76-2f31-4fe5-9d1d-c9d077160a6c container client-container: <nil>
STEP: delete the pod
May 26 23:02:15.822: INFO: Waiting for pod downwardapi-volume-cab17a76-2f31-4fe5-9d1d-c9d077160a6c to disappear
May 26 23:02:15.832: INFO: Pod downwardapi-volume-cab17a76-2f31-4fe5-9d1d-c9d077160a6c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:02:15.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1606" for this suite.
May 26 23:02:23.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:02:25.759: INFO: namespace downward-api-1606 deletion completed in 9.905727709s

â€¢ [SLOW TEST:12.176 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:02:25.759: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
May 26 23:02:25.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 create -f - --namespace=kubectl-4846'
May 26 23:02:26.646: INFO: stderr: ""
May 26 23:02:26.646: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 26 23:02:27.657: INFO: Selector matched 1 pods for map[app:redis]
May 26 23:02:27.657: INFO: Found 0 / 1
May 26 23:02:28.656: INFO: Selector matched 1 pods for map[app:redis]
May 26 23:02:28.656: INFO: Found 1 / 1
May 26 23:02:28.656: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 26 23:02:28.665: INFO: Selector matched 1 pods for map[app:redis]
May 26 23:02:28.665: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 26 23:02:28.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 patch pod redis-master-hw8v2 --namespace=kubectl-4846 -p {"metadata":{"annotations":{"x":"y"}}}'
May 26 23:02:28.835: INFO: stderr: ""
May 26 23:02:28.835: INFO: stdout: "pod/redis-master-hw8v2 patched\n"
STEP: checking annotations
May 26 23:02:28.844: INFO: Selector matched 1 pods for map[app:redis]
May 26 23:02:28.844: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:02:28.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4846" for this suite.
May 26 23:02:42.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:02:44.754: INFO: namespace kubectl-4846 deletion completed in 15.885562477s

â€¢ [SLOW TEST:18.995 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:02:44.755: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:02:52.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9179" for this suite.
May 26 23:03:00.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:03:02.849: INFO: namespace job-9179 deletion completed in 9.891243684s

â€¢ [SLOW TEST:18.094 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:03:02.849: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
May 26 23:03:13.163: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0526 23:03:13.163860      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:03:13.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1738" for this suite.
May 26 23:03:21.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:03:23.071: INFO: namespace gc-1738 deletion completed in 9.887933376s

â€¢ [SLOW TEST:20.222 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:03:23.074: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 23:03:23.297: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"4a4d366b-d3b2-4db6-98d7-356890f4e3d4", Controller:(*bool)(0xc004d8d0e6), BlockOwnerDeletion:(*bool)(0xc004d8d0e7)}}
May 26 23:03:23.319: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"4b7b4868-2e48-44f7-bc18-46f796561cbf", Controller:(*bool)(0xc003ffc9b2), BlockOwnerDeletion:(*bool)(0xc003ffc9b3)}}
May 26 23:03:23.335: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"163efa18-0022-4bc7-be40-ce46e9c13000", Controller:(*bool)(0xc004d8d332), BlockOwnerDeletion:(*bool)(0xc004d8d333)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:03:28.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5832" for this suite.
May 26 23:03:36.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:03:38.261: INFO: namespace gc-5832 deletion completed in 9.886665286s

â€¢ [SLOW TEST:15.187 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:03:38.262: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4967
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-4967
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4967
May 26 23:03:38.502: INFO: Found 0 stateful pods, waiting for 1
May 26 23:03:48.515: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 26 23:03:48.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 26 23:03:48.859: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 26 23:03:48.859: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 26 23:03:48.859: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 26 23:03:48.869: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 26 23:03:58.879: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 26 23:03:58.879: INFO: Waiting for statefulset status.replicas updated to 0
May 26 23:03:58.917: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 26 23:03:58.917: INFO: ss-0  10.215.60.34  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:38 +0000 UTC  }]
May 26 23:03:58.917: INFO: 
May 26 23:03:58.917: INFO: StatefulSet ss has not reached scale 3, at 1
May 26 23:03:59.927: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990770884s
May 26 23:04:00.937: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.980764094s
May 26 23:04:01.949: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.970074312s
May 26 23:04:02.960: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.958262028s
May 26 23:04:03.970: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.94725905s
May 26 23:04:04.981: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.93722701s
May 26 23:04:05.992: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.926756956s
May 26 23:04:07.003: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.915833777s
May 26 23:04:08.014: INFO: Verifying statefulset ss doesn't scale past 3 for another 904.078957ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4967
May 26 23:04:09.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:04:09.352: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 26 23:04:09.352: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 26 23:04:09.352: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 26 23:04:09.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:04:09.714: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 26 23:04:09.714: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 26 23:04:09.714: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 26 23:04:09.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:04:10.056: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 26 23:04:10.056: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 26 23:04:10.056: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 26 23:04:10.068: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
May 26 23:04:20.079: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 26 23:04:20.079: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 26 23:04:20.079: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 26 23:04:20.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 26 23:04:20.428: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 26 23:04:20.428: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 26 23:04:20.428: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 26 23:04:20.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 26 23:04:20.754: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 26 23:04:20.754: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 26 23:04:20.754: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 26 23:04:20.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 26 23:04:21.129: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 26 23:04:21.129: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 26 23:04:21.129: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 26 23:04:21.129: INFO: Waiting for statefulset status.replicas updated to 0
May 26 23:04:21.137: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 26 23:04:31.173: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 26 23:04:31.173: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 26 23:04:31.173: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 26 23:04:31.203: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 26 23:04:31.203: INFO: ss-0  10.215.60.34  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:38 +0000 UTC  }]
May 26 23:04:31.203: INFO: ss-1  10.215.60.11  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  }]
May 26 23:04:31.203: INFO: ss-2  10.215.60.10  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  }]
May 26 23:04:31.203: INFO: 
May 26 23:04:31.203: INFO: StatefulSet ss has not reached scale 0, at 3
May 26 23:04:32.214: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 26 23:04:32.214: INFO: ss-0  10.215.60.34  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:38 +0000 UTC  }]
May 26 23:04:32.214: INFO: ss-1  10.215.60.11  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  }]
May 26 23:04:32.214: INFO: ss-2  10.215.60.10  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  }]
May 26 23:04:32.214: INFO: 
May 26 23:04:32.214: INFO: StatefulSet ss has not reached scale 0, at 3
May 26 23:04:33.225: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 26 23:04:33.225: INFO: ss-0  10.215.60.34  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:38 +0000 UTC  }]
May 26 23:04:33.225: INFO: ss-1  10.215.60.11  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  }]
May 26 23:04:33.225: INFO: ss-2  10.215.60.10  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  }]
May 26 23:04:33.225: INFO: 
May 26 23:04:33.225: INFO: StatefulSet ss has not reached scale 0, at 3
May 26 23:04:34.239: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 26 23:04:34.239: INFO: ss-0  10.215.60.34  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:38 +0000 UTC  }]
May 26 23:04:34.239: INFO: ss-1  10.215.60.11  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  }]
May 26 23:04:34.239: INFO: ss-2  10.215.60.10  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  }]
May 26 23:04:34.239: INFO: 
May 26 23:04:34.239: INFO: StatefulSet ss has not reached scale 0, at 3
May 26 23:04:35.254: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 26 23:04:35.254: INFO: ss-0  10.215.60.34  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:38 +0000 UTC  }]
May 26 23:04:35.254: INFO: ss-1  10.215.60.11  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  }]
May 26 23:04:35.254: INFO: ss-2  10.215.60.10  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  }]
May 26 23:04:35.254: INFO: 
May 26 23:04:35.254: INFO: StatefulSet ss has not reached scale 0, at 3
May 26 23:04:36.265: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 26 23:04:36.265: INFO: ss-0  10.215.60.34  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:38 +0000 UTC  }]
May 26 23:04:36.265: INFO: ss-1  10.215.60.11  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  }]
May 26 23:04:36.265: INFO: ss-2  10.215.60.10  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  }]
May 26 23:04:36.265: INFO: 
May 26 23:04:36.265: INFO: StatefulSet ss has not reached scale 0, at 3
May 26 23:04:37.276: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 26 23:04:37.276: INFO: ss-0  10.215.60.34  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:38 +0000 UTC  }]
May 26 23:04:37.276: INFO: ss-1  10.215.60.11  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  }]
May 26 23:04:37.276: INFO: ss-2  10.215.60.10  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  }]
May 26 23:04:37.276: INFO: 
May 26 23:04:37.276: INFO: StatefulSet ss has not reached scale 0, at 3
May 26 23:04:38.286: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 26 23:04:38.286: INFO: ss-0  10.215.60.34  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:38 +0000 UTC  }]
May 26 23:04:38.286: INFO: ss-1  10.215.60.11  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  }]
May 26 23:04:38.287: INFO: 
May 26 23:04:38.287: INFO: StatefulSet ss has not reached scale 0, at 2
May 26 23:04:39.297: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 26 23:04:39.297: INFO: ss-0  10.215.60.34  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:38 +0000 UTC  }]
May 26 23:04:39.297: INFO: ss-1  10.215.60.11  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  }]
May 26 23:04:39.297: INFO: 
May 26 23:04:39.297: INFO: StatefulSet ss has not reached scale 0, at 2
May 26 23:04:40.326: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 26 23:04:40.326: INFO: ss-0  10.215.60.34  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:38 +0000 UTC  }]
May 26 23:04:40.326: INFO: ss-1  10.215.60.11  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-26 23:03:58 +0000 UTC  }]
May 26 23:04:40.326: INFO: 
May 26 23:04:40.326: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4967
May 26 23:04:41.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:04:41.564: INFO: rc: 1
May 26 23:04:41.564: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  error: unable to upgrade connection: container not found ("webserver")
 [] <nil> 0xc00a655cb0 exit status 1 <nil> <nil> true [0xc003ea1478 0xc003ea1490 0xc003ea14a8] [0xc003ea1478 0xc003ea1490 0xc003ea14a8] [0xc003ea1488 0xc003ea14a0] [0x10efce0 0x10efce0] 0xc001fa4960 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
May 26 23:04:51.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:04:51.710: INFO: rc: 1
May 26 23:04:51.710: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003d74360 exit status 1 <nil> <nil> true [0xc0014e00c8 0xc0014e0178 0xc0014e02b8] [0xc0014e00c8 0xc0014e0178 0xc0014e02b8] [0xc0014e0140 0xc0014e02a0] [0x10efce0 0x10efce0] 0xc0029077a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:05:01.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:05:01.864: INFO: rc: 1
May 26 23:05:01.864: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00183c360 exit status 1 <nil> <nil> true [0xc0004a09f8 0xc0004a0e88 0xc0004a0f98] [0xc0004a09f8 0xc0004a0e88 0xc0004a0f98] [0xc0004a0e58 0xc0004a0ee8] [0x10efce0 0x10efce0] 0xc00032cae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:05:11.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:05:12.011: INFO: rc: 1
May 26 23:05:12.011: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0034223c0 exit status 1 <nil> <nil> true [0xc0005226e0 0xc000522ae8 0xc000523020] [0xc0005226e0 0xc000522ae8 0xc000523020] [0xc0005229b8 0xc000522d08] [0x10efce0 0x10efce0] 0xc0028e9980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:05:22.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:05:22.150: INFO: rc: 1
May 26 23:05:22.150: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003d747b0 exit status 1 <nil> <nil> true [0xc0014e0310 0xc0014e03c8 0xc0014e07d8] [0xc0014e0310 0xc0014e03c8 0xc0014e07d8] [0xc0014e03a0 0xc0014e06e8] [0x10efce0 0x10efce0] 0xc002410120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:05:32.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:05:32.297: INFO: rc: 1
May 26 23:05:32.298: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003422780 exit status 1 <nil> <nil> true [0xc000523148 0xc000523410 0xc000523670] [0xc000523148 0xc000523410 0xc000523670] [0xc000523310 0xc0005235d8] [0x10efce0 0x10efce0] 0xc001e40de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:05:42.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:05:42.446: INFO: rc: 1
May 26 23:05:42.447: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003d74b40 exit status 1 <nil> <nil> true [0xc0014e0838 0xc0014e0940 0xc0014e0a90] [0xc0014e0838 0xc0014e0940 0xc0014e0a90] [0xc0014e08d0 0xc0014e0a58] [0x10efce0 0x10efce0] 0xc0022d1800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:05:52.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:05:52.615: INFO: rc: 1
May 26 23:05:52.615: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003422ae0 exit status 1 <nil> <nil> true [0xc000523760 0xc000523a70 0xc000523bd0] [0xc000523760 0xc000523a70 0xc000523bd0] [0xc0005239e8 0xc000523b78] [0x10efce0 0x10efce0] 0xc002dc5920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:06:02.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:06:02.788: INFO: rc: 1
May 26 23:06:02.788: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003422e70 exit status 1 <nil> <nil> true [0xc000523c08 0xc000523df0 0xc0004189d8] [0xc000523c08 0xc000523df0 0xc0004189d8] [0xc000523db8 0xc000418650] [0x10efce0 0x10efce0] 0xc0023a1b00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:06:12.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:06:12.938: INFO: rc: 1
May 26 23:06:12.938: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003423260 exit status 1 <nil> <nil> true [0xc000418d50 0xc000419358 0xc000419578] [0xc000418d50 0xc000419358 0xc000419578] [0xc0004191c8 0xc000419510] [0x10efce0 0x10efce0] 0xc00222ef00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:06:22.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:06:23.081: INFO: rc: 1
May 26 23:06:23.081: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003d74ed0 exit status 1 <nil> <nil> true [0xc0014e0b10 0xc0014e0c18 0xc0014e0de0] [0xc0014e0b10 0xc0014e0c18 0xc0014e0de0] [0xc0014e0bf0 0xc0014e0cb8] [0x10efce0 0x10efce0] 0xc002532480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:06:33.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:06:33.238: INFO: rc: 1
May 26 23:06:33.238: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0034235f0 exit status 1 <nil> <nil> true [0xc0004195c0 0xc00179e0c0 0xc00179e270] [0xc0004195c0 0xc00179e0c0 0xc00179e270] [0xc00179e030 0xc00179e238] [0x10efce0 0x10efce0] 0xc002ba2c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:06:43.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:06:43.390: INFO: rc: 1
May 26 23:06:43.390: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00183c7b0 exit status 1 <nil> <nil> true [0xc0004a1300 0xc0004a17c0 0xc0004a1d38] [0xc0004a1300 0xc0004a17c0 0xc0004a1d38] [0xc0004a1590 0xc0004a19e0] [0x10efce0 0x10efce0] 0xc0024cb8c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:06:53.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:06:53.533: INFO: rc: 1
May 26 23:06:53.533: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003422360 exit status 1 <nil> <nil> true [0xc0004189d8 0xc0004191c8 0xc000419510] [0xc0004189d8 0xc0004191c8 0xc000419510] [0xc000419010 0xc0004194e8] [0x10efce0 0x10efce0] 0xc0029f5800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:07:03.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:07:03.689: INFO: rc: 1
May 26 23:07:03.689: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003d74390 exit status 1 <nil> <nil> true [0xc0005226e0 0xc000522ae8 0xc000523020] [0xc0005226e0 0xc000522ae8 0xc000523020] [0xc0005229b8 0xc000522d08] [0x10efce0 0x10efce0] 0xc0021e6c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:07:13.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:07:13.842: INFO: rc: 1
May 26 23:07:13.842: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003d74810 exit status 1 <nil> <nil> true [0xc000523148 0xc000523410 0xc000523670] [0xc000523148 0xc000523410 0xc000523670] [0xc000523310 0xc0005235d8] [0x10efce0 0x10efce0] 0xc0022d0a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:07:23.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:07:23.996: INFO: rc: 1
May 26 23:07:23.996: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003422720 exit status 1 <nil> <nil> true [0xc000419578 0xc00179e030 0xc00179e238] [0xc000419578 0xc00179e030 0xc00179e238] [0xc000419610 0xc00179e1e8] [0x10efce0 0x10efce0] 0xc002dc48a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:07:33.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:07:34.136: INFO: rc: 1
May 26 23:07:34.136: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003422b10 exit status 1 <nil> <nil> true [0xc00179e270 0xc00179e3d8 0xc00179e5c8] [0xc00179e270 0xc00179e3d8 0xc00179e5c8] [0xc00179e328 0xc00179e580] [0x10efce0 0x10efce0] 0xc0017d0780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:07:44.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:07:44.294: INFO: rc: 1
May 26 23:07:44.294: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00183c3c0 exit status 1 <nil> <nil> true [0xc0004a09f8 0xc0004a0e88 0xc0004a0f98] [0xc0004a09f8 0xc0004a0e88 0xc0004a0f98] [0xc0004a0e58 0xc0004a0ee8] [0x10efce0 0x10efce0] 0xc002442ae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:07:54.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:07:54.439: INFO: rc: 1
May 26 23:07:54.440: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00183c780 exit status 1 <nil> <nil> true [0xc0004a1300 0xc0004a17c0 0xc0004a1d38] [0xc0004a1300 0xc0004a17c0 0xc0004a1d38] [0xc0004a1590 0xc0004a19e0] [0x10efce0 0x10efce0] 0xc0028e8720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:08:04.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:08:04.591: INFO: rc: 1
May 26 23:08:04.592: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003d74c00 exit status 1 <nil> <nil> true [0xc000523760 0xc000523a70 0xc000523bd0] [0xc000523760 0xc000523a70 0xc000523bd0] [0xc0005239e8 0xc000523b78] [0x10efce0 0x10efce0] 0xc00032cae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:08:14.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:08:14.756: INFO: rc: 1
May 26 23:08:14.756: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00183cb40 exit status 1 <nil> <nil> true [0xc0004a1d60 0xc0014e0000 0xc0014e0140] [0xc0004a1d60 0xc0014e0000 0xc0014e0140] [0xc0004a1f08 0xc0014e0108] [0x10efce0 0x10efce0] 0xc002395f20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:08:24.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:08:24.892: INFO: rc: 1
May 26 23:08:24.893: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003d74fc0 exit status 1 <nil> <nil> true [0xc000523c08 0xc000523df0 0xc002d52010] [0xc000523c08 0xc000523df0 0xc002d52010] [0xc000523db8 0xc002d52008] [0x10efce0 0x10efce0] 0xc002a630e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:08:34.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:08:35.033: INFO: rc: 1
May 26 23:08:35.033: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc005c50330 exit status 1 <nil> <nil> true [0xc001a9c000 0xc001a9c018 0xc001a9c030] [0xc001a9c000 0xc001a9c018 0xc001a9c030] [0xc001a9c010 0xc001a9c028] [0x10efce0 0x10efce0] 0xc002ba3da0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:08:45.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:08:45.179: INFO: rc: 1
May 26 23:08:45.179: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003d75350 exit status 1 <nil> <nil> true [0xc002d52018 0xc002d52030 0xc002d52058] [0xc002d52018 0xc002d52030 0xc002d52058] [0xc002d52028 0xc002d52050] [0x10efce0 0x10efce0] 0xc002533aa0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:08:55.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:08:55.326: INFO: rc: 1
May 26 23:08:55.326: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc005c50360 exit status 1 <nil> <nil> true [0xc0004a0b90 0xc0004a0ed0 0xc0004a1300] [0xc0004a0b90 0xc0004a0ed0 0xc0004a1300] [0xc0004a0e88 0xc0004a0f98] [0x10efce0 0x10efce0] 0xc002ba3da0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:09:05.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:09:05.482: INFO: rc: 1
May 26 23:09:05.482: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00183c360 exit status 1 <nil> <nil> true [0xc0005226e0 0xc000522ae8 0xc000523020] [0xc0005226e0 0xc000522ae8 0xc000523020] [0xc0005229b8 0xc000522d08] [0x10efce0 0x10efce0] 0xc002395f20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:09:15.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:09:15.637: INFO: rc: 1
May 26 23:09:15.637: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc005c50720 exit status 1 <nil> <nil> true [0xc0004a1480 0xc0004a18f8 0xc0004a1d60] [0xc0004a1480 0xc0004a18f8 0xc0004a1d60] [0xc0004a17c0 0xc0004a1d38] [0x10efce0 0x10efce0] 0xc0028e8300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:09:25.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:09:25.784: INFO: rc: 1
May 26 23:09:25.784: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00183c720 exit status 1 <nil> <nil> true [0xc000523148 0xc000523410 0xc000523670] [0xc000523148 0xc000523410 0xc000523670] [0xc000523310 0xc0005235d8] [0x10efce0 0x10efce0] 0xc002443320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:09:35.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:09:35.927: INFO: rc: 1
May 26 23:09:35.927: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc005c50ae0 exit status 1 <nil> <nil> true [0xc0004a1df0 0xc0004189d8 0xc0004191c8] [0xc0004a1df0 0xc0004189d8 0xc0004191c8] [0xc000418650 0xc000419010] [0x10efce0 0x10efce0] 0xc00223b740 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 26 23:09:45.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=statefulset-4967 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 26 23:09:46.070: INFO: rc: 1
May 26 23:09:46.071: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
May 26 23:09:46.071: INFO: Scaling statefulset ss to 0
May 26 23:09:46.097: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May 26 23:09:46.104: INFO: Deleting all statefulset in ns statefulset-4967
May 26 23:09:46.112: INFO: Scaling statefulset ss to 0
May 26 23:09:46.141: INFO: Waiting for statefulset status.replicas updated to 0
May 26 23:09:46.151: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:09:46.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4967" for this suite.
May 26 23:09:54.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:09:56.102: INFO: namespace statefulset-4967 deletion completed in 9.892500435s

â€¢ [SLOW TEST:377.840 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:09:56.102: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 26 23:09:56.272: INFO: Waiting up to 5m0s for pod "pod-d7b94b94-dbdf-4f7b-8d98-ec6d82b86dfe" in namespace "emptydir-9071" to be "success or failure"
May 26 23:09:56.280: INFO: Pod "pod-d7b94b94-dbdf-4f7b-8d98-ec6d82b86dfe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.289184ms
May 26 23:09:58.290: INFO: Pod "pod-d7b94b94-dbdf-4f7b-8d98-ec6d82b86dfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017835755s
STEP: Saw pod success
May 26 23:09:58.290: INFO: Pod "pod-d7b94b94-dbdf-4f7b-8d98-ec6d82b86dfe" satisfied condition "success or failure"
May 26 23:09:58.300: INFO: Trying to get logs from node 10.215.60.34 pod pod-d7b94b94-dbdf-4f7b-8d98-ec6d82b86dfe container test-container: <nil>
STEP: delete the pod
May 26 23:09:58.387: INFO: Waiting for pod pod-d7b94b94-dbdf-4f7b-8d98-ec6d82b86dfe to disappear
May 26 23:09:58.396: INFO: Pod pod-d7b94b94-dbdf-4f7b-8d98-ec6d82b86dfe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:09:58.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9071" for this suite.
May 26 23:10:06.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:10:08.294: INFO: namespace emptydir-9071 deletion completed in 9.884854303s

â€¢ [SLOW TEST:12.192 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:10:08.294: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 23:10:08.478: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 26 23:10:16.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 --namespace=crd-publish-openapi-4594 create -f -'
May 26 23:10:17.158: INFO: stderr: ""
May 26 23:10:17.158: INFO: stdout: "e2e-test-crd-publish-openapi-7591-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 26 23:10:17.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 --namespace=crd-publish-openapi-4594 delete e2e-test-crd-publish-openapi-7591-crds test-cr'
May 26 23:10:17.318: INFO: stderr: ""
May 26 23:10:17.318: INFO: stdout: "e2e-test-crd-publish-openapi-7591-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May 26 23:10:17.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 --namespace=crd-publish-openapi-4594 apply -f -'
May 26 23:10:17.860: INFO: stderr: ""
May 26 23:10:17.860: INFO: stdout: "e2e-test-crd-publish-openapi-7591-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 26 23:10:17.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 --namespace=crd-publish-openapi-4594 delete e2e-test-crd-publish-openapi-7591-crds test-cr'
May 26 23:10:18.027: INFO: stderr: ""
May 26 23:10:18.027: INFO: stdout: "e2e-test-crd-publish-openapi-7591-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
May 26 23:10:18.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 explain e2e-test-crd-publish-openapi-7591-crds'
May 26 23:10:18.604: INFO: stderr: ""
May 26 23:10:18.604: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7591-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:10:26.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4594" for this suite.
May 26 23:10:34.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:10:36.215: INFO: namespace crd-publish-openapi-4594 deletion completed in 9.888227588s

â€¢ [SLOW TEST:27.921 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:10:36.216: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-fc013887-5bf6-4ccc-8234-28d42db4c645
STEP: Creating a pod to test consume secrets
May 26 23:10:36.427: INFO: Waiting up to 5m0s for pod "pod-secrets-fc8ea0c8-cc66-48aa-b100-cabbf0f98f9d" in namespace "secrets-3440" to be "success or failure"
May 26 23:10:36.437: INFO: Pod "pod-secrets-fc8ea0c8-cc66-48aa-b100-cabbf0f98f9d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.338663ms
May 26 23:10:38.449: INFO: Pod "pod-secrets-fc8ea0c8-cc66-48aa-b100-cabbf0f98f9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021637019s
STEP: Saw pod success
May 26 23:10:38.449: INFO: Pod "pod-secrets-fc8ea0c8-cc66-48aa-b100-cabbf0f98f9d" satisfied condition "success or failure"
May 26 23:10:38.459: INFO: Trying to get logs from node 10.215.60.34 pod pod-secrets-fc8ea0c8-cc66-48aa-b100-cabbf0f98f9d container secret-volume-test: <nil>
STEP: delete the pod
May 26 23:10:38.513: INFO: Waiting for pod pod-secrets-fc8ea0c8-cc66-48aa-b100-cabbf0f98f9d to disappear
May 26 23:10:38.523: INFO: Pod pod-secrets-fc8ea0c8-cc66-48aa-b100-cabbf0f98f9d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:10:38.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3440" for this suite.
May 26 23:10:46.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:10:48.426: INFO: namespace secrets-3440 deletion completed in 9.889974205s

â€¢ [SLOW TEST:12.210 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:10:48.426: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-0b9dfcfc-d33a-453f-9b4d-763653f2efce
STEP: Creating a pod to test consume secrets
May 26 23:10:48.622: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fbf9bc5d-a629-4a0c-988b-ecba90161983" in namespace "projected-7882" to be "success or failure"
May 26 23:10:48.635: INFO: Pod "pod-projected-secrets-fbf9bc5d-a629-4a0c-988b-ecba90161983": Phase="Pending", Reason="", readiness=false. Elapsed: 12.707094ms
May 26 23:10:50.645: INFO: Pod "pod-projected-secrets-fbf9bc5d-a629-4a0c-988b-ecba90161983": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022661292s
May 26 23:10:52.655: INFO: Pod "pod-projected-secrets-fbf9bc5d-a629-4a0c-988b-ecba90161983": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032751799s
STEP: Saw pod success
May 26 23:10:52.655: INFO: Pod "pod-projected-secrets-fbf9bc5d-a629-4a0c-988b-ecba90161983" satisfied condition "success or failure"
May 26 23:10:52.664: INFO: Trying to get logs from node 10.215.60.34 pod pod-projected-secrets-fbf9bc5d-a629-4a0c-988b-ecba90161983 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 26 23:10:52.722: INFO: Waiting for pod pod-projected-secrets-fbf9bc5d-a629-4a0c-988b-ecba90161983 to disappear
May 26 23:10:52.731: INFO: Pod pod-projected-secrets-fbf9bc5d-a629-4a0c-988b-ecba90161983 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:10:52.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7882" for this suite.
May 26 23:11:00.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:11:02.634: INFO: namespace projected-7882 deletion completed in 9.889261943s

â€¢ [SLOW TEST:14.208 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:11:02.634: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 26 23:11:02.816: INFO: Waiting up to 5m0s for pod "downwardapi-volume-66ea5366-17a5-488f-86f9-8ffe6f6a558e" in namespace "projected-6918" to be "success or failure"
May 26 23:11:02.828: INFO: Pod "downwardapi-volume-66ea5366-17a5-488f-86f9-8ffe6f6a558e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.864933ms
May 26 23:11:04.841: INFO: Pod "downwardapi-volume-66ea5366-17a5-488f-86f9-8ffe6f6a558e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024381375s
May 26 23:11:06.851: INFO: Pod "downwardapi-volume-66ea5366-17a5-488f-86f9-8ffe6f6a558e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034432789s
STEP: Saw pod success
May 26 23:11:06.851: INFO: Pod "downwardapi-volume-66ea5366-17a5-488f-86f9-8ffe6f6a558e" satisfied condition "success or failure"
May 26 23:11:06.862: INFO: Trying to get logs from node 10.215.60.34 pod downwardapi-volume-66ea5366-17a5-488f-86f9-8ffe6f6a558e container client-container: <nil>
STEP: delete the pod
May 26 23:11:06.912: INFO: Waiting for pod downwardapi-volume-66ea5366-17a5-488f-86f9-8ffe6f6a558e to disappear
May 26 23:11:06.921: INFO: Pod downwardapi-volume-66ea5366-17a5-488f-86f9-8ffe6f6a558e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:11:06.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6918" for this suite.
May 26 23:11:14.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:11:16.843: INFO: namespace projected-6918 deletion completed in 9.900854735s

â€¢ [SLOW TEST:14.209 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:11:16.844: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
May 26 23:11:17.001: INFO: Waiting up to 5m0s for pod "pod-888c521b-0fd2-4d70-b905-65bdfd9fcd67" in namespace "emptydir-9675" to be "success or failure"
May 26 23:11:17.010: INFO: Pod "pod-888c521b-0fd2-4d70-b905-65bdfd9fcd67": Phase="Pending", Reason="", readiness=false. Elapsed: 8.068226ms
May 26 23:11:19.019: INFO: Pod "pod-888c521b-0fd2-4d70-b905-65bdfd9fcd67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017424147s
STEP: Saw pod success
May 26 23:11:19.019: INFO: Pod "pod-888c521b-0fd2-4d70-b905-65bdfd9fcd67" satisfied condition "success or failure"
May 26 23:11:19.028: INFO: Trying to get logs from node 10.215.60.34 pod pod-888c521b-0fd2-4d70-b905-65bdfd9fcd67 container test-container: <nil>
STEP: delete the pod
May 26 23:11:19.096: INFO: Waiting for pod pod-888c521b-0fd2-4d70-b905-65bdfd9fcd67 to disappear
May 26 23:11:19.104: INFO: Pod pod-888c521b-0fd2-4d70-b905-65bdfd9fcd67 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:11:19.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9675" for this suite.
May 26 23:11:27.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:11:29.412: INFO: namespace emptydir-9675 deletion completed in 10.294350497s

â€¢ [SLOW TEST:12.569 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:11:29.413: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-a9887c0a-9d11-4c2a-86f3-c4d7c5db82e2
STEP: Creating a pod to test consume configMaps
May 26 23:11:29.680: INFO: Waiting up to 5m0s for pod "pod-configmaps-00490ee8-a053-4401-818c-fef428998b12" in namespace "configmap-4755" to be "success or failure"
May 26 23:11:29.689: INFO: Pod "pod-configmaps-00490ee8-a053-4401-818c-fef428998b12": Phase="Pending", Reason="", readiness=false. Elapsed: 8.806824ms
May 26 23:11:31.699: INFO: Pod "pod-configmaps-00490ee8-a053-4401-818c-fef428998b12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019487697s
May 26 23:11:33.713: INFO: Pod "pod-configmaps-00490ee8-a053-4401-818c-fef428998b12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033068719s
STEP: Saw pod success
May 26 23:11:33.713: INFO: Pod "pod-configmaps-00490ee8-a053-4401-818c-fef428998b12" satisfied condition "success or failure"
May 26 23:11:33.723: INFO: Trying to get logs from node 10.215.60.34 pod pod-configmaps-00490ee8-a053-4401-818c-fef428998b12 container configmap-volume-test: <nil>
STEP: delete the pod
May 26 23:11:33.808: INFO: Waiting for pod pod-configmaps-00490ee8-a053-4401-818c-fef428998b12 to disappear
May 26 23:11:33.817: INFO: Pod pod-configmaps-00490ee8-a053-4401-818c-fef428998b12 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:11:33.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4755" for this suite.
May 26 23:11:41.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:11:43.722: INFO: namespace configmap-4755 deletion completed in 9.891718168s

â€¢ [SLOW TEST:14.309 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:11:43.722: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-ccf4d0ef-18ba-4f0b-b977-9d83f93d00a5
STEP: Creating a pod to test consume configMaps
May 26 23:11:43.906: INFO: Waiting up to 5m0s for pod "pod-configmaps-3580cf0f-b985-4e84-a30d-2bada482f20a" in namespace "configmap-1885" to be "success or failure"
May 26 23:11:43.915: INFO: Pod "pod-configmaps-3580cf0f-b985-4e84-a30d-2bada482f20a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.853143ms
May 26 23:11:45.930: INFO: Pod "pod-configmaps-3580cf0f-b985-4e84-a30d-2bada482f20a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023908317s
May 26 23:11:47.942: INFO: Pod "pod-configmaps-3580cf0f-b985-4e84-a30d-2bada482f20a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035695153s
STEP: Saw pod success
May 26 23:11:47.942: INFO: Pod "pod-configmaps-3580cf0f-b985-4e84-a30d-2bada482f20a" satisfied condition "success or failure"
May 26 23:11:47.951: INFO: Trying to get logs from node 10.215.60.34 pod pod-configmaps-3580cf0f-b985-4e84-a30d-2bada482f20a container configmap-volume-test: <nil>
STEP: delete the pod
May 26 23:11:48.008: INFO: Waiting for pod pod-configmaps-3580cf0f-b985-4e84-a30d-2bada482f20a to disappear
May 26 23:11:48.017: INFO: Pod pod-configmaps-3580cf0f-b985-4e84-a30d-2bada482f20a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:11:48.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1885" for this suite.
May 26 23:11:56.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:11:57.933: INFO: namespace configmap-1885 deletion completed in 9.902226275s

â€¢ [SLOW TEST:14.211 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:11:57.933: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-b807866e-0d4f-4b4f-b642-841538232d80
STEP: Creating a pod to test consume secrets
May 26 23:11:59.121: INFO: Waiting up to 5m0s for pod "pod-secrets-a0676cc6-c3d9-4c6d-8781-069a77eed13e" in namespace "secrets-756" to be "success or failure"
May 26 23:11:59.130: INFO: Pod "pod-secrets-a0676cc6-c3d9-4c6d-8781-069a77eed13e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.249105ms
May 26 23:12:01.140: INFO: Pod "pod-secrets-a0676cc6-c3d9-4c6d-8781-069a77eed13e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018979177s
STEP: Saw pod success
May 26 23:12:01.140: INFO: Pod "pod-secrets-a0676cc6-c3d9-4c6d-8781-069a77eed13e" satisfied condition "success or failure"
May 26 23:12:01.148: INFO: Trying to get logs from node 10.215.60.34 pod pod-secrets-a0676cc6-c3d9-4c6d-8781-069a77eed13e container secret-env-test: <nil>
STEP: delete the pod
May 26 23:12:01.216: INFO: Waiting for pod pod-secrets-a0676cc6-c3d9-4c6d-8781-069a77eed13e to disappear
May 26 23:12:01.225: INFO: Pod pod-secrets-a0676cc6-c3d9-4c6d-8781-069a77eed13e no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:12:01.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-756" for this suite.
May 26 23:12:09.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:12:11.138: INFO: namespace secrets-756 deletion completed in 9.897658898s

â€¢ [SLOW TEST:13.205 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:12:11.139: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 23:12:11.289: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-c3540d5a-9a84-46f4-a345-f9acc1aad76a
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-c3540d5a-9a84-46f4-a345-f9acc1aad76a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:12:15.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7461" for this suite.
May 26 23:12:45.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:12:47.376: INFO: namespace configmap-7461 deletion completed in 31.905736385s

â€¢ [SLOW TEST:36.237 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:12:47.376: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:12:47.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-178" for this suite.
May 26 23:12:55.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:12:57.395: INFO: namespace services-178 deletion completed in 9.88637295s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:10.019 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:12:57.397: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-9896/configmap-test-87ff8690-3f77-48de-b5bb-b944fe603ec9
STEP: Creating a pod to test consume configMaps
May 26 23:12:57.587: INFO: Waiting up to 5m0s for pod "pod-configmaps-90b109e3-7fa8-489e-9695-e1a8a34ccc85" in namespace "configmap-9896" to be "success or failure"
May 26 23:12:57.606: INFO: Pod "pod-configmaps-90b109e3-7fa8-489e-9695-e1a8a34ccc85": Phase="Pending", Reason="", readiness=false. Elapsed: 19.393793ms
May 26 23:12:59.616: INFO: Pod "pod-configmaps-90b109e3-7fa8-489e-9695-e1a8a34ccc85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029068543s
STEP: Saw pod success
May 26 23:12:59.616: INFO: Pod "pod-configmaps-90b109e3-7fa8-489e-9695-e1a8a34ccc85" satisfied condition "success or failure"
May 26 23:12:59.625: INFO: Trying to get logs from node 10.215.60.34 pod pod-configmaps-90b109e3-7fa8-489e-9695-e1a8a34ccc85 container env-test: <nil>
STEP: delete the pod
May 26 23:12:59.677: INFO: Waiting for pod pod-configmaps-90b109e3-7fa8-489e-9695-e1a8a34ccc85 to disappear
May 26 23:12:59.686: INFO: Pod pod-configmaps-90b109e3-7fa8-489e-9695-e1a8a34ccc85 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:12:59.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9896" for this suite.
May 26 23:13:07.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:13:09.589: INFO: namespace configmap-9896 deletion completed in 9.888097431s

â€¢ [SLOW TEST:12.192 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:13:09.590: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May 26 23:13:10.776: INFO: Waiting up to 5m0s for pod "downward-api-7ff1ef66-428d-4ecc-abc6-d396ad7f9915" in namespace "downward-api-1706" to be "success or failure"
May 26 23:13:10.785: INFO: Pod "downward-api-7ff1ef66-428d-4ecc-abc6-d396ad7f9915": Phase="Pending", Reason="", readiness=false. Elapsed: 9.270836ms
May 26 23:13:12.794: INFO: Pod "downward-api-7ff1ef66-428d-4ecc-abc6-d396ad7f9915": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018640725s
STEP: Saw pod success
May 26 23:13:12.794: INFO: Pod "downward-api-7ff1ef66-428d-4ecc-abc6-d396ad7f9915" satisfied condition "success or failure"
May 26 23:13:12.803: INFO: Trying to get logs from node 10.215.60.34 pod downward-api-7ff1ef66-428d-4ecc-abc6-d396ad7f9915 container dapi-container: <nil>
STEP: delete the pod
May 26 23:13:12.861: INFO: Waiting for pod downward-api-7ff1ef66-428d-4ecc-abc6-d396ad7f9915 to disappear
May 26 23:13:12.870: INFO: Pod downward-api-7ff1ef66-428d-4ecc-abc6-d396ad7f9915 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:13:12.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1706" for this suite.
May 26 23:13:20.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:13:22.807: INFO: namespace downward-api-1706 deletion completed in 9.917563539s

â€¢ [SLOW TEST:13.218 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:13:22.808: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 26 23:13:25.545: INFO: Successfully updated pod "pod-update-ccfe8ea1-9ca4-405c-bcdd-5a3e64067e1a"
STEP: verifying the updated pod is in kubernetes
May 26 23:13:25.565: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:13:25.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3030" for this suite.
May 26 23:13:57.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:13:59.473: INFO: namespace pods-3030 deletion completed in 33.889322939s

â€¢ [SLOW TEST:36.666 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:13:59.473: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-25539a1d-212f-4b24-9fbb-e4d78b407661
STEP: Creating a pod to test consume configMaps
May 26 23:13:59.660: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2dac489c-5bf7-4892-90fe-8543a87e9dd9" in namespace "projected-9152" to be "success or failure"
May 26 23:13:59.670: INFO: Pod "pod-projected-configmaps-2dac489c-5bf7-4892-90fe-8543a87e9dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.446727ms
May 26 23:14:01.681: INFO: Pod "pod-projected-configmaps-2dac489c-5bf7-4892-90fe-8543a87e9dd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020461393s
STEP: Saw pod success
May 26 23:14:01.681: INFO: Pod "pod-projected-configmaps-2dac489c-5bf7-4892-90fe-8543a87e9dd9" satisfied condition "success or failure"
May 26 23:14:01.693: INFO: Trying to get logs from node 10.215.60.34 pod pod-projected-configmaps-2dac489c-5bf7-4892-90fe-8543a87e9dd9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 26 23:14:01.767: INFO: Waiting for pod pod-projected-configmaps-2dac489c-5bf7-4892-90fe-8543a87e9dd9 to disappear
May 26 23:14:01.777: INFO: Pod pod-projected-configmaps-2dac489c-5bf7-4892-90fe-8543a87e9dd9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:14:01.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9152" for this suite.
May 26 23:14:09.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:14:11.686: INFO: namespace projected-9152 deletion completed in 9.888189063s

â€¢ [SLOW TEST:12.212 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:14:11.686: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-862150fe-a764-4283-a3fa-1497d41bf335
STEP: Creating a pod to test consume secrets
May 26 23:14:11.925: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-64f358f9-e7af-4d27-a106-ca0919e5aaf7" in namespace "projected-4509" to be "success or failure"
May 26 23:14:11.934: INFO: Pod "pod-projected-secrets-64f358f9-e7af-4d27-a106-ca0919e5aaf7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.814826ms
May 26 23:14:13.947: INFO: Pod "pod-projected-secrets-64f358f9-e7af-4d27-a106-ca0919e5aaf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021614876s
STEP: Saw pod success
May 26 23:14:13.947: INFO: Pod "pod-projected-secrets-64f358f9-e7af-4d27-a106-ca0919e5aaf7" satisfied condition "success or failure"
May 26 23:14:13.957: INFO: Trying to get logs from node 10.215.60.34 pod pod-projected-secrets-64f358f9-e7af-4d27-a106-ca0919e5aaf7 container secret-volume-test: <nil>
STEP: delete the pod
May 26 23:14:14.011: INFO: Waiting for pod pod-projected-secrets-64f358f9-e7af-4d27-a106-ca0919e5aaf7 to disappear
May 26 23:14:14.021: INFO: Pod pod-projected-secrets-64f358f9-e7af-4d27-a106-ca0919e5aaf7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:14:14.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4509" for this suite.
May 26 23:14:22.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:14:23.923: INFO: namespace projected-4509 deletion completed in 9.887951157s

â€¢ [SLOW TEST:12.237 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:14:23.923: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:14:26.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8070" for this suite.
May 26 23:15:14.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:15:16.045: INFO: namespace kubelet-test-8070 deletion completed in 49.890991492s

â€¢ [SLOW TEST:52.122 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:15:16.045: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-0b8c806a-22d6-48da-b67a-28a759a1f36d
STEP: Creating a pod to test consume configMaps
May 26 23:15:16.248: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c880b8df-ad10-44d1-9438-2a49144c378e" in namespace "projected-266" to be "success or failure"
May 26 23:15:16.258: INFO: Pod "pod-projected-configmaps-c880b8df-ad10-44d1-9438-2a49144c378e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.456875ms
May 26 23:15:18.269: INFO: Pod "pod-projected-configmaps-c880b8df-ad10-44d1-9438-2a49144c378e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020913466s
STEP: Saw pod success
May 26 23:15:18.269: INFO: Pod "pod-projected-configmaps-c880b8df-ad10-44d1-9438-2a49144c378e" satisfied condition "success or failure"
May 26 23:15:18.278: INFO: Trying to get logs from node 10.215.60.34 pod pod-projected-configmaps-c880b8df-ad10-44d1-9438-2a49144c378e container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 26 23:15:18.345: INFO: Waiting for pod pod-projected-configmaps-c880b8df-ad10-44d1-9438-2a49144c378e to disappear
May 26 23:15:18.355: INFO: Pod pod-projected-configmaps-c880b8df-ad10-44d1-9438-2a49144c378e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:15:18.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-266" for this suite.
May 26 23:15:26.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:15:28.261: INFO: namespace projected-266 deletion completed in 9.887000366s

â€¢ [SLOW TEST:12.216 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:15:28.262: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 26 23:15:29.239: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 26 23:15:32.298: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:15:32.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-708" for this suite.
May 26 23:15:40.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:15:42.375: INFO: namespace webhook-708 deletion completed in 9.886166002s
STEP: Destroying namespace "webhook-708-markers" for this suite.
May 26 23:15:50.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:15:52.275: INFO: namespace webhook-708-markers deletion completed in 9.900015435s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:24.059 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:15:52.321: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-a324d434-f8d5-41d8-8a8f-4f540ea9470e in namespace container-probe-1354
May 26 23:15:56.571: INFO: Started pod liveness-a324d434-f8d5-41d8-8a8f-4f540ea9470e in namespace container-probe-1354
STEP: checking the pod's current state and verifying that restartCount is present
May 26 23:15:56.581: INFO: Initial restart count of pod liveness-a324d434-f8d5-41d8-8a8f-4f540ea9470e is 0
May 26 23:16:14.686: INFO: Restart count of pod container-probe-1354/liveness-a324d434-f8d5-41d8-8a8f-4f540ea9470e is now 1 (18.104790722s elapsed)
May 26 23:16:34.796: INFO: Restart count of pod container-probe-1354/liveness-a324d434-f8d5-41d8-8a8f-4f540ea9470e is now 2 (38.21430139s elapsed)
May 26 23:16:54.908: INFO: Restart count of pod container-probe-1354/liveness-a324d434-f8d5-41d8-8a8f-4f540ea9470e is now 3 (58.326807388s elapsed)
May 26 23:17:15.027: INFO: Restart count of pod container-probe-1354/liveness-a324d434-f8d5-41d8-8a8f-4f540ea9470e is now 4 (1m18.445998836s elapsed)
May 26 23:18:17.346: INFO: Restart count of pod container-probe-1354/liveness-a324d434-f8d5-41d8-8a8f-4f540ea9470e is now 5 (2m20.764262052s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:18:17.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1354" for this suite.
May 26 23:18:25.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:18:27.284: INFO: namespace container-probe-1354 deletion completed in 9.890683565s

â€¢ [SLOW TEST:154.963 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:18:27.284: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
May 26 23:18:29.541: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-297629347 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
May 26 23:18:34.743: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:18:34.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7483" for this suite.
May 26 23:18:42.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:18:44.689: INFO: namespace pods-7483 deletion completed in 9.912583952s

â€¢ [SLOW TEST:17.404 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:18:44.689: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May 26 23:18:44.946: INFO: Waiting up to 5m0s for pod "downward-api-8f2e9c11-1c46-410a-8ae1-b45cb92be454" in namespace "downward-api-4390" to be "success or failure"
May 26 23:18:44.956: INFO: Pod "downward-api-8f2e9c11-1c46-410a-8ae1-b45cb92be454": Phase="Pending", Reason="", readiness=false. Elapsed: 9.38613ms
May 26 23:18:46.966: INFO: Pod "downward-api-8f2e9c11-1c46-410a-8ae1-b45cb92be454": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019222315s
STEP: Saw pod success
May 26 23:18:46.966: INFO: Pod "downward-api-8f2e9c11-1c46-410a-8ae1-b45cb92be454" satisfied condition "success or failure"
May 26 23:18:46.975: INFO: Trying to get logs from node 10.215.60.34 pod downward-api-8f2e9c11-1c46-410a-8ae1-b45cb92be454 container dapi-container: <nil>
STEP: delete the pod
May 26 23:18:47.029: INFO: Waiting for pod downward-api-8f2e9c11-1c46-410a-8ae1-b45cb92be454 to disappear
May 26 23:18:47.039: INFO: Pod downward-api-8f2e9c11-1c46-410a-8ae1-b45cb92be454 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:18:47.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4390" for this suite.
May 26 23:18:55.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:18:56.943: INFO: namespace downward-api-4390 deletion completed in 9.887477491s

â€¢ [SLOW TEST:12.254 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:18:56.943: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
May 26 23:19:01.972: INFO: Successfully updated pod "annotationupdatea6409a1a-6bc8-424f-a8e7-76e19107679c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:19:04.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-174" for this suite.
May 26 23:19:36.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:19:37.930: INFO: namespace projected-174 deletion completed in 33.886978002s

â€¢ [SLOW TEST:40.987 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:19:37.930: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
May 26 23:20:08.205: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0526 23:20:08.205701      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:20:08.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2503" for this suite.
May 26 23:20:16.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:20:18.111: INFO: namespace gc-2503 deletion completed in 9.88886039s

â€¢ [SLOW TEST:40.180 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:20:18.111: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 23:20:18.279: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-91e13273-b2fb-4edc-9a38-e49d3eedc096
STEP: Creating configMap with name cm-test-opt-upd-8d877820-2740-4702-9c1b-4aadac827768
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-91e13273-b2fb-4edc-9a38-e49d3eedc096
STEP: Updating configmap cm-test-opt-upd-8d877820-2740-4702-9c1b-4aadac827768
STEP: Creating configMap with name cm-test-opt-create-f5c9c9a4-8f24-4027-9469-45965f305b4e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:20:24.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9496" for this suite.
May 26 23:20:38.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:20:40.496: INFO: namespace projected-9496 deletion completed in 15.887311767s

â€¢ [SLOW TEST:22.385 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:20:40.498: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 23:20:40.688: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 26 23:20:40.734: INFO: Number of nodes with available pods: 0
May 26 23:20:40.734: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 23:20:41.764: INFO: Number of nodes with available pods: 0
May 26 23:20:41.764: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 23:20:42.758: INFO: Number of nodes with available pods: 1
May 26 23:20:42.758: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 23:20:43.757: INFO: Number of nodes with available pods: 3
May 26 23:20:43.757: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 26 23:20:43.829: INFO: Wrong image for pod: daemon-set-4pj44. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:43.830: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:43.830: INFO: Wrong image for pod: daemon-set-tlbf6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:44.855: INFO: Wrong image for pod: daemon-set-4pj44. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:44.855: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:44.855: INFO: Wrong image for pod: daemon-set-tlbf6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:45.852: INFO: Wrong image for pod: daemon-set-4pj44. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:45.852: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:45.852: INFO: Wrong image for pod: daemon-set-tlbf6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:46.851: INFO: Wrong image for pod: daemon-set-4pj44. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:46.852: INFO: Pod daemon-set-4pj44 is not available
May 26 23:20:46.852: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:46.852: INFO: Wrong image for pod: daemon-set-tlbf6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:47.852: INFO: Wrong image for pod: daemon-set-4pj44. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:47.852: INFO: Pod daemon-set-4pj44 is not available
May 26 23:20:47.852: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:47.852: INFO: Wrong image for pod: daemon-set-tlbf6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:48.852: INFO: Wrong image for pod: daemon-set-4pj44. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:48.852: INFO: Pod daemon-set-4pj44 is not available
May 26 23:20:48.852: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:48.852: INFO: Wrong image for pod: daemon-set-tlbf6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:49.852: INFO: Wrong image for pod: daemon-set-4pj44. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:49.852: INFO: Pod daemon-set-4pj44 is not available
May 26 23:20:49.852: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:49.852: INFO: Wrong image for pod: daemon-set-tlbf6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:50.852: INFO: Pod daemon-set-2mqs6 is not available
May 26 23:20:50.852: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:50.852: INFO: Wrong image for pod: daemon-set-tlbf6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:51.851: INFO: Pod daemon-set-2mqs6 is not available
May 26 23:20:51.851: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:51.851: INFO: Wrong image for pod: daemon-set-tlbf6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:52.851: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:52.851: INFO: Wrong image for pod: daemon-set-tlbf6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:53.855: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:53.856: INFO: Wrong image for pod: daemon-set-tlbf6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:54.854: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:54.854: INFO: Wrong image for pod: daemon-set-tlbf6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:54.854: INFO: Pod daemon-set-tlbf6 is not available
May 26 23:20:55.853: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:55.853: INFO: Wrong image for pod: daemon-set-tlbf6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:55.853: INFO: Pod daemon-set-tlbf6 is not available
May 26 23:20:56.851: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:56.851: INFO: Wrong image for pod: daemon-set-tlbf6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:56.851: INFO: Pod daemon-set-tlbf6 is not available
May 26 23:20:57.853: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:58.853: INFO: Pod daemon-set-4hbsr is not available
May 26 23:20:58.853: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:20:59.851: INFO: Pod daemon-set-4hbsr is not available
May 26 23:20:59.852: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:21:00.851: INFO: Pod daemon-set-4hbsr is not available
May 26 23:21:00.851: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:21:01.852: INFO: Pod daemon-set-4hbsr is not available
May 26 23:21:01.852: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:21:02.852: INFO: Pod daemon-set-4hbsr is not available
May 26 23:21:02.852: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:21:03.851: INFO: Pod daemon-set-4hbsr is not available
May 26 23:21:03.851: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:21:04.855: INFO: Pod daemon-set-4hbsr is not available
May 26 23:21:04.855: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:21:05.852: INFO: Pod daemon-set-4hbsr is not available
May 26 23:21:05.852: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:21:06.853: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:21:07.852: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:21:07.852: INFO: Pod daemon-set-szllv is not available
May 26 23:21:08.852: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:21:08.852: INFO: Pod daemon-set-szllv is not available
May 26 23:21:09.852: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:21:09.852: INFO: Pod daemon-set-szllv is not available
May 26 23:21:10.852: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:21:10.852: INFO: Pod daemon-set-szllv is not available
May 26 23:21:11.852: INFO: Wrong image for pod: daemon-set-szllv. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 26 23:21:11.852: INFO: Pod daemon-set-szllv is not available
May 26 23:21:12.853: INFO: Pod daemon-set-gtgw4 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
May 26 23:21:12.889: INFO: Number of nodes with available pods: 2
May 26 23:21:12.889: INFO: Node 10.215.60.11 is running more than one daemon pod
May 26 23:21:13.912: INFO: Number of nodes with available pods: 2
May 26 23:21:13.912: INFO: Node 10.215.60.11 is running more than one daemon pod
May 26 23:21:14.913: INFO: Number of nodes with available pods: 3
May 26 23:21:14.913: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2607, will wait for the garbage collector to delete the pods
May 26 23:21:15.038: INFO: Deleting DaemonSet.extensions daemon-set took: 21.304079ms
May 26 23:21:15.438: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.26499ms
May 26 23:21:20.547: INFO: Number of nodes with available pods: 0
May 26 23:21:20.547: INFO: Number of running nodes: 0, number of available pods: 0
May 26 23:21:20.556: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2607/daemonsets","resourceVersion":"90847"},"items":null}

May 26 23:21:20.565: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2607/pods","resourceVersion":"90848"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:21:20.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2607" for this suite.
May 26 23:21:28.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:21:30.508: INFO: namespace daemonsets-2607 deletion completed in 9.885088169s

â€¢ [SLOW TEST:50.011 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:21:30.513: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3175.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3175.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3175.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3175.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 26 23:21:34.824: INFO: DNS probes using dns-test-acee9dd7-a4e9-4e53-bbe1-a81a145d33a0 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3175.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3175.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3175.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3175.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 26 23:21:36.961: INFO: File wheezy_udp@dns-test-service-3.dns-3175.svc.cluster.local from pod  dns-3175/dns-test-be16c615-a3e1-4b4d-b7c8-75459522fccc contains 'foo.example.com.
' instead of 'bar.example.com.'
May 26 23:21:36.975: INFO: File jessie_udp@dns-test-service-3.dns-3175.svc.cluster.local from pod  dns-3175/dns-test-be16c615-a3e1-4b4d-b7c8-75459522fccc contains 'foo.example.com.
' instead of 'bar.example.com.'
May 26 23:21:36.975: INFO: Lookups using dns-3175/dns-test-be16c615-a3e1-4b4d-b7c8-75459522fccc failed for: [wheezy_udp@dns-test-service-3.dns-3175.svc.cluster.local jessie_udp@dns-test-service-3.dns-3175.svc.cluster.local]

May 26 23:21:42.240: INFO: DNS probes using dns-test-be16c615-a3e1-4b4d-b7c8-75459522fccc succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3175.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3175.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3175.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3175.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 26 23:21:46.438: INFO: DNS probes using dns-test-11748557-0698-4780-aa22-881cf28725e7 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:21:46.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3175" for this suite.
May 26 23:21:54.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:21:56.435: INFO: namespace dns-3175 deletion completed in 9.900245361s

â€¢ [SLOW TEST:25.922 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:21:56.435: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
May 26 23:21:56.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 create -f - --namespace=kubectl-3311'
May 26 23:21:57.244: INFO: stderr: ""
May 26 23:21:57.244: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 26 23:21:57.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3311'
May 26 23:21:57.405: INFO: stderr: ""
May 26 23:21:57.405: INFO: stdout: "update-demo-nautilus-7tr2c update-demo-nautilus-nm498 "
May 26 23:21:57.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-nautilus-7tr2c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3311'
May 26 23:21:57.551: INFO: stderr: ""
May 26 23:21:57.551: INFO: stdout: ""
May 26 23:21:57.551: INFO: update-demo-nautilus-7tr2c is created but not running
May 26 23:22:02.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3311'
May 26 23:22:02.699: INFO: stderr: ""
May 26 23:22:02.699: INFO: stdout: "update-demo-nautilus-7tr2c update-demo-nautilus-nm498 "
May 26 23:22:02.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-nautilus-7tr2c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3311'
May 26 23:22:02.836: INFO: stderr: ""
May 26 23:22:02.836: INFO: stdout: "true"
May 26 23:22:02.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-nautilus-7tr2c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3311'
May 26 23:22:02.973: INFO: stderr: ""
May 26 23:22:02.973: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 26 23:22:02.973: INFO: validating pod update-demo-nautilus-7tr2c
May 26 23:22:02.999: INFO: got data: {
  "image": "nautilus.jpg"
}

May 26 23:22:02.999: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 26 23:22:02.999: INFO: update-demo-nautilus-7tr2c is verified up and running
May 26 23:22:02.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-nautilus-nm498 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3311'
May 26 23:22:03.172: INFO: stderr: ""
May 26 23:22:03.172: INFO: stdout: "true"
May 26 23:22:03.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-nautilus-nm498 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3311'
May 26 23:22:03.309: INFO: stderr: ""
May 26 23:22:03.309: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 26 23:22:03.309: INFO: validating pod update-demo-nautilus-nm498
May 26 23:22:03.337: INFO: got data: {
  "image": "nautilus.jpg"
}

May 26 23:22:03.337: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 26 23:22:03.337: INFO: update-demo-nautilus-nm498 is verified up and running
STEP: rolling-update to new replication controller
May 26 23:22:03.343: INFO: scanned /root for discovery docs: <nil>
May 26 23:22:03.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-3311'
May 26 23:22:28.539: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 26 23:22:28.539: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 26 23:22:28.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3311'
May 26 23:22:28.691: INFO: stderr: ""
May 26 23:22:28.691: INFO: stdout: "update-demo-kitten-2lc78 update-demo-kitten-krvvq "
May 26 23:22:28.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-kitten-2lc78 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3311'
May 26 23:22:28.819: INFO: stderr: ""
May 26 23:22:28.819: INFO: stdout: "true"
May 26 23:22:28.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-kitten-2lc78 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3311'
May 26 23:22:28.948: INFO: stderr: ""
May 26 23:22:28.948: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 26 23:22:28.948: INFO: validating pod update-demo-kitten-2lc78
May 26 23:22:28.968: INFO: got data: {
  "image": "kitten.jpg"
}

May 26 23:22:28.968: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 26 23:22:28.969: INFO: update-demo-kitten-2lc78 is verified up and running
May 26 23:22:28.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-kitten-krvvq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3311'
May 26 23:22:29.116: INFO: stderr: ""
May 26 23:22:29.116: INFO: stdout: "true"
May 26 23:22:29.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-kitten-krvvq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3311'
May 26 23:22:29.257: INFO: stderr: ""
May 26 23:22:29.257: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 26 23:22:29.257: INFO: validating pod update-demo-kitten-krvvq
May 26 23:22:29.280: INFO: got data: {
  "image": "kitten.jpg"
}

May 26 23:22:29.280: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 26 23:22:29.280: INFO: update-demo-kitten-krvvq is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:22:29.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3311" for this suite.
May 26 23:22:43.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:22:45.224: INFO: namespace kubectl-3311 deletion completed in 15.92194327s

â€¢ [SLOW TEST:48.789 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:22:45.224: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8045.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8045.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8045.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8045.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8045.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8045.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8045.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8045.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8045.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8045.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8045.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8045.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8045.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8045.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8045.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8045.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8045.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8045.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 26 23:22:49.464: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8045.svc.cluster.local from pod dns-8045/dns-test-04c4dfcf-bc16-4519-a50c-6b0a5445a632: the server could not find the requested resource (get pods dns-test-04c4dfcf-bc16-4519-a50c-6b0a5445a632)
May 26 23:22:49.507: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8045.svc.cluster.local from pod dns-8045/dns-test-04c4dfcf-bc16-4519-a50c-6b0a5445a632: the server could not find the requested resource (get pods dns-test-04c4dfcf-bc16-4519-a50c-6b0a5445a632)
May 26 23:22:49.522: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8045.svc.cluster.local from pod dns-8045/dns-test-04c4dfcf-bc16-4519-a50c-6b0a5445a632: the server could not find the requested resource (get pods dns-test-04c4dfcf-bc16-4519-a50c-6b0a5445a632)
May 26 23:22:49.587: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8045.svc.cluster.local from pod dns-8045/dns-test-04c4dfcf-bc16-4519-a50c-6b0a5445a632: the server could not find the requested resource (get pods dns-test-04c4dfcf-bc16-4519-a50c-6b0a5445a632)
May 26 23:22:49.605: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8045.svc.cluster.local from pod dns-8045/dns-test-04c4dfcf-bc16-4519-a50c-6b0a5445a632: the server could not find the requested resource (get pods dns-test-04c4dfcf-bc16-4519-a50c-6b0a5445a632)
May 26 23:22:49.621: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8045.svc.cluster.local from pod dns-8045/dns-test-04c4dfcf-bc16-4519-a50c-6b0a5445a632: the server could not find the requested resource (get pods dns-test-04c4dfcf-bc16-4519-a50c-6b0a5445a632)
May 26 23:22:49.658: INFO: Lookups using dns-8045/dns-test-04c4dfcf-bc16-4519-a50c-6b0a5445a632 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8045.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8045.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8045.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8045.svc.cluster.local jessie_udp@dns-test-service-2.dns-8045.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8045.svc.cluster.local]

May 26 23:22:54.848: INFO: DNS probes using dns-8045/dns-test-04c4dfcf-bc16-4519-a50c-6b0a5445a632 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:22:54.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8045" for this suite.
May 26 23:23:02.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:23:04.855: INFO: namespace dns-8045 deletion completed in 9.888299635s

â€¢ [SLOW TEST:19.631 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:23:04.858: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 23:23:05.114: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:23:05.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6795" for this suite.
May 26 23:23:13.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:23:15.620: INFO: namespace custom-resource-definition-6795 deletion completed in 9.896437672s

â€¢ [SLOW TEST:10.762 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:23:15.621: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 26 23:23:16.828: INFO: Waiting up to 5m0s for pod "pod-94bee5f0-15cd-485b-bff6-971b26b4e029" in namespace "emptydir-6226" to be "success or failure"
May 26 23:23:16.837: INFO: Pod "pod-94bee5f0-15cd-485b-bff6-971b26b4e029": Phase="Pending", Reason="", readiness=false. Elapsed: 8.747545ms
May 26 23:23:18.847: INFO: Pod "pod-94bee5f0-15cd-485b-bff6-971b26b4e029": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018696392s
STEP: Saw pod success
May 26 23:23:18.847: INFO: Pod "pod-94bee5f0-15cd-485b-bff6-971b26b4e029" satisfied condition "success or failure"
May 26 23:23:18.858: INFO: Trying to get logs from node 10.215.60.34 pod pod-94bee5f0-15cd-485b-bff6-971b26b4e029 container test-container: <nil>
STEP: delete the pod
May 26 23:23:18.951: INFO: Waiting for pod pod-94bee5f0-15cd-485b-bff6-971b26b4e029 to disappear
May 26 23:23:18.959: INFO: Pod pod-94bee5f0-15cd-485b-bff6-971b26b4e029 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:23:18.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6226" for this suite.
May 26 23:23:27.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:23:28.897: INFO: namespace emptydir-6226 deletion completed in 9.922885047s

â€¢ [SLOW TEST:13.276 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:23:28.899: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 23:23:29.095: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 26 23:23:34.105: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 26 23:23:34.105: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May 26 23:23:34.159: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4111 /apis/apps/v1/namespaces/deployment-4111/deployments/test-cleanup-deployment 1793aa1b-1841-4461-94da-f33a51da0cba 92060 1 2020-05-26 23:23:34 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001599478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

May 26 23:23:34.169: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-4111 /apis/apps/v1/namespaces/deployment-4111/replicasets/test-cleanup-deployment-65db99849b f8852dc6-009a-4928-88bd-214a1fc7b72e 92062 1 2020-05-26 23:23:34 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 1793aa1b-1841-4461-94da-f33a51da0cba 0xc004d20f17 0xc004d20f18}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004d20f78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 26 23:23:34.169: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
May 26 23:23:34.169: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4111 /apis/apps/v1/namespaces/deployment-4111/replicasets/test-cleanup-controller 1b8514de-09e2-4d75-9ad6-e760af03e979 92061 1 2020-05-26 23:23:29 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 1793aa1b-1841-4461-94da-f33a51da0cba 0xc004d20e47 0xc004d20e48}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004d20ea8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 26 23:23:34.179: INFO: Pod "test-cleanup-controller-xgx42" is available:
&Pod{ObjectMeta:{test-cleanup-controller-xgx42 test-cleanup-controller- deployment-4111 /api/v1/namespaces/deployment-4111/pods/test-cleanup-controller-xgx42 cca9cd7f-c739-43ce-8860-69445dfe03b6 92047 0 2020-05-26 23:23:29 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:172.30.182.89/32 cni.projectcalico.org/podIPs:172.30.182.89/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.182.89"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-cleanup-controller 1b8514de-09e2-4d75-9ad6-e760af03e979 0xc004d21417 0xc004d21418}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v8wh5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v8wh5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v8wh5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.34,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-mzm7f,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 23:23:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 23:23:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 23:23:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 23:23:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.34,PodIP:172.30.182.89,StartTime:2020-05-26 23:23:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-26 23:23:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://e454c223d519091567a72daf6a22123e642583f35d1270f5e02383ae2d696d49,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.182.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:23:34.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4111" for this suite.
May 26 23:23:42.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:23:44.325: INFO: namespace deployment-4111 deletion completed in 9.915972621s

â€¢ [SLOW TEST:15.427 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:23:44.326: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
May 26 23:23:46.557: INFO: Pod pod-hostip-bb789ea6-2261-4b5b-8c19-1b1413bd5493 has hostIP: 10.215.60.34
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:23:46.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6273" for this suite.
May 26 23:24:18.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:24:20.473: INFO: namespace pods-6273 deletion completed in 33.897151608s

â€¢ [SLOW TEST:36.148 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:24:20.474: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5984.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5984.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5984.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5984.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5984.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5984.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 26 23:24:22.860: INFO: DNS probes using dns-5984/dns-test-89f4b55f-070b-4916-a197-3a87e5460308 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:24:22.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5984" for this suite.
May 26 23:24:30.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:24:32.855: INFO: namespace dns-5984 deletion completed in 9.891975686s

â€¢ [SLOW TEST:12.382 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:24:32.860: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 26 23:24:35.137: INFO: &Pod{ObjectMeta:{send-events-66fa2829-0216-456a-bdbc-d2b9b7d806f6  events-268 /api/v1/namespaces/events-268/pods/send-events-66fa2829-0216-456a-bdbc-d2b9b7d806f6 415839b9-e818-483c-b003-aebb6324dd1b 92582 0 2020-05-26 23:24:33 +0000 UTC <nil> <nil> map[name:foo time:23244219] map[cni.projectcalico.org/podIP:172.30.182.82/32 cni.projectcalico.org/podIPs:172.30.182.82/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.182.82"
    ],
    "dns": {}
}] openshift.io/scc:anyuid] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v6b4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v6b4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v6b4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.215.60.34,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c65,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 23:24:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 23:24:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 23:24:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-26 23:24:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.215.60.34,PodIP:172.30.182.82,StartTime:2020-05-26 23:24:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-26 23:24:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:cri-o://ba231888d8c463eaa381ef8397bb7cecfe06582926a8eabaa347a9324dabdabb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.182.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
May 26 23:24:37.162: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 26 23:24:39.172: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:24:39.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-268" for this suite.
May 26 23:25:23.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:25:25.118: INFO: namespace events-268 deletion completed in 45.909837612s

â€¢ [SLOW TEST:52.259 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:25:25.119: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-2433
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2433
STEP: creating replication controller externalsvc in namespace services-2433
I0526 23:25:25.384031      23 runners.go:184] Created replication controller with name: externalsvc, namespace: services-2433, replica count: 2
I0526 23:25:28.434852      23 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
May 26 23:25:28.498: INFO: Creating new exec pod
May 26 23:25:30.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 exec --namespace=services-2433 execpodgzwzp -- /bin/sh -x -c nslookup nodeport-service'
May 26 23:25:30.929: INFO: stderr: "+ nslookup nodeport-service\n"
May 26 23:25:30.929: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-2433.svc.cluster.local\tcanonical name = externalsvc.services-2433.svc.cluster.local.\nName:\texternalsvc.services-2433.svc.cluster.local\nAddress: 172.21.149.21\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2433, will wait for the garbage collector to delete the pods
May 26 23:25:31.013: INFO: Deleting ReplicationController externalsvc took: 24.476865ms
May 26 23:25:31.413: INFO: Terminating ReplicationController externalsvc pods took: 400.315955ms
May 26 23:25:42.266: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:25:42.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2433" for this suite.
May 26 23:25:50.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:25:52.237: INFO: namespace services-2433 deletion completed in 9.904830599s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:27.118 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:25:52.238: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 23:25:52.374: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:25:54.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2181" for this suite.
May 26 23:26:42.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:26:44.426: INFO: namespace pods-2181 deletion completed in 49.899169708s

â€¢ [SLOW TEST:52.188 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:26:44.427: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 26 23:26:45.200: INFO: Pod name wrapped-volume-race-8808766a-e3bf-4256-819e-617bfa4ed8dc: Found 0 pods out of 5
May 26 23:26:50.218: INFO: Pod name wrapped-volume-race-8808766a-e3bf-4256-819e-617bfa4ed8dc: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8808766a-e3bf-4256-819e-617bfa4ed8dc in namespace emptydir-wrapper-9277, will wait for the garbage collector to delete the pods
May 26 23:27:00.356: INFO: Deleting ReplicationController wrapped-volume-race-8808766a-e3bf-4256-819e-617bfa4ed8dc took: 23.274481ms
May 26 23:27:00.757: INFO: Terminating ReplicationController wrapped-volume-race-8808766a-e3bf-4256-819e-617bfa4ed8dc pods took: 400.369482ms
STEP: Creating RC which spawns configmap-volume pods
May 26 23:27:38.209: INFO: Pod name wrapped-volume-race-a59c3eba-e2eb-41f5-9364-2e6a040d865d: Found 0 pods out of 5
May 26 23:27:43.225: INFO: Pod name wrapped-volume-race-a59c3eba-e2eb-41f5-9364-2e6a040d865d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a59c3eba-e2eb-41f5-9364-2e6a040d865d in namespace emptydir-wrapper-9277, will wait for the garbage collector to delete the pods
May 26 23:27:43.373: INFO: Deleting ReplicationController wrapped-volume-race-a59c3eba-e2eb-41f5-9364-2e6a040d865d took: 26.759276ms
May 26 23:27:43.774: INFO: Terminating ReplicationController wrapped-volume-race-a59c3eba-e2eb-41f5-9364-2e6a040d865d pods took: 400.389964ms
STEP: Creating RC which spawns configmap-volume pods
May 26 23:28:28.019: INFO: Pod name wrapped-volume-race-7aab9bff-71d8-49e9-a5c3-f2e767b48067: Found 0 pods out of 5
May 26 23:28:33.037: INFO: Pod name wrapped-volume-race-7aab9bff-71d8-49e9-a5c3-f2e767b48067: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7aab9bff-71d8-49e9-a5c3-f2e767b48067 in namespace emptydir-wrapper-9277, will wait for the garbage collector to delete the pods
May 26 23:28:33.177: INFO: Deleting ReplicationController wrapped-volume-race-7aab9bff-71d8-49e9-a5c3-f2e767b48067 took: 23.710827ms
May 26 23:28:33.577: INFO: Terminating ReplicationController wrapped-volume-race-7aab9bff-71d8-49e9-a5c3-f2e767b48067 pods took: 400.526205ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:29:19.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9277" for this suite.
May 26 23:29:29.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:29:31.072: INFO: namespace emptydir-wrapper-9277 deletion completed in 11.88958578s

â€¢ [SLOW TEST:166.645 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:29:31.072: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-9e62c563-41c0-4476-b9db-9e3c6308f112
STEP: Creating a pod to test consume secrets
May 26 23:29:31.253: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f5568ad7-35d5-4a1a-b9ea-dfe06d01029c" in namespace "projected-1232" to be "success or failure"
May 26 23:29:31.262: INFO: Pod "pod-projected-secrets-f5568ad7-35d5-4a1a-b9ea-dfe06d01029c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.99963ms
May 26 23:29:33.272: INFO: Pod "pod-projected-secrets-f5568ad7-35d5-4a1a-b9ea-dfe06d01029c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018837458s
STEP: Saw pod success
May 26 23:29:33.272: INFO: Pod "pod-projected-secrets-f5568ad7-35d5-4a1a-b9ea-dfe06d01029c" satisfied condition "success or failure"
May 26 23:29:33.280: INFO: Trying to get logs from node 10.215.60.34 pod pod-projected-secrets-f5568ad7-35d5-4a1a-b9ea-dfe06d01029c container projected-secret-volume-test: <nil>
STEP: delete the pod
May 26 23:29:33.361: INFO: Waiting for pod pod-projected-secrets-f5568ad7-35d5-4a1a-b9ea-dfe06d01029c to disappear
May 26 23:29:33.369: INFO: Pod pod-projected-secrets-f5568ad7-35d5-4a1a-b9ea-dfe06d01029c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:29:33.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1232" for this suite.
May 26 23:29:41.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:29:43.278: INFO: namespace projected-1232 deletion completed in 9.892747121s

â€¢ [SLOW TEST:12.206 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:29:43.279: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 26 23:29:51.588: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 26 23:29:51.598: INFO: Pod pod-with-poststart-http-hook still exists
May 26 23:29:53.598: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 26 23:29:53.607: INFO: Pod pod-with-poststart-http-hook still exists
May 26 23:29:55.598: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 26 23:29:55.607: INFO: Pod pod-with-poststart-http-hook still exists
May 26 23:29:57.598: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 26 23:29:57.607: INFO: Pod pod-with-poststart-http-hook still exists
May 26 23:29:59.598: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 26 23:29:59.608: INFO: Pod pod-with-poststart-http-hook still exists
May 26 23:30:01.598: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 26 23:30:01.609: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:30:01.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5344" for this suite.
May 26 23:30:15.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:30:17.517: INFO: namespace container-lifecycle-hook-5344 deletion completed in 15.890978146s

â€¢ [SLOW TEST:34.238 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:30:17.517: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
May 26 23:30:22.333: INFO: Successfully updated pod "labelsupdate11a58771-d948-4ea3-b679-9d9b9ec25fab"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:30:24.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9198" for this suite.
May 26 23:30:42.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:30:44.307: INFO: namespace projected-9198 deletion completed in 19.899669582s

â€¢ [SLOW TEST:26.790 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:30:44.307: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 26 23:30:45.208: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 26 23:30:48.264: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 23:30:48.277: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:30:49.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5778" for this suite.
May 26 23:30:57.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:30:59.599: INFO: namespace crd-webhook-5778 deletion completed in 9.889227361s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:15.338 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:30:59.646: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-9924
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9924 to expose endpoints map[]
May 26 23:30:59.900: INFO: successfully validated that service endpoint-test2 in namespace services-9924 exposes endpoints map[] (9.117699ms elapsed)
STEP: Creating pod pod1 in namespace services-9924
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9924 to expose endpoints map[pod1:[80]]
May 26 23:31:02.023: INFO: successfully validated that service endpoint-test2 in namespace services-9924 exposes endpoints map[pod1:[80]] (2.079718901s elapsed)
STEP: Creating pod pod2 in namespace services-9924
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9924 to expose endpoints map[pod1:[80] pod2:[80]]
May 26 23:31:04.147: INFO: successfully validated that service endpoint-test2 in namespace services-9924 exposes endpoints map[pod1:[80] pod2:[80]] (2.093137816s elapsed)
STEP: Deleting pod pod1 in namespace services-9924
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9924 to expose endpoints map[pod2:[80]]
May 26 23:31:04.186: INFO: successfully validated that service endpoint-test2 in namespace services-9924 exposes endpoints map[pod2:[80]] (21.775881ms elapsed)
STEP: Deleting pod pod2 in namespace services-9924
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9924 to expose endpoints map[]
May 26 23:31:05.228: INFO: successfully validated that service endpoint-test2 in namespace services-9924 exposes endpoints map[] (1.022224522s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:31:05.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9924" for this suite.
May 26 23:31:19.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:31:21.181: INFO: namespace services-9924 deletion completed in 15.885607921s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:21.535 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:31:21.181: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 26 23:31:23.956: INFO: Successfully updated pod "pod-update-activedeadlineseconds-eed3d397-80fd-41b6-a020-ac4ac4b09cbc"
May 26 23:31:23.956: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-eed3d397-80fd-41b6-a020-ac4ac4b09cbc" in namespace "pods-9685" to be "terminated due to deadline exceeded"
May 26 23:31:23.964: INFO: Pod "pod-update-activedeadlineseconds-eed3d397-80fd-41b6-a020-ac4ac4b09cbc": Phase="Running", Reason="", readiness=true. Elapsed: 8.021533ms
May 26 23:31:25.974: INFO: Pod "pod-update-activedeadlineseconds-eed3d397-80fd-41b6-a020-ac4ac4b09cbc": Phase="Running", Reason="", readiness=true. Elapsed: 2.017839395s
May 26 23:31:27.984: INFO: Pod "pod-update-activedeadlineseconds-eed3d397-80fd-41b6-a020-ac4ac4b09cbc": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.028048414s
May 26 23:31:27.984: INFO: Pod "pod-update-activedeadlineseconds-eed3d397-80fd-41b6-a020-ac4ac4b09cbc" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:31:27.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9685" for this suite.
May 26 23:31:36.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:31:37.890: INFO: namespace pods-9685 deletion completed in 9.887521098s

â€¢ [SLOW TEST:16.709 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:31:37.890: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 26 23:31:38.084: INFO: Pod name pod-release: Found 0 pods out of 1
May 26 23:31:43.094: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:31:44.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9873" for this suite.
May 26 23:31:52.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:31:54.068: INFO: namespace replication-controller-9873 deletion completed in 9.887341821s

â€¢ [SLOW TEST:16.178 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:31:54.068: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:32:01.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7465" for this suite.
May 26 23:32:09.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:32:11.159: INFO: namespace resourcequota-7465 deletion completed in 9.886922084s

â€¢ [SLOW TEST:17.091 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:32:11.159: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 26 23:32:15.453: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 26 23:32:15.461: INFO: Pod pod-with-prestop-http-hook still exists
May 26 23:32:17.462: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 26 23:32:17.471: INFO: Pod pod-with-prestop-http-hook still exists
May 26 23:32:19.462: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 26 23:32:19.472: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:32:19.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1257" for this suite.
May 26 23:32:33.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:32:35.448: INFO: namespace container-lifecycle-hook-1257 deletion completed in 15.88607422s

â€¢ [SLOW TEST:24.289 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:32:35.449: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 26 23:32:36.263: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 26 23:32:38.293: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726132756, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726132756, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726132756, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726132756, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 26 23:32:41.324: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:32:41.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-490" for this suite.
May 26 23:32:49.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:32:51.403: INFO: namespace webhook-490 deletion completed in 9.898227632s
STEP: Destroying namespace "webhook-490-markers" for this suite.
May 26 23:32:59.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:33:01.288: INFO: namespace webhook-490-markers deletion completed in 9.885023266s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:25.887 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:33:01.336: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 26 23:33:01.712: INFO: Waiting up to 5m0s for pod "downwardapi-volume-948425b1-2b87-4552-8898-8f3b2eacbdad" in namespace "downward-api-2829" to be "success or failure"
May 26 23:33:01.721: INFO: Pod "downwardapi-volume-948425b1-2b87-4552-8898-8f3b2eacbdad": Phase="Pending", Reason="", readiness=false. Elapsed: 9.070536ms
May 26 23:33:03.731: INFO: Pod "downwardapi-volume-948425b1-2b87-4552-8898-8f3b2eacbdad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019307004s
STEP: Saw pod success
May 26 23:33:03.731: INFO: Pod "downwardapi-volume-948425b1-2b87-4552-8898-8f3b2eacbdad" satisfied condition "success or failure"
May 26 23:33:03.741: INFO: Trying to get logs from node 10.215.60.34 pod downwardapi-volume-948425b1-2b87-4552-8898-8f3b2eacbdad container client-container: <nil>
STEP: delete the pod
May 26 23:33:03.800: INFO: Waiting for pod downwardapi-volume-948425b1-2b87-4552-8898-8f3b2eacbdad to disappear
May 26 23:33:03.809: INFO: Pod downwardapi-volume-948425b1-2b87-4552-8898-8f3b2eacbdad no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:33:03.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2829" for this suite.
May 26 23:33:11.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:33:13.722: INFO: namespace downward-api-2829 deletion completed in 9.886667391s

â€¢ [SLOW TEST:12.386 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:33:13.722: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 26 23:33:14.527: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 26 23:33:16.556: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726132794, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726132794, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726132794, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726132794, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 26 23:33:19.594: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:33:29.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8166" for this suite.
May 26 23:33:38.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:33:39.862: INFO: namespace webhook-8166 deletion completed in 9.896394073s
STEP: Destroying namespace "webhook-8166-markers" for this suite.
May 26 23:33:47.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:33:49.748: INFO: namespace webhook-8166-markers deletion completed in 9.885074428s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:36.072 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:33:49.796: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 26 23:33:49.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-6592'
May 26 23:33:50.311: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 26 23:33:50.312: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
May 26 23:33:50.326: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
May 26 23:33:50.369: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
May 26 23:33:50.412: INFO: scanned /root for discovery docs: <nil>
May 26 23:33:50.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-6592'
May 26 23:34:06.494: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 26 23:34:06.494: INFO: stdout: "Created e2e-test-httpd-rc-f294ac2572e3b99139a0cd31b7713156\nScaling up e2e-test-httpd-rc-f294ac2572e3b99139a0cd31b7713156 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-f294ac2572e3b99139a0cd31b7713156 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-f294ac2572e3b99139a0cd31b7713156 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
May 26 23:34:06.494: INFO: stdout: "Created e2e-test-httpd-rc-f294ac2572e3b99139a0cd31b7713156\nScaling up e2e-test-httpd-rc-f294ac2572e3b99139a0cd31b7713156 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-f294ac2572e3b99139a0cd31b7713156 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-f294ac2572e3b99139a0cd31b7713156 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
May 26 23:34:06.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-6592'
May 26 23:34:06.645: INFO: stderr: ""
May 26 23:34:06.645: INFO: stdout: "e2e-test-httpd-rc-f294ac2572e3b99139a0cd31b7713156-vdrw7 "
May 26 23:34:06.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods e2e-test-httpd-rc-f294ac2572e3b99139a0cd31b7713156-vdrw7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6592'
May 26 23:34:06.785: INFO: stderr: ""
May 26 23:34:06.785: INFO: stdout: "true"
May 26 23:34:06.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods e2e-test-httpd-rc-f294ac2572e3b99139a0cd31b7713156-vdrw7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6592'
May 26 23:34:06.915: INFO: stderr: ""
May 26 23:34:06.915: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
May 26 23:34:06.915: INFO: e2e-test-httpd-rc-f294ac2572e3b99139a0cd31b7713156-vdrw7 is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
May 26 23:34:06.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 delete rc e2e-test-httpd-rc --namespace=kubectl-6592'
May 26 23:34:07.086: INFO: stderr: ""
May 26 23:34:07.086: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:34:07.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6592" for this suite.
May 26 23:34:39.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:34:41.002: INFO: namespace kubectl-6592 deletion completed in 33.887855471s

â€¢ [SLOW TEST:51.206 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:34:41.003: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 23:34:41.134: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:34:47.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7233" for this suite.
May 26 23:34:55.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:34:57.450: INFO: namespace custom-resource-definition-7233 deletion completed in 9.89307531s

â€¢ [SLOW TEST:16.447 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:34:57.451: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-34e2bc52-ab0f-42d3-a97c-5be1d6530080
STEP: Creating a pod to test consume configMaps
May 26 23:34:57.633: INFO: Waiting up to 5m0s for pod "pod-configmaps-8dda6ff8-e55a-4e06-b073-d276e5b8af36" in namespace "configmap-2388" to be "success or failure"
May 26 23:34:57.644: INFO: Pod "pod-configmaps-8dda6ff8-e55a-4e06-b073-d276e5b8af36": Phase="Pending", Reason="", readiness=false. Elapsed: 10.23458ms
May 26 23:34:59.659: INFO: Pod "pod-configmaps-8dda6ff8-e55a-4e06-b073-d276e5b8af36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025391296s
May 26 23:35:01.669: INFO: Pod "pod-configmaps-8dda6ff8-e55a-4e06-b073-d276e5b8af36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035226365s
STEP: Saw pod success
May 26 23:35:01.669: INFO: Pod "pod-configmaps-8dda6ff8-e55a-4e06-b073-d276e5b8af36" satisfied condition "success or failure"
May 26 23:35:01.678: INFO: Trying to get logs from node 10.215.60.34 pod pod-configmaps-8dda6ff8-e55a-4e06-b073-d276e5b8af36 container configmap-volume-test: <nil>
STEP: delete the pod
May 26 23:35:02.010: INFO: Waiting for pod pod-configmaps-8dda6ff8-e55a-4e06-b073-d276e5b8af36 to disappear
May 26 23:35:02.019: INFO: Pod pod-configmaps-8dda6ff8-e55a-4e06-b073-d276e5b8af36 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:35:02.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2388" for this suite.
May 26 23:35:10.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:35:11.922: INFO: namespace configmap-2388 deletion completed in 9.888984404s

â€¢ [SLOW TEST:14.472 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:35:11.923: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
May 26 23:35:12.095: INFO: Waiting up to 5m0s for pod "var-expansion-cf685a7f-5eee-4706-8033-0200e027165f" in namespace "var-expansion-3917" to be "success or failure"
May 26 23:35:12.106: INFO: Pod "var-expansion-cf685a7f-5eee-4706-8033-0200e027165f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.037459ms
May 26 23:35:14.116: INFO: Pod "var-expansion-cf685a7f-5eee-4706-8033-0200e027165f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021154854s
STEP: Saw pod success
May 26 23:35:14.116: INFO: Pod "var-expansion-cf685a7f-5eee-4706-8033-0200e027165f" satisfied condition "success or failure"
May 26 23:35:14.125: INFO: Trying to get logs from node 10.215.60.34 pod var-expansion-cf685a7f-5eee-4706-8033-0200e027165f container dapi-container: <nil>
STEP: delete the pod
May 26 23:35:14.188: INFO: Waiting for pod var-expansion-cf685a7f-5eee-4706-8033-0200e027165f to disappear
May 26 23:35:14.199: INFO: Pod var-expansion-cf685a7f-5eee-4706-8033-0200e027165f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:35:14.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3917" for this suite.
May 26 23:35:22.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:35:24.115: INFO: namespace var-expansion-3917 deletion completed in 9.900012524s

â€¢ [SLOW TEST:12.193 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:35:24.115: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-4jvk
STEP: Creating a pod to test atomic-volume-subpath
May 26 23:35:24.348: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-4jvk" in namespace "subpath-3727" to be "success or failure"
May 26 23:35:24.359: INFO: Pod "pod-subpath-test-downwardapi-4jvk": Phase="Pending", Reason="", readiness=false. Elapsed: 10.568894ms
May 26 23:35:26.369: INFO: Pod "pod-subpath-test-downwardapi-4jvk": Phase="Running", Reason="", readiness=true. Elapsed: 2.020845563s
May 26 23:35:28.379: INFO: Pod "pod-subpath-test-downwardapi-4jvk": Phase="Running", Reason="", readiness=true. Elapsed: 4.030751814s
May 26 23:35:30.389: INFO: Pod "pod-subpath-test-downwardapi-4jvk": Phase="Running", Reason="", readiness=true. Elapsed: 6.041250715s
May 26 23:35:32.399: INFO: Pod "pod-subpath-test-downwardapi-4jvk": Phase="Running", Reason="", readiness=true. Elapsed: 8.051211688s
May 26 23:35:34.409: INFO: Pod "pod-subpath-test-downwardapi-4jvk": Phase="Running", Reason="", readiness=true. Elapsed: 10.061061376s
May 26 23:35:36.420: INFO: Pod "pod-subpath-test-downwardapi-4jvk": Phase="Running", Reason="", readiness=true. Elapsed: 12.071319715s
May 26 23:35:38.430: INFO: Pod "pod-subpath-test-downwardapi-4jvk": Phase="Running", Reason="", readiness=true. Elapsed: 14.081453436s
May 26 23:35:40.440: INFO: Pod "pod-subpath-test-downwardapi-4jvk": Phase="Running", Reason="", readiness=true. Elapsed: 16.092140139s
May 26 23:35:42.451: INFO: Pod "pod-subpath-test-downwardapi-4jvk": Phase="Running", Reason="", readiness=true. Elapsed: 18.102729743s
May 26 23:35:44.461: INFO: Pod "pod-subpath-test-downwardapi-4jvk": Phase="Running", Reason="", readiness=true. Elapsed: 20.112912278s
May 26 23:35:46.472: INFO: Pod "pod-subpath-test-downwardapi-4jvk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.124005682s
STEP: Saw pod success
May 26 23:35:46.472: INFO: Pod "pod-subpath-test-downwardapi-4jvk" satisfied condition "success or failure"
May 26 23:35:46.482: INFO: Trying to get logs from node 10.215.60.34 pod pod-subpath-test-downwardapi-4jvk container test-container-subpath-downwardapi-4jvk: <nil>
STEP: delete the pod
May 26 23:35:46.544: INFO: Waiting for pod pod-subpath-test-downwardapi-4jvk to disappear
May 26 23:35:46.553: INFO: Pod pod-subpath-test-downwardapi-4jvk no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-4jvk
May 26 23:35:46.553: INFO: Deleting pod "pod-subpath-test-downwardapi-4jvk" in namespace "subpath-3727"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:35:46.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3727" for this suite.
May 26 23:35:54.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:35:56.479: INFO: namespace subpath-3727 deletion completed in 9.899911024s

â€¢ [SLOW TEST:32.364 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:35:56.479: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:36:03.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7262" for this suite.
May 26 23:36:12.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:36:13.877: INFO: namespace namespaces-7262 deletion completed in 9.908000428s
STEP: Destroying namespace "nsdeletetest-1918" for this suite.
May 26 23:36:13.886: INFO: Namespace nsdeletetest-1918 was already deleted
STEP: Destroying namespace "nsdeletetest-7795" for this suite.
May 26 23:36:21.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:36:23.771: INFO: namespace nsdeletetest-7795 deletion completed in 9.885503931s

â€¢ [SLOW TEST:27.292 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:36:23.772: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 23:36:24.962: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-1989b9cd-f4f8-429d-92fe-b960319bf3d8" in namespace "security-context-test-6755" to be "success or failure"
May 26 23:36:24.971: INFO: Pod "busybox-privileged-false-1989b9cd-f4f8-429d-92fe-b960319bf3d8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.242325ms
May 26 23:36:26.981: INFO: Pod "busybox-privileged-false-1989b9cd-f4f8-429d-92fe-b960319bf3d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018729209s
May 26 23:36:28.991: INFO: Pod "busybox-privileged-false-1989b9cd-f4f8-429d-92fe-b960319bf3d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029073196s
May 26 23:36:28.991: INFO: Pod "busybox-privileged-false-1989b9cd-f4f8-429d-92fe-b960319bf3d8" satisfied condition "success or failure"
May 26 23:36:29.015: INFO: Got logs for pod "busybox-privileged-false-1989b9cd-f4f8-429d-92fe-b960319bf3d8": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:36:29.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6755" for this suite.
May 26 23:36:37.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:36:38.938: INFO: namespace security-context-test-6755 deletion completed in 9.906766109s

â€¢ [SLOW TEST:15.167 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:36:38.939: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 26 23:36:39.851: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 26 23:36:41.878: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726132999, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726132999, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726132999, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726132999, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 26 23:36:44.914: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:36:45.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7064" for this suite.
May 26 23:36:53.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:36:55.381: INFO: namespace webhook-7064 deletion completed in 9.89851906s
STEP: Destroying namespace "webhook-7064-markers" for this suite.
May 26 23:37:03.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:37:05.281: INFO: namespace webhook-7064-markers deletion completed in 9.900477163s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:26.387 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:37:05.326: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
May 26 23:37:05.444: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:37:09.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7407" for this suite.
May 26 23:37:23.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:37:25.175: INFO: namespace init-container-7407 deletion completed in 15.89769557s

â€¢ [SLOW TEST:19.849 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:37:25.176: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 23:37:25.400: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 26 23:37:25.428: INFO: Number of nodes with available pods: 0
May 26 23:37:25.428: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 26 23:37:25.496: INFO: Number of nodes with available pods: 0
May 26 23:37:25.496: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 23:37:26.506: INFO: Number of nodes with available pods: 0
May 26 23:37:26.507: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 23:37:27.510: INFO: Number of nodes with available pods: 0
May 26 23:37:27.510: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 23:37:28.508: INFO: Number of nodes with available pods: 1
May 26 23:37:28.509: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 26 23:37:28.558: INFO: Number of nodes with available pods: 1
May 26 23:37:28.558: INFO: Number of running nodes: 0, number of available pods: 1
May 26 23:37:29.568: INFO: Number of nodes with available pods: 0
May 26 23:37:29.568: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 26 23:37:29.592: INFO: Number of nodes with available pods: 0
May 26 23:37:29.592: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 23:37:30.603: INFO: Number of nodes with available pods: 0
May 26 23:37:30.603: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 23:37:31.602: INFO: Number of nodes with available pods: 0
May 26 23:37:31.602: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 23:37:32.603: INFO: Number of nodes with available pods: 0
May 26 23:37:32.603: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 23:37:33.603: INFO: Number of nodes with available pods: 0
May 26 23:37:33.603: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 23:37:34.602: INFO: Number of nodes with available pods: 0
May 26 23:37:34.603: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 23:37:35.603: INFO: Number of nodes with available pods: 0
May 26 23:37:35.603: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 23:37:36.603: INFO: Number of nodes with available pods: 0
May 26 23:37:36.603: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 23:37:37.602: INFO: Number of nodes with available pods: 0
May 26 23:37:37.602: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 23:37:38.602: INFO: Number of nodes with available pods: 0
May 26 23:37:38.602: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 23:37:39.602: INFO: Number of nodes with available pods: 0
May 26 23:37:39.602: INFO: Node 10.215.60.10 is running more than one daemon pod
May 26 23:37:40.618: INFO: Number of nodes with available pods: 1
May 26 23:37:40.618: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5043, will wait for the garbage collector to delete the pods
May 26 23:37:40.728: INFO: Deleting DaemonSet.extensions daemon-set took: 31.316179ms
May 26 23:37:41.128: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.384117ms
May 26 23:37:47.939: INFO: Number of nodes with available pods: 0
May 26 23:37:47.940: INFO: Number of running nodes: 0, number of available pods: 0
May 26 23:37:47.948: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5043/daemonsets","resourceVersion":"98880"},"items":null}

May 26 23:37:47.957: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5043/pods","resourceVersion":"98880"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:37:48.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5043" for this suite.
May 26 23:37:56.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:37:58.268: INFO: namespace daemonsets-5043 deletion completed in 10.217763139s

â€¢ [SLOW TEST:33.092 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:37:58.268: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:37:58.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5772" for this suite.
May 26 23:38:12.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:38:14.365: INFO: namespace pods-5772 deletion completed in 15.902003189s

â€¢ [SLOW TEST:16.097 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:38:14.366: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 26 23:38:14.593: INFO: Waiting up to 5m0s for pod "downwardapi-volume-55539937-0683-47e2-aa59-407b17598919" in namespace "downward-api-3126" to be "success or failure"
May 26 23:38:14.603: INFO: Pod "downwardapi-volume-55539937-0683-47e2-aa59-407b17598919": Phase="Pending", Reason="", readiness=false. Elapsed: 9.384594ms
May 26 23:38:16.614: INFO: Pod "downwardapi-volume-55539937-0683-47e2-aa59-407b17598919": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020874753s
STEP: Saw pod success
May 26 23:38:16.614: INFO: Pod "downwardapi-volume-55539937-0683-47e2-aa59-407b17598919" satisfied condition "success or failure"
May 26 23:38:16.623: INFO: Trying to get logs from node 10.215.60.34 pod downwardapi-volume-55539937-0683-47e2-aa59-407b17598919 container client-container: <nil>
STEP: delete the pod
May 26 23:38:16.710: INFO: Waiting for pod downwardapi-volume-55539937-0683-47e2-aa59-407b17598919 to disappear
May 26 23:38:16.722: INFO: Pod downwardapi-volume-55539937-0683-47e2-aa59-407b17598919 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:38:16.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3126" for this suite.
May 26 23:38:24.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:38:26.625: INFO: namespace downward-api-3126 deletion completed in 9.887265868s

â€¢ [SLOW TEST:12.259 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:38:26.626: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
May 26 23:38:29.428: INFO: Successfully updated pod "annotationupdatef87ee175-fe7e-4ab7-84f7-98861d367ab0"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:38:31.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8837" for this suite.
May 26 23:38:45.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:38:47.393: INFO: namespace downward-api-8837 deletion completed in 15.902266582s

â€¢ [SLOW TEST:20.768 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:38:47.394: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 23:38:48.585: INFO: Waiting up to 5m0s for pod "busybox-user-65534-680619a7-b8da-4946-a27e-293d4a74c9df" in namespace "security-context-test-1756" to be "success or failure"
May 26 23:38:48.595: INFO: Pod "busybox-user-65534-680619a7-b8da-4946-a27e-293d4a74c9df": Phase="Pending", Reason="", readiness=false. Elapsed: 9.732959ms
May 26 23:38:50.604: INFO: Pod "busybox-user-65534-680619a7-b8da-4946-a27e-293d4a74c9df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018771929s
May 26 23:38:50.604: INFO: Pod "busybox-user-65534-680619a7-b8da-4946-a27e-293d4a74c9df" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:38:50.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1756" for this suite.
May 26 23:38:58.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:39:00.521: INFO: namespace security-context-test-1756 deletion completed in 9.900783536s

â€¢ [SLOW TEST:13.127 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:39:00.521: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 26 23:39:00.753: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1f9ad707-850c-48ab-9919-8cf7404241b8" in namespace "projected-9835" to be "success or failure"
May 26 23:39:00.766: INFO: Pod "downwardapi-volume-1f9ad707-850c-48ab-9919-8cf7404241b8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.84664ms
May 26 23:39:02.779: INFO: Pod "downwardapi-volume-1f9ad707-850c-48ab-9919-8cf7404241b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025300675s
STEP: Saw pod success
May 26 23:39:02.779: INFO: Pod "downwardapi-volume-1f9ad707-850c-48ab-9919-8cf7404241b8" satisfied condition "success or failure"
May 26 23:39:02.789: INFO: Trying to get logs from node 10.215.60.34 pod downwardapi-volume-1f9ad707-850c-48ab-9919-8cf7404241b8 container client-container: <nil>
STEP: delete the pod
May 26 23:39:02.859: INFO: Waiting for pod downwardapi-volume-1f9ad707-850c-48ab-9919-8cf7404241b8 to disappear
May 26 23:39:02.879: INFO: Pod downwardapi-volume-1f9ad707-850c-48ab-9919-8cf7404241b8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:39:02.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9835" for this suite.
May 26 23:39:10.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:39:12.804: INFO: namespace projected-9835 deletion completed in 9.90926471s

â€¢ [SLOW TEST:12.283 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:39:12.804: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
May 26 23:39:12.978: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:39:54.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7306" for this suite.
May 26 23:40:02.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:40:04.631: INFO: namespace crd-publish-openapi-7306 deletion completed in 9.886637535s

â€¢ [SLOW TEST:51.826 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:40:04.631: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 23:40:04.796: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-709444d8-feeb-4f3e-9d41-fa2df9680356
STEP: Creating configMap with name cm-test-opt-upd-0a7108bd-23a7-4f9b-8056-a8143a4789af
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-709444d8-feeb-4f3e-9d41-fa2df9680356
STEP: Updating configmap cm-test-opt-upd-0a7108bd-23a7-4f9b-8056-a8143a4789af
STEP: Creating configMap with name cm-test-opt-create-966c7e78-211b-444a-9b44-1b435c8f9e2a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:41:18.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7430" for this suite.
May 26 23:41:32.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:41:34.132: INFO: namespace configmap-7430 deletion completed in 15.885829806s

â€¢ [SLOW TEST:89.501 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:41:34.132: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
May 26 23:41:36.919: INFO: Successfully updated pod "labelsupdate0137bea3-1bf8-44d9-8c60-18690bca93db"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:41:38.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4410" for this suite.
May 26 23:41:53.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:41:54.868: INFO: namespace downward-api-4410 deletion completed in 15.88595698s

â€¢ [SLOW TEST:20.736 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:41:54.868: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
May 26 23:41:55.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 create -f - --namespace=kubectl-61'
May 26 23:41:55.561: INFO: stderr: ""
May 26 23:41:55.561: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 26 23:41:55.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-61'
May 26 23:41:55.718: INFO: stderr: ""
May 26 23:41:55.718: INFO: stdout: "update-demo-nautilus-qxx9x update-demo-nautilus-tgwdc "
May 26 23:41:55.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-nautilus-qxx9x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-61'
May 26 23:41:55.860: INFO: stderr: ""
May 26 23:41:55.860: INFO: stdout: ""
May 26 23:41:55.860: INFO: update-demo-nautilus-qxx9x is created but not running
May 26 23:42:00.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-61'
May 26 23:42:01.017: INFO: stderr: ""
May 26 23:42:01.017: INFO: stdout: "update-demo-nautilus-qxx9x update-demo-nautilus-tgwdc "
May 26 23:42:01.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-nautilus-qxx9x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-61'
May 26 23:42:01.150: INFO: stderr: ""
May 26 23:42:01.150: INFO: stdout: "true"
May 26 23:42:01.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-nautilus-qxx9x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-61'
May 26 23:42:01.279: INFO: stderr: ""
May 26 23:42:01.279: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 26 23:42:01.279: INFO: validating pod update-demo-nautilus-qxx9x
May 26 23:42:01.312: INFO: got data: {
  "image": "nautilus.jpg"
}

May 26 23:42:01.312: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 26 23:42:01.312: INFO: update-demo-nautilus-qxx9x is verified up and running
May 26 23:42:01.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-nautilus-tgwdc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-61'
May 26 23:42:01.454: INFO: stderr: ""
May 26 23:42:01.454: INFO: stdout: "true"
May 26 23:42:01.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-nautilus-tgwdc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-61'
May 26 23:42:01.590: INFO: stderr: ""
May 26 23:42:01.590: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 26 23:42:01.590: INFO: validating pod update-demo-nautilus-tgwdc
May 26 23:42:01.623: INFO: got data: {
  "image": "nautilus.jpg"
}

May 26 23:42:01.623: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 26 23:42:01.623: INFO: update-demo-nautilus-tgwdc is verified up and running
STEP: scaling down the replication controller
May 26 23:42:01.627: INFO: scanned /root for discovery docs: <nil>
May 26 23:42:01.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-61'
May 26 23:42:02.913: INFO: stderr: ""
May 26 23:42:02.913: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 26 23:42:02.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-61'
May 26 23:42:03.058: INFO: stderr: ""
May 26 23:42:03.058: INFO: stdout: "update-demo-nautilus-qxx9x update-demo-nautilus-tgwdc "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 26 23:42:08.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-61'
May 26 23:42:08.202: INFO: stderr: ""
May 26 23:42:08.202: INFO: stdout: "update-demo-nautilus-qxx9x update-demo-nautilus-tgwdc "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 26 23:42:13.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-61'
May 26 23:42:13.348: INFO: stderr: ""
May 26 23:42:13.348: INFO: stdout: "update-demo-nautilus-qxx9x "
May 26 23:42:13.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-nautilus-qxx9x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-61'
May 26 23:42:13.476: INFO: stderr: ""
May 26 23:42:13.476: INFO: stdout: "true"
May 26 23:42:13.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-nautilus-qxx9x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-61'
May 26 23:42:13.612: INFO: stderr: ""
May 26 23:42:13.613: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 26 23:42:13.613: INFO: validating pod update-demo-nautilus-qxx9x
May 26 23:42:13.628: INFO: got data: {
  "image": "nautilus.jpg"
}

May 26 23:42:13.628: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 26 23:42:13.628: INFO: update-demo-nautilus-qxx9x is verified up and running
STEP: scaling up the replication controller
May 26 23:42:13.633: INFO: scanned /root for discovery docs: <nil>
May 26 23:42:13.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-61'
May 26 23:42:14.849: INFO: stderr: ""
May 26 23:42:14.849: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 26 23:42:14.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-61'
May 26 23:42:15.041: INFO: stderr: ""
May 26 23:42:15.041: INFO: stdout: "update-demo-nautilus-67v97 update-demo-nautilus-qxx9x "
May 26 23:42:15.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-nautilus-67v97 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-61'
May 26 23:42:15.211: INFO: stderr: ""
May 26 23:42:15.211: INFO: stdout: ""
May 26 23:42:15.211: INFO: update-demo-nautilus-67v97 is created but not running
May 26 23:42:20.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-61'
May 26 23:42:20.360: INFO: stderr: ""
May 26 23:42:20.360: INFO: stdout: "update-demo-nautilus-67v97 update-demo-nautilus-qxx9x "
May 26 23:42:20.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-nautilus-67v97 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-61'
May 26 23:42:20.483: INFO: stderr: ""
May 26 23:42:20.483: INFO: stdout: "true"
May 26 23:42:20.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-nautilus-67v97 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-61'
May 26 23:42:20.628: INFO: stderr: ""
May 26 23:42:20.628: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 26 23:42:20.628: INFO: validating pod update-demo-nautilus-67v97
May 26 23:42:20.650: INFO: got data: {
  "image": "nautilus.jpg"
}

May 26 23:42:20.650: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 26 23:42:20.650: INFO: update-demo-nautilus-67v97 is verified up and running
May 26 23:42:20.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-nautilus-qxx9x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-61'
May 26 23:42:20.789: INFO: stderr: ""
May 26 23:42:20.790: INFO: stdout: "true"
May 26 23:42:20.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods update-demo-nautilus-qxx9x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-61'
May 26 23:42:20.935: INFO: stderr: ""
May 26 23:42:20.935: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 26 23:42:20.935: INFO: validating pod update-demo-nautilus-qxx9x
May 26 23:42:20.952: INFO: got data: {
  "image": "nautilus.jpg"
}

May 26 23:42:20.952: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 26 23:42:20.952: INFO: update-demo-nautilus-qxx9x is verified up and running
STEP: using delete to clean up resources
May 26 23:42:20.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 delete --grace-period=0 --force -f - --namespace=kubectl-61'
May 26 23:42:21.113: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 26 23:42:21.113: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 26 23:42:21.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-61'
May 26 23:42:21.287: INFO: stderr: "No resources found in kubectl-61 namespace.\n"
May 26 23:42:21.287: INFO: stdout: ""
May 26 23:42:21.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods -l name=update-demo --namespace=kubectl-61 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 26 23:42:21.430: INFO: stderr: ""
May 26 23:42:21.430: INFO: stdout: "update-demo-nautilus-qxx9x\n"
May 26 23:42:21.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-61'
May 26 23:42:22.101: INFO: stderr: "No resources found in kubectl-61 namespace.\n"
May 26 23:42:22.101: INFO: stdout: ""
May 26 23:42:22.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 get pods -l name=update-demo --namespace=kubectl-61 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 26 23:42:22.241: INFO: stderr: ""
May 26 23:42:22.241: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:42:22.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-61" for this suite.
May 26 23:42:36.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:42:38.168: INFO: namespace kubectl-61 deletion completed in 15.911390544s

â€¢ [SLOW TEST:43.301 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:42:38.169: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
May 26 23:42:38.311: INFO: Waiting up to 1m0s for all nodes to be ready
May 26 23:43:38.401: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 23:43:38.416: INFO: Starting informer...
STEP: Starting pods...
May 26 23:43:38.681: INFO: Pod1 is running on 10.215.60.34. Tainting Node
May 26 23:43:40.941: INFO: Pod2 is running on 10.215.60.34. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
May 26 23:43:48.605: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May 26 23:44:08.793: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:44:08.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7807" for this suite.
May 26 23:44:16.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:44:18.745: INFO: namespace taint-multiple-pods-7807 deletion completed in 9.894559462s

â€¢ [SLOW TEST:100.576 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:44:18.745: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 26 23:44:19.925: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8a901c1e-500a-4261-b5c0-bf6975e7b6b8" in namespace "downward-api-9479" to be "success or failure"
May 26 23:44:19.934: INFO: Pod "downwardapi-volume-8a901c1e-500a-4261-b5c0-bf6975e7b6b8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.728968ms
May 26 23:44:21.946: INFO: Pod "downwardapi-volume-8a901c1e-500a-4261-b5c0-bf6975e7b6b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020463427s
STEP: Saw pod success
May 26 23:44:21.946: INFO: Pod "downwardapi-volume-8a901c1e-500a-4261-b5c0-bf6975e7b6b8" satisfied condition "success or failure"
May 26 23:44:21.957: INFO: Trying to get logs from node 10.215.60.34 pod downwardapi-volume-8a901c1e-500a-4261-b5c0-bf6975e7b6b8 container client-container: <nil>
STEP: delete the pod
May 26 23:44:22.045: INFO: Waiting for pod downwardapi-volume-8a901c1e-500a-4261-b5c0-bf6975e7b6b8 to disappear
May 26 23:44:22.054: INFO: Pod downwardapi-volume-8a901c1e-500a-4261-b5c0-bf6975e7b6b8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:44:22.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9479" for this suite.
May 26 23:44:30.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:44:31.953: INFO: namespace downward-api-9479 deletion completed in 9.884902029s

â€¢ [SLOW TEST:13.209 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:44:31.954: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
May 26 23:44:32.071: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
May 26 23:44:39.810: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:45:10.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9489" for this suite.
May 26 23:45:18.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:45:20.317: INFO: namespace crd-publish-openapi-9489 deletion completed in 9.944893134s

â€¢ [SLOW TEST:48.363 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:45:20.317: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 26 23:45:20.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-8561'
May 26 23:45:20.929: INFO: stderr: ""
May 26 23:45:20.929: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
May 26 23:45:20.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-297629347 delete pods e2e-test-httpd-pod --namespace=kubectl-8561'
May 26 23:45:30.449: INFO: stderr: ""
May 26 23:45:30.449: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:45:30.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8561" for this suite.
May 26 23:45:38.498: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:45:40.366: INFO: namespace kubectl-8561 deletion completed in 9.89959034s

â€¢ [SLOW TEST:20.048 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:45:40.366: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
May 26 23:45:40.566: INFO: Waiting up to 5m0s for pod "pod-5d241d83-d954-4429-b0d0-75f7bd84d693" in namespace "emptydir-2755" to be "success or failure"
May 26 23:45:40.580: INFO: Pod "pod-5d241d83-d954-4429-b0d0-75f7bd84d693": Phase="Pending", Reason="", readiness=false. Elapsed: 13.943959ms
May 26 23:45:42.591: INFO: Pod "pod-5d241d83-d954-4429-b0d0-75f7bd84d693": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025032567s
STEP: Saw pod success
May 26 23:45:42.591: INFO: Pod "pod-5d241d83-d954-4429-b0d0-75f7bd84d693" satisfied condition "success or failure"
May 26 23:45:42.621: INFO: Trying to get logs from node 10.215.60.34 pod pod-5d241d83-d954-4429-b0d0-75f7bd84d693 container test-container: <nil>
STEP: delete the pod
May 26 23:45:42.690: INFO: Waiting for pod pod-5d241d83-d954-4429-b0d0-75f7bd84d693 to disappear
May 26 23:45:42.700: INFO: Pod pod-5d241d83-d954-4429-b0d0-75f7bd84d693 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:45:42.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2755" for this suite.
May 26 23:45:50.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:45:52.616: INFO: namespace emptydir-2755 deletion completed in 9.897977122s

â€¢ [SLOW TEST:12.250 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:45:52.617: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
May 26 23:45:52.759: INFO: PodSpec: initContainers in spec.initContainers
May 26 23:46:40.376: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-2ed74b56-344f-4d57-8dff-4669233932f5", GenerateName:"", Namespace:"init-container-3039", SelfLink:"/api/v1/namespaces/init-container-3039/pods/pod-init-2ed74b56-344f-4d57-8dff-4669233932f5", UID:"fe782dc1-6fb3-4106-8f7e-d32ab636481c", ResourceVersion:"102553", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63726133552, loc:(*time.Location)(0x84c02a0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"759294215"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.30.182.83/32", "cni.projectcalico.org/podIPs":"172.30.182.83/32", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.182.83\"\n    ],\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-ssbwc", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0048a8540), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ssbwc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc004f7ed70), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ssbwc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc004f7ee10), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ssbwc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc004f7ecd0), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004495780), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.215.60.34", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002ff4d80), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0044959c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0044959e0)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0044959fc), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004495a00), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726133552, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726133552, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726133552, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726133552, loc:(*time.Location)(0x84c02a0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.215.60.34", PodIP:"172.30.182.83", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.182.83"}}, StartTime:(*v1.Time)(0xc003827de0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00074a000)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00074a0e0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"cri-o://0dd4e9aad7bf8510a4e21820d9425492ead7d47144dcdf54f83e357c1386d541", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003827e20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003827e00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc004495a9f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:46:40.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3039" for this suite.
May 26 23:46:54.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:46:56.295: INFO: namespace init-container-3039 deletion completed in 15.899447316s

â€¢ [SLOW TEST:63.678 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:46:56.295: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-687fe4d8-254b-4e35-9cf4-00cf4b8c2d2f
STEP: Creating a pod to test consume secrets
May 26 23:46:56.526: INFO: Waiting up to 5m0s for pod "pod-secrets-450fd9c8-96d6-412f-934a-9ac8d6f876a5" in namespace "secrets-1752" to be "success or failure"
May 26 23:46:56.539: INFO: Pod "pod-secrets-450fd9c8-96d6-412f-934a-9ac8d6f876a5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.71784ms
May 26 23:46:58.552: INFO: Pod "pod-secrets-450fd9c8-96d6-412f-934a-9ac8d6f876a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02539595s
STEP: Saw pod success
May 26 23:46:58.552: INFO: Pod "pod-secrets-450fd9c8-96d6-412f-934a-9ac8d6f876a5" satisfied condition "success or failure"
May 26 23:46:58.563: INFO: Trying to get logs from node 10.215.60.34 pod pod-secrets-450fd9c8-96d6-412f-934a-9ac8d6f876a5 container secret-volume-test: <nil>
STEP: delete the pod
May 26 23:46:58.627: INFO: Waiting for pod pod-secrets-450fd9c8-96d6-412f-934a-9ac8d6f876a5 to disappear
May 26 23:46:58.639: INFO: Pod pod-secrets-450fd9c8-96d6-412f-934a-9ac8d6f876a5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:46:58.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1752" for this suite.
May 26 23:47:06.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:47:08.737: INFO: namespace secrets-1752 deletion completed in 10.076251758s

â€¢ [SLOW TEST:12.442 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:47:08.741: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:47:12.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-960" for this suite.
May 26 23:47:26.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:47:27.971: INFO: namespace replication-controller-960 deletion completed in 15.886403029s

â€¢ [SLOW TEST:19.230 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:47:27.971: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-0f4a6479-a315-42d1-8409-5d33b3f6da03
STEP: Creating a pod to test consume configMaps
May 26 23:47:28.166: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a0874159-f9e2-4078-bd24-4b723465dc38" in namespace "projected-5072" to be "success or failure"
May 26 23:47:28.174: INFO: Pod "pod-projected-configmaps-a0874159-f9e2-4078-bd24-4b723465dc38": Phase="Pending", Reason="", readiness=false. Elapsed: 8.152177ms
May 26 23:47:30.184: INFO: Pod "pod-projected-configmaps-a0874159-f9e2-4078-bd24-4b723465dc38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017996565s
STEP: Saw pod success
May 26 23:47:30.184: INFO: Pod "pod-projected-configmaps-a0874159-f9e2-4078-bd24-4b723465dc38" satisfied condition "success or failure"
May 26 23:47:30.192: INFO: Trying to get logs from node 10.215.60.34 pod pod-projected-configmaps-a0874159-f9e2-4078-bd24-4b723465dc38 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 26 23:47:30.246: INFO: Waiting for pod pod-projected-configmaps-a0874159-f9e2-4078-bd24-4b723465dc38 to disappear
May 26 23:47:30.258: INFO: Pod pod-projected-configmaps-a0874159-f9e2-4078-bd24-4b723465dc38 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:47:30.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5072" for this suite.
May 26 23:47:38.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:47:40.164: INFO: namespace projected-5072 deletion completed in 9.886430485s

â€¢ [SLOW TEST:12.194 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:47:40.165: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 23:48:00.379: INFO: Container started at 2020-05-26 23:47:41 +0000 UTC, pod became ready at 2020-05-26 23:47:58 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:48:00.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1000" for this suite.
May 26 23:48:14.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:48:16.295: INFO: namespace container-probe-1000 deletion completed in 15.897071933s

â€¢ [SLOW TEST:36.130 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:48:16.296: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-59f39730-2881-4382-bbf9-984b14600da0
STEP: Creating a pod to test consume configMaps
May 26 23:48:16.573: INFO: Waiting up to 5m0s for pod "pod-configmaps-b9e40712-f6d6-4403-890c-ef1970dd8bcb" in namespace "configmap-9286" to be "success or failure"
May 26 23:48:16.584: INFO: Pod "pod-configmaps-b9e40712-f6d6-4403-890c-ef1970dd8bcb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.053472ms
May 26 23:48:18.594: INFO: Pod "pod-configmaps-b9e40712-f6d6-4403-890c-ef1970dd8bcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021216666s
May 26 23:48:20.604: INFO: Pod "pod-configmaps-b9e40712-f6d6-4403-890c-ef1970dd8bcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030825403s
STEP: Saw pod success
May 26 23:48:20.604: INFO: Pod "pod-configmaps-b9e40712-f6d6-4403-890c-ef1970dd8bcb" satisfied condition "success or failure"
May 26 23:48:20.614: INFO: Trying to get logs from node 10.215.60.34 pod pod-configmaps-b9e40712-f6d6-4403-890c-ef1970dd8bcb container configmap-volume-test: <nil>
STEP: delete the pod
May 26 23:48:20.664: INFO: Waiting for pod pod-configmaps-b9e40712-f6d6-4403-890c-ef1970dd8bcb to disappear
May 26 23:48:20.673: INFO: Pod pod-configmaps-b9e40712-f6d6-4403-890c-ef1970dd8bcb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:48:20.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9286" for this suite.
May 26 23:48:28.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:48:30.587: INFO: namespace configmap-9286 deletion completed in 9.898465334s

â€¢ [SLOW TEST:14.291 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:48:30.590: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:48:35.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4274" for this suite.
May 26 23:48:44.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:48:45.949: INFO: namespace watch-4274 deletion completed in 9.96074875s

â€¢ [SLOW TEST:15.359 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:48:45.949: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 26 23:48:46.169: INFO: (0) /api/v1/nodes/10.215.60.10:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 50.864945ms)
May 26 23:48:46.187: INFO: (1) /api/v1/nodes/10.215.60.10:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 17.453268ms)
May 26 23:48:46.203: INFO: (2) /api/v1/nodes/10.215.60.10:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 16.049041ms)
May 26 23:48:46.219: INFO: (3) /api/v1/nodes/10.215.60.10:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 16.300004ms)
May 26 23:48:46.236: INFO: (4) /api/v1/nodes/10.215.60.10:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 16.996479ms)
May 26 23:48:46.253: INFO: (5) /api/v1/nodes/10.215.60.10:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 16.484051ms)
May 26 23:48:46.269: INFO: (6) /api/v1/nodes/10.215.60.10:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 16.187169ms)
May 26 23:48:46.284: INFO: (7) /api/v1/nodes/10.215.60.10:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 15.061069ms)
May 26 23:48:46.300: INFO: (8) /api/v1/nodes/10.215.60.10:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 16.068919ms)
May 26 23:48:46.334: INFO: (9) /api/v1/nodes/10.215.60.10:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 33.954644ms)
May 26 23:48:46.350: INFO: (10) /api/v1/nodes/10.215.60.10:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 15.791207ms)
May 26 23:48:46.367: INFO: (11) /api/v1/nodes/10.215.60.10:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 16.889162ms)
May 26 23:48:46.382: INFO: (12) /api/v1/nodes/10.215.60.10:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 14.903422ms)
May 26 23:48:46.400: INFO: (13) /api/v1/nodes/10.215.60.10:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 17.763993ms)
May 26 23:48:46.416: INFO: (14) /api/v1/nodes/10.215.60.10:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 16.292425ms)
May 26 23:48:46.435: INFO: (15) /api/v1/nodes/10.215.60.10:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 19.517298ms)
May 26 23:48:46.453: INFO: (16) /api/v1/nodes/10.215.60.10:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 17.946212ms)
May 26 23:48:46.470: INFO: (17) /api/v1/nodes/10.215.60.10:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 16.535913ms)
May 26 23:48:46.486: INFO: (18) /api/v1/nodes/10.215.60.10:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 16.012403ms)
May 26 23:48:46.502: INFO: (19) /api/v1/nodes/10.215.60.10:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 15.884438ms)
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:48:46.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3118" for this suite.
May 26 23:48:54.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:48:55.307: INFO: namespace proxy-3118 deletion completed in 8.789893646s

â€¢ [SLOW TEST:9.358 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:48:55.307: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 26 23:49:00.633: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:49:01.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9602" for this suite.
May 26 23:49:33.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:49:35.602: INFO: namespace replicaset-9602 deletion completed in 33.89555526s

â€¢ [SLOW TEST:40.295 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:49:35.603: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 26 23:49:36.785: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4bff10e8-53e9-444f-90e7-74cb9e43b638" in namespace "projected-3558" to be "success or failure"
May 26 23:49:36.794: INFO: Pod "downwardapi-volume-4bff10e8-53e9-444f-90e7-74cb9e43b638": Phase="Pending", Reason="", readiness=false. Elapsed: 8.694057ms
May 26 23:49:38.806: INFO: Pod "downwardapi-volume-4bff10e8-53e9-444f-90e7-74cb9e43b638": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021006878s
May 26 23:49:40.817: INFO: Pod "downwardapi-volume-4bff10e8-53e9-444f-90e7-74cb9e43b638": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031819041s
STEP: Saw pod success
May 26 23:49:40.817: INFO: Pod "downwardapi-volume-4bff10e8-53e9-444f-90e7-74cb9e43b638" satisfied condition "success or failure"
May 26 23:49:40.829: INFO: Trying to get logs from node 10.215.60.34 pod downwardapi-volume-4bff10e8-53e9-444f-90e7-74cb9e43b638 container client-container: <nil>
STEP: delete the pod
May 26 23:49:40.889: INFO: Waiting for pod downwardapi-volume-4bff10e8-53e9-444f-90e7-74cb9e43b638 to disappear
May 26 23:49:40.901: INFO: Pod downwardapi-volume-4bff10e8-53e9-444f-90e7-74cb9e43b638 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:49:40.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3558" for this suite.
May 26 23:49:48.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:49:50.818: INFO: namespace projected-3558 deletion completed in 9.901397259s

â€¢ [SLOW TEST:15.216 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:49:50.819: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:49:53.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9812" for this suite.
May 26 23:50:03.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:50:04.969: INFO: namespace containers-9812 deletion completed in 11.896371737s

â€¢ [SLOW TEST:14.150 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:50:04.969: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:50:25.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3713" for this suite.
May 26 23:50:33.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:50:35.605: INFO: namespace container-runtime-3713 deletion completed in 9.885811421s

â€¢ [SLOW TEST:30.636 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 26 23:50:35.606: INFO: >>> kubeConfig: /tmp/kubeconfig-297629347
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-00924493-ddd9-4146-9823-4dfb32769a27
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 26 23:50:35.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8017" for this suite.
May 26 23:50:43.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 26 23:50:45.651: INFO: namespace configmap-8017 deletion completed in 9.886760002s

â€¢ [SLOW TEST:10.046 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSMay 26 23:50:45.652: INFO: Running AfterSuite actions on all nodes
May 26 23:50:45.652: INFO: Running AfterSuite actions on node 1
May 26 23:50:45.652: INFO: Skipping dumping logs from cluster

Ran 276 of 4897 Specs in 8655.011 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4621 Skipped
PASS

Ginkgo ran 1 suite in 2h24m17.052188364s
Test Suite Passed
