I0720 18:23:45.798145      25 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-291352008
I0720 18:23:45.798251      25 e2e.go:92] Starting e2e run "f223d469-6a1f-47ea-9bdf-d0495435c689" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1595269424 - Will randomize all specs
Will run 276 of 4847 specs

Jul 20 18:23:45.807: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 18:23:45.810: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul 20 18:23:45.847: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul 20 18:23:45.916: INFO: 22 / 22 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 20 18:23:45.916: INFO: expected 13 pod replicas in namespace 'kube-system', 13 are Running and Ready.
Jul 20 18:23:45.916: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul 20 18:23:45.943: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jul 20 18:23:45.943: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Jul 20 18:23:45.943: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Jul 20 18:23:45.943: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-driver-installer' (0 seconds elapsed)
Jul 20 18:23:45.943: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
Jul 20 18:23:45.943: INFO: e2e test version: v1.16.13
Jul 20 18:23:45.947: INFO: kube-apiserver version: v1.16.13+IKS
Jul 20 18:23:45.947: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 18:23:45.973: INFO: Cluster IP family: ipv4
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:23:45.973: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
Jul 20 18:23:46.088: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Jul 20 18:23:46.153: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6757
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jul 20 18:23:46.323: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85651545-a6ce-48a8-9619-a4b7acec1142" in namespace "projected-6757" to be "success or failure"
Jul 20 18:23:46.339: INFO: Pod "downwardapi-volume-85651545-a6ce-48a8-9619-a4b7acec1142": Phase="Pending", Reason="", readiness=false. Elapsed: 16.243096ms
Jul 20 18:23:48.351: INFO: Pod "downwardapi-volume-85651545-a6ce-48a8-9619-a4b7acec1142": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027570434s
Jul 20 18:23:50.361: INFO: Pod "downwardapi-volume-85651545-a6ce-48a8-9619-a4b7acec1142": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037642665s
STEP: Saw pod success
Jul 20 18:23:50.361: INFO: Pod "downwardapi-volume-85651545-a6ce-48a8-9619-a4b7acec1142" satisfied condition "success or failure"
Jul 20 18:23:50.370: INFO: Trying to get logs from node 10.123.236.230 pod downwardapi-volume-85651545-a6ce-48a8-9619-a4b7acec1142 container client-container: <nil>
STEP: delete the pod
Jul 20 18:23:50.480: INFO: Waiting for pod downwardapi-volume-85651545-a6ce-48a8-9619-a4b7acec1142 to disappear
Jul 20 18:23:50.491: INFO: Pod downwardapi-volume-85651545-a6ce-48a8-9619-a4b7acec1142 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:23:50.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6757" for this suite.
Jul 20 18:23:56.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:23:57.212: INFO: namespace projected-6757 deletion completed in 6.705376101s

â€¢ [SLOW TEST:11.239 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:23:57.212: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1732
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-sjdmq in namespace proxy-1732
I0720 18:23:57.525261      25 runners.go:184] Created replication controller with name: proxy-service-sjdmq, namespace: proxy-1732, replica count: 1
I0720 18:23:58.581489      25 runners.go:184] proxy-service-sjdmq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0720 18:23:59.581988      25 runners.go:184] proxy-service-sjdmq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0720 18:24:00.582468      25 runners.go:184] proxy-service-sjdmq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0720 18:24:01.582806      25 runners.go:184] proxy-service-sjdmq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0720 18:24:02.583192      25 runners.go:184] proxy-service-sjdmq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0720 18:24:03.583737      25 runners.go:184] proxy-service-sjdmq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0720 18:24:04.583936      25 runners.go:184] proxy-service-sjdmq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0720 18:24:05.584232      25 runners.go:184] proxy-service-sjdmq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0720 18:24:06.584699      25 runners.go:184] proxy-service-sjdmq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0720 18:24:07.584994      25 runners.go:184] proxy-service-sjdmq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0720 18:24:08.585214      25 runners.go:184] proxy-service-sjdmq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0720 18:24:09.585493      25 runners.go:184] proxy-service-sjdmq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0720 18:24:10.585893      25 runners.go:184] proxy-service-sjdmq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0720 18:24:11.586193      25 runners.go:184] proxy-service-sjdmq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0720 18:24:12.586691      25 runners.go:184] proxy-service-sjdmq Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 20 18:24:12.600: INFO: setup took 15.148210361s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul 20 18:24:12.781: INFO: (0) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">test<... (200; 180.355319ms)
Jul 20 18:24:12.782: INFO: (0) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname1/proxy/: foo (200; 180.763832ms)
Jul 20 18:24:12.782: INFO: (0) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname2/proxy/: bar (200; 180.571646ms)
Jul 20 18:24:12.782: INFO: (0) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/rewriteme">test</a> (200; 180.615189ms)
Jul 20 18:24:12.782: INFO: (0) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">... (200; 180.855373ms)
Jul 20 18:24:12.782: INFO: (0) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 181.126188ms)
Jul 20 18:24:12.785: INFO: (0) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname2/proxy/: bar (200; 183.964962ms)
Jul 20 18:24:12.785: INFO: (0) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname1/proxy/: foo (200; 184.003011ms)
Jul 20 18:24:12.799: INFO: (0) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname2/proxy/: tls qux (200; 197.556814ms)
Jul 20 18:24:12.799: INFO: (0) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:462/proxy/: tls qux (200; 197.793197ms)
Jul 20 18:24:12.799: INFO: (0) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/tlsrewritem... (200; 198.354748ms)
Jul 20 18:24:12.801: INFO: (0) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname1/proxy/: tls baz (200; 199.654533ms)
Jul 20 18:24:12.801: INFO: (0) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:460/proxy/: tls baz (200; 199.762173ms)
Jul 20 18:24:13.218: INFO: (0) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 617.451761ms)
Jul 20 18:24:13.218: INFO: (0) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 617.564185ms)
Jul 20 18:24:13.218: INFO: (0) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 617.295048ms)
Jul 20 18:24:13.235: INFO: (1) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">... (200; 16.596872ms)
Jul 20 18:24:13.236: INFO: (1) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 17.285416ms)
Jul 20 18:24:13.236: INFO: (1) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 17.466054ms)
Jul 20 18:24:13.237: INFO: (1) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:462/proxy/: tls qux (200; 17.654604ms)
Jul 20 18:24:13.237: INFO: (1) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 17.697034ms)
Jul 20 18:24:13.237: INFO: (1) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 17.844808ms)
Jul 20 18:24:13.237: INFO: (1) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">test<... (200; 18.173163ms)
Jul 20 18:24:13.237: INFO: (1) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/tlsrewritem... (200; 18.286012ms)
Jul 20 18:24:13.237: INFO: (1) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:460/proxy/: tls baz (200; 18.254329ms)
Jul 20 18:24:13.238: INFO: (1) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/rewriteme">test</a> (200; 18.537496ms)
Jul 20 18:24:13.245: INFO: (1) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname2/proxy/: bar (200; 26.226159ms)
Jul 20 18:24:13.245: INFO: (1) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname1/proxy/: foo (200; 26.240812ms)
Jul 20 18:24:13.245: INFO: (1) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname2/proxy/: bar (200; 26.292394ms)
Jul 20 18:24:13.245: INFO: (1) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname1/proxy/: foo (200; 26.126124ms)
Jul 20 18:24:13.245: INFO: (1) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname2/proxy/: tls qux (200; 26.21856ms)
Jul 20 18:24:13.245: INFO: (1) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname1/proxy/: tls baz (200; 26.443627ms)
Jul 20 18:24:13.263: INFO: (2) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">test<... (200; 17.608646ms)
Jul 20 18:24:13.265: INFO: (2) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:460/proxy/: tls baz (200; 18.782272ms)
Jul 20 18:24:13.265: INFO: (2) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 18.874868ms)
Jul 20 18:24:13.265: INFO: (2) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:462/proxy/: tls qux (200; 19.134588ms)
Jul 20 18:24:13.266: INFO: (2) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 20.224469ms)
Jul 20 18:24:13.267: INFO: (2) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/rewriteme">test</a> (200; 20.890388ms)
Jul 20 18:24:13.267: INFO: (2) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 21.134038ms)
Jul 20 18:24:13.267: INFO: (2) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname2/proxy/: tls qux (200; 21.169622ms)
Jul 20 18:24:13.267: INFO: (2) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">... (200; 21.607748ms)
Jul 20 18:24:13.267: INFO: (2) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 21.345282ms)
Jul 20 18:24:13.267: INFO: (2) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/tlsrewritem... (200; 21.412383ms)
Jul 20 18:24:13.272: INFO: (2) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname2/proxy/: bar (200; 25.854724ms)
Jul 20 18:24:13.274: INFO: (2) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname2/proxy/: bar (200; 28.673498ms)
Jul 20 18:24:13.274: INFO: (2) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname1/proxy/: tls baz (200; 28.530461ms)
Jul 20 18:24:13.276: INFO: (2) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname1/proxy/: foo (200; 29.706242ms)
Jul 20 18:24:13.276: INFO: (2) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname1/proxy/: foo (200; 30.263662ms)
Jul 20 18:24:13.299: INFO: (3) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/tlsrewritem... (200; 23.221242ms)
Jul 20 18:24:13.299: INFO: (3) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">... (200; 22.440123ms)
Jul 20 18:24:13.301: INFO: (3) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 24.36573ms)
Jul 20 18:24:13.301: INFO: (3) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:460/proxy/: tls baz (200; 24.566314ms)
Jul 20 18:24:13.301: INFO: (3) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/rewriteme">test</a> (200; 24.069071ms)
Jul 20 18:24:13.302: INFO: (3) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 25.391938ms)
Jul 20 18:24:13.303: INFO: (3) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">test<... (200; 27.063149ms)
Jul 20 18:24:13.303: INFO: (3) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 27.03417ms)
Jul 20 18:24:13.303: INFO: (3) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:462/proxy/: tls qux (200; 27.190148ms)
Jul 20 18:24:13.305: INFO: (3) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 28.11064ms)
Jul 20 18:24:13.311: INFO: (3) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname1/proxy/: foo (200; 34.337275ms)
Jul 20 18:24:13.313: INFO: (3) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname2/proxy/: tls qux (200; 36.397985ms)
Jul 20 18:24:13.313: INFO: (3) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname2/proxy/: bar (200; 37.373068ms)
Jul 20 18:24:13.314: INFO: (3) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname2/proxy/: bar (200; 37.750192ms)
Jul 20 18:24:13.314: INFO: (3) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname1/proxy/: foo (200; 37.920188ms)
Jul 20 18:24:13.314: INFO: (3) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname1/proxy/: tls baz (200; 37.730867ms)
Jul 20 18:24:13.331: INFO: (4) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:462/proxy/: tls qux (200; 16.923632ms)
Jul 20 18:24:13.340: INFO: (4) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 26.171096ms)
Jul 20 18:24:13.341: INFO: (4) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:460/proxy/: tls baz (200; 26.110549ms)
Jul 20 18:24:13.340: INFO: (4) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/rewriteme">test</a> (200; 26.253091ms)
Jul 20 18:24:13.340: INFO: (4) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">... (200; 26.1409ms)
Jul 20 18:24:13.340: INFO: (4) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 26.235183ms)
Jul 20 18:24:13.342: INFO: (4) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">test<... (200; 27.22235ms)
Jul 20 18:24:13.342: INFO: (4) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 27.967159ms)
Jul 20 18:24:13.342: INFO: (4) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname2/proxy/: tls qux (200; 28.085725ms)
Jul 20 18:24:13.343: INFO: (4) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 28.363967ms)
Jul 20 18:24:13.343: INFO: (4) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/tlsrewritem... (200; 28.515447ms)
Jul 20 18:24:13.348: INFO: (4) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname1/proxy/: foo (200; 33.449085ms)
Jul 20 18:24:13.348: INFO: (4) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname1/proxy/: tls baz (200; 33.592362ms)
Jul 20 18:24:13.348: INFO: (4) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname2/proxy/: bar (200; 33.584244ms)
Jul 20 18:24:13.353: INFO: (4) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname2/proxy/: bar (200; 38.606905ms)
Jul 20 18:24:13.353: INFO: (4) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname1/proxy/: foo (200; 38.792445ms)
Jul 20 18:24:13.371: INFO: (5) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/tlsrewritem... (200; 17.710514ms)
Jul 20 18:24:13.373: INFO: (5) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">... (200; 20.213622ms)
Jul 20 18:24:13.373: INFO: (5) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:460/proxy/: tls baz (200; 19.654426ms)
Jul 20 18:24:13.373: INFO: (5) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">test<... (200; 19.550453ms)
Jul 20 18:24:13.374: INFO: (5) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:462/proxy/: tls qux (200; 19.894205ms)
Jul 20 18:24:13.374: INFO: (5) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 19.61551ms)
Jul 20 18:24:13.374: INFO: (5) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 19.651337ms)
Jul 20 18:24:13.375: INFO: (5) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 20.557075ms)
Jul 20 18:24:13.375: INFO: (5) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/rewriteme">test</a> (200; 21.507355ms)
Jul 20 18:24:13.375: INFO: (5) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 21.580566ms)
Jul 20 18:24:13.387: INFO: (5) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname2/proxy/: tls qux (200; 33.37287ms)
Jul 20 18:24:13.392: INFO: (5) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname2/proxy/: bar (200; 38.361435ms)
Jul 20 18:24:13.392: INFO: (5) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname1/proxy/: foo (200; 38.310717ms)
Jul 20 18:24:13.393: INFO: (5) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname1/proxy/: tls baz (200; 38.907663ms)
Jul 20 18:24:13.393: INFO: (5) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname2/proxy/: bar (200; 39.350076ms)
Jul 20 18:24:13.393: INFO: (5) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname1/proxy/: foo (200; 39.509198ms)
Jul 20 18:24:13.411: INFO: (6) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 17.639662ms)
Jul 20 18:24:13.414: INFO: (6) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 20.118766ms)
Jul 20 18:24:13.414: INFO: (6) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 20.127866ms)
Jul 20 18:24:13.415: INFO: (6) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:462/proxy/: tls qux (200; 21.063727ms)
Jul 20 18:24:13.415: INFO: (6) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 20.67656ms)
Jul 20 18:24:13.415: INFO: (6) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:460/proxy/: tls baz (200; 21.621707ms)
Jul 20 18:24:13.415: INFO: (6) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/tlsrewritem... (200; 21.160934ms)
Jul 20 18:24:13.415: INFO: (6) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/rewriteme">test</a> (200; 21.210458ms)
Jul 20 18:24:13.415: INFO: (6) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">... (200; 21.506326ms)
Jul 20 18:24:13.415: INFO: (6) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">test<... (200; 21.632262ms)
Jul 20 18:24:13.421: INFO: (6) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname2/proxy/: tls qux (200; 27.36674ms)
Jul 20 18:24:13.421: INFO: (6) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname1/proxy/: foo (200; 27.884308ms)
Jul 20 18:24:13.423: INFO: (6) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname2/proxy/: bar (200; 29.559154ms)
Jul 20 18:24:13.425: INFO: (6) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname2/proxy/: bar (200; 31.094683ms)
Jul 20 18:24:13.425: INFO: (6) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname1/proxy/: tls baz (200; 31.125307ms)
Jul 20 18:24:13.425: INFO: (6) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname1/proxy/: foo (200; 31.15288ms)
Jul 20 18:24:13.442: INFO: (7) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 17.205585ms)
Jul 20 18:24:13.443: INFO: (7) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 17.30459ms)
Jul 20 18:24:13.446: INFO: (7) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">... (200; 20.417976ms)
Jul 20 18:24:13.446: INFO: (7) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 20.644946ms)
Jul 20 18:24:13.446: INFO: (7) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 20.611233ms)
Jul 20 18:24:13.446: INFO: (7) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/tlsrewritem... (200; 20.476164ms)
Jul 20 18:24:13.446: INFO: (7) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:460/proxy/: tls baz (200; 20.962246ms)
Jul 20 18:24:13.447: INFO: (7) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:462/proxy/: tls qux (200; 21.241386ms)
Jul 20 18:24:13.447: INFO: (7) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">test<... (200; 21.217647ms)
Jul 20 18:24:13.448: INFO: (7) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/rewriteme">test</a> (200; 22.133083ms)
Jul 20 18:24:13.448: INFO: (7) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname1/proxy/: tls baz (200; 22.420439ms)
Jul 20 18:24:13.451: INFO: (7) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname2/proxy/: bar (200; 25.585266ms)
Jul 20 18:24:13.454: INFO: (7) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname2/proxy/: bar (200; 28.78589ms)
Jul 20 18:24:13.454: INFO: (7) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname1/proxy/: foo (200; 28.872286ms)
Jul 20 18:24:13.454: INFO: (7) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname1/proxy/: foo (200; 28.908383ms)
Jul 20 18:24:13.456: INFO: (7) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname2/proxy/: tls qux (200; 30.802913ms)
Jul 20 18:24:13.473: INFO: (8) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">... (200; 16.3084ms)
Jul 20 18:24:13.474: INFO: (8) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/rewriteme">test</a> (200; 17.741022ms)
Jul 20 18:24:13.475: INFO: (8) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:460/proxy/: tls baz (200; 18.317317ms)
Jul 20 18:24:13.475: INFO: (8) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 18.415903ms)
Jul 20 18:24:13.480: INFO: (8) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 22.936559ms)
Jul 20 18:24:13.480: INFO: (8) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 22.995891ms)
Jul 20 18:24:13.480: INFO: (8) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:462/proxy/: tls qux (200; 23.157238ms)
Jul 20 18:24:13.481: INFO: (8) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/tlsrewritem... (200; 23.936098ms)
Jul 20 18:24:13.481: INFO: (8) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 25.060493ms)
Jul 20 18:24:13.482: INFO: (8) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">test<... (200; 25.197998ms)
Jul 20 18:24:13.483: INFO: (8) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname2/proxy/: tls qux (200; 25.973859ms)
Jul 20 18:24:13.488: INFO: (8) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname2/proxy/: bar (200; 31.421052ms)
Jul 20 18:24:13.492: INFO: (8) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname1/proxy/: foo (200; 35.023287ms)
Jul 20 18:24:13.492: INFO: (8) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname2/proxy/: bar (200; 34.921042ms)
Jul 20 18:24:13.492: INFO: (8) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname1/proxy/: tls baz (200; 34.939566ms)
Jul 20 18:24:13.492: INFO: (8) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname1/proxy/: foo (200; 34.936077ms)
Jul 20 18:24:13.509: INFO: (9) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 17.411525ms)
Jul 20 18:24:13.515: INFO: (9) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:462/proxy/: tls qux (200; 22.672458ms)
Jul 20 18:24:13.516: INFO: (9) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:460/proxy/: tls baz (200; 23.834623ms)
Jul 20 18:24:13.519: INFO: (9) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 26.460533ms)
Jul 20 18:24:13.519: INFO: (9) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">test<... (200; 26.576012ms)
Jul 20 18:24:13.519: INFO: (9) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/tlsrewritem... (200; 26.486992ms)
Jul 20 18:24:13.519: INFO: (9) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname1/proxy/: foo (200; 26.67378ms)
Jul 20 18:24:13.519: INFO: (9) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">... (200; 26.470648ms)
Jul 20 18:24:13.519: INFO: (9) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/rewriteme">test</a> (200; 26.463957ms)
Jul 20 18:24:13.519: INFO: (9) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 26.698572ms)
Jul 20 18:24:13.521: INFO: (9) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname1/proxy/: tls baz (200; 28.822695ms)
Jul 20 18:24:13.534: INFO: (9) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname2/proxy/: bar (200; 42.085365ms)
Jul 20 18:24:13.540: INFO: (9) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 47.909429ms)
Jul 20 18:24:13.540: INFO: (9) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname2/proxy/: bar (200; 48.207801ms)
Jul 20 18:24:13.540: INFO: (9) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname2/proxy/: tls qux (200; 47.886394ms)
Jul 20 18:24:13.541: INFO: (9) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname1/proxy/: foo (200; 48.627217ms)
Jul 20 18:24:13.559: INFO: (10) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:462/proxy/: tls qux (200; 18.226446ms)
Jul 20 18:24:13.560: INFO: (10) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 18.923674ms)
Jul 20 18:24:13.560: INFO: (10) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 19.110406ms)
Jul 20 18:24:13.560: INFO: (10) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:460/proxy/: tls baz (200; 19.228166ms)
Jul 20 18:24:13.561: INFO: (10) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 20.05735ms)
Jul 20 18:24:13.561: INFO: (10) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 20.03363ms)
Jul 20 18:24:13.561: INFO: (10) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">... (200; 20.077609ms)
Jul 20 18:24:13.562: INFO: (10) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">test<... (200; 20.389801ms)
Jul 20 18:24:13.562: INFO: (10) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/rewriteme">test</a> (200; 20.569162ms)
Jul 20 18:24:13.568: INFO: (10) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname2/proxy/: tls qux (200; 27.040634ms)
Jul 20 18:24:13.568: INFO: (10) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/tlsrewritem... (200; 26.74176ms)
Jul 20 18:24:13.570: INFO: (10) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname1/proxy/: foo (200; 29.345346ms)
Jul 20 18:24:13.571: INFO: (10) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname2/proxy/: bar (200; 29.952781ms)
Jul 20 18:24:13.571: INFO: (10) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname2/proxy/: bar (200; 30.089302ms)
Jul 20 18:24:13.572: INFO: (10) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname1/proxy/: foo (200; 30.792478ms)
Jul 20 18:24:13.572: INFO: (10) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname1/proxy/: tls baz (200; 30.833283ms)
Jul 20 18:24:13.590: INFO: (11) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 18.302804ms)
Jul 20 18:24:13.596: INFO: (11) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:460/proxy/: tls baz (200; 23.621765ms)
Jul 20 18:24:13.600: INFO: (11) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/rewriteme">test</a> (200; 27.784243ms)
Jul 20 18:24:13.600: INFO: (11) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname2/proxy/: tls qux (200; 27.886907ms)
Jul 20 18:24:13.600: INFO: (11) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:462/proxy/: tls qux (200; 27.983469ms)
Jul 20 18:24:13.600: INFO: (11) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname1/proxy/: foo (200; 27.989031ms)
Jul 20 18:24:13.600: INFO: (11) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 27.97883ms)
Jul 20 18:24:13.600: INFO: (11) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 28.093291ms)
Jul 20 18:24:13.600: INFO: (11) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 28.17773ms)
Jul 20 18:24:13.600: INFO: (11) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/tlsrewritem... (200; 28.177625ms)
Jul 20 18:24:13.600: INFO: (11) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">test<... (200; 28.270461ms)
Jul 20 18:24:13.600: INFO: (11) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">... (200; 28.084242ms)
Jul 20 18:24:13.602: INFO: (11) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname1/proxy/: tls baz (200; 30.330877ms)
Jul 20 18:24:13.605: INFO: (11) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname2/proxy/: bar (200; 33.391999ms)
Jul 20 18:24:13.606: INFO: (11) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname1/proxy/: foo (200; 33.291783ms)
Jul 20 18:24:13.606: INFO: (11) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname2/proxy/: bar (200; 33.469369ms)
Jul 20 18:24:13.629: INFO: (12) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 22.462671ms)
Jul 20 18:24:13.635: INFO: (12) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:462/proxy/: tls qux (200; 28.58042ms)
Jul 20 18:24:13.635: INFO: (12) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/rewriteme">test</a> (200; 28.820189ms)
Jul 20 18:24:13.636: INFO: (12) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/tlsrewritem... (200; 29.798159ms)
Jul 20 18:24:13.636: INFO: (12) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:460/proxy/: tls baz (200; 29.659896ms)
Jul 20 18:24:13.636: INFO: (12) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 30.328536ms)
Jul 20 18:24:13.636: INFO: (12) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">test<... (200; 30.360445ms)
Jul 20 18:24:13.637: INFO: (12) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">... (200; 30.754953ms)
Jul 20 18:24:13.637: INFO: (12) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname1/proxy/: foo (200; 30.890458ms)
Jul 20 18:24:13.637: INFO: (12) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 30.524905ms)
Jul 20 18:24:13.641: INFO: (12) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 34.983329ms)
Jul 20 18:24:13.641: INFO: (12) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname1/proxy/: foo (200; 35.096682ms)
Jul 20 18:24:13.642: INFO: (12) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname2/proxy/: bar (200; 35.799416ms)
Jul 20 18:24:13.642: INFO: (12) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname2/proxy/: tls qux (200; 36.294386ms)
Jul 20 18:24:13.642: INFO: (12) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname2/proxy/: bar (200; 36.397387ms)
Jul 20 18:24:13.642: INFO: (12) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname1/proxy/: tls baz (200; 36.256204ms)
Jul 20 18:24:13.667: INFO: (13) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">... (200; 24.379286ms)
Jul 20 18:24:13.667: INFO: (13) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname2/proxy/: bar (200; 24.230048ms)
Jul 20 18:24:13.667: INFO: (13) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:462/proxy/: tls qux (200; 24.501893ms)
Jul 20 18:24:13.672: INFO: (13) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/tlsrewritem... (200; 29.728898ms)
Jul 20 18:24:13.672: INFO: (13) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 29.727454ms)
Jul 20 18:24:13.672: INFO: (13) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/rewriteme">test</a> (200; 29.824728ms)
Jul 20 18:24:13.672: INFO: (13) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 29.68424ms)
Jul 20 18:24:13.673: INFO: (13) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">test<... (200; 29.795595ms)
Jul 20 18:24:13.673: INFO: (13) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 30.669186ms)
Jul 20 18:24:13.675: INFO: (13) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 32.768908ms)
Jul 20 18:24:13.675: INFO: (13) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:460/proxy/: tls baz (200; 32.86209ms)
Jul 20 18:24:13.675: INFO: (13) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname1/proxy/: tls baz (200; 32.79294ms)
Jul 20 18:24:13.675: INFO: (13) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname2/proxy/: tls qux (200; 32.876546ms)
Jul 20 18:24:13.681: INFO: (13) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname1/proxy/: foo (200; 37.835843ms)
Jul 20 18:24:13.681: INFO: (13) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname1/proxy/: foo (200; 38.067593ms)
Jul 20 18:24:13.682: INFO: (13) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname2/proxy/: bar (200; 39.92205ms)
Jul 20 18:24:13.699: INFO: (14) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">test<... (200; 16.886561ms)
Jul 20 18:24:13.702: INFO: (14) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 19.711732ms)
Jul 20 18:24:13.702: INFO: (14) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 19.486577ms)
Jul 20 18:24:13.702: INFO: (14) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 19.641662ms)
Jul 20 18:24:13.702: INFO: (14) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">... (200; 19.620754ms)
Jul 20 18:24:13.702: INFO: (14) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:460/proxy/: tls baz (200; 19.416281ms)
Jul 20 18:24:13.702: INFO: (14) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 19.510991ms)
Jul 20 18:24:13.702: INFO: (14) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/tlsrewritem... (200; 19.624484ms)
Jul 20 18:24:13.703: INFO: (14) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:462/proxy/: tls qux (200; 20.64578ms)
Jul 20 18:24:13.705: INFO: (14) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/rewriteme">test</a> (200; 22.592796ms)
Jul 20 18:24:13.707: INFO: (14) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname2/proxy/: bar (200; 24.394523ms)
Jul 20 18:24:13.711: INFO: (14) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname1/proxy/: foo (200; 27.969516ms)
Jul 20 18:24:13.711: INFO: (14) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname2/proxy/: bar (200; 28.019297ms)
Jul 20 18:24:13.712: INFO: (14) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname2/proxy/: tls qux (200; 29.337818ms)
Jul 20 18:24:13.716: INFO: (14) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname1/proxy/: tls baz (200; 33.103074ms)
Jul 20 18:24:13.716: INFO: (14) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname1/proxy/: foo (200; 33.896853ms)
Jul 20 18:24:13.735: INFO: (15) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 18.545273ms)
Jul 20 18:24:13.740: INFO: (15) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 23.560576ms)
Jul 20 18:24:13.741: INFO: (15) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/tlsrewritem... (200; 24.627533ms)
Jul 20 18:24:13.741: INFO: (15) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 24.554123ms)
Jul 20 18:24:13.742: INFO: (15) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname2/proxy/: tls qux (200; 25.319987ms)
Jul 20 18:24:13.742: INFO: (15) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:462/proxy/: tls qux (200; 25.392357ms)
Jul 20 18:24:13.747: INFO: (15) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname1/proxy/: foo (200; 30.151077ms)
Jul 20 18:24:13.747: INFO: (15) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/rewriteme">test</a> (200; 30.344555ms)
Jul 20 18:24:13.747: INFO: (15) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:460/proxy/: tls baz (200; 30.511767ms)
Jul 20 18:24:13.747: INFO: (15) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 30.46316ms)
Jul 20 18:24:13.747: INFO: (15) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">... (200; 30.649789ms)
Jul 20 18:24:13.747: INFO: (15) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">test<... (200; 30.901139ms)
Jul 20 18:24:13.752: INFO: (15) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname1/proxy/: foo (200; 35.034291ms)
Jul 20 18:24:13.752: INFO: (15) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname2/proxy/: bar (200; 35.151515ms)
Jul 20 18:24:13.755: INFO: (15) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname1/proxy/: tls baz (200; 38.484826ms)
Jul 20 18:24:13.755: INFO: (15) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname2/proxy/: bar (200; 38.721315ms)
Jul 20 18:24:13.775: INFO: (16) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">... (200; 19.43465ms)
Jul 20 18:24:13.775: INFO: (16) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:462/proxy/: tls qux (200; 18.82682ms)
Jul 20 18:24:13.781: INFO: (16) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/rewriteme">test</a> (200; 24.821658ms)
Jul 20 18:24:13.781: INFO: (16) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">test<... (200; 24.353121ms)
Jul 20 18:24:13.781: INFO: (16) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 24.115581ms)
Jul 20 18:24:13.781: INFO: (16) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 24.661303ms)
Jul 20 18:24:13.781: INFO: (16) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:460/proxy/: tls baz (200; 25.152103ms)
Jul 20 18:24:13.781: INFO: (16) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/tlsrewritem... (200; 25.441384ms)
Jul 20 18:24:13.785: INFO: (16) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 27.973491ms)
Jul 20 18:24:13.785: INFO: (16) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 29.495669ms)
Jul 20 18:24:13.794: INFO: (16) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname2/proxy/: bar (200; 38.194803ms)
Jul 20 18:24:13.794: INFO: (16) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname1/proxy/: foo (200; 37.399465ms)
Jul 20 18:24:13.794: INFO: (16) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname2/proxy/: bar (200; 37.849625ms)
Jul 20 18:24:13.794: INFO: (16) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname1/proxy/: tls baz (200; 38.61349ms)
Jul 20 18:24:13.798: INFO: (16) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname1/proxy/: foo (200; 41.88663ms)
Jul 20 18:24:13.798: INFO: (16) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname2/proxy/: tls qux (200; 42.764247ms)
Jul 20 18:24:13.814: INFO: (17) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 14.954719ms)
Jul 20 18:24:13.816: INFO: (17) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">... (200; 16.629178ms)
Jul 20 18:24:13.816: INFO: (17) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/rewriteme">test</a> (200; 16.611292ms)
Jul 20 18:24:13.816: INFO: (17) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/tlsrewritem... (200; 16.965797ms)
Jul 20 18:24:13.819: INFO: (17) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">test<... (200; 19.886004ms)
Jul 20 18:24:13.819: INFO: (17) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 20.094435ms)
Jul 20 18:24:13.819: INFO: (17) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:462/proxy/: tls qux (200; 19.490273ms)
Jul 20 18:24:13.819: INFO: (17) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 20.291879ms)
Jul 20 18:24:13.820: INFO: (17) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:460/proxy/: tls baz (200; 20.781506ms)
Jul 20 18:24:13.820: INFO: (17) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 20.602846ms)
Jul 20 18:24:13.821: INFO: (17) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname2/proxy/: tls qux (200; 22.128188ms)
Jul 20 18:24:13.829: INFO: (17) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname1/proxy/: foo (200; 29.190153ms)
Jul 20 18:24:13.832: INFO: (17) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname1/proxy/: foo (200; 32.855565ms)
Jul 20 18:24:13.832: INFO: (17) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname2/proxy/: bar (200; 32.437512ms)
Jul 20 18:24:13.832: INFO: (17) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname2/proxy/: bar (200; 32.442877ms)
Jul 20 18:24:13.832: INFO: (17) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname1/proxy/: tls baz (200; 32.497414ms)
Jul 20 18:24:13.847: INFO: (18) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/tlsrewritem... (200; 15.351902ms)
Jul 20 18:24:13.849: INFO: (18) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/rewriteme">test</a> (200; 16.995319ms)
Jul 20 18:24:13.850: INFO: (18) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">test<... (200; 17.719617ms)
Jul 20 18:24:13.851: INFO: (18) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 19.140225ms)
Jul 20 18:24:13.851: INFO: (18) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 19.346131ms)
Jul 20 18:24:13.852: INFO: (18) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">... (200; 19.763254ms)
Jul 20 18:24:13.852: INFO: (18) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 20.117281ms)
Jul 20 18:24:13.852: INFO: (18) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 20.075447ms)
Jul 20 18:24:13.852: INFO: (18) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:462/proxy/: tls qux (200; 20.10449ms)
Jul 20 18:24:13.853: INFO: (18) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:460/proxy/: tls baz (200; 20.617763ms)
Jul 20 18:24:13.862: INFO: (18) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname1/proxy/: foo (200; 29.703946ms)
Jul 20 18:24:13.862: INFO: (18) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname2/proxy/: bar (200; 29.584461ms)
Jul 20 18:24:13.864: INFO: (18) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname1/proxy/: tls baz (200; 31.470107ms)
Jul 20 18:24:13.864: INFO: (18) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname2/proxy/: bar (200; 31.560434ms)
Jul 20 18:24:13.864: INFO: (18) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname2/proxy/: tls qux (200; 31.672385ms)
Jul 20 18:24:13.864: INFO: (18) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname1/proxy/: foo (200; 31.798262ms)
Jul 20 18:24:13.886: INFO: (19) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:443/proxy/tlsrewritem... (200; 21.709331ms)
Jul 20 18:24:13.891: INFO: (19) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 26.685655ms)
Jul 20 18:24:13.892: INFO: (19) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:462/proxy/: tls qux (200; 27.532837ms)
Jul 20 18:24:13.892: INFO: (19) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 27.711405ms)
Jul 20 18:24:13.892: INFO: (19) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm/proxy/rewriteme">test</a> (200; 27.832501ms)
Jul 20 18:24:13.892: INFO: (19) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">test<... (200; 27.658168ms)
Jul 20 18:24:13.892: INFO: (19) /api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-1732/pods/http:proxy-service-sjdmq-xjjdm:1080/proxy/rewriteme">... (200; 27.861441ms)
Jul 20 18:24:13.896: INFO: (19) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname2/proxy/: tls qux (200; 31.764591ms)
Jul 20 18:24:13.901: INFO: (19) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname2/proxy/: bar (200; 36.332307ms)
Jul 20 18:24:13.901: INFO: (19) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname2/proxy/: bar (200; 36.406375ms)
Jul 20 18:24:13.901: INFO: (19) /api/v1/namespaces/proxy-1732/services/proxy-service-sjdmq:portname1/proxy/: foo (200; 36.331535ms)
Jul 20 18:24:13.901: INFO: (19) /api/v1/namespaces/proxy-1732/services/https:proxy-service-sjdmq:tlsportname1/proxy/: tls baz (200; 36.446347ms)
Jul 20 18:24:13.901: INFO: (19) /api/v1/namespaces/proxy-1732/services/http:proxy-service-sjdmq:portname1/proxy/: foo (200; 36.498369ms)
Jul 20 18:24:13.901: INFO: (19) /api/v1/namespaces/proxy-1732/pods/https:proxy-service-sjdmq-xjjdm:460/proxy/: tls baz (200; 36.653445ms)
Jul 20 18:24:14.102: INFO: (19) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:162/proxy/: bar (200; 237.888476ms)
Jul 20 18:24:14.102: INFO: (19) /api/v1/namespaces/proxy-1732/pods/proxy-service-sjdmq-xjjdm:160/proxy/: foo (200; 237.782411ms)
STEP: deleting ReplicationController proxy-service-sjdmq in namespace proxy-1732, will wait for the garbage collector to delete the pods
Jul 20 18:24:14.192: INFO: Deleting ReplicationController proxy-service-sjdmq took: 27.869883ms
Jul 20 18:24:14.393: INFO: Terminating ReplicationController proxy-service-sjdmq pods took: 200.652524ms
[AfterEach] version v1
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:24:21.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1732" for this suite.
Jul 20 18:24:27.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:24:28.095: INFO: namespace proxy-1732 deletion completed in 6.778837639s

â€¢ [SLOW TEST:30.883 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:24:28.095: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-2114
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jul 20 18:24:32.406: INFO: &Pod{ObjectMeta:{send-events-91018be8-ae2f-479d-bd0d-d5b6ae29c73e  events-2114 /api/v1/namespaces/events-2114/pods/send-events-91018be8-ae2f-479d-bd0d-d5b6ae29c73e de76dc08-0d21-4fd2-9052-3d36d59b0fed 19542 0 2020-07-20 18:24:28 +0000 UTC <nil> <nil> map[name:foo time:344837010] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-r9thc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-r9thc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-r9thc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.220,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 18:24:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 18:24:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 18:24:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 18:24:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.220,PodIP:172.30.159.120,StartTime:2020-07-20 18:24:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-20 18:24:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:containerd://9e8ad405bc726f6e7de35642a1e9e8ca42be71ae004195aaea11fefdc01fe3de,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.159.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jul 20 18:24:34.425: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jul 20 18:24:36.483: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:24:36.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2114" for this suite.
Jul 20 18:25:22.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:25:23.086: INFO: namespace events-2114 deletion completed in 46.570160804s

â€¢ [SLOW TEST:54.991 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:25:23.086: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-401
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:25:27.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-401" for this suite.
Jul 20 18:25:33.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:25:34.466: INFO: namespace watch-401 deletion completed in 6.646424738s

â€¢ [SLOW TEST:11.380 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:25:34.467: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2413
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Jul 20 18:25:34.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 create -f - --namespace=kubectl-2413'
Jul 20 18:25:35.171: INFO: stderr: ""
Jul 20 18:25:35.171: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 20 18:25:35.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2413'
Jul 20 18:25:35.393: INFO: stderr: ""
Jul 20 18:25:35.393: INFO: stdout: "update-demo-nautilus-4qcqd update-demo-nautilus-mj72m "
Jul 20 18:25:35.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-nautilus-4qcqd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2413'
Jul 20 18:25:35.491: INFO: stderr: ""
Jul 20 18:25:35.491: INFO: stdout: ""
Jul 20 18:25:35.491: INFO: update-demo-nautilus-4qcqd is created but not running
Jul 20 18:25:40.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2413'
Jul 20 18:25:40.593: INFO: stderr: ""
Jul 20 18:25:40.593: INFO: stdout: "update-demo-nautilus-4qcqd update-demo-nautilus-mj72m "
Jul 20 18:25:40.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-nautilus-4qcqd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2413'
Jul 20 18:25:40.669: INFO: stderr: ""
Jul 20 18:25:40.669: INFO: stdout: "true"
Jul 20 18:25:40.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-nautilus-4qcqd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2413'
Jul 20 18:25:40.751: INFO: stderr: ""
Jul 20 18:25:40.751: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 20 18:25:40.751: INFO: validating pod update-demo-nautilus-4qcqd
Jul 20 18:25:40.791: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 20 18:25:40.791: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 20 18:25:40.791: INFO: update-demo-nautilus-4qcqd is verified up and running
Jul 20 18:25:40.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-nautilus-mj72m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2413'
Jul 20 18:25:40.861: INFO: stderr: ""
Jul 20 18:25:40.861: INFO: stdout: "true"
Jul 20 18:25:40.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-nautilus-mj72m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2413'
Jul 20 18:25:40.945: INFO: stderr: ""
Jul 20 18:25:40.945: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 20 18:25:40.945: INFO: validating pod update-demo-nautilus-mj72m
Jul 20 18:25:40.970: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 20 18:25:40.970: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 20 18:25:40.970: INFO: update-demo-nautilus-mj72m is verified up and running
STEP: using delete to clean up resources
Jul 20 18:25:40.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 delete --grace-period=0 --force -f - --namespace=kubectl-2413'
Jul 20 18:25:41.073: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 20 18:25:41.073: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 20 18:25:41.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2413'
Jul 20 18:25:41.223: INFO: stderr: "No resources found in kubectl-2413 namespace.\n"
Jul 20 18:25:41.223: INFO: stdout: ""
Jul 20 18:25:41.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods -l name=update-demo --namespace=kubectl-2413 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 20 18:25:41.302: INFO: stderr: ""
Jul 20 18:25:41.302: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:25:41.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2413" for this suite.
Jul 20 18:25:47.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:25:48.056: INFO: namespace kubectl-2413 deletion completed in 6.729281636s

â€¢ [SLOW TEST:13.589 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:25:48.056: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-7652
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul 20 18:25:56.497: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7652 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 20 18:25:56.497: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 18:25:56.724: INFO: Exec stderr: ""
Jul 20 18:25:56.724: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7652 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 20 18:25:56.725: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 18:25:56.849: INFO: Exec stderr: ""
Jul 20 18:25:56.849: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7652 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 20 18:25:56.849: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 18:25:56.980: INFO: Exec stderr: ""
Jul 20 18:25:56.980: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7652 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 20 18:25:56.980: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 18:25:57.119: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul 20 18:25:57.119: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7652 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 20 18:25:57.119: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 18:25:57.316: INFO: Exec stderr: ""
Jul 20 18:25:57.317: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7652 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 20 18:25:57.317: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 18:25:57.450: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul 20 18:25:57.450: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7652 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 20 18:25:57.450: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 18:25:57.662: INFO: Exec stderr: ""
Jul 20 18:25:57.662: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7652 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 20 18:25:57.662: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 18:25:57.858: INFO: Exec stderr: ""
Jul 20 18:25:57.858: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7652 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 20 18:25:57.858: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 18:25:57.996: INFO: Exec stderr: ""
Jul 20 18:25:57.996: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7652 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 20 18:25:57.996: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 18:25:58.236: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:25:58.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-7652" for this suite.
Jul 20 18:26:46.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:26:46.967: INFO: namespace e2e-kubelet-etc-hosts-7652 deletion completed in 48.714742336s

â€¢ [SLOW TEST:58.911 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:26:46.968: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4244
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jul 20 18:26:47.270: INFO: Waiting up to 5m0s for pod "downward-api-323848da-7eea-479f-8980-0762e670b96c" in namespace "downward-api-4244" to be "success or failure"
Jul 20 18:26:47.285: INFO: Pod "downward-api-323848da-7eea-479f-8980-0762e670b96c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.090368ms
Jul 20 18:26:49.294: INFO: Pod "downward-api-323848da-7eea-479f-8980-0762e670b96c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023372395s
Jul 20 18:26:51.305: INFO: Pod "downward-api-323848da-7eea-479f-8980-0762e670b96c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034015561s
STEP: Saw pod success
Jul 20 18:26:51.305: INFO: Pod "downward-api-323848da-7eea-479f-8980-0762e670b96c" satisfied condition "success or failure"
Jul 20 18:26:51.315: INFO: Trying to get logs from node 10.123.236.227 pod downward-api-323848da-7eea-479f-8980-0762e670b96c container dapi-container: <nil>
STEP: delete the pod
Jul 20 18:26:51.404: INFO: Waiting for pod downward-api-323848da-7eea-479f-8980-0762e670b96c to disappear
Jul 20 18:26:51.412: INFO: Pod downward-api-323848da-7eea-479f-8980-0762e670b96c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:26:51.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4244" for this suite.
Jul 20 18:26:57.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:26:58.092: INFO: namespace downward-api-4244 deletion completed in 6.660803538s

â€¢ [SLOW TEST:11.124 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:26:58.092: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6108
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-7bd4e9ec-6bbd-4d20-82d8-218f3eef0de3
STEP: Creating configMap with name cm-test-opt-upd-b1fafab4-2d94-4317-bbc6-951daf696391
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-7bd4e9ec-6bbd-4d20-82d8-218f3eef0de3
STEP: Updating configmap cm-test-opt-upd-b1fafab4-2d94-4317-bbc6-951daf696391
STEP: Creating configMap with name cm-test-opt-create-fce25272-7c23-44a1-818c-6c2efeff1d43
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:27:06.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6108" for this suite.
Jul 20 18:27:35.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:27:35.545: INFO: namespace configmap-6108 deletion completed in 28.610646667s

â€¢ [SLOW TEST:37.454 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:27:35.546: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3920
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Jul 20 18:27:38.595: INFO: Successfully updated pod "labelsupdate0be7a019-a53c-475c-8f75-30c141b4e9cc"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:27:40.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3920" for this suite.
Jul 20 18:27:52.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:27:53.289: INFO: namespace projected-3920 deletion completed in 12.60135522s

â€¢ [SLOW TEST:17.744 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:27:53.290: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9243
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:27:53.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9243" for this suite.
Jul 20 18:27:59.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:28:00.427: INFO: namespace resourcequota-9243 deletion completed in 6.814513388s

â€¢ [SLOW TEST:7.137 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:28:00.428: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7643
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-f640b378-8fd6-451d-8636-fb4120348f29 in namespace container-probe-7643
Jul 20 18:28:06.730: INFO: Started pod busybox-f640b378-8fd6-451d-8636-fb4120348f29 in namespace container-probe-7643
STEP: checking the pod's current state and verifying that restartCount is present
Jul 20 18:28:06.740: INFO: Initial restart count of pod busybox-f640b378-8fd6-451d-8636-fb4120348f29 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:32:08.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7643" for this suite.
Jul 20 18:32:14.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:32:15.093: INFO: namespace container-probe-7643 deletion completed in 6.646247688s

â€¢ [SLOW TEST:254.666 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:32:15.100: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2496
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:32:27.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2496" for this suite.
Jul 20 18:32:33.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:32:33.680: INFO: namespace resourcequota-2496 deletion completed in 6.572339092s

â€¢ [SLOW TEST:18.580 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:32:33.681: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7770
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 20 18:32:34.530: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 20 18:32:36.557: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730866754, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730866754, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730866754, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730866754, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 20 18:32:39.634: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:32:50.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7770" for this suite.
Jul 20 18:32:56.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:32:56.669: INFO: namespace webhook-7770 deletion completed in 6.603191899s
STEP: Destroying namespace "webhook-7770-markers" for this suite.
Jul 20 18:33:02.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:33:03.272: INFO: namespace webhook-7770-markers deletion completed in 6.602277784s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:29.748 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:33:03.430: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4940
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 18:33:03.664: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:33:04.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4940" for this suite.
Jul 20 18:33:10.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:33:10.808: INFO: namespace custom-resource-definition-4940 deletion completed in 6.550213378s

â€¢ [SLOW TEST:7.378 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:33:10.808: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5318
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jul 20 18:33:21.303: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:33:21.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0720 18:33:21.303250      25 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-5318" for this suite.
Jul 20 18:33:29.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:33:29.909: INFO: namespace gc-5318 deletion completed in 8.577152594s

â€¢ [SLOW TEST:19.100 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:33:29.909: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-604
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jul 20 18:33:30.150: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 20 18:33:30.194: INFO: Waiting for terminating namespaces to be deleted...
Jul 20 18:33:30.223: INFO: 
Logging pods the kubelet thinks is on node 10.123.236.220 before test
Jul 20 18:33:30.304: INFO: ibm-master-proxy-static-10.123.236.220 from kube-system started at 2020-07-20 16:37:55 +0000 UTC (2 container statuses recorded)
Jul 20 18:33:30.304: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jul 20 18:33:30.304: INFO: 	Container pause ready: true, restart count 0
Jul 20 18:33:30.304: INFO: ibm-keepalived-watcher-69dl6 from kube-system started at 2020-07-20 16:38:01 +0000 UTC (1 container statuses recorded)
Jul 20 18:33:30.304: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jul 20 18:33:30.304: INFO: sonobuoy-systemd-logs-daemon-set-2fac7b176f1f4511-zngst from sonobuoy started at 2020-07-20 18:23:20 +0000 UTC (2 container statuses recorded)
Jul 20 18:33:30.304: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 20 18:33:30.304: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 20 18:33:30.304: INFO: coredns-54f55c7c7c-gw4fd from kube-system started at 2020-07-20 17:02:33 +0000 UTC (1 container statuses recorded)
Jul 20 18:33:30.304: INFO: 	Container coredns ready: true, restart count 0
Jul 20 18:33:30.304: INFO: calico-node-6z6jp from kube-system started at 2020-07-20 16:38:01 +0000 UTC (1 container statuses recorded)
Jul 20 18:33:30.304: INFO: 	Container calico-node ready: true, restart count 0
Jul 20 18:33:30.304: INFO: ibm-cloud-provider-ip-149-81-145-66-6c99b7f77d-6xqwt from ibm-system started at 2020-07-20 16:38:20 +0000 UTC (1 container statuses recorded)
Jul 20 18:33:30.304: INFO: 	Container ibm-cloud-provider-ip-149-81-145-66 ready: true, restart count 0
Jul 20 18:33:30.304: INFO: public-crbsas6ivf0c8md2c63apg-alb1-5f96ffbc77-c5tzp from kube-system started at 2020-07-20 16:39:13 +0000 UTC (4 container statuses recorded)
Jul 20 18:33:30.304: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Jul 20 18:33:30.304: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Jul 20 18:33:30.304: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Jul 20 18:33:30.304: INFO: 	Container nginx-ingress ready: true, restart count 0
Jul 20 18:33:30.304: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-07-20 16:40:09 +0000 UTC (1 container statuses recorded)
Jul 20 18:33:30.304: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jul 20 18:33:30.304: INFO: 
Logging pods the kubelet thinks is on node 10.123.236.227 before test
Jul 20 18:33:30.624: INFO: dashboard-metrics-scraper-5789d44f58-mr88k from kube-system started at 2020-07-20 16:37:01 +0000 UTC (1 container statuses recorded)
Jul 20 18:33:30.624: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jul 20 18:33:30.624: INFO: kubernetes-dashboard-984c5c57-cf2cf from kube-system started at 2020-07-20 16:37:01 +0000 UTC (1 container statuses recorded)
Jul 20 18:33:30.624: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul 20 18:33:30.624: INFO: ibm-storage-watcher-5955dd9995-ft6sf from kube-system started at 2020-07-20 16:37:05 +0000 UTC (1 container statuses recorded)
Jul 20 18:33:30.624: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jul 20 18:33:30.624: INFO: coredns-autoscaler-6bc79bb9db-nsbq2 from kube-system started at 2020-07-20 16:37:01 +0000 UTC (1 container statuses recorded)
Jul 20 18:33:30.624: INFO: 	Container autoscaler ready: true, restart count 0
Jul 20 18:33:30.624: INFO: catalog-operator-67646bfcdb-49pmq from ibm-system started at 2020-07-20 16:37:01 +0000 UTC (1 container statuses recorded)
Jul 20 18:33:30.624: INFO: 	Container catalog-operator ready: true, restart count 0
Jul 20 18:33:30.624: INFO: coredns-54f55c7c7c-fr2hn from kube-system started at 2020-07-20 17:02:34 +0000 UTC (1 container statuses recorded)
Jul 20 18:33:30.624: INFO: 	Container coredns ready: true, restart count 0
Jul 20 18:33:30.624: INFO: sonobuoy-systemd-logs-daemon-set-2fac7b176f1f4511-mktvj from sonobuoy started at 2020-07-20 18:23:20 +0000 UTC (2 container statuses recorded)
Jul 20 18:33:30.624: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 20 18:33:30.624: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 20 18:33:30.624: INFO: vpn-f66c45467-f46vm from kube-system started at 2020-07-20 17:02:12 +0000 UTC (1 container statuses recorded)
Jul 20 18:33:30.624: INFO: 	Container vpn ready: true, restart count 0
Jul 20 18:33:30.624: INFO: calico-kube-controllers-69c4db5c8d-44fh7 from kube-system started at 2020-07-20 16:37:01 +0000 UTC (1 container statuses recorded)
Jul 20 18:33:30.624: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 20 18:33:30.624: INFO: ibm-file-plugin-586dc4596c-gxcnd from kube-system started at 2020-07-20 16:37:01 +0000 UTC (1 container statuses recorded)
Jul 20 18:33:30.624: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jul 20 18:33:30.624: INFO: olm-operator-787498c9b7-mnb2k from ibm-system started at 2020-07-20 16:37:01 +0000 UTC (1 container statuses recorded)
Jul 20 18:33:30.624: INFO: 	Container olm-operator ready: true, restart count 0
Jul 20 18:33:30.624: INFO: addon-catalog-source-c4rf5 from ibm-system started at 2020-07-20 16:37:18 +0000 UTC (1 container statuses recorded)
Jul 20 18:33:30.624: INFO: 	Container configmap-registry-server ready: true, restart count 0
Jul 20 18:33:30.624: INFO: ibm-master-proxy-static-10.123.236.227 from kube-system started at 2020-07-20 16:36:44 +0000 UTC (2 container statuses recorded)
Jul 20 18:33:30.625: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jul 20 18:33:30.625: INFO: 	Container pause ready: true, restart count 0
Jul 20 18:33:30.625: INFO: ibm-keepalived-watcher-m9bwq from kube-system started at 2020-07-20 16:36:51 +0000 UTC (1 container statuses recorded)
Jul 20 18:33:30.625: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jul 20 18:33:30.625: INFO: calico-node-85dkz from kube-system started at 2020-07-20 16:36:51 +0000 UTC (1 container statuses recorded)
Jul 20 18:33:30.625: INFO: 	Container calico-node ready: true, restart count 0
Jul 20 18:33:30.625: INFO: metrics-server-7d665f557c-wfj45 from kube-system started at 2020-07-20 16:37:17 +0000 UTC (2 container statuses recorded)
Jul 20 18:33:30.625: INFO: 	Container metrics-server ready: true, restart count 0
Jul 20 18:33:30.625: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jul 20 18:33:30.625: INFO: 
Logging pods the kubelet thinks is on node 10.123.236.230 before test
Jul 20 18:33:30.724: INFO: ibm-master-proxy-static-10.123.236.230 from kube-system started at 2020-07-20 16:38:01 +0000 UTC (2 container statuses recorded)
Jul 20 18:33:30.724: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jul 20 18:33:30.724: INFO: 	Container pause ready: true, restart count 0
Jul 20 18:33:30.724: INFO: calico-node-wpsss from kube-system started at 2020-07-20 16:38:02 +0000 UTC (1 container statuses recorded)
Jul 20 18:33:30.724: INFO: 	Container calico-node ready: true, restart count 0
Jul 20 18:33:30.724: INFO: ibm-keepalived-watcher-fhgvr from kube-system started at 2020-07-20 16:38:02 +0000 UTC (1 container statuses recorded)
Jul 20 18:33:30.724: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jul 20 18:33:30.724: INFO: public-crbsas6ivf0c8md2c63apg-alb1-5f96ffbc77-lpqp5 from kube-system started at 2020-07-20 16:39:13 +0000 UTC (4 container statuses recorded)
Jul 20 18:33:30.724: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Jul 20 18:33:30.724: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Jul 20 18:33:30.724: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Jul 20 18:33:30.724: INFO: 	Container nginx-ingress ready: true, restart count 0
Jul 20 18:33:30.724: INFO: ibm-cloud-provider-ip-149-81-145-66-6c99b7f77d-4rckp from ibm-system started at 2020-07-20 16:38:20 +0000 UTC (1 container statuses recorded)
Jul 20 18:33:30.724: INFO: 	Container ibm-cloud-provider-ip-149-81-145-66 ready: true, restart count 0
Jul 20 18:33:30.724: INFO: coredns-54f55c7c7c-xgw9w from kube-system started at 2020-07-20 17:02:33 +0000 UTC (1 container statuses recorded)
Jul 20 18:33:30.724: INFO: 	Container coredns ready: true, restart count 0
Jul 20 18:33:30.724: INFO: sonobuoy from sonobuoy started at 2020-07-20 18:23:14 +0000 UTC (1 container statuses recorded)
Jul 20 18:33:30.724: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 20 18:33:30.724: INFO: sonobuoy-e2e-job-0f6e1d1f061d4a08 from sonobuoy started at 2020-07-20 18:23:20 +0000 UTC (2 container statuses recorded)
Jul 20 18:33:30.724: INFO: 	Container e2e ready: true, restart count 0
Jul 20 18:33:30.724: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 20 18:33:30.724: INFO: sonobuoy-systemd-logs-daemon-set-2fac7b176f1f4511-xh4r9 from sonobuoy started at 2020-07-20 18:23:20 +0000 UTC (2 container statuses recorded)
Jul 20 18:33:30.724: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 20 18:33:30.724: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node 10.123.236.220
STEP: verifying the node has the label node 10.123.236.227
STEP: verifying the node has the label node 10.123.236.230
Jul 20 18:33:30.860: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.123.236.220
Jul 20 18:33:30.860: INFO: Pod addon-catalog-source-c4rf5 requesting resource cpu=10m on Node 10.123.236.227
Jul 20 18:33:30.860: INFO: Pod catalog-operator-67646bfcdb-49pmq requesting resource cpu=10m on Node 10.123.236.227
Jul 20 18:33:30.860: INFO: Pod ibm-cloud-provider-ip-149-81-145-66-6c99b7f77d-4rckp requesting resource cpu=5m on Node 10.123.236.230
Jul 20 18:33:30.860: INFO: Pod ibm-cloud-provider-ip-149-81-145-66-6c99b7f77d-6xqwt requesting resource cpu=5m on Node 10.123.236.220
Jul 20 18:33:30.860: INFO: Pod olm-operator-787498c9b7-mnb2k requesting resource cpu=10m on Node 10.123.236.227
Jul 20 18:33:30.860: INFO: Pod calico-kube-controllers-69c4db5c8d-44fh7 requesting resource cpu=10m on Node 10.123.236.227
Jul 20 18:33:30.860: INFO: Pod calico-node-6z6jp requesting resource cpu=250m on Node 10.123.236.220
Jul 20 18:33:30.860: INFO: Pod calico-node-85dkz requesting resource cpu=250m on Node 10.123.236.227
Jul 20 18:33:30.860: INFO: Pod calico-node-wpsss requesting resource cpu=250m on Node 10.123.236.230
Jul 20 18:33:30.860: INFO: Pod coredns-54f55c7c7c-fr2hn requesting resource cpu=100m on Node 10.123.236.227
Jul 20 18:33:30.860: INFO: Pod coredns-54f55c7c7c-gw4fd requesting resource cpu=100m on Node 10.123.236.220
Jul 20 18:33:30.860: INFO: Pod coredns-54f55c7c7c-xgw9w requesting resource cpu=100m on Node 10.123.236.230
Jul 20 18:33:30.860: INFO: Pod coredns-autoscaler-6bc79bb9db-nsbq2 requesting resource cpu=20m on Node 10.123.236.227
Jul 20 18:33:30.860: INFO: Pod dashboard-metrics-scraper-5789d44f58-mr88k requesting resource cpu=1m on Node 10.123.236.227
Jul 20 18:33:30.860: INFO: Pod ibm-file-plugin-586dc4596c-gxcnd requesting resource cpu=50m on Node 10.123.236.227
Jul 20 18:33:30.860: INFO: Pod ibm-keepalived-watcher-69dl6 requesting resource cpu=5m on Node 10.123.236.220
Jul 20 18:33:30.860: INFO: Pod ibm-keepalived-watcher-fhgvr requesting resource cpu=5m on Node 10.123.236.230
Jul 20 18:33:30.860: INFO: Pod ibm-keepalived-watcher-m9bwq requesting resource cpu=5m on Node 10.123.236.227
Jul 20 18:33:30.860: INFO: Pod ibm-master-proxy-static-10.123.236.220 requesting resource cpu=25m on Node 10.123.236.220
Jul 20 18:33:30.860: INFO: Pod ibm-master-proxy-static-10.123.236.227 requesting resource cpu=25m on Node 10.123.236.227
Jul 20 18:33:30.860: INFO: Pod ibm-master-proxy-static-10.123.236.230 requesting resource cpu=25m on Node 10.123.236.230
Jul 20 18:33:30.860: INFO: Pod ibm-storage-watcher-5955dd9995-ft6sf requesting resource cpu=50m on Node 10.123.236.227
Jul 20 18:33:30.860: INFO: Pod kubernetes-dashboard-984c5c57-cf2cf requesting resource cpu=50m on Node 10.123.236.227
Jul 20 18:33:30.860: INFO: Pod metrics-server-7d665f557c-wfj45 requesting resource cpu=121m on Node 10.123.236.227
Jul 20 18:33:30.860: INFO: Pod public-crbsas6ivf0c8md2c63apg-alb1-5f96ffbc77-c5tzp requesting resource cpu=10m on Node 10.123.236.220
Jul 20 18:33:30.860: INFO: Pod public-crbsas6ivf0c8md2c63apg-alb1-5f96ffbc77-lpqp5 requesting resource cpu=10m on Node 10.123.236.230
Jul 20 18:33:30.860: INFO: Pod vpn-f66c45467-f46vm requesting resource cpu=5m on Node 10.123.236.227
Jul 20 18:33:30.860: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.123.236.230
Jul 20 18:33:30.860: INFO: Pod sonobuoy-e2e-job-0f6e1d1f061d4a08 requesting resource cpu=0m on Node 10.123.236.230
Jul 20 18:33:30.860: INFO: Pod sonobuoy-systemd-logs-daemon-set-2fac7b176f1f4511-mktvj requesting resource cpu=0m on Node 10.123.236.227
Jul 20 18:33:30.860: INFO: Pod sonobuoy-systemd-logs-daemon-set-2fac7b176f1f4511-xh4r9 requesting resource cpu=0m on Node 10.123.236.230
Jul 20 18:33:30.860: INFO: Pod sonobuoy-systemd-logs-daemon-set-2fac7b176f1f4511-zngst requesting resource cpu=0m on Node 10.123.236.220
STEP: Starting Pods to consume most of the cluster CPU.
Jul 20 18:33:30.860: INFO: Creating a pod which consumes cpu=2460m on Node 10.123.236.230
Jul 20 18:33:30.882: INFO: Creating a pod which consumes cpu=2460m on Node 10.123.236.220
Jul 20 18:33:30.907: INFO: Creating a pod which consumes cpu=2235m on Node 10.123.236.227
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3d506a64-72bb-47fd-ac0e-5a17447f7517.1623899f738e9390], Reason = [Scheduled], Message = [Successfully assigned sched-pred-604/filler-pod-3d506a64-72bb-47fd-ac0e-5a17447f7517 to 10.123.236.220]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3d506a64-72bb-47fd-ac0e-5a17447f7517.1623899fab1d6653], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3d506a64-72bb-47fd-ac0e-5a17447f7517.1623899fade63d71], Reason = [Created], Message = [Created container filler-pod-3d506a64-72bb-47fd-ac0e-5a17447f7517]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3d506a64-72bb-47fd-ac0e-5a17447f7517.1623899fb653082c], Reason = [Started], Message = [Started container filler-pod-3d506a64-72bb-47fd-ac0e-5a17447f7517]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c3ac67d5-3abd-4cfd-a58a-fb4435673a62.1623899f725bab0f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-604/filler-pod-c3ac67d5-3abd-4cfd-a58a-fb4435673a62 to 10.123.236.230]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c3ac67d5-3abd-4cfd-a58a-fb4435673a62.1623899fb3390c5c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c3ac67d5-3abd-4cfd-a58a-fb4435673a62.1623899fb607ae6a], Reason = [Created], Message = [Created container filler-pod-c3ac67d5-3abd-4cfd-a58a-fb4435673a62]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c3ac67d5-3abd-4cfd-a58a-fb4435673a62.1623899fbf6419ab], Reason = [Started], Message = [Started container filler-pod-c3ac67d5-3abd-4cfd-a58a-fb4435673a62]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f2dae77c-e816-4a3c-b28a-097ff7db6030.1623899f74259318], Reason = [Scheduled], Message = [Successfully assigned sched-pred-604/filler-pod-f2dae77c-e816-4a3c-b28a-097ff7db6030 to 10.123.236.227]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f2dae77c-e816-4a3c-b28a-097ff7db6030.1623899fbb095866], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f2dae77c-e816-4a3c-b28a-097ff7db6030.1623899fbd7b28b7], Reason = [Created], Message = [Created container filler-pod-f2dae77c-e816-4a3c-b28a-097ff7db6030]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f2dae77c-e816-4a3c-b28a-097ff7db6030.1623899fc5f77f39], Reason = [Started], Message = [Started container filler-pod-f2dae77c-e816-4a3c-b28a-097ff7db6030]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.162389a0669c3410], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.123.236.227
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.123.236.230
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.123.236.220
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:33:36.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-604" for this suite.
Jul 20 18:33:42.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:33:42.775: INFO: namespace sched-pred-604 deletion completed in 6.590730748s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:12.866 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:33:42.776: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3837
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 18:33:43.060: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul 20 18:33:48.071: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 20 18:33:50.090: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 20 18:33:56.182: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3837 /apis/apps/v1/namespaces/deployment-3837/deployments/test-cleanup-deployment 55f7bf40-b013-4a1b-ab46-91efa015e2b0 21367 1 2020-07-20 18:33:50 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0031e75a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-07-20 18:33:50 +0000 UTC,LastTransitionTime:2020-07-20 18:33:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-65db99849b" has successfully progressed.,LastUpdateTime:2020-07-20 18:33:55 +0000 UTC,LastTransitionTime:2020-07-20 18:33:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 20 18:33:56.202: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-3837 /apis/apps/v1/namespaces/deployment-3837/replicasets/test-cleanup-deployment-65db99849b a4961e4a-1f6d-47d8-a9c5-405ebf1c6b0c 21357 1 2020-07-20 18:33:50 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 55f7bf40-b013-4a1b-ab46-91efa015e2b0 0xc0031e7c97 0xc0031e7c98}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0031e7d48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 20 18:33:56.213: INFO: Pod "test-cleanup-deployment-65db99849b-vgd8c" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-65db99849b-vgd8c test-cleanup-deployment-65db99849b- deployment-3837 /api/v1/namespaces/deployment-3837/pods/test-cleanup-deployment-65db99849b-vgd8c 3cb09d69-665a-4fb0-a6df-a0f0145159cd 21356 0 2020-07-20 18:33:50 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-deployment-65db99849b a4961e4a-1f6d-47d8-a9c5-405ebf1c6b0c 0xc003114157 0xc003114158}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-drpcb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-drpcb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-drpcb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 18:33:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 18:33:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 18:33:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 18:33:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.227,PodIP:172.30.106.162,StartTime:2020-07-20 18:33:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-20 18:33:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:containerd://27ebd826bbea5c6d6bc91304021cbf4dd822393396d54ba4166cfcdba05cdac2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.106.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:33:56.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3837" for this suite.
Jul 20 18:34:02.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:34:02.799: INFO: namespace deployment-3837 deletion completed in 6.564103154s

â€¢ [SLOW TEST:20.023 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:34:02.800: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1531
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-4248
STEP: Creating secret with name secret-test-e81da48a-76a9-4aa5-a92c-cd30fbde4981
STEP: Creating a pod to test consume secrets
Jul 20 18:34:03.364: INFO: Waiting up to 5m0s for pod "pod-secrets-d3089170-8914-41a4-950b-41e667118919" in namespace "secrets-1531" to be "success or failure"
Jul 20 18:34:03.374: INFO: Pod "pod-secrets-d3089170-8914-41a4-950b-41e667118919": Phase="Pending", Reason="", readiness=false. Elapsed: 9.493258ms
Jul 20 18:34:05.387: INFO: Pod "pod-secrets-d3089170-8914-41a4-950b-41e667118919": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022521429s
Jul 20 18:34:07.400: INFO: Pod "pod-secrets-d3089170-8914-41a4-950b-41e667118919": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035938342s
STEP: Saw pod success
Jul 20 18:34:07.400: INFO: Pod "pod-secrets-d3089170-8914-41a4-950b-41e667118919" satisfied condition "success or failure"
Jul 20 18:34:07.410: INFO: Trying to get logs from node 10.123.236.220 pod pod-secrets-d3089170-8914-41a4-950b-41e667118919 container secret-volume-test: <nil>
STEP: delete the pod
Jul 20 18:34:07.479: INFO: Waiting for pod pod-secrets-d3089170-8914-41a4-950b-41e667118919 to disappear
Jul 20 18:34:07.493: INFO: Pod pod-secrets-d3089170-8914-41a4-950b-41e667118919 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:34:07.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1531" for this suite.
Jul 20 18:34:13.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:34:14.061: INFO: namespace secrets-1531 deletion completed in 6.532345213s
STEP: Destroying namespace "secret-namespace-4248" for this suite.
Jul 20 18:34:20.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:34:20.707: INFO: namespace secret-namespace-4248 deletion completed in 6.646620609s

â€¢ [SLOW TEST:17.908 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:34:20.708: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9962
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 20 18:34:21.067: INFO: Number of nodes with available pods: 0
Jul 20 18:34:21.067: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 18:34:22.100: INFO: Number of nodes with available pods: 0
Jul 20 18:34:22.100: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 18:34:23.091: INFO: Number of nodes with available pods: 0
Jul 20 18:34:23.091: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 18:34:24.094: INFO: Number of nodes with available pods: 1
Jul 20 18:34:24.094: INFO: Node 10.123.236.227 is running more than one daemon pod
Jul 20 18:34:25.096: INFO: Number of nodes with available pods: 1
Jul 20 18:34:25.096: INFO: Node 10.123.236.227 is running more than one daemon pod
Jul 20 18:34:26.094: INFO: Number of nodes with available pods: 1
Jul 20 18:34:26.095: INFO: Node 10.123.236.227 is running more than one daemon pod
Jul 20 18:34:27.097: INFO: Number of nodes with available pods: 1
Jul 20 18:34:27.097: INFO: Node 10.123.236.227 is running more than one daemon pod
Jul 20 18:34:28.097: INFO: Number of nodes with available pods: 1
Jul 20 18:34:28.097: INFO: Node 10.123.236.227 is running more than one daemon pod
Jul 20 18:34:29.106: INFO: Number of nodes with available pods: 3
Jul 20 18:34:29.106: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul 20 18:34:29.205: INFO: Number of nodes with available pods: 2
Jul 20 18:34:29.205: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 18:34:30.230: INFO: Number of nodes with available pods: 2
Jul 20 18:34:30.230: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 18:34:31.327: INFO: Number of nodes with available pods: 2
Jul 20 18:34:31.327: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 18:34:32.233: INFO: Number of nodes with available pods: 2
Jul 20 18:34:32.233: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 18:34:33.235: INFO: Number of nodes with available pods: 2
Jul 20 18:34:33.235: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 18:34:34.231: INFO: Number of nodes with available pods: 2
Jul 20 18:34:34.231: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 18:34:35.238: INFO: Number of nodes with available pods: 2
Jul 20 18:34:35.238: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 18:34:36.233: INFO: Number of nodes with available pods: 2
Jul 20 18:34:36.233: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 18:34:37.234: INFO: Number of nodes with available pods: 3
Jul 20 18:34:37.234: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9962, will wait for the garbage collector to delete the pods
Jul 20 18:34:37.333: INFO: Deleting DaemonSet.extensions daemon-set took: 25.23468ms
Jul 20 18:34:37.534: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.303478ms
Jul 20 18:34:45.246: INFO: Number of nodes with available pods: 0
Jul 20 18:34:45.246: INFO: Number of running nodes: 0, number of available pods: 0
Jul 20 18:34:45.257: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9962/daemonsets","resourceVersion":"21635"},"items":null}

Jul 20 18:34:45.268: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9962/pods","resourceVersion":"21635"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:34:45.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9962" for this suite.
Jul 20 18:34:53.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:34:53.910: INFO: namespace daemonsets-9962 deletion completed in 8.575329592s

â€¢ [SLOW TEST:33.202 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:34:53.911: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8780
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Jul 20 18:34:54.758: INFO: created pod pod-service-account-defaultsa
Jul 20 18:34:54.758: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul 20 18:34:54.784: INFO: created pod pod-service-account-mountsa
Jul 20 18:34:54.784: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul 20 18:34:54.797: INFO: created pod pod-service-account-nomountsa
Jul 20 18:34:54.797: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul 20 18:34:54.816: INFO: created pod pod-service-account-defaultsa-mountspec
Jul 20 18:34:54.816: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul 20 18:34:54.832: INFO: created pod pod-service-account-mountsa-mountspec
Jul 20 18:34:54.832: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul 20 18:34:54.845: INFO: created pod pod-service-account-nomountsa-mountspec
Jul 20 18:34:54.845: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul 20 18:34:54.864: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul 20 18:34:54.864: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul 20 18:34:54.877: INFO: created pod pod-service-account-mountsa-nomountspec
Jul 20 18:34:54.877: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul 20 18:34:54.890: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul 20 18:34:54.891: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:34:54.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8780" for this suite.
Jul 20 18:35:02.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:35:03.623: INFO: namespace svcaccounts-8780 deletion completed in 8.709297756s

â€¢ [SLOW TEST:9.712 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:35:03.624: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7366
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jul 20 18:35:33.971: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0720 18:35:33.971242      25 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 20 18:35:33.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7366" for this suite.
Jul 20 18:35:40.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:35:40.529: INFO: namespace gc-7366 deletion completed in 6.541428801s

â€¢ [SLOW TEST:36.905 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:35:40.529: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8260
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8260
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-8260
I0720 18:35:41.136765      25 runners.go:184] Created replication controller with name: externalname-service, namespace: services-8260, replica count: 2
Jul 20 18:35:44.187: INFO: Creating new exec pod
I0720 18:35:44.187315      25 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 20 18:35:47.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=services-8260 execpod7przt -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jul 20 18:35:47.728: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 20 18:35:47.728: INFO: stdout: ""
Jul 20 18:35:47.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=services-8260 execpod7przt -- /bin/sh -x -c nc -zv -t -w 2 172.21.230.167 80'
Jul 20 18:35:47.987: INFO: stderr: "+ nc -zv -t -w 2 172.21.230.167 80\nConnection to 172.21.230.167 80 port [tcp/http] succeeded!\n"
Jul 20 18:35:47.987: INFO: stdout: ""
Jul 20 18:35:47.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=services-8260 execpod7przt -- /bin/sh -x -c nc -zv -t -w 2 10.123.236.220 30889'
Jul 20 18:35:48.222: INFO: stderr: "+ nc -zv -t -w 2 10.123.236.220 30889\nConnection to 10.123.236.220 30889 port [tcp/30889] succeeded!\n"
Jul 20 18:35:48.222: INFO: stdout: ""
Jul 20 18:35:48.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=services-8260 execpod7przt -- /bin/sh -x -c nc -zv -t -w 2 10.123.236.227 30889'
Jul 20 18:35:48.574: INFO: stderr: "+ nc -zv -t -w 2 10.123.236.227 30889\nConnection to 10.123.236.227 30889 port [tcp/30889] succeeded!\n"
Jul 20 18:35:48.574: INFO: stdout: ""
Jul 20 18:35:48.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=services-8260 execpod7przt -- /bin/sh -x -c nc -zv -t -w 2 149.81.110.243 30889'
Jul 20 18:35:48.807: INFO: stderr: "+ nc -zv -t -w 2 149.81.110.243 30889\nConnection to 149.81.110.243 30889 port [tcp/30889] succeeded!\n"
Jul 20 18:35:48.807: INFO: stdout: ""
Jul 20 18:35:48.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=services-8260 execpod7przt -- /bin/sh -x -c nc -zv -t -w 2 149.81.110.250 30889'
Jul 20 18:35:49.030: INFO: stderr: "+ nc -zv -t -w 2 149.81.110.250 30889\nConnection to 149.81.110.250 30889 port [tcp/30889] succeeded!\n"
Jul 20 18:35:49.030: INFO: stdout: ""
Jul 20 18:35:49.030: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:35:49.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8260" for this suite.
Jul 20 18:35:57.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:35:57.999: INFO: namespace services-8260 deletion completed in 8.835984506s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:17.470 seconds]
[sig-network] Services
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:35:58.002: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1903
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jul 20 18:35:58.296: INFO: Waiting up to 5m0s for pod "downwardapi-volume-992da386-b777-41fe-9588-af2fe96fc791" in namespace "projected-1903" to be "success or failure"
Jul 20 18:35:58.313: INFO: Pod "downwardapi-volume-992da386-b777-41fe-9588-af2fe96fc791": Phase="Pending", Reason="", readiness=false. Elapsed: 16.078186ms
Jul 20 18:36:00.325: INFO: Pod "downwardapi-volume-992da386-b777-41fe-9588-af2fe96fc791": Phase="Running", Reason="", readiness=true. Elapsed: 2.029004783s
Jul 20 18:36:02.339: INFO: Pod "downwardapi-volume-992da386-b777-41fe-9588-af2fe96fc791": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04224756s
STEP: Saw pod success
Jul 20 18:36:02.339: INFO: Pod "downwardapi-volume-992da386-b777-41fe-9588-af2fe96fc791" satisfied condition "success or failure"
Jul 20 18:36:02.354: INFO: Trying to get logs from node 10.123.236.227 pod downwardapi-volume-992da386-b777-41fe-9588-af2fe96fc791 container client-container: <nil>
STEP: delete the pod
Jul 20 18:36:02.434: INFO: Waiting for pod downwardapi-volume-992da386-b777-41fe-9588-af2fe96fc791 to disappear
Jul 20 18:36:02.448: INFO: Pod downwardapi-volume-992da386-b777-41fe-9588-af2fe96fc791 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:36:02.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1903" for this suite.
Jul 20 18:36:08.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:36:09.287: INFO: namespace projected-1903 deletion completed in 6.811086339s

â€¢ [SLOW TEST:11.285 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:36:09.288: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4579
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 18:36:09.629: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"d3c1c752-e68b-4e8a-98fb-3457082df662", Controller:(*bool)(0xc002cfcede), BlockOwnerDeletion:(*bool)(0xc002cfcedf)}}
Jul 20 18:36:09.647: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"72f932c7-e0bb-4e8a-a246-eb5854126fbe", Controller:(*bool)(0xc000396c06), BlockOwnerDeletion:(*bool)(0xc000396c07)}}
Jul 20 18:36:09.660: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"3c7fc94b-1723-42c9-84b9-46c1672242cc", Controller:(*bool)(0xc002cfd096), BlockOwnerDeletion:(*bool)(0xc002cfd097)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:36:14.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4579" for this suite.
Jul 20 18:36:20.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:36:21.350: INFO: namespace gc-4579 deletion completed in 6.646645858s

â€¢ [SLOW TEST:12.063 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:36:21.351: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6369
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Jul 20 18:36:21.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 create -f - --namespace=kubectl-6369'
Jul 20 18:36:21.914: INFO: stderr: ""
Jul 20 18:36:21.914: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 20 18:36:21.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6369'
Jul 20 18:36:22.007: INFO: stderr: ""
Jul 20 18:36:22.007: INFO: stdout: "update-demo-nautilus-4c6bm update-demo-nautilus-g4hfc "
Jul 20 18:36:22.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-nautilus-4c6bm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6369'
Jul 20 18:36:22.086: INFO: stderr: ""
Jul 20 18:36:22.086: INFO: stdout: ""
Jul 20 18:36:22.086: INFO: update-demo-nautilus-4c6bm is created but not running
Jul 20 18:36:27.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6369'
Jul 20 18:36:27.184: INFO: stderr: ""
Jul 20 18:36:27.185: INFO: stdout: "update-demo-nautilus-4c6bm update-demo-nautilus-g4hfc "
Jul 20 18:36:27.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-nautilus-4c6bm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6369'
Jul 20 18:36:27.264: INFO: stderr: ""
Jul 20 18:36:27.264: INFO: stdout: "true"
Jul 20 18:36:27.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-nautilus-4c6bm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6369'
Jul 20 18:36:27.355: INFO: stderr: ""
Jul 20 18:36:27.355: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 20 18:36:27.355: INFO: validating pod update-demo-nautilus-4c6bm
Jul 20 18:36:27.377: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 20 18:36:27.377: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 20 18:36:27.377: INFO: update-demo-nautilus-4c6bm is verified up and running
Jul 20 18:36:27.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-nautilus-g4hfc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6369'
Jul 20 18:36:27.488: INFO: stderr: ""
Jul 20 18:36:27.488: INFO: stdout: "true"
Jul 20 18:36:27.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-nautilus-g4hfc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6369'
Jul 20 18:36:27.573: INFO: stderr: ""
Jul 20 18:36:27.573: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 20 18:36:27.573: INFO: validating pod update-demo-nautilus-g4hfc
Jul 20 18:36:27.600: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 20 18:36:27.600: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 20 18:36:27.600: INFO: update-demo-nautilus-g4hfc is verified up and running
STEP: rolling-update to new replication controller
Jul 20 18:36:27.602: INFO: scanned /root for discovery docs: <nil>
Jul 20 18:36:27.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-6369'
Jul 20 18:36:51.114: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul 20 18:36:51.114: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 20 18:36:51.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6369'
Jul 20 18:36:51.199: INFO: stderr: ""
Jul 20 18:36:51.199: INFO: stdout: "update-demo-kitten-dw9lp update-demo-kitten-hmp8d "
Jul 20 18:36:51.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-kitten-dw9lp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6369'
Jul 20 18:36:51.275: INFO: stderr: ""
Jul 20 18:36:51.275: INFO: stdout: "true"
Jul 20 18:36:51.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-kitten-dw9lp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6369'
Jul 20 18:36:51.367: INFO: stderr: ""
Jul 20 18:36:51.367: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul 20 18:36:51.367: INFO: validating pod update-demo-kitten-dw9lp
Jul 20 18:36:51.389: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 20 18:36:51.389: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 20 18:36:51.389: INFO: update-demo-kitten-dw9lp is verified up and running
Jul 20 18:36:51.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-kitten-hmp8d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6369'
Jul 20 18:36:51.463: INFO: stderr: ""
Jul 20 18:36:51.463: INFO: stdout: "true"
Jul 20 18:36:51.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-kitten-hmp8d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6369'
Jul 20 18:36:51.548: INFO: stderr: ""
Jul 20 18:36:51.548: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul 20 18:36:51.548: INFO: validating pod update-demo-kitten-hmp8d
Jul 20 18:36:51.575: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 20 18:36:51.575: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 20 18:36:51.575: INFO: update-demo-kitten-hmp8d is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:36:51.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6369" for this suite.
Jul 20 18:37:23.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:37:24.205: INFO: namespace kubectl-6369 deletion completed in 32.615434136s

â€¢ [SLOW TEST:62.854 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:37:24.206: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9663
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 18:37:24.464: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-e6a8f10b-b4fe-477d-b5a4-72c41997252f" in namespace "security-context-test-9663" to be "success or failure"
Jul 20 18:37:24.473: INFO: Pod "busybox-readonly-false-e6a8f10b-b4fe-477d-b5a4-72c41997252f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.654124ms
Jul 20 18:37:26.488: INFO: Pod "busybox-readonly-false-e6a8f10b-b4fe-477d-b5a4-72c41997252f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024428505s
Jul 20 18:37:28.498: INFO: Pod "busybox-readonly-false-e6a8f10b-b4fe-477d-b5a4-72c41997252f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034334436s
Jul 20 18:37:30.515: INFO: Pod "busybox-readonly-false-e6a8f10b-b4fe-477d-b5a4-72c41997252f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051831969s
Jul 20 18:37:30.516: INFO: Pod "busybox-readonly-false-e6a8f10b-b4fe-477d-b5a4-72c41997252f" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:37:30.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9663" for this suite.
Jul 20 18:37:36.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:37:37.059: INFO: namespace security-context-test-9663 deletion completed in 6.52683758s

â€¢ [SLOW TEST:12.853 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:37:37.059: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3867
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 20 18:37:37.422: INFO: Number of nodes with available pods: 0
Jul 20 18:37:37.422: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 18:37:38.455: INFO: Number of nodes with available pods: 0
Jul 20 18:37:38.455: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 18:37:39.450: INFO: Number of nodes with available pods: 2
Jul 20 18:37:39.450: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 18:37:40.448: INFO: Number of nodes with available pods: 3
Jul 20 18:37:40.448: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul 20 18:37:40.508: INFO: Number of nodes with available pods: 2
Jul 20 18:37:40.508: INFO: Node 10.123.236.227 is running more than one daemon pod
Jul 20 18:37:41.534: INFO: Number of nodes with available pods: 2
Jul 20 18:37:41.535: INFO: Node 10.123.236.227 is running more than one daemon pod
Jul 20 18:37:42.534: INFO: Number of nodes with available pods: 2
Jul 20 18:37:42.534: INFO: Node 10.123.236.227 is running more than one daemon pod
Jul 20 18:37:43.535: INFO: Number of nodes with available pods: 3
Jul 20 18:37:43.535: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3867, will wait for the garbage collector to delete the pods
Jul 20 18:37:43.632: INFO: Deleting DaemonSet.extensions daemon-set took: 19.946751ms
Jul 20 18:37:43.832: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.340381ms
Jul 20 18:37:51.241: INFO: Number of nodes with available pods: 0
Jul 20 18:37:51.241: INFO: Number of running nodes: 0, number of available pods: 0
Jul 20 18:37:51.250: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3867/daemonsets","resourceVersion":"22642"},"items":null}

Jul 20 18:37:51.265: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3867/pods","resourceVersion":"22642"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:37:51.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3867" for this suite.
Jul 20 18:37:59.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:37:59.970: INFO: namespace daemonsets-3867 deletion completed in 8.628715756s

â€¢ [SLOW TEST:22.911 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:37:59.971: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9569
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 20 18:38:08.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 20 18:38:08.428: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 20 18:38:10.429: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 20 18:38:10.441: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 20 18:38:12.429: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 20 18:38:12.440: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 20 18:38:14.429: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 20 18:38:14.440: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 20 18:38:16.429: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 20 18:38:16.439: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:38:16.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9569" for this suite.
Jul 20 18:38:28.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:38:29.061: INFO: namespace container-lifecycle-hook-9569 deletion completed in 12.60740762s

â€¢ [SLOW TEST:29.090 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:38:29.062: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1999
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1999
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-1999
I0720 18:38:29.474550      25 runners.go:184] Created replication controller with name: externalname-service, namespace: services-1999, replica count: 2
Jul 20 18:38:32.525: INFO: Creating new exec pod
I0720 18:38:32.525137      25 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 20 18:38:37.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=services-1999 execpodvntwv -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jul 20 18:38:37.808: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 20 18:38:37.808: INFO: stdout: ""
Jul 20 18:38:37.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=services-1999 execpodvntwv -- /bin/sh -x -c nc -zv -t -w 2 172.21.44.97 80'
Jul 20 18:38:38.038: INFO: stderr: "+ nc -zv -t -w 2 172.21.44.97 80\nConnection to 172.21.44.97 80 port [tcp/http] succeeded!\n"
Jul 20 18:38:38.038: INFO: stdout: ""
Jul 20 18:38:38.038: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:38:38.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1999" for this suite.
Jul 20 18:38:44.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:38:44.933: INFO: namespace services-1999 deletion completed in 6.783372893s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:15.871 seconds]
[sig-network] Services
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:38:44.933: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4146
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:38:45.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4146" for this suite.
Jul 20 18:38:57.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:38:57.922: INFO: namespace pods-4146 deletion completed in 12.704754381s

â€¢ [SLOW TEST:12.989 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:38:57.925: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8927
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-0fa7ecf5-dc54-4fb2-a17d-956237f1983e
STEP: Creating a pod to test consume configMaps
Jul 20 18:38:58.210: INFO: Waiting up to 5m0s for pod "pod-configmaps-cea7be6f-6003-4bee-a883-5b613fa63c04" in namespace "configmap-8927" to be "success or failure"
Jul 20 18:38:58.244: INFO: Pod "pod-configmaps-cea7be6f-6003-4bee-a883-5b613fa63c04": Phase="Pending", Reason="", readiness=false. Elapsed: 33.683882ms
Jul 20 18:39:00.259: INFO: Pod "pod-configmaps-cea7be6f-6003-4bee-a883-5b613fa63c04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.048277105s
STEP: Saw pod success
Jul 20 18:39:00.259: INFO: Pod "pod-configmaps-cea7be6f-6003-4bee-a883-5b613fa63c04" satisfied condition "success or failure"
Jul 20 18:39:00.268: INFO: Trying to get logs from node 10.123.236.220 pod pod-configmaps-cea7be6f-6003-4bee-a883-5b613fa63c04 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 20 18:39:00.349: INFO: Waiting for pod pod-configmaps-cea7be6f-6003-4bee-a883-5b613fa63c04 to disappear
Jul 20 18:39:00.361: INFO: Pod pod-configmaps-cea7be6f-6003-4bee-a883-5b613fa63c04 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:39:00.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8927" for this suite.
Jul 20 18:39:06.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:39:06.991: INFO: namespace configmap-8927 deletion completed in 6.614709222s

â€¢ [SLOW TEST:9.066 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:39:06.992: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-1258
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 18:39:07.220: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Creating first CR 
Jul 20 18:39:07.464: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-07-20T18:39:07Z generation:1 name:name1 resourceVersion:23022 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:28a8e394-ea0b-4341-9b3e-ccfa0d165915] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jul 20 18:39:17.477: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-07-20T18:39:17Z generation:1 name:name2 resourceVersion:23037 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:b227676f-79d8-4151-bbfc-7c0a77a2c853] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jul 20 18:39:27.494: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-07-20T18:39:07Z generation:2 name:name1 resourceVersion:23052 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:28a8e394-ea0b-4341-9b3e-ccfa0d165915] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jul 20 18:39:37.509: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-07-20T18:39:17Z generation:2 name:name2 resourceVersion:23066 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:b227676f-79d8-4151-bbfc-7c0a77a2c853] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jul 20 18:39:47.529: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-07-20T18:39:07Z generation:2 name:name1 resourceVersion:23079 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:28a8e394-ea0b-4341-9b3e-ccfa0d165915] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jul 20 18:39:57.551: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-07-20T18:39:17Z generation:2 name:name2 resourceVersion:23095 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:b227676f-79d8-4151-bbfc-7c0a77a2c853] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:40:08.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-1258" for this suite.
Jul 20 18:40:14.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:40:14.649: INFO: namespace crd-watch-1258 deletion completed in 6.553001312s

â€¢ [SLOW TEST:67.657 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:40:14.650: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6624
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-b41d2b2c-c536-428a-909f-913ab98c2285
STEP: Creating configMap with name cm-test-opt-upd-c4b96029-44a6-44b0-b2b0-864821e293cd
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b41d2b2c-c536-428a-909f-913ab98c2285
STEP: Updating configmap cm-test-opt-upd-c4b96029-44a6-44b0-b2b0-864821e293cd
STEP: Creating configMap with name cm-test-opt-create-cc7fc75c-fa93-48a8-8322-061a87c5299f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:41:32.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6624" for this suite.
Jul 20 18:42:02.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:42:03.655: INFO: namespace projected-6624 deletion completed in 30.78566507s

â€¢ [SLOW TEST:109.006 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:42:03.656: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6184
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-e7bc68d7-b934-4e48-b650-391715926e99
STEP: Creating a pod to test consume secrets
Jul 20 18:42:04.025: INFO: Waiting up to 5m0s for pod "pod-secrets-b6bc1c72-9ac4-4b1e-9787-b643ad238575" in namespace "secrets-6184" to be "success or failure"
Jul 20 18:42:04.035: INFO: Pod "pod-secrets-b6bc1c72-9ac4-4b1e-9787-b643ad238575": Phase="Pending", Reason="", readiness=false. Elapsed: 9.958197ms
Jul 20 18:42:06.045: INFO: Pod "pod-secrets-b6bc1c72-9ac4-4b1e-9787-b643ad238575": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020447985s
STEP: Saw pod success
Jul 20 18:42:06.045: INFO: Pod "pod-secrets-b6bc1c72-9ac4-4b1e-9787-b643ad238575" satisfied condition "success or failure"
Jul 20 18:42:06.055: INFO: Trying to get logs from node 10.123.236.230 pod pod-secrets-b6bc1c72-9ac4-4b1e-9787-b643ad238575 container secret-volume-test: <nil>
STEP: delete the pod
Jul 20 18:42:06.373: INFO: Waiting for pod pod-secrets-b6bc1c72-9ac4-4b1e-9787-b643ad238575 to disappear
Jul 20 18:42:06.383: INFO: Pod pod-secrets-b6bc1c72-9ac4-4b1e-9787-b643ad238575 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:42:06.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6184" for this suite.
Jul 20 18:42:12.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:42:12.932: INFO: namespace secrets-6184 deletion completed in 6.533831001s

â€¢ [SLOW TEST:9.276 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:42:12.932: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9646
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 18:42:13.184: INFO: Waiting up to 5m0s for pod "busybox-user-65534-ba904574-e26c-4009-9edb-69068a10d147" in namespace "security-context-test-9646" to be "success or failure"
Jul 20 18:42:13.193: INFO: Pod "busybox-user-65534-ba904574-e26c-4009-9edb-69068a10d147": Phase="Pending", Reason="", readiness=false. Elapsed: 7.944544ms
Jul 20 18:42:15.212: INFO: Pod "busybox-user-65534-ba904574-e26c-4009-9edb-69068a10d147": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027703479s
Jul 20 18:42:15.212: INFO: Pod "busybox-user-65534-ba904574-e26c-4009-9edb-69068a10d147" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:42:15.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9646" for this suite.
Jul 20 18:42:21.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:42:21.896: INFO: namespace security-context-test-9646 deletion completed in 6.580196858s

â€¢ [SLOW TEST:8.964 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:42:21.896: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-842
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:42:22.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-842" for this suite.
Jul 20 18:42:28.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:42:28.792: INFO: namespace custom-resource-definition-842 deletion completed in 6.627052808s

â€¢ [SLOW TEST:6.896 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:42:28.792: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8501
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-cc2822cf-20bf-429f-9fa3-07cc1c881c30
STEP: Creating secret with name s-test-opt-upd-0768d68b-3559-4c03-b435-27adb70264fd
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-cc2822cf-20bf-429f-9fa3-07cc1c881c30
STEP: Updating secret s-test-opt-upd-0768d68b-3559-4c03-b435-27adb70264fd
STEP: Creating secret with name s-test-opt-create-f1c530bb-9d38-4160-bc56-af0e75fab425
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:43:55.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8501" for this suite.
Jul 20 18:44:07.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:44:07.698: INFO: namespace projected-8501 deletion completed in 12.602811087s

â€¢ [SLOW TEST:98.906 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:44:07.699: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1929
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Jul 20 18:44:07.928: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-291352008 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:44:08.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1929" for this suite.
Jul 20 18:44:14.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:44:14.643: INFO: namespace kubectl-1929 deletion completed in 6.628550432s

â€¢ [SLOW TEST:6.944 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:44:14.644: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6960
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 20 18:44:14.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-6960'
Jul 20 18:44:14.985: INFO: stderr: ""
Jul 20 18:44:14.985: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Jul 20 18:44:14.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 delete pods e2e-test-httpd-pod --namespace=kubectl-6960'
Jul 20 18:44:25.229: INFO: stderr: ""
Jul 20 18:44:25.229: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:44:25.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6960" for this suite.
Jul 20 18:44:31.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:44:32.031: INFO: namespace kubectl-6960 deletion completed in 6.784156134s

â€¢ [SLOW TEST:17.387 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:44:32.032: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7167
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-f6fc2570-9288-45ed-a72c-e3fd8f7c9c7b
STEP: Creating a pod to test consume configMaps
Jul 20 18:44:32.329: INFO: Waiting up to 5m0s for pod "pod-configmaps-509e84d9-d179-4579-aca9-78822ebcf744" in namespace "configmap-7167" to be "success or failure"
Jul 20 18:44:32.338: INFO: Pod "pod-configmaps-509e84d9-d179-4579-aca9-78822ebcf744": Phase="Pending", Reason="", readiness=false. Elapsed: 8.338369ms
Jul 20 18:44:34.350: INFO: Pod "pod-configmaps-509e84d9-d179-4579-aca9-78822ebcf744": Phase="Running", Reason="", readiness=true. Elapsed: 2.02043475s
Jul 20 18:44:36.360: INFO: Pod "pod-configmaps-509e84d9-d179-4579-aca9-78822ebcf744": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030509409s
STEP: Saw pod success
Jul 20 18:44:36.360: INFO: Pod "pod-configmaps-509e84d9-d179-4579-aca9-78822ebcf744" satisfied condition "success or failure"
Jul 20 18:44:36.371: INFO: Trying to get logs from node 10.123.236.227 pod pod-configmaps-509e84d9-d179-4579-aca9-78822ebcf744 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 20 18:44:36.431: INFO: Waiting for pod pod-configmaps-509e84d9-d179-4579-aca9-78822ebcf744 to disappear
Jul 20 18:44:36.442: INFO: Pod pod-configmaps-509e84d9-d179-4579-aca9-78822ebcf744 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:44:36.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7167" for this suite.
Jul 20 18:44:42.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:44:43.039: INFO: namespace configmap-7167 deletion completed in 6.581209561s

â€¢ [SLOW TEST:11.007 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:44:43.041: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3643
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 20 18:44:43.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-3643'
Jul 20 18:44:43.396: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 20 18:44:43.396: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Jul 20 18:44:45.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 delete deployment e2e-test-httpd-deployment --namespace=kubectl-3643'
Jul 20 18:44:45.526: INFO: stderr: ""
Jul 20 18:44:45.526: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:44:45.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3643" for this suite.
Jul 20 18:44:51.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:44:52.223: INFO: namespace kubectl-3643 deletion completed in 6.681464444s

â€¢ [SLOW TEST:9.182 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:44:52.223: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8230
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8230.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8230.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8230.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8230.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8230.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8230.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 20 18:45:04.707: INFO: DNS probes using dns-8230/dns-test-e1fc1e09-bfb6-479c-8f5d-1459f65fbf43 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:45:04.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8230" for this suite.
Jul 20 18:45:10.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:45:11.565: INFO: namespace dns-8230 deletion completed in 6.808169335s

â€¢ [SLOW TEST:19.341 seconds]
[sig-network] DNS
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:45:11.565: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4216
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4216
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Jul 20 18:45:11.924: INFO: Found 0 stateful pods, waiting for 3
Jul 20 18:45:21.937: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 20 18:45:21.937: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 20 18:45:21.937: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jul 20 18:45:22.017: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul 20 18:45:32.111: INFO: Updating stateful set ss2
Jul 20 18:45:32.130: INFO: Waiting for Pod statefulset-4216/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Jul 20 18:45:42.234: INFO: Found 2 stateful pods, waiting for 3
Jul 20 18:45:52.252: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 20 18:45:52.252: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 20 18:45:52.252: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul 20 18:45:52.306: INFO: Updating stateful set ss2
Jul 20 18:45:52.329: INFO: Waiting for Pod statefulset-4216/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 20 18:46:02.348: INFO: Waiting for Pod statefulset-4216/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 20 18:46:12.380: INFO: Updating stateful set ss2
Jul 20 18:46:12.400: INFO: Waiting for StatefulSet statefulset-4216/ss2 to complete update
Jul 20 18:46:12.400: INFO: Waiting for Pod statefulset-4216/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 20 18:46:22.420: INFO: Waiting for StatefulSet statefulset-4216/ss2 to complete update
Jul 20 18:46:22.420: INFO: Waiting for Pod statefulset-4216/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jul 20 18:46:32.420: INFO: Deleting all statefulset in ns statefulset-4216
Jul 20 18:46:32.428: INFO: Scaling statefulset ss2 to 0
Jul 20 18:47:02.481: INFO: Waiting for statefulset status.replicas updated to 0
Jul 20 18:47:02.490: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:47:02.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4216" for this suite.
Jul 20 18:47:10.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:47:11.553: INFO: namespace statefulset-4216 deletion completed in 9.008094277s

â€¢ [SLOW TEST:119.988 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:47:11.555: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2843
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Jul 20 18:47:11.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 --namespace=kubectl-2843 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jul 20 18:47:14.312: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jul 20 18:47:14.312: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:47:16.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2843" for this suite.
Jul 20 18:47:26.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:47:26.904: INFO: namespace kubectl-2843 deletion completed in 10.555603501s

â€¢ [SLOW TEST:15.349 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:47:26.905: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6147
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 20 18:47:27.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-6147'
Jul 20 18:47:27.220: INFO: stderr: ""
Jul 20 18:47:27.220: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jul 20 18:47:32.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pod e2e-test-httpd-pod --namespace=kubectl-6147 -o json'
Jul 20 18:47:32.356: INFO: stderr: ""
Jul 20 18:47:32.356: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2020-07-20T18:47:27Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6147\",\n        \"resourceVersion\": \"24507\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6147/pods/e2e-test-httpd-pod\",\n        \"uid\": \"bc05f27e-314b-4677-9287-16d59e64247e\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-mxf2s\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.123.236.227\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-mxf2s\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-mxf2s\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-07-20T18:47:27Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-07-20T18:47:29Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-07-20T18:47:29Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-07-20T18:47:27Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://b7e2278cf6c050b07d82e598813dac77259255a571ac64e9c9b5da7453be665a\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-07-20T18:47:28Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.123.236.227\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.106.186\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.106.186\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-07-20T18:47:27Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul 20 18:47:32.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 replace -f - --namespace=kubectl-6147'
Jul 20 18:47:32.710: INFO: stderr: ""
Jul 20 18:47:32.710: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Jul 20 18:47:32.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 delete pods e2e-test-httpd-pod --namespace=kubectl-6147'
Jul 20 18:47:44.762: INFO: stderr: ""
Jul 20 18:47:44.762: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:47:44.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6147" for this suite.
Jul 20 18:47:50.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:47:51.485: INFO: namespace kubectl-6147 deletion completed in 6.706211562s

â€¢ [SLOW TEST:24.580 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:47:51.485: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-966
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-3d20649b-d26f-4221-b8ac-c67c3b517cfb
STEP: Creating a pod to test consume configMaps
Jul 20 18:47:51.781: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-58f695be-e95a-48de-97fa-f2e6a2c09a8e" in namespace "projected-966" to be "success or failure"
Jul 20 18:47:51.790: INFO: Pod "pod-projected-configmaps-58f695be-e95a-48de-97fa-f2e6a2c09a8e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.779242ms
Jul 20 18:47:53.800: INFO: Pod "pod-projected-configmaps-58f695be-e95a-48de-97fa-f2e6a2c09a8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019112494s
Jul 20 18:47:55.812: INFO: Pod "pod-projected-configmaps-58f695be-e95a-48de-97fa-f2e6a2c09a8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030931867s
STEP: Saw pod success
Jul 20 18:47:55.812: INFO: Pod "pod-projected-configmaps-58f695be-e95a-48de-97fa-f2e6a2c09a8e" satisfied condition "success or failure"
Jul 20 18:47:55.820: INFO: Trying to get logs from node 10.123.236.227 pod pod-projected-configmaps-58f695be-e95a-48de-97fa-f2e6a2c09a8e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 20 18:47:55.898: INFO: Waiting for pod pod-projected-configmaps-58f695be-e95a-48de-97fa-f2e6a2c09a8e to disappear
Jul 20 18:47:55.909: INFO: Pod pod-projected-configmaps-58f695be-e95a-48de-97fa-f2e6a2c09a8e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:47:55.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-966" for this suite.
Jul 20 18:48:01.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:48:02.467: INFO: namespace projected-966 deletion completed in 6.546546506s

â€¢ [SLOW TEST:10.982 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:48:02.468: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7069
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jul 20 18:48:02.780: INFO: Waiting up to 5m0s for pod "downward-api-66f13c15-e637-4c96-9e16-bea41f8134c7" in namespace "downward-api-7069" to be "success or failure"
Jul 20 18:48:02.789: INFO: Pod "downward-api-66f13c15-e637-4c96-9e16-bea41f8134c7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.251395ms
Jul 20 18:48:04.799: INFO: Pod "downward-api-66f13c15-e637-4c96-9e16-bea41f8134c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019230344s
STEP: Saw pod success
Jul 20 18:48:04.799: INFO: Pod "downward-api-66f13c15-e637-4c96-9e16-bea41f8134c7" satisfied condition "success or failure"
Jul 20 18:48:04.808: INFO: Trying to get logs from node 10.123.236.220 pod downward-api-66f13c15-e637-4c96-9e16-bea41f8134c7 container dapi-container: <nil>
STEP: delete the pod
Jul 20 18:48:05.171: INFO: Waiting for pod downward-api-66f13c15-e637-4c96-9e16-bea41f8134c7 to disappear
Jul 20 18:48:05.181: INFO: Pod downward-api-66f13c15-e637-4c96-9e16-bea41f8134c7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:48:05.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7069" for this suite.
Jul 20 18:48:11.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:48:11.884: INFO: namespace downward-api-7069 deletion completed in 6.687699477s

â€¢ [SLOW TEST:9.416 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:48:11.884: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8653
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Jul 20 18:48:14.756: INFO: Successfully updated pod "annotationupdate230e4a9f-d52f-4a34-bc81-5e8bae19353f"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:48:18.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8653" for this suite.
Jul 20 18:48:46.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:48:47.504: INFO: namespace projected-8653 deletion completed in 28.641856453s

â€¢ [SLOW TEST:35.620 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:48:47.506: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6611
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Jul 20 18:48:47.771: INFO: Waiting up to 5m0s for pod "pod-f0a0ca2d-ea87-4503-a354-59e96b597fc1" in namespace "emptydir-6611" to be "success or failure"
Jul 20 18:48:47.783: INFO: Pod "pod-f0a0ca2d-ea87-4503-a354-59e96b597fc1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.621627ms
Jul 20 18:48:49.794: INFO: Pod "pod-f0a0ca2d-ea87-4503-a354-59e96b597fc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022676982s
STEP: Saw pod success
Jul 20 18:48:49.794: INFO: Pod "pod-f0a0ca2d-ea87-4503-a354-59e96b597fc1" satisfied condition "success or failure"
Jul 20 18:48:49.804: INFO: Trying to get logs from node 10.123.236.227 pod pod-f0a0ca2d-ea87-4503-a354-59e96b597fc1 container test-container: <nil>
STEP: delete the pod
Jul 20 18:48:49.894: INFO: Waiting for pod pod-f0a0ca2d-ea87-4503-a354-59e96b597fc1 to disappear
Jul 20 18:48:49.912: INFO: Pod pod-f0a0ca2d-ea87-4503-a354-59e96b597fc1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:48:49.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6611" for this suite.
Jul 20 18:48:56.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:48:56.513: INFO: namespace emptydir-6611 deletion completed in 6.583133688s

â€¢ [SLOW TEST:9.007 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:48:56.515: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5289
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:48:59.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5289" for this suite.
Jul 20 18:49:12.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:49:12.619: INFO: namespace replication-controller-5289 deletion completed in 12.763347303s

â€¢ [SLOW TEST:16.103 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:49:12.619: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7140
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Jul 20 18:49:12.912: INFO: Waiting up to 5m0s for pod "client-containers-1faf1cc0-3467-47ca-83aa-4f2657442248" in namespace "containers-7140" to be "success or failure"
Jul 20 18:49:12.921: INFO: Pod "client-containers-1faf1cc0-3467-47ca-83aa-4f2657442248": Phase="Pending", Reason="", readiness=false. Elapsed: 8.487836ms
Jul 20 18:49:14.936: INFO: Pod "client-containers-1faf1cc0-3467-47ca-83aa-4f2657442248": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023126541s
STEP: Saw pod success
Jul 20 18:49:14.936: INFO: Pod "client-containers-1faf1cc0-3467-47ca-83aa-4f2657442248" satisfied condition "success or failure"
Jul 20 18:49:14.948: INFO: Trying to get logs from node 10.123.236.227 pod client-containers-1faf1cc0-3467-47ca-83aa-4f2657442248 container test-container: <nil>
STEP: delete the pod
Jul 20 18:49:15.017: INFO: Waiting for pod client-containers-1faf1cc0-3467-47ca-83aa-4f2657442248 to disappear
Jul 20 18:49:15.027: INFO: Pod client-containers-1faf1cc0-3467-47ca-83aa-4f2657442248 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:49:15.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7140" for this suite.
Jul 20 18:49:21.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:49:21.666: INFO: namespace containers-7140 deletion completed in 6.62447998s

â€¢ [SLOW TEST:9.047 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:49:21.666: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3600
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:49:24.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3600" for this suite.
Jul 20 18:49:36.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:49:36.957: INFO: namespace containers-3600 deletion completed in 12.924352287s

â€¢ [SLOW TEST:15.291 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:49:36.957: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4381
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 18:49:37.392: INFO: Create a RollingUpdate DaemonSet
Jul 20 18:49:37.405: INFO: Check that daemon pods launch on every node of the cluster
Jul 20 18:49:37.432: INFO: Number of nodes with available pods: 0
Jul 20 18:49:37.432: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 18:49:38.461: INFO: Number of nodes with available pods: 0
Jul 20 18:49:38.461: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 18:49:39.459: INFO: Number of nodes with available pods: 2
Jul 20 18:49:39.459: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 18:49:40.458: INFO: Number of nodes with available pods: 3
Jul 20 18:49:40.458: INFO: Number of running nodes: 3, number of available pods: 3
Jul 20 18:49:40.458: INFO: Update the DaemonSet to trigger a rollout
Jul 20 18:49:40.477: INFO: Updating DaemonSet daemon-set
Jul 20 18:49:45.543: INFO: Roll back the DaemonSet before rollout is complete
Jul 20 18:49:45.565: INFO: Updating DaemonSet daemon-set
Jul 20 18:49:45.565: INFO: Make sure DaemonSet rollback is complete
Jul 20 18:49:45.576: INFO: Wrong image for pod: daemon-set-dtkc9. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 20 18:49:45.576: INFO: Pod daemon-set-dtkc9 is not available
Jul 20 18:49:46.602: INFO: Wrong image for pod: daemon-set-dtkc9. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 20 18:49:46.602: INFO: Pod daemon-set-dtkc9 is not available
Jul 20 18:49:47.603: INFO: Pod daemon-set-hth42 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4381, will wait for the garbage collector to delete the pods
Jul 20 18:49:47.731: INFO: Deleting DaemonSet.extensions daemon-set took: 21.886692ms
Jul 20 18:49:47.931: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.195045ms
Jul 20 18:49:54.848: INFO: Number of nodes with available pods: 0
Jul 20 18:49:54.848: INFO: Number of running nodes: 0, number of available pods: 0
Jul 20 18:49:54.857: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4381/daemonsets","resourceVersion":"25113"},"items":null}

Jul 20 18:49:54.869: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4381/pods","resourceVersion":"25113"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:49:54.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4381" for this suite.
Jul 20 18:50:01.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:50:01.743: INFO: namespace daemonsets-4381 deletion completed in 6.816940011s

â€¢ [SLOW TEST:24.786 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:50:01.745: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5413
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 18:50:02.031: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:50:04.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5413" for this suite.
Jul 20 18:50:50.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:50:51.245: INFO: namespace pods-5413 deletion completed in 46.705152674s

â€¢ [SLOW TEST:49.501 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:50:51.245: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4054
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul 20 18:50:51.576: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4054 /api/v1/namespaces/watch-4054/configmaps/e2e-watch-test-resource-version 6a59fc41-885b-435e-94e1-27163534c409 25276 0 2020-07-20 18:50:51 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 20 18:50:51.576: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4054 /api/v1/namespaces/watch-4054/configmaps/e2e-watch-test-resource-version 6a59fc41-885b-435e-94e1-27163534c409 25277 0 2020-07-20 18:50:51 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:50:51.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4054" for this suite.
Jul 20 18:50:57.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:50:58.202: INFO: namespace watch-4054 deletion completed in 6.611535295s

â€¢ [SLOW TEST:6.956 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:50:58.204: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-365
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-805f1097-a044-4d8c-bd86-62be6ba84cc9 in namespace container-probe-365
Jul 20 18:51:00.509: INFO: Started pod liveness-805f1097-a044-4d8c-bd86-62be6ba84cc9 in namespace container-probe-365
STEP: checking the pod's current state and verifying that restartCount is present
Jul 20 18:51:00.519: INFO: Initial restart count of pod liveness-805f1097-a044-4d8c-bd86-62be6ba84cc9 is 0
Jul 20 18:51:18.632: INFO: Restart count of pod container-probe-365/liveness-805f1097-a044-4d8c-bd86-62be6ba84cc9 is now 1 (18.112914723s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:51:18.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-365" for this suite.
Jul 20 18:51:24.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:51:25.606: INFO: namespace container-probe-365 deletion completed in 6.912308553s

â€¢ [SLOW TEST:27.402 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:51:25.607: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5856
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 20 18:51:27.975: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:51:28.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5856" for this suite.
Jul 20 18:51:34.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:51:34.693: INFO: namespace container-runtime-5856 deletion completed in 6.646229028s

â€¢ [SLOW TEST:9.086 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:51:34.694: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1271
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Jul 20 18:51:34.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 api-versions'
Jul 20 18:51:35.027: INFO: stderr: ""
Jul 20 18:51:35.027: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:51:35.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1271" for this suite.
Jul 20 18:51:41.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:51:41.668: INFO: namespace kubectl-1271 deletion completed in 6.628680519s

â€¢ [SLOW TEST:6.974 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:51:41.669: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1103
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-ae827b20-1db8-40ba-b194-9a8b9171f79b
STEP: Creating a pod to test consume secrets
Jul 20 18:51:42.039: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7f2eda65-fff4-4523-8eec-5106cdc174e3" in namespace "projected-1103" to be "success or failure"
Jul 20 18:51:42.055: INFO: Pod "pod-projected-secrets-7f2eda65-fff4-4523-8eec-5106cdc174e3": Phase="Pending", Reason="", readiness=false. Elapsed: 15.949641ms
Jul 20 18:51:44.066: INFO: Pod "pod-projected-secrets-7f2eda65-fff4-4523-8eec-5106cdc174e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026751231s
STEP: Saw pod success
Jul 20 18:51:44.066: INFO: Pod "pod-projected-secrets-7f2eda65-fff4-4523-8eec-5106cdc174e3" satisfied condition "success or failure"
Jul 20 18:51:44.076: INFO: Trying to get logs from node 10.123.236.220 pod pod-projected-secrets-7f2eda65-fff4-4523-8eec-5106cdc174e3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 20 18:51:44.154: INFO: Waiting for pod pod-projected-secrets-7f2eda65-fff4-4523-8eec-5106cdc174e3 to disappear
Jul 20 18:51:44.162: INFO: Pod pod-projected-secrets-7f2eda65-fff4-4523-8eec-5106cdc174e3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:51:44.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1103" for this suite.
Jul 20 18:51:50.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:51:50.903: INFO: namespace projected-1103 deletion completed in 6.72493538s

â€¢ [SLOW TEST:9.235 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:51:50.907: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2258
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-cb3dffa7-083d-42ef-ab1d-339606c3a76e
STEP: Creating a pod to test consume configMaps
Jul 20 18:51:51.187: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b246a24a-1b66-4155-b3c2-d01317c829c1" in namespace "projected-2258" to be "success or failure"
Jul 20 18:51:51.197: INFO: Pod "pod-projected-configmaps-b246a24a-1b66-4155-b3c2-d01317c829c1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.447545ms
Jul 20 18:51:53.207: INFO: Pod "pod-projected-configmaps-b246a24a-1b66-4155-b3c2-d01317c829c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019959524s
Jul 20 18:51:55.216: INFO: Pod "pod-projected-configmaps-b246a24a-1b66-4155-b3c2-d01317c829c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029588108s
STEP: Saw pod success
Jul 20 18:51:55.216: INFO: Pod "pod-projected-configmaps-b246a24a-1b66-4155-b3c2-d01317c829c1" satisfied condition "success or failure"
Jul 20 18:51:55.225: INFO: Trying to get logs from node 10.123.236.227 pod pod-projected-configmaps-b246a24a-1b66-4155-b3c2-d01317c829c1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 20 18:51:55.302: INFO: Waiting for pod pod-projected-configmaps-b246a24a-1b66-4155-b3c2-d01317c829c1 to disappear
Jul 20 18:51:55.311: INFO: Pod pod-projected-configmaps-b246a24a-1b66-4155-b3c2-d01317c829c1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:51:55.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2258" for this suite.
Jul 20 18:52:01.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:52:01.933: INFO: namespace projected-2258 deletion completed in 6.605055134s

â€¢ [SLOW TEST:11.027 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:52:01.934: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5700
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jul 20 18:52:02.173: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 20 18:52:02.315: INFO: Waiting for terminating namespaces to be deleted...
Jul 20 18:52:02.340: INFO: 
Logging pods the kubelet thinks is on node 10.123.236.220 before test
Jul 20 18:52:02.392: INFO: calico-node-6z6jp from kube-system started at 2020-07-20 16:38:01 +0000 UTC (1 container statuses recorded)
Jul 20 18:52:02.392: INFO: 	Container calico-node ready: true, restart count 0
Jul 20 18:52:02.392: INFO: ibm-cloud-provider-ip-149-81-145-66-6c99b7f77d-6xqwt from ibm-system started at 2020-07-20 16:38:20 +0000 UTC (1 container statuses recorded)
Jul 20 18:52:02.392: INFO: 	Container ibm-cloud-provider-ip-149-81-145-66 ready: true, restart count 0
Jul 20 18:52:02.392: INFO: public-crbsas6ivf0c8md2c63apg-alb1-5f96ffbc77-c5tzp from kube-system started at 2020-07-20 16:39:13 +0000 UTC (4 container statuses recorded)
Jul 20 18:52:02.392: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Jul 20 18:52:02.392: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Jul 20 18:52:02.392: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Jul 20 18:52:02.392: INFO: 	Container nginx-ingress ready: true, restart count 0
Jul 20 18:52:02.392: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-07-20 16:40:09 +0000 UTC (1 container statuses recorded)
Jul 20 18:52:02.392: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jul 20 18:52:02.392: INFO: sonobuoy-systemd-logs-daemon-set-2fac7b176f1f4511-zngst from sonobuoy started at 2020-07-20 18:23:20 +0000 UTC (2 container statuses recorded)
Jul 20 18:52:02.392: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 20 18:52:02.392: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 20 18:52:02.392: INFO: coredns-54f55c7c7c-gw4fd from kube-system started at 2020-07-20 17:02:33 +0000 UTC (1 container statuses recorded)
Jul 20 18:52:02.392: INFO: 	Container coredns ready: true, restart count 0
Jul 20 18:52:02.392: INFO: ibm-master-proxy-static-10.123.236.220 from kube-system started at 2020-07-20 16:37:55 +0000 UTC (2 container statuses recorded)
Jul 20 18:52:02.392: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jul 20 18:52:02.392: INFO: 	Container pause ready: true, restart count 0
Jul 20 18:52:02.392: INFO: ibm-keepalived-watcher-69dl6 from kube-system started at 2020-07-20 16:38:01 +0000 UTC (1 container statuses recorded)
Jul 20 18:52:02.392: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jul 20 18:52:02.392: INFO: 
Logging pods the kubelet thinks is on node 10.123.236.227 before test
Jul 20 18:52:02.466: INFO: coredns-54f55c7c7c-fr2hn from kube-system started at 2020-07-20 17:02:34 +0000 UTC (1 container statuses recorded)
Jul 20 18:52:02.467: INFO: 	Container coredns ready: true, restart count 0
Jul 20 18:52:02.467: INFO: sonobuoy-systemd-logs-daemon-set-2fac7b176f1f4511-mktvj from sonobuoy started at 2020-07-20 18:23:20 +0000 UTC (2 container statuses recorded)
Jul 20 18:52:02.467: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 20 18:52:02.467: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 20 18:52:02.467: INFO: coredns-autoscaler-6bc79bb9db-nsbq2 from kube-system started at 2020-07-20 16:37:01 +0000 UTC (1 container statuses recorded)
Jul 20 18:52:02.467: INFO: 	Container autoscaler ready: true, restart count 0
Jul 20 18:52:02.467: INFO: catalog-operator-67646bfcdb-49pmq from ibm-system started at 2020-07-20 16:37:01 +0000 UTC (1 container statuses recorded)
Jul 20 18:52:02.467: INFO: 	Container catalog-operator ready: true, restart count 0
Jul 20 18:52:02.467: INFO: olm-operator-787498c9b7-mnb2k from ibm-system started at 2020-07-20 16:37:01 +0000 UTC (1 container statuses recorded)
Jul 20 18:52:02.467: INFO: 	Container olm-operator ready: true, restart count 0
Jul 20 18:52:02.467: INFO: addon-catalog-source-c4rf5 from ibm-system started at 2020-07-20 16:37:18 +0000 UTC (1 container statuses recorded)
Jul 20 18:52:02.467: INFO: 	Container configmap-registry-server ready: true, restart count 0
Jul 20 18:52:02.467: INFO: vpn-f66c45467-f46vm from kube-system started at 2020-07-20 17:02:12 +0000 UTC (1 container statuses recorded)
Jul 20 18:52:02.467: INFO: 	Container vpn ready: true, restart count 0
Jul 20 18:52:02.467: INFO: calico-kube-controllers-69c4db5c8d-44fh7 from kube-system started at 2020-07-20 16:37:01 +0000 UTC (1 container statuses recorded)
Jul 20 18:52:02.467: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 20 18:52:02.467: INFO: ibm-file-plugin-586dc4596c-gxcnd from kube-system started at 2020-07-20 16:37:01 +0000 UTC (1 container statuses recorded)
Jul 20 18:52:02.467: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jul 20 18:52:02.467: INFO: calico-node-85dkz from kube-system started at 2020-07-20 16:36:51 +0000 UTC (1 container statuses recorded)
Jul 20 18:52:02.467: INFO: 	Container calico-node ready: true, restart count 0
Jul 20 18:52:02.467: INFO: metrics-server-7d665f557c-wfj45 from kube-system started at 2020-07-20 16:37:17 +0000 UTC (2 container statuses recorded)
Jul 20 18:52:02.467: INFO: 	Container metrics-server ready: true, restart count 0
Jul 20 18:52:02.467: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jul 20 18:52:02.467: INFO: ibm-master-proxy-static-10.123.236.227 from kube-system started at 2020-07-20 16:36:44 +0000 UTC (2 container statuses recorded)
Jul 20 18:52:02.467: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jul 20 18:52:02.467: INFO: 	Container pause ready: true, restart count 0
Jul 20 18:52:02.467: INFO: ibm-keepalived-watcher-m9bwq from kube-system started at 2020-07-20 16:36:51 +0000 UTC (1 container statuses recorded)
Jul 20 18:52:02.467: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jul 20 18:52:02.467: INFO: ibm-storage-watcher-5955dd9995-ft6sf from kube-system started at 2020-07-20 16:37:05 +0000 UTC (1 container statuses recorded)
Jul 20 18:52:02.467: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jul 20 18:52:02.467: INFO: dashboard-metrics-scraper-5789d44f58-mr88k from kube-system started at 2020-07-20 16:37:01 +0000 UTC (1 container statuses recorded)
Jul 20 18:52:02.467: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jul 20 18:52:02.467: INFO: kubernetes-dashboard-984c5c57-cf2cf from kube-system started at 2020-07-20 16:37:01 +0000 UTC (1 container statuses recorded)
Jul 20 18:52:02.467: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul 20 18:52:02.467: INFO: 
Logging pods the kubelet thinks is on node 10.123.236.230 before test
Jul 20 18:52:02.530: INFO: ibm-cloud-provider-ip-149-81-145-66-6c99b7f77d-4rckp from ibm-system started at 2020-07-20 16:38:20 +0000 UTC (1 container statuses recorded)
Jul 20 18:52:02.530: INFO: 	Container ibm-cloud-provider-ip-149-81-145-66 ready: true, restart count 0
Jul 20 18:52:02.530: INFO: coredns-54f55c7c7c-xgw9w from kube-system started at 2020-07-20 17:02:33 +0000 UTC (1 container statuses recorded)
Jul 20 18:52:02.530: INFO: 	Container coredns ready: true, restart count 0
Jul 20 18:52:02.530: INFO: sonobuoy from sonobuoy started at 2020-07-20 18:23:14 +0000 UTC (1 container statuses recorded)
Jul 20 18:52:02.530: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 20 18:52:02.530: INFO: sonobuoy-e2e-job-0f6e1d1f061d4a08 from sonobuoy started at 2020-07-20 18:23:20 +0000 UTC (2 container statuses recorded)
Jul 20 18:52:02.530: INFO: 	Container e2e ready: true, restart count 0
Jul 20 18:52:02.530: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 20 18:52:02.530: INFO: sonobuoy-systemd-logs-daemon-set-2fac7b176f1f4511-xh4r9 from sonobuoy started at 2020-07-20 18:23:20 +0000 UTC (2 container statuses recorded)
Jul 20 18:52:02.530: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 20 18:52:02.530: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 20 18:52:02.530: INFO: ibm-master-proxy-static-10.123.236.230 from kube-system started at 2020-07-20 16:38:01 +0000 UTC (2 container statuses recorded)
Jul 20 18:52:02.530: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jul 20 18:52:02.530: INFO: 	Container pause ready: true, restart count 0
Jul 20 18:52:02.530: INFO: calico-node-wpsss from kube-system started at 2020-07-20 16:38:02 +0000 UTC (1 container statuses recorded)
Jul 20 18:52:02.530: INFO: 	Container calico-node ready: true, restart count 0
Jul 20 18:52:02.530: INFO: ibm-keepalived-watcher-fhgvr from kube-system started at 2020-07-20 16:38:02 +0000 UTC (1 container statuses recorded)
Jul 20 18:52:02.530: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jul 20 18:52:02.530: INFO: public-crbsas6ivf0c8md2c63apg-alb1-5f96ffbc77-lpqp5 from kube-system started at 2020-07-20 16:39:13 +0000 UTC (4 container statuses recorded)
Jul 20 18:52:02.530: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Jul 20 18:52:02.530: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Jul 20 18:52:02.530: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Jul 20 18:52:02.530: INFO: 	Container nginx-ingress ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-9c2d5f6f-b3f5-4022-9ae3-7a5034e8da6e 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-9c2d5f6f-b3f5-4022-9ae3-7a5034e8da6e off the node 10.123.236.220
STEP: verifying the node doesn't have the label kubernetes.io/e2e-9c2d5f6f-b3f5-4022-9ae3-7a5034e8da6e
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:52:10.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5700" for this suite.
Jul 20 18:52:28.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:52:29.353: INFO: namespace sched-pred-5700 deletion completed in 18.577957649s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:27.420 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:52:29.354: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4936
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jul 20 18:52:29.616: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d57bbdd0-d55e-493d-bf2d-33d97e4be088" in namespace "projected-4936" to be "success or failure"
Jul 20 18:52:29.625: INFO: Pod "downwardapi-volume-d57bbdd0-d55e-493d-bf2d-33d97e4be088": Phase="Pending", Reason="", readiness=false. Elapsed: 8.343018ms
Jul 20 18:52:31.635: INFO: Pod "downwardapi-volume-d57bbdd0-d55e-493d-bf2d-33d97e4be088": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018779833s
Jul 20 18:52:33.645: INFO: Pod "downwardapi-volume-d57bbdd0-d55e-493d-bf2d-33d97e4be088": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02810279s
STEP: Saw pod success
Jul 20 18:52:33.645: INFO: Pod "downwardapi-volume-d57bbdd0-d55e-493d-bf2d-33d97e4be088" satisfied condition "success or failure"
Jul 20 18:52:33.654: INFO: Trying to get logs from node 10.123.236.230 pod downwardapi-volume-d57bbdd0-d55e-493d-bf2d-33d97e4be088 container client-container: <nil>
STEP: delete the pod
Jul 20 18:52:33.724: INFO: Waiting for pod downwardapi-volume-d57bbdd0-d55e-493d-bf2d-33d97e4be088 to disappear
Jul 20 18:52:33.736: INFO: Pod downwardapi-volume-d57bbdd0-d55e-493d-bf2d-33d97e4be088 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:52:33.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4936" for this suite.
Jul 20 18:52:40.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:52:40.768: INFO: namespace projected-4936 deletion completed in 7.010237442s

â€¢ [SLOW TEST:11.414 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:52:40.769: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8128
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-6365b0b9-e317-4e70-9803-ba3d25daf4ee
Jul 20 18:52:41.027: INFO: Pod name my-hostname-basic-6365b0b9-e317-4e70-9803-ba3d25daf4ee: Found 0 pods out of 1
Jul 20 18:52:46.039: INFO: Pod name my-hostname-basic-6365b0b9-e317-4e70-9803-ba3d25daf4ee: Found 1 pods out of 1
Jul 20 18:52:46.039: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-6365b0b9-e317-4e70-9803-ba3d25daf4ee" are running
Jul 20 18:52:46.052: INFO: Pod "my-hostname-basic-6365b0b9-e317-4e70-9803-ba3d25daf4ee-6zc4m" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-20 18:52:41 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-20 18:52:43 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-20 18:52:43 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-20 18:52:41 +0000 UTC Reason: Message:}])
Jul 20 18:52:46.052: INFO: Trying to dial the pod
Jul 20 18:52:51.100: INFO: Controller my-hostname-basic-6365b0b9-e317-4e70-9803-ba3d25daf4ee: Got expected result from replica 1 [my-hostname-basic-6365b0b9-e317-4e70-9803-ba3d25daf4ee-6zc4m]: "my-hostname-basic-6365b0b9-e317-4e70-9803-ba3d25daf4ee-6zc4m", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:52:51.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8128" for this suite.
Jul 20 18:52:59.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:52:59.753: INFO: namespace replication-controller-8128 deletion completed in 8.635218825s

â€¢ [SLOW TEST:18.984 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:52:59.754: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7557
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 18:53:00.028: INFO: Creating deployment "test-recreate-deployment"
Jul 20 18:53:00.041: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul 20 18:53:00.066: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jul 20 18:53:02.086: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul 20 18:53:02.094: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730867980, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730867980, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730867980, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730867980, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 20 18:53:04.103: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730867980, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730867980, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730867980, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730867980, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 20 18:53:06.103: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730867980, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730867980, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730867980, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730867980, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 20 18:53:08.107: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul 20 18:53:08.129: INFO: Updating deployment test-recreate-deployment
Jul 20 18:53:08.129: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 20 18:53:08.226: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-7557 /apis/apps/v1/namespaces/deployment-7557/deployments/test-recreate-deployment 40f3740b-7941-41aa-a871-778b88f985f8 25833 2 2020-07-20 18:53:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002714208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-07-20 18:53:08 +0000 UTC,LastTransitionTime:2020-07-20 18:53:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-07-20 18:53:08 +0000 UTC,LastTransitionTime:2020-07-20 18:53:00 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jul 20 18:53:08.238: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-7557 /apis/apps/v1/namespaces/deployment-7557/replicasets/test-recreate-deployment-5f94c574ff f8dabcdb-d30b-416a-8515-a0b370e7c7ff 25831 1 2020-07-20 18:53:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 40f3740b-7941-41aa-a871-778b88f985f8 0xc0031042a7 0xc0031042a8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003104308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 20 18:53:08.238: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul 20 18:53:08.238: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-7557 /apis/apps/v1/namespaces/deployment-7557/replicasets/test-recreate-deployment-68fc85c7bb e2ae88d9-f687-49fd-80a3-7fca1deb91eb 25822 2 2020-07-20 18:53:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 40f3740b-7941-41aa-a871-778b88f985f8 0xc003104377 0xc003104378}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0031043d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 20 18:53:08.252: INFO: Pod "test-recreate-deployment-5f94c574ff-ss4x8" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-ss4x8 test-recreate-deployment-5f94c574ff- deployment-7557 /api/v1/namespaces/deployment-7557/pods/test-recreate-deployment-5f94c574ff-ss4x8 2737444a-e7d1-4acb-9647-16dae11053c4 25834 0 2020-07-20 18:53:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff f8dabcdb-d30b-416a-8515-a0b370e7c7ff 0xc0027145d7 0xc0027145d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lbhqx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lbhqx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lbhqx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.230,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 18:53:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 18:53:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 18:53:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 18:53:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.230,PodIP:,StartTime:2020-07-20 18:53:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:53:08.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7557" for this suite.
Jul 20 18:53:14.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:53:14.935: INFO: namespace deployment-7557 deletion completed in 6.646111224s

â€¢ [SLOW TEST:15.181 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:53:14.936: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5049
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 20 18:53:15.567: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 20 18:53:17.597: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730867995, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730867995, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730867995, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730867995, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 20 18:53:20.653: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:53:20.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5049" for this suite.
Jul 20 18:53:26.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:53:27.784: INFO: namespace webhook-5049 deletion completed in 6.889779672s
STEP: Destroying namespace "webhook-5049-markers" for this suite.
Jul 20 18:53:33.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:53:34.275: INFO: namespace webhook-5049-markers deletion completed in 6.490997438s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:19.451 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:53:34.387: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3780
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 20 18:53:35.136: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 20 18:53:37.165: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730868015, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730868015, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730868015, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730868015, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 20 18:53:40.214: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
Jul 20 18:53:50.362: INFO: Waiting for webhook configuration to be ready...
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:53:50.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3780" for this suite.
Jul 20 18:53:56.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:53:57.260: INFO: namespace webhook-3780 deletion completed in 6.602505075s
STEP: Destroying namespace "webhook-3780-markers" for this suite.
Jul 20 18:54:03.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:54:03.983: INFO: namespace webhook-3780-markers deletion completed in 6.723263523s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:29.691 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:54:04.080: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2905
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-2905/secret-test-d37890bc-c09f-40c2-9d29-442be051c5f4
STEP: Creating a pod to test consume secrets
Jul 20 18:54:04.433: INFO: Waiting up to 5m0s for pod "pod-configmaps-bc0a64c6-f8f2-4c7a-a5ab-46746cbc477a" in namespace "secrets-2905" to be "success or failure"
Jul 20 18:54:04.541: INFO: Pod "pod-configmaps-bc0a64c6-f8f2-4c7a-a5ab-46746cbc477a": Phase="Pending", Reason="", readiness=false. Elapsed: 107.13734ms
Jul 20 18:54:06.596: INFO: Pod "pod-configmaps-bc0a64c6-f8f2-4c7a-a5ab-46746cbc477a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.162086373s
STEP: Saw pod success
Jul 20 18:54:06.596: INFO: Pod "pod-configmaps-bc0a64c6-f8f2-4c7a-a5ab-46746cbc477a" satisfied condition "success or failure"
Jul 20 18:54:06.608: INFO: Trying to get logs from node 10.123.236.230 pod pod-configmaps-bc0a64c6-f8f2-4c7a-a5ab-46746cbc477a container env-test: <nil>
STEP: delete the pod
Jul 20 18:54:06.689: INFO: Waiting for pod pod-configmaps-bc0a64c6-f8f2-4c7a-a5ab-46746cbc477a to disappear
Jul 20 18:54:06.700: INFO: Pod pod-configmaps-bc0a64c6-f8f2-4c7a-a5ab-46746cbc477a no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:54:06.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2905" for this suite.
Jul 20 18:54:12.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:54:13.404: INFO: namespace secrets-2905 deletion completed in 6.683890736s

â€¢ [SLOW TEST:9.324 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:54:13.409: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-4579
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jul 20 18:54:14.028: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jul 20 18:54:16.059: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730868054, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730868054, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730868054, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730868054, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 20 18:54:19.131: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 18:54:19.142: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:54:20.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4579" for this suite.
Jul 20 18:54:26.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:54:27.297: INFO: namespace crd-webhook-4579 deletion completed in 6.69977908s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:14.049 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:54:27.460: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2958
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Jul 20 18:54:27.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 create -f - --namespace=kubectl-2958'
Jul 20 18:54:27.987: INFO: stderr: ""
Jul 20 18:54:27.987: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 20 18:54:27.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2958'
Jul 20 18:54:28.214: INFO: stderr: ""
Jul 20 18:54:28.214: INFO: stdout: "update-demo-nautilus-jjt4c update-demo-nautilus-tdf4f "
Jul 20 18:54:28.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-nautilus-jjt4c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2958'
Jul 20 18:54:28.344: INFO: stderr: ""
Jul 20 18:54:28.344: INFO: stdout: ""
Jul 20 18:54:28.344: INFO: update-demo-nautilus-jjt4c is created but not running
Jul 20 18:54:33.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2958'
Jul 20 18:54:33.422: INFO: stderr: ""
Jul 20 18:54:33.422: INFO: stdout: "update-demo-nautilus-jjt4c update-demo-nautilus-tdf4f "
Jul 20 18:54:33.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-nautilus-jjt4c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2958'
Jul 20 18:54:33.497: INFO: stderr: ""
Jul 20 18:54:33.498: INFO: stdout: "true"
Jul 20 18:54:33.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-nautilus-jjt4c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2958'
Jul 20 18:54:33.641: INFO: stderr: ""
Jul 20 18:54:33.641: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 20 18:54:33.641: INFO: validating pod update-demo-nautilus-jjt4c
Jul 20 18:54:33.664: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 20 18:54:33.664: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 20 18:54:33.664: INFO: update-demo-nautilus-jjt4c is verified up and running
Jul 20 18:54:33.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-nautilus-tdf4f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2958'
Jul 20 18:54:33.738: INFO: stderr: ""
Jul 20 18:54:33.738: INFO: stdout: "true"
Jul 20 18:54:33.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-nautilus-tdf4f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2958'
Jul 20 18:54:33.842: INFO: stderr: ""
Jul 20 18:54:33.842: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 20 18:54:33.842: INFO: validating pod update-demo-nautilus-tdf4f
Jul 20 18:54:33.869: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 20 18:54:33.869: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 20 18:54:33.869: INFO: update-demo-nautilus-tdf4f is verified up and running
STEP: scaling down the replication controller
Jul 20 18:54:33.870: INFO: scanned /root for discovery docs: <nil>
Jul 20 18:54:33.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-2958'
Jul 20 18:54:35.035: INFO: stderr: ""
Jul 20 18:54:35.035: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 20 18:54:35.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2958'
Jul 20 18:54:35.172: INFO: stderr: ""
Jul 20 18:54:35.172: INFO: stdout: "update-demo-nautilus-jjt4c update-demo-nautilus-tdf4f "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 20 18:54:40.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2958'
Jul 20 18:54:40.248: INFO: stderr: ""
Jul 20 18:54:40.248: INFO: stdout: "update-demo-nautilus-jjt4c update-demo-nautilus-tdf4f "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 20 18:54:45.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2958'
Jul 20 18:54:45.328: INFO: stderr: ""
Jul 20 18:54:45.328: INFO: stdout: "update-demo-nautilus-tdf4f "
Jul 20 18:54:45.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-nautilus-tdf4f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2958'
Jul 20 18:54:45.419: INFO: stderr: ""
Jul 20 18:54:45.419: INFO: stdout: "true"
Jul 20 18:54:45.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-nautilus-tdf4f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2958'
Jul 20 18:54:45.500: INFO: stderr: ""
Jul 20 18:54:45.500: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 20 18:54:45.500: INFO: validating pod update-demo-nautilus-tdf4f
Jul 20 18:54:45.516: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 20 18:54:45.516: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 20 18:54:45.516: INFO: update-demo-nautilus-tdf4f is verified up and running
STEP: scaling up the replication controller
Jul 20 18:54:45.517: INFO: scanned /root for discovery docs: <nil>
Jul 20 18:54:45.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-2958'
Jul 20 18:54:46.636: INFO: stderr: ""
Jul 20 18:54:46.636: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 20 18:54:46.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2958'
Jul 20 18:54:46.718: INFO: stderr: ""
Jul 20 18:54:46.718: INFO: stdout: "update-demo-nautilus-7tp7r update-demo-nautilus-tdf4f "
Jul 20 18:54:46.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-nautilus-7tp7r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2958'
Jul 20 18:54:46.808: INFO: stderr: ""
Jul 20 18:54:46.808: INFO: stdout: ""
Jul 20 18:54:46.808: INFO: update-demo-nautilus-7tp7r is created but not running
Jul 20 18:54:51.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2958'
Jul 20 18:54:51.898: INFO: stderr: ""
Jul 20 18:54:51.898: INFO: stdout: "update-demo-nautilus-7tp7r update-demo-nautilus-tdf4f "
Jul 20 18:54:51.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-nautilus-7tp7r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2958'
Jul 20 18:54:51.986: INFO: stderr: ""
Jul 20 18:54:51.986: INFO: stdout: "true"
Jul 20 18:54:51.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-nautilus-7tp7r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2958'
Jul 20 18:54:52.072: INFO: stderr: ""
Jul 20 18:54:52.072: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 20 18:54:52.072: INFO: validating pod update-demo-nautilus-7tp7r
Jul 20 18:54:52.105: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 20 18:54:52.105: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 20 18:54:52.105: INFO: update-demo-nautilus-7tp7r is verified up and running
Jul 20 18:54:52.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-nautilus-tdf4f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2958'
Jul 20 18:54:52.196: INFO: stderr: ""
Jul 20 18:54:52.196: INFO: stdout: "true"
Jul 20 18:54:52.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods update-demo-nautilus-tdf4f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2958'
Jul 20 18:54:52.272: INFO: stderr: ""
Jul 20 18:54:52.272: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 20 18:54:52.272: INFO: validating pod update-demo-nautilus-tdf4f
Jul 20 18:54:52.291: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 20 18:54:52.291: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 20 18:54:52.291: INFO: update-demo-nautilus-tdf4f is verified up and running
STEP: using delete to clean up resources
Jul 20 18:54:52.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 delete --grace-period=0 --force -f - --namespace=kubectl-2958'
Jul 20 18:54:52.392: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 20 18:54:52.392: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 20 18:54:52.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2958'
Jul 20 18:54:52.507: INFO: stderr: "No resources found in kubectl-2958 namespace.\n"
Jul 20 18:54:52.507: INFO: stdout: ""
Jul 20 18:54:52.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods -l name=update-demo --namespace=kubectl-2958 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 20 18:54:52.591: INFO: stderr: ""
Jul 20 18:54:52.591: INFO: stdout: "update-demo-nautilus-7tp7r\nupdate-demo-nautilus-tdf4f\n"
Jul 20 18:54:53.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2958'
Jul 20 18:54:53.197: INFO: stderr: "No resources found in kubectl-2958 namespace.\n"
Jul 20 18:54:53.197: INFO: stdout: ""
Jul 20 18:54:53.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods -l name=update-demo --namespace=kubectl-2958 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 20 18:54:53.276: INFO: stderr: ""
Jul 20 18:54:53.276: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:54:53.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2958" for this suite.
Jul 20 18:55:23.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:55:23.838: INFO: namespace kubectl-2958 deletion completed in 30.543977038s

â€¢ [SLOW TEST:56.378 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:55:23.839: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7750
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul 20 18:55:24.174: INFO: Pod name pod-release: Found 0 pods out of 1
Jul 20 18:55:29.185: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:55:29.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7750" for this suite.
Jul 20 18:55:35.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:55:35.787: INFO: namespace replication-controller-7750 deletion completed in 6.551650587s

â€¢ [SLOW TEST:11.948 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:55:35.790: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-2582
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2582
I0720 18:55:36.022927      25 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2582, replica count: 1
I0720 18:55:37.073395      25 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0720 18:55:38.073665      25 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0720 18:55:39.073923      25 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 20 18:55:39.230: INFO: Created: latency-svc-zbc2r
Jul 20 18:55:39.230: INFO: Got endpoints: latency-svc-zbc2r [56.444915ms]
Jul 20 18:55:39.272: INFO: Created: latency-svc-s489r
Jul 20 18:55:39.273: INFO: Got endpoints: latency-svc-s489r [42.759719ms]
Jul 20 18:55:39.283: INFO: Created: latency-svc-b26wg
Jul 20 18:55:39.286: INFO: Got endpoints: latency-svc-b26wg [55.108693ms]
Jul 20 18:55:39.312: INFO: Created: latency-svc-d9ncf
Jul 20 18:55:39.312: INFO: Got endpoints: latency-svc-d9ncf [81.488265ms]
Jul 20 18:55:39.312: INFO: Created: latency-svc-p6m6v
Jul 20 18:55:39.312: INFO: Got endpoints: latency-svc-p6m6v [81.682335ms]
Jul 20 18:55:39.320: INFO: Created: latency-svc-gv8rb
Jul 20 18:55:39.333: INFO: Got endpoints: latency-svc-gv8rb [102.35926ms]
Jul 20 18:55:39.334: INFO: Created: latency-svc-dmhkg
Jul 20 18:55:39.336: INFO: Got endpoints: latency-svc-dmhkg [105.257978ms]
Jul 20 18:55:39.348: INFO: Created: latency-svc-l78p2
Jul 20 18:55:39.348: INFO: Got endpoints: latency-svc-l78p2 [117.237863ms]
Jul 20 18:55:39.372: INFO: Created: latency-svc-sqqhj
Jul 20 18:55:39.372: INFO: Got endpoints: latency-svc-sqqhj [141.135586ms]
Jul 20 18:55:39.372: INFO: Created: latency-svc-mzzw2
Jul 20 18:55:39.372: INFO: Got endpoints: latency-svc-mzzw2 [141.035817ms]
Jul 20 18:55:39.394: INFO: Created: latency-svc-7f7jx
Jul 20 18:55:39.394: INFO: Got endpoints: latency-svc-7f7jx [162.839964ms]
Jul 20 18:55:39.440: INFO: Created: latency-svc-5tpgs
Jul 20 18:55:39.440: INFO: Got endpoints: latency-svc-5tpgs [209.390849ms]
Jul 20 18:55:39.450: INFO: Created: latency-svc-nd5ts
Jul 20 18:55:39.450: INFO: Got endpoints: latency-svc-nd5ts [219.582646ms]
Jul 20 18:55:39.450: INFO: Created: latency-svc-gs8kb
Jul 20 18:55:39.450: INFO: Got endpoints: latency-svc-gs8kb [219.601268ms]
Jul 20 18:55:39.464: INFO: Created: latency-svc-lnk7h
Jul 20 18:55:39.464: INFO: Got endpoints: latency-svc-lnk7h [233.901449ms]
Jul 20 18:55:39.482: INFO: Created: latency-svc-pcxxp
Jul 20 18:55:39.482: INFO: Got endpoints: latency-svc-pcxxp [250.698995ms]
Jul 20 18:55:39.491: INFO: Created: latency-svc-h2hsz
Jul 20 18:55:39.491: INFO: Got endpoints: latency-svc-h2hsz [218.02335ms]
Jul 20 18:55:39.506: INFO: Created: latency-svc-w4smv
Jul 20 18:55:39.506: INFO: Got endpoints: latency-svc-w4smv [220.076463ms]
Jul 20 18:55:39.519: INFO: Created: latency-svc-xvd4b
Jul 20 18:55:39.519: INFO: Got endpoints: latency-svc-xvd4b [206.643688ms]
Jul 20 18:55:39.519: INFO: Created: latency-svc-bx5w4
Jul 20 18:55:39.519: INFO: Got endpoints: latency-svc-bx5w4 [206.53221ms]
Jul 20 18:55:39.528: INFO: Created: latency-svc-g8sxx
Jul 20 18:55:39.528: INFO: Got endpoints: latency-svc-g8sxx [195.013006ms]
Jul 20 18:55:39.540: INFO: Created: latency-svc-kdnh2
Jul 20 18:55:39.540: INFO: Created: latency-svc-kg9qt
Jul 20 18:55:39.540: INFO: Got endpoints: latency-svc-kdnh2 [192.216217ms]
Jul 20 18:55:39.540: INFO: Got endpoints: latency-svc-kg9qt [204.111531ms]
Jul 20 18:55:39.544: INFO: Created: latency-svc-znsd4
Jul 20 18:55:39.544: INFO: Created: latency-svc-tdpdt
Jul 20 18:55:39.544: INFO: Got endpoints: latency-svc-tdpdt [172.514802ms]
Jul 20 18:55:39.551: INFO: Got endpoints: latency-svc-znsd4 [179.210976ms]
Jul 20 18:55:39.569: INFO: Created: latency-svc-9pxwn
Jul 20 18:55:39.569: INFO: Got endpoints: latency-svc-9pxwn [175.633196ms]
Jul 20 18:55:39.580: INFO: Created: latency-svc-mqb7x
Jul 20 18:55:39.580: INFO: Got endpoints: latency-svc-mqb7x [140.167147ms]
Jul 20 18:55:39.589: INFO: Created: latency-svc-8vrcw
Jul 20 18:55:39.589: INFO: Got endpoints: latency-svc-8vrcw [138.940718ms]
Jul 20 18:55:39.594: INFO: Created: latency-svc-6z488
Jul 20 18:55:39.594: INFO: Got endpoints: latency-svc-6z488 [144.119023ms]
Jul 20 18:55:39.604: INFO: Created: latency-svc-qfxnl
Jul 20 18:55:39.604: INFO: Got endpoints: latency-svc-qfxnl [139.733651ms]
Jul 20 18:55:39.615: INFO: Created: latency-svc-q4sjr
Jul 20 18:55:39.616: INFO: Got endpoints: latency-svc-q4sjr [133.965479ms]
Jul 20 18:55:39.640: INFO: Created: latency-svc-xls4m
Jul 20 18:55:39.640: INFO: Got endpoints: latency-svc-xls4m [148.410059ms]
Jul 20 18:55:39.640: INFO: Created: latency-svc-zpsqn
Jul 20 18:55:39.644: INFO: Got endpoints: latency-svc-zpsqn [138.335872ms]
Jul 20 18:55:39.656: INFO: Created: latency-svc-pklfh
Jul 20 18:55:39.661: INFO: Got endpoints: latency-svc-pklfh [141.513761ms]
Jul 20 18:55:39.665: INFO: Created: latency-svc-7pzdx
Jul 20 18:55:39.674: INFO: Got endpoints: latency-svc-7pzdx [155.141381ms]
Jul 20 18:55:39.681: INFO: Created: latency-svc-pfk78
Jul 20 18:55:39.684: INFO: Got endpoints: latency-svc-pfk78 [155.56939ms]
Jul 20 18:55:39.694: INFO: Created: latency-svc-mmmrd
Jul 20 18:55:39.694: INFO: Got endpoints: latency-svc-mmmrd [154.194663ms]
Jul 20 18:55:39.700: INFO: Created: latency-svc-npjqp
Jul 20 18:55:39.714: INFO: Got endpoints: latency-svc-npjqp [173.281006ms]
Jul 20 18:55:39.732: INFO: Created: latency-svc-wtsv6
Jul 20 18:55:39.732: INFO: Got endpoints: latency-svc-wtsv6 [187.857558ms]
Jul 20 18:55:39.765: INFO: Created: latency-svc-57g5j
Jul 20 18:55:39.765: INFO: Got endpoints: latency-svc-57g5j [213.776898ms]
Jul 20 18:55:39.772: INFO: Created: latency-svc-8dn5v
Jul 20 18:55:39.772: INFO: Got endpoints: latency-svc-8dn5v [202.571928ms]
Jul 20 18:55:39.783: INFO: Created: latency-svc-4725c
Jul 20 18:55:39.783: INFO: Got endpoints: latency-svc-4725c [194.082702ms]
Jul 20 18:55:39.783: INFO: Created: latency-svc-ttfzl
Jul 20 18:55:39.783: INFO: Got endpoints: latency-svc-ttfzl [203.006986ms]
Jul 20 18:55:39.798: INFO: Created: latency-svc-q4drz
Jul 20 18:55:39.798: INFO: Got endpoints: latency-svc-q4drz [203.201366ms]
Jul 20 18:55:39.812: INFO: Created: latency-svc-hmj8l
Jul 20 18:55:39.812: INFO: Got endpoints: latency-svc-hmj8l [208.014287ms]
Jul 20 18:55:39.821: INFO: Created: latency-svc-4vc8g
Jul 20 18:55:39.821: INFO: Got endpoints: latency-svc-4vc8g [205.876067ms]
Jul 20 18:55:39.840: INFO: Created: latency-svc-gnxsd
Jul 20 18:55:39.840: INFO: Got endpoints: latency-svc-gnxsd [200.268631ms]
Jul 20 18:55:39.854: INFO: Created: latency-svc-24hnp
Jul 20 18:55:39.854: INFO: Got endpoints: latency-svc-24hnp [209.09548ms]
Jul 20 18:55:39.854: INFO: Created: latency-svc-rgmct
Jul 20 18:55:39.854: INFO: Got endpoints: latency-svc-rgmct [193.520243ms]
Jul 20 18:55:39.887: INFO: Created: latency-svc-6gwg6
Jul 20 18:55:39.887: INFO: Got endpoints: latency-svc-6gwg6 [213.092176ms]
Jul 20 18:55:39.887: INFO: Created: latency-svc-vqj62
Jul 20 18:55:39.887: INFO: Got endpoints: latency-svc-vqj62 [203.487537ms]
Jul 20 18:55:39.888: INFO: Created: latency-svc-95wfg
Jul 20 18:55:39.897: INFO: Got endpoints: latency-svc-95wfg [202.845814ms]
Jul 20 18:55:39.902: INFO: Created: latency-svc-zk862
Jul 20 18:55:39.907: INFO: Got endpoints: latency-svc-zk862 [193.25887ms]
Jul 20 18:55:39.916: INFO: Created: latency-svc-pwc5b
Jul 20 18:55:39.923: INFO: Got endpoints: latency-svc-pwc5b [190.378543ms]
Jul 20 18:55:39.925: INFO: Created: latency-svc-mzpmm
Jul 20 18:55:39.931: INFO: Got endpoints: latency-svc-mzpmm [166.412085ms]
Jul 20 18:55:39.945: INFO: Created: latency-svc-99xb4
Jul 20 18:55:39.954: INFO: Created: latency-svc-p8lrq
Jul 20 18:55:39.954: INFO: Got endpoints: latency-svc-p8lrq [170.533659ms]
Jul 20 18:55:39.954: INFO: Got endpoints: latency-svc-99xb4 [182.260972ms]
Jul 20 18:55:39.983: INFO: Created: latency-svc-rj4v2
Jul 20 18:55:39.983: INFO: Got endpoints: latency-svc-rj4v2 [199.532167ms]
Jul 20 18:55:39.992: INFO: Created: latency-svc-s4qp7
Jul 20 18:55:39.995: INFO: Got endpoints: latency-svc-s4qp7 [197.531096ms]
Jul 20 18:55:40.016: INFO: Created: latency-svc-dhpx7
Jul 20 18:55:40.021: INFO: Got endpoints: latency-svc-dhpx7 [208.960873ms]
Jul 20 18:55:40.029: INFO: Created: latency-svc-ls8wd
Jul 20 18:55:40.031: INFO: Got endpoints: latency-svc-ls8wd [209.69429ms]
Jul 20 18:55:40.039: INFO: Created: latency-svc-l45jt
Jul 20 18:55:40.049: INFO: Got endpoints: latency-svc-l45jt [208.991092ms]
Jul 20 18:55:40.059: INFO: Created: latency-svc-t9q42
Jul 20 18:55:40.061: INFO: Got endpoints: latency-svc-t9q42 [207.412438ms]
Jul 20 18:55:40.074: INFO: Created: latency-svc-p4chs
Jul 20 18:55:40.075: INFO: Got endpoints: latency-svc-p4chs [221.393659ms]
Jul 20 18:55:40.087: INFO: Created: latency-svc-4x6vh
Jul 20 18:55:40.087: INFO: Got endpoints: latency-svc-4x6vh [199.388835ms]
Jul 20 18:55:40.105: INFO: Created: latency-svc-4vflg
Jul 20 18:55:40.105: INFO: Got endpoints: latency-svc-4vflg [217.310263ms]
Jul 20 18:55:40.112: INFO: Created: latency-svc-kb97l
Jul 20 18:55:40.112: INFO: Got endpoints: latency-svc-kb97l [214.850395ms]
Jul 20 18:55:40.127: INFO: Created: latency-svc-lc44z
Jul 20 18:55:40.127: INFO: Got endpoints: latency-svc-lc44z [220.559488ms]
Jul 20 18:55:40.144: INFO: Created: latency-svc-69hq8
Jul 20 18:55:40.150: INFO: Got endpoints: latency-svc-69hq8 [227.191972ms]
Jul 20 18:55:40.157: INFO: Created: latency-svc-bhz66
Jul 20 18:55:40.157: INFO: Got endpoints: latency-svc-bhz66 [225.564222ms]
Jul 20 18:55:40.163: INFO: Created: latency-svc-6cwm7
Jul 20 18:55:40.171: INFO: Got endpoints: latency-svc-6cwm7 [217.112109ms]
Jul 20 18:55:40.173: INFO: Created: latency-svc-lwvqg
Jul 20 18:55:40.184: INFO: Got endpoints: latency-svc-lwvqg [230.073485ms]
Jul 20 18:55:40.188: INFO: Created: latency-svc-tk4vc
Jul 20 18:55:40.193: INFO: Got endpoints: latency-svc-tk4vc [210.070736ms]
Jul 20 18:55:40.200: INFO: Created: latency-svc-f62p8
Jul 20 18:55:40.206: INFO: Got endpoints: latency-svc-f62p8 [210.501922ms]
Jul 20 18:55:40.213: INFO: Created: latency-svc-8q4gz
Jul 20 18:55:40.230: INFO: Created: latency-svc-2dskg
Jul 20 18:55:40.230: INFO: Got endpoints: latency-svc-2dskg [199.281587ms]
Jul 20 18:55:40.230: INFO: Got endpoints: latency-svc-8q4gz [209.137083ms]
Jul 20 18:55:40.247: INFO: Created: latency-svc-hqq88
Jul 20 18:55:40.253: INFO: Got endpoints: latency-svc-hqq88 [203.337587ms]
Jul 20 18:55:40.259: INFO: Created: latency-svc-pzdhq
Jul 20 18:55:40.264: INFO: Got endpoints: latency-svc-pzdhq [202.720749ms]
Jul 20 18:55:40.271: INFO: Created: latency-svc-gdkdb
Jul 20 18:55:40.276: INFO: Got endpoints: latency-svc-gdkdb [200.394826ms]
Jul 20 18:55:40.285: INFO: Created: latency-svc-dcqsr
Jul 20 18:55:40.294: INFO: Got endpoints: latency-svc-dcqsr [207.267271ms]
Jul 20 18:55:40.296: INFO: Created: latency-svc-jflvl
Jul 20 18:55:40.303: INFO: Got endpoints: latency-svc-jflvl [198.014963ms]
Jul 20 18:55:40.308: INFO: Created: latency-svc-ftxdb
Jul 20 18:55:40.320: INFO: Got endpoints: latency-svc-ftxdb [207.935976ms]
Jul 20 18:55:40.327: INFO: Created: latency-svc-npzjr
Jul 20 18:55:40.334: INFO: Got endpoints: latency-svc-npzjr [206.627839ms]
Jul 20 18:55:40.341: INFO: Created: latency-svc-mpd2f
Jul 20 18:55:40.348: INFO: Got endpoints: latency-svc-mpd2f [198.299276ms]
Jul 20 18:55:40.353: INFO: Created: latency-svc-k2bjz
Jul 20 18:55:40.356: INFO: Got endpoints: latency-svc-k2bjz [199.072463ms]
Jul 20 18:55:40.380: INFO: Created: latency-svc-j2n4q
Jul 20 18:55:40.380: INFO: Got endpoints: latency-svc-j2n4q [208.589041ms]
Jul 20 18:55:40.383: INFO: Created: latency-svc-fl95v
Jul 20 18:55:40.391: INFO: Got endpoints: latency-svc-fl95v [206.630831ms]
Jul 20 18:55:40.407: INFO: Created: latency-svc-7sxmt
Jul 20 18:55:40.407: INFO: Got endpoints: latency-svc-7sxmt [213.996756ms]
Jul 20 18:55:40.425: INFO: Created: latency-svc-p6j8t
Jul 20 18:55:40.425: INFO: Got endpoints: latency-svc-p6j8t [219.04872ms]
Jul 20 18:55:40.427: INFO: Created: latency-svc-6ggkp
Jul 20 18:55:40.437: INFO: Got endpoints: latency-svc-6ggkp [206.647854ms]
Jul 20 18:55:40.447: INFO: Created: latency-svc-mb6mq
Jul 20 18:55:40.452: INFO: Created: latency-svc-q888w
Jul 20 18:55:40.456: INFO: Got endpoints: latency-svc-mb6mq [225.235822ms]
Jul 20 18:55:40.460: INFO: Got endpoints: latency-svc-q888w [207.074915ms]
Jul 20 18:55:40.467: INFO: Created: latency-svc-hz6cc
Jul 20 18:55:40.471: INFO: Got endpoints: latency-svc-hz6cc [207.002877ms]
Jul 20 18:55:40.478: INFO: Created: latency-svc-kl6r7
Jul 20 18:55:40.481: INFO: Got endpoints: latency-svc-kl6r7 [204.779796ms]
Jul 20 18:55:40.485: INFO: Created: latency-svc-744bv
Jul 20 18:55:40.491: INFO: Got endpoints: latency-svc-744bv [196.688449ms]
Jul 20 18:55:40.508: INFO: Created: latency-svc-2ck7x
Jul 20 18:55:40.508: INFO: Got endpoints: latency-svc-2ck7x [205.759766ms]
Jul 20 18:55:40.514: INFO: Created: latency-svc-r2rg4
Jul 20 18:55:40.523: INFO: Got endpoints: latency-svc-r2rg4 [202.352227ms]
Jul 20 18:55:40.529: INFO: Created: latency-svc-xljlh
Jul 20 18:55:40.534: INFO: Got endpoints: latency-svc-xljlh [199.866661ms]
Jul 20 18:55:40.543: INFO: Created: latency-svc-wqj8s
Jul 20 18:55:40.550: INFO: Got endpoints: latency-svc-wqj8s [201.443338ms]
Jul 20 18:55:40.557: INFO: Created: latency-svc-8kgzg
Jul 20 18:55:40.560: INFO: Got endpoints: latency-svc-8kgzg [203.284482ms]
Jul 20 18:55:40.584: INFO: Created: latency-svc-txltd
Jul 20 18:55:40.585: INFO: Got endpoints: latency-svc-txltd [204.901249ms]
Jul 20 18:55:40.585: INFO: Created: latency-svc-spqfn
Jul 20 18:55:40.585: INFO: Got endpoints: latency-svc-spqfn [194.069811ms]
Jul 20 18:55:40.595: INFO: Created: latency-svc-c867k
Jul 20 18:55:40.595: INFO: Got endpoints: latency-svc-c867k [188.324992ms]
Jul 20 18:55:40.603: INFO: Created: latency-svc-n7g4j
Jul 20 18:55:40.610: INFO: Got endpoints: latency-svc-n7g4j [185.22673ms]
Jul 20 18:55:40.621: INFO: Created: latency-svc-9kkss
Jul 20 18:55:40.621: INFO: Got endpoints: latency-svc-9kkss [183.553263ms]
Jul 20 18:55:40.635: INFO: Created: latency-svc-72s9n
Jul 20 18:55:40.635: INFO: Got endpoints: latency-svc-72s9n [179.691369ms]
Jul 20 18:55:40.649: INFO: Created: latency-svc-5fszl
Jul 20 18:55:40.649: INFO: Got endpoints: latency-svc-5fszl [188.966744ms]
Jul 20 18:55:40.653: INFO: Created: latency-svc-nnwfs
Jul 20 18:55:40.655: INFO: Got endpoints: latency-svc-nnwfs [183.474115ms]
Jul 20 18:55:40.671: INFO: Created: latency-svc-8bnhh
Jul 20 18:55:40.672: INFO: Got endpoints: latency-svc-8bnhh [191.646009ms]
Jul 20 18:55:40.681: INFO: Created: latency-svc-gbgcv
Jul 20 18:55:40.682: INFO: Got endpoints: latency-svc-gbgcv [190.493364ms]
Jul 20 18:55:40.731: INFO: Created: latency-svc-2sbh4
Jul 20 18:55:40.732: INFO: Got endpoints: latency-svc-2sbh4 [76.655759ms]
Jul 20 18:55:40.747: INFO: Created: latency-svc-tjltz
Jul 20 18:55:40.747: INFO: Got endpoints: latency-svc-tjltz [224.584632ms]
Jul 20 18:55:40.747: INFO: Created: latency-svc-r4jjf
Jul 20 18:55:40.747: INFO: Got endpoints: latency-svc-r4jjf [238.825308ms]
Jul 20 18:55:40.752: INFO: Created: latency-svc-v9bpq
Jul 20 18:55:40.752: INFO: Got endpoints: latency-svc-v9bpq [217.646744ms]
Jul 20 18:55:40.769: INFO: Created: latency-svc-6sdms
Jul 20 18:55:40.769: INFO: Got endpoints: latency-svc-6sdms [219.620014ms]
Jul 20 18:55:40.771: INFO: Created: latency-svc-94r75
Jul 20 18:55:40.771: INFO: Got endpoints: latency-svc-94r75 [211.266026ms]
Jul 20 18:55:40.783: INFO: Created: latency-svc-dqkhl
Jul 20 18:55:40.783: INFO: Got endpoints: latency-svc-dqkhl [197.877391ms]
Jul 20 18:55:40.783: INFO: Created: latency-svc-9fsfz
Jul 20 18:55:40.785: INFO: Got endpoints: latency-svc-9fsfz [200.098995ms]
Jul 20 18:55:40.789: INFO: Created: latency-svc-db57v
Jul 20 18:55:40.794: INFO: Got endpoints: latency-svc-db57v [198.972539ms]
Jul 20 18:55:40.809: INFO: Created: latency-svc-j4hxs
Jul 20 18:55:40.809: INFO: Got endpoints: latency-svc-j4hxs [198.968911ms]
Jul 20 18:55:40.825: INFO: Created: latency-svc-9nxqj
Jul 20 18:55:40.825: INFO: Got endpoints: latency-svc-9nxqj [204.385076ms]
Jul 20 18:55:40.842: INFO: Created: latency-svc-vg8rf
Jul 20 18:55:40.842: INFO: Got endpoints: latency-svc-vg8rf [206.800865ms]
Jul 20 18:55:40.851: INFO: Created: latency-svc-9wwhw
Jul 20 18:55:40.851: INFO: Got endpoints: latency-svc-9wwhw [202.470449ms]
Jul 20 18:55:40.862: INFO: Created: latency-svc-ggpqp
Jul 20 18:55:40.862: INFO: Got endpoints: latency-svc-ggpqp [189.187876ms]
Jul 20 18:55:40.863: INFO: Created: latency-svc-4vvgd
Jul 20 18:55:40.864: INFO: Got endpoints: latency-svc-4vvgd [182.366574ms]
Jul 20 18:55:40.885: INFO: Created: latency-svc-pxdlg
Jul 20 18:55:40.885: INFO: Got endpoints: latency-svc-pxdlg [153.24837ms]
Jul 20 18:55:40.903: INFO: Created: latency-svc-g78dd
Jul 20 18:55:40.903: INFO: Got endpoints: latency-svc-g78dd [155.384323ms]
Jul 20 18:55:40.915: INFO: Created: latency-svc-r42q6
Jul 20 18:55:40.915: INFO: Got endpoints: latency-svc-r42q6 [162.896865ms]
Jul 20 18:55:40.915: INFO: Created: latency-svc-8np6k
Jul 20 18:55:40.915: INFO: Got endpoints: latency-svc-8np6k [167.618318ms]
Jul 20 18:55:40.923: INFO: Created: latency-svc-l97rf
Jul 20 18:55:40.923: INFO: Got endpoints: latency-svc-l97rf [153.073399ms]
Jul 20 18:55:40.931: INFO: Created: latency-svc-d4mjl
Jul 20 18:55:40.932: INFO: Got endpoints: latency-svc-d4mjl [160.959154ms]
Jul 20 18:55:40.949: INFO: Created: latency-svc-8q6nn
Jul 20 18:55:40.949: INFO: Got endpoints: latency-svc-8q6nn [166.011901ms]
Jul 20 18:55:40.965: INFO: Created: latency-svc-9f4nn
Jul 20 18:55:40.965: INFO: Got endpoints: latency-svc-9f4nn [180.349783ms]
Jul 20 18:55:40.981: INFO: Created: latency-svc-z9fzx
Jul 20 18:55:40.987: INFO: Got endpoints: latency-svc-z9fzx [192.365495ms]
Jul 20 18:55:40.989: INFO: Created: latency-svc-52t56
Jul 20 18:55:40.995: INFO: Got endpoints: latency-svc-52t56 [186.214464ms]
Jul 20 18:55:41.006: INFO: Created: latency-svc-7bw4t
Jul 20 18:55:41.011: INFO: Got endpoints: latency-svc-7bw4t [186.103853ms]
Jul 20 18:55:41.023: INFO: Created: latency-svc-2tvdh
Jul 20 18:55:41.026: INFO: Got endpoints: latency-svc-2tvdh [184.139612ms]
Jul 20 18:55:41.034: INFO: Created: latency-svc-zrszd
Jul 20 18:55:41.034: INFO: Got endpoints: latency-svc-zrszd [182.654739ms]
Jul 20 18:55:41.044: INFO: Created: latency-svc-m97rf
Jul 20 18:55:41.045: INFO: Got endpoints: latency-svc-m97rf [182.994982ms]
Jul 20 18:55:41.060: INFO: Created: latency-svc-t962x
Jul 20 18:55:41.064: INFO: Got endpoints: latency-svc-t962x [199.708949ms]
Jul 20 18:55:41.079: INFO: Created: latency-svc-pfg8v
Jul 20 18:55:41.079: INFO: Got endpoints: latency-svc-pfg8v [193.8478ms]
Jul 20 18:55:41.088: INFO: Created: latency-svc-7m54k
Jul 20 18:55:41.089: INFO: Got endpoints: latency-svc-7m54k [185.820331ms]
Jul 20 18:55:41.104: INFO: Created: latency-svc-f4fkv
Jul 20 18:55:41.104: INFO: Got endpoints: latency-svc-f4fkv [189.031857ms]
Jul 20 18:55:41.116: INFO: Created: latency-svc-ns6bk
Jul 20 18:55:41.116: INFO: Got endpoints: latency-svc-ns6bk [201.737355ms]
Jul 20 18:55:41.139: INFO: Created: latency-svc-hd8dk
Jul 20 18:55:41.139: INFO: Got endpoints: latency-svc-hd8dk [207.548263ms]
Jul 20 18:55:41.139: INFO: Created: latency-svc-bjqdw
Jul 20 18:55:41.139: INFO: Got endpoints: latency-svc-bjqdw [216.79518ms]
Jul 20 18:55:41.157: INFO: Created: latency-svc-5kfqk
Jul 20 18:55:41.157: INFO: Got endpoints: latency-svc-5kfqk [207.667661ms]
Jul 20 18:55:41.165: INFO: Created: latency-svc-k4xtx
Jul 20 18:55:41.165: INFO: Got endpoints: latency-svc-k4xtx [199.770184ms]
Jul 20 18:55:41.177: INFO: Created: latency-svc-gbq85
Jul 20 18:55:41.177: INFO: Got endpoints: latency-svc-gbq85 [190.392093ms]
Jul 20 18:55:41.192: INFO: Created: latency-svc-wc5rf
Jul 20 18:55:41.192: INFO: Got endpoints: latency-svc-wc5rf [196.722026ms]
Jul 20 18:55:41.200: INFO: Created: latency-svc-tqb6c
Jul 20 18:55:41.200: INFO: Got endpoints: latency-svc-tqb6c [188.447792ms]
Jul 20 18:55:41.222: INFO: Created: latency-svc-qchrd
Jul 20 18:55:41.222: INFO: Got endpoints: latency-svc-qchrd [195.911755ms]
Jul 20 18:55:41.229: INFO: Created: latency-svc-l69cl
Jul 20 18:55:41.230: INFO: Got endpoints: latency-svc-l69cl [195.540128ms]
Jul 20 18:55:41.243: INFO: Created: latency-svc-z58v7
Jul 20 18:55:41.243: INFO: Got endpoints: latency-svc-z58v7 [198.546364ms]
Jul 20 18:55:41.256: INFO: Created: latency-svc-x8wcl
Jul 20 18:55:41.256: INFO: Got endpoints: latency-svc-x8wcl [191.750181ms]
Jul 20 18:55:41.273: INFO: Created: latency-svc-hgsqf
Jul 20 18:55:41.274: INFO: Got endpoints: latency-svc-hgsqf [194.620772ms]
Jul 20 18:55:41.289: INFO: Created: latency-svc-lbr89
Jul 20 18:55:41.289: INFO: Got endpoints: latency-svc-lbr89 [200.584325ms]
Jul 20 18:55:41.297: INFO: Created: latency-svc-ktst8
Jul 20 18:55:41.297: INFO: Got endpoints: latency-svc-ktst8 [192.648081ms]
Jul 20 18:55:41.323: INFO: Created: latency-svc-k2lkq
Jul 20 18:55:41.323: INFO: Got endpoints: latency-svc-k2lkq [206.960189ms]
Jul 20 18:55:41.332: INFO: Created: latency-svc-xxpkz
Jul 20 18:55:41.334: INFO: Got endpoints: latency-svc-xxpkz [195.020216ms]
Jul 20 18:55:41.355: INFO: Created: latency-svc-2d5bz
Jul 20 18:55:41.355: INFO: Got endpoints: latency-svc-2d5bz [215.716609ms]
Jul 20 18:55:41.369: INFO: Created: latency-svc-5wxkq
Jul 20 18:55:41.369: INFO: Got endpoints: latency-svc-5wxkq [211.434644ms]
Jul 20 18:55:41.385: INFO: Created: latency-svc-xcbhr
Jul 20 18:55:41.385: INFO: Got endpoints: latency-svc-xcbhr [219.936552ms]
Jul 20 18:55:41.406: INFO: Created: latency-svc-7d6sr
Jul 20 18:55:41.406: INFO: Got endpoints: latency-svc-7d6sr [229.20454ms]
Jul 20 18:55:41.414: INFO: Created: latency-svc-q8zdq
Jul 20 18:55:41.414: INFO: Got endpoints: latency-svc-q8zdq [221.609375ms]
Jul 20 18:55:41.432: INFO: Created: latency-svc-krcjk
Jul 20 18:55:41.432: INFO: Got endpoints: latency-svc-krcjk [231.921778ms]
Jul 20 18:55:41.437: INFO: Created: latency-svc-l8ks7
Jul 20 18:55:41.437: INFO: Got endpoints: latency-svc-l8ks7 [214.844819ms]
Jul 20 18:55:41.454: INFO: Created: latency-svc-qg7st
Jul 20 18:55:41.454: INFO: Got endpoints: latency-svc-qg7st [224.454ms]
Jul 20 18:55:41.485: INFO: Created: latency-svc-q27rf
Jul 20 18:55:41.485: INFO: Got endpoints: latency-svc-q27rf [229.059418ms]
Jul 20 18:55:41.485: INFO: Created: latency-svc-s2qkx
Jul 20 18:55:41.485: INFO: Got endpoints: latency-svc-s2qkx [242.109517ms]
Jul 20 18:55:41.491: INFO: Created: latency-svc-26d4n
Jul 20 18:55:41.491: INFO: Got endpoints: latency-svc-26d4n [217.71101ms]
Jul 20 18:55:41.503: INFO: Created: latency-svc-nhzsp
Jul 20 18:55:41.511: INFO: Got endpoints: latency-svc-nhzsp [222.1593ms]
Jul 20 18:55:41.523: INFO: Created: latency-svc-ndswh
Jul 20 18:55:41.523: INFO: Got endpoints: latency-svc-ndswh [225.870941ms]
Jul 20 18:55:41.532: INFO: Created: latency-svc-8vgpx
Jul 20 18:55:41.537: INFO: Got endpoints: latency-svc-8vgpx [213.839163ms]
Jul 20 18:55:41.548: INFO: Created: latency-svc-bwpvq
Jul 20 18:55:41.551: INFO: Got endpoints: latency-svc-bwpvq [216.310165ms]
Jul 20 18:55:41.565: INFO: Created: latency-svc-5mb8v
Jul 20 18:55:41.577: INFO: Got endpoints: latency-svc-5mb8v [221.707695ms]
Jul 20 18:55:41.595: INFO: Created: latency-svc-bk7br
Jul 20 18:55:41.595: INFO: Got endpoints: latency-svc-bk7br [226.484258ms]
Jul 20 18:55:41.607: INFO: Created: latency-svc-c9d2v
Jul 20 18:55:41.607: INFO: Got endpoints: latency-svc-c9d2v [221.634535ms]
Jul 20 18:55:41.613: INFO: Created: latency-svc-dsnkt
Jul 20 18:55:41.616: INFO: Got endpoints: latency-svc-dsnkt [209.318777ms]
Jul 20 18:55:41.623: INFO: Created: latency-svc-htk7s
Jul 20 18:55:41.638: INFO: Got endpoints: latency-svc-htk7s [223.946586ms]
Jul 20 18:55:41.649: INFO: Created: latency-svc-6v9tn
Jul 20 18:55:41.653: INFO: Got endpoints: latency-svc-6v9tn [221.233853ms]
Jul 20 18:55:41.664: INFO: Created: latency-svc-4lck9
Jul 20 18:55:41.664: INFO: Got endpoints: latency-svc-4lck9 [226.671499ms]
Jul 20 18:55:41.675: INFO: Created: latency-svc-wrvkv
Jul 20 18:55:41.687: INFO: Got endpoints: latency-svc-wrvkv [232.657236ms]
Jul 20 18:55:41.697: INFO: Created: latency-svc-87l47
Jul 20 18:55:41.701: INFO: Got endpoints: latency-svc-87l47 [216.526018ms]
Jul 20 18:55:41.709: INFO: Created: latency-svc-mcp8v
Jul 20 18:55:41.709: INFO: Got endpoints: latency-svc-mcp8v [223.396632ms]
Jul 20 18:55:41.731: INFO: Created: latency-svc-szrl6
Jul 20 18:55:41.734: INFO: Got endpoints: latency-svc-szrl6 [243.001942ms]
Jul 20 18:55:41.751: INFO: Created: latency-svc-lkdpz
Jul 20 18:55:41.751: INFO: Got endpoints: latency-svc-lkdpz [239.850888ms]
Jul 20 18:55:41.766: INFO: Created: latency-svc-c2rrn
Jul 20 18:55:41.766: INFO: Got endpoints: latency-svc-c2rrn [242.900536ms]
Jul 20 18:55:41.781: INFO: Created: latency-svc-p8477
Jul 20 18:55:41.781: INFO: Got endpoints: latency-svc-p8477 [243.685711ms]
Jul 20 18:55:41.791: INFO: Created: latency-svc-psgds
Jul 20 18:55:41.791: INFO: Got endpoints: latency-svc-psgds [240.270889ms]
Jul 20 18:55:41.816: INFO: Created: latency-svc-wrl2d
Jul 20 18:55:41.816: INFO: Got endpoints: latency-svc-wrl2d [239.280206ms]
Jul 20 18:55:41.827: INFO: Created: latency-svc-wp4x7
Jul 20 18:55:41.827: INFO: Got endpoints: latency-svc-wp4x7 [231.290402ms]
Jul 20 18:55:41.842: INFO: Created: latency-svc-nftq5
Jul 20 18:55:41.842: INFO: Got endpoints: latency-svc-nftq5 [235.199652ms]
Jul 20 18:55:41.851: INFO: Created: latency-svc-hxztv
Jul 20 18:55:41.854: INFO: Got endpoints: latency-svc-hxztv [237.921713ms]
Jul 20 18:55:41.868: INFO: Created: latency-svc-wxj5h
Jul 20 18:55:41.878: INFO: Got endpoints: latency-svc-wxj5h [239.730466ms]
Jul 20 18:55:41.881: INFO: Created: latency-svc-6cqzf
Jul 20 18:55:41.886: INFO: Got endpoints: latency-svc-6cqzf [232.522794ms]
Jul 20 18:55:41.905: INFO: Created: latency-svc-cncp5
Jul 20 18:55:41.905: INFO: Got endpoints: latency-svc-cncp5 [240.786018ms]
Jul 20 18:55:41.928: INFO: Created: latency-svc-zcbz9
Jul 20 18:55:41.928: INFO: Got endpoints: latency-svc-zcbz9 [240.952066ms]
Jul 20 18:55:41.941: INFO: Created: latency-svc-z5clf
Jul 20 18:55:41.941: INFO: Got endpoints: latency-svc-z5clf [239.754161ms]
Jul 20 18:55:41.944: INFO: Created: latency-svc-n6qm2
Jul 20 18:55:41.944: INFO: Got endpoints: latency-svc-n6qm2 [235.242661ms]
Jul 20 18:55:41.956: INFO: Created: latency-svc-lf7v4
Jul 20 18:55:41.956: INFO: Got endpoints: latency-svc-lf7v4 [221.701047ms]
Jul 20 18:55:41.956: INFO: Latencies: [42.759719ms 55.108693ms 76.655759ms 81.488265ms 81.682335ms 102.35926ms 105.257978ms 117.237863ms 133.965479ms 138.335872ms 138.940718ms 139.733651ms 140.167147ms 141.035817ms 141.135586ms 141.513761ms 144.119023ms 148.410059ms 153.073399ms 153.24837ms 154.194663ms 155.141381ms 155.384323ms 155.56939ms 160.959154ms 162.839964ms 162.896865ms 166.011901ms 166.412085ms 167.618318ms 170.533659ms 172.514802ms 173.281006ms 175.633196ms 179.210976ms 179.691369ms 180.349783ms 182.260972ms 182.366574ms 182.654739ms 182.994982ms 183.474115ms 183.553263ms 184.139612ms 185.22673ms 185.820331ms 186.103853ms 186.214464ms 187.857558ms 188.324992ms 188.447792ms 188.966744ms 189.031857ms 189.187876ms 190.378543ms 190.392093ms 190.493364ms 191.646009ms 191.750181ms 192.216217ms 192.365495ms 192.648081ms 193.25887ms 193.520243ms 193.8478ms 194.069811ms 194.082702ms 194.620772ms 195.013006ms 195.020216ms 195.540128ms 195.911755ms 196.688449ms 196.722026ms 197.531096ms 197.877391ms 198.014963ms 198.299276ms 198.546364ms 198.968911ms 198.972539ms 199.072463ms 199.281587ms 199.388835ms 199.532167ms 199.708949ms 199.770184ms 199.866661ms 200.098995ms 200.268631ms 200.394826ms 200.584325ms 201.443338ms 201.737355ms 202.352227ms 202.470449ms 202.571928ms 202.720749ms 202.845814ms 203.006986ms 203.201366ms 203.284482ms 203.337587ms 203.487537ms 204.111531ms 204.385076ms 204.779796ms 204.901249ms 205.759766ms 205.876067ms 206.53221ms 206.627839ms 206.630831ms 206.643688ms 206.647854ms 206.800865ms 206.960189ms 207.002877ms 207.074915ms 207.267271ms 207.412438ms 207.548263ms 207.667661ms 207.935976ms 208.014287ms 208.589041ms 208.960873ms 208.991092ms 209.09548ms 209.137083ms 209.318777ms 209.390849ms 209.69429ms 210.070736ms 210.501922ms 211.266026ms 211.434644ms 213.092176ms 213.776898ms 213.839163ms 213.996756ms 214.844819ms 214.850395ms 215.716609ms 216.310165ms 216.526018ms 216.79518ms 217.112109ms 217.310263ms 217.646744ms 217.71101ms 218.02335ms 219.04872ms 219.582646ms 219.601268ms 219.620014ms 219.936552ms 220.076463ms 220.559488ms 221.233853ms 221.393659ms 221.609375ms 221.634535ms 221.701047ms 221.707695ms 222.1593ms 223.396632ms 223.946586ms 224.454ms 224.584632ms 225.235822ms 225.564222ms 225.870941ms 226.484258ms 226.671499ms 227.191972ms 229.059418ms 229.20454ms 230.073485ms 231.290402ms 231.921778ms 232.522794ms 232.657236ms 233.901449ms 235.199652ms 235.242661ms 237.921713ms 238.825308ms 239.280206ms 239.730466ms 239.754161ms 239.850888ms 240.270889ms 240.786018ms 240.952066ms 242.109517ms 242.900536ms 243.001942ms 243.685711ms 250.698995ms]
Jul 20 18:55:41.956: INFO: 50 %ile: 203.201366ms
Jul 20 18:55:41.956: INFO: 90 %ile: 231.921778ms
Jul 20 18:55:41.956: INFO: 99 %ile: 243.685711ms
Jul 20 18:55:41.956: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:55:41.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-2582" for this suite.
Jul 20 18:56:14.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:56:14.672: INFO: namespace svc-latency-2582 deletion completed in 32.690967152s

â€¢ [SLOW TEST:38.882 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:56:14.672: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8440
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:56:14.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8440" for this suite.
Jul 20 18:56:21.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:56:21.580: INFO: namespace kubelet-test-8440 deletion completed in 6.568167895s

â€¢ [SLOW TEST:6.908 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:56:21.580: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6889
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 20 18:56:22.235: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 20 18:56:24.263: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730868182, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730868182, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730868182, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730868182, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 20 18:56:27.313: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:56:27.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6889" for this suite.
Jul 20 18:56:33.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:56:34.152: INFO: namespace webhook-6889 deletion completed in 6.5578219s
STEP: Destroying namespace "webhook-6889-markers" for this suite.
Jul 20 18:56:40.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:56:40.731: INFO: namespace webhook-6889-markers deletion completed in 6.578955539s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:19.274 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:56:40.854: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2146
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:56:45.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2146" for this suite.
Jul 20 18:56:51.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:56:51.758: INFO: namespace kubelet-test-2146 deletion completed in 6.576074907s

â€¢ [SLOW TEST:10.903 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:56:51.758: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1753
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1753
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1753
STEP: creating replication controller externalsvc in namespace services-1753
I0720 18:56:52.099367      25 runners.go:184] Created replication controller with name: externalsvc, namespace: services-1753, replica count: 2
I0720 18:56:55.150136      25 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jul 20 18:56:55.243: INFO: Creating new exec pod
Jul 20 18:56:59.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=services-1753 execpodbq9n2 -- /bin/sh -x -c nslookup clusterip-service'
Jul 20 18:56:59.511: INFO: stderr: "+ nslookup clusterip-service\n"
Jul 20 18:56:59.511: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-1753.svc.cluster.local\tcanonical name = externalsvc.services-1753.svc.cluster.local.\nName:\texternalsvc.services-1753.svc.cluster.local\nAddress: 172.21.243.156\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1753, will wait for the garbage collector to delete the pods
Jul 20 18:56:59.596: INFO: Deleting ReplicationController externalsvc took: 25.314268ms
Jul 20 18:56:59.796: INFO: Terminating ReplicationController externalsvc pods took: 200.261477ms
Jul 20 18:57:03.891: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:57:03.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1753" for this suite.
Jul 20 18:57:10.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:57:10.913: INFO: namespace services-1753 deletion completed in 6.908303828s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:19.155 seconds]
[sig-network] Services
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:57:10.915: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-686
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Jul 20 18:57:11.220: INFO: Waiting up to 5m0s for pod "var-expansion-991c606f-c1a6-488b-a881-ffcf0d8c7608" in namespace "var-expansion-686" to be "success or failure"
Jul 20 18:57:11.242: INFO: Pod "var-expansion-991c606f-c1a6-488b-a881-ffcf0d8c7608": Phase="Pending", Reason="", readiness=false. Elapsed: 21.807305ms
Jul 20 18:57:13.259: INFO: Pod "var-expansion-991c606f-c1a6-488b-a881-ffcf0d8c7608": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038325754s
Jul 20 18:57:15.270: INFO: Pod "var-expansion-991c606f-c1a6-488b-a881-ffcf0d8c7608": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049687175s
STEP: Saw pod success
Jul 20 18:57:15.270: INFO: Pod "var-expansion-991c606f-c1a6-488b-a881-ffcf0d8c7608" satisfied condition "success or failure"
Jul 20 18:57:15.282: INFO: Trying to get logs from node 10.123.236.230 pod var-expansion-991c606f-c1a6-488b-a881-ffcf0d8c7608 container dapi-container: <nil>
STEP: delete the pod
Jul 20 18:57:15.395: INFO: Waiting for pod var-expansion-991c606f-c1a6-488b-a881-ffcf0d8c7608 to disappear
Jul 20 18:57:15.432: INFO: Pod var-expansion-991c606f-c1a6-488b-a881-ffcf0d8c7608 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:57:15.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-686" for this suite.
Jul 20 18:57:21.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:57:22.066: INFO: namespace var-expansion-686 deletion completed in 6.614884249s

â€¢ [SLOW TEST:11.151 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:57:22.066: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4575
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 20 18:57:22.364: INFO: Waiting up to 5m0s for pod "pod-635b4d00-5057-4732-a75c-2f731896fe0d" in namespace "emptydir-4575" to be "success or failure"
Jul 20 18:57:22.375: INFO: Pod "pod-635b4d00-5057-4732-a75c-2f731896fe0d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.018938ms
Jul 20 18:57:24.385: INFO: Pod "pod-635b4d00-5057-4732-a75c-2f731896fe0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020323857s
Jul 20 18:57:26.394: INFO: Pod "pod-635b4d00-5057-4732-a75c-2f731896fe0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030047314s
STEP: Saw pod success
Jul 20 18:57:26.394: INFO: Pod "pod-635b4d00-5057-4732-a75c-2f731896fe0d" satisfied condition "success or failure"
Jul 20 18:57:26.403: INFO: Trying to get logs from node 10.123.236.220 pod pod-635b4d00-5057-4732-a75c-2f731896fe0d container test-container: <nil>
STEP: delete the pod
Jul 20 18:57:26.494: INFO: Waiting for pod pod-635b4d00-5057-4732-a75c-2f731896fe0d to disappear
Jul 20 18:57:26.505: INFO: Pod pod-635b4d00-5057-4732-a75c-2f731896fe0d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:57:26.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4575" for this suite.
Jul 20 18:57:32.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:57:33.240: INFO: namespace emptydir-4575 deletion completed in 6.717155301s

â€¢ [SLOW TEST:11.174 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:57:33.241: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3158
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-31341e0f-1633-4e93-ae50-d71139a90760
STEP: Creating a pod to test consume configMaps
Jul 20 18:57:33.503: INFO: Waiting up to 5m0s for pod "pod-configmaps-5dbcae14-3c1b-44c2-bc7f-55e999222382" in namespace "configmap-3158" to be "success or failure"
Jul 20 18:57:33.519: INFO: Pod "pod-configmaps-5dbcae14-3c1b-44c2-bc7f-55e999222382": Phase="Pending", Reason="", readiness=false. Elapsed: 15.275533ms
Jul 20 18:57:35.529: INFO: Pod "pod-configmaps-5dbcae14-3c1b-44c2-bc7f-55e999222382": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025936196s
Jul 20 18:57:37.539: INFO: Pod "pod-configmaps-5dbcae14-3c1b-44c2-bc7f-55e999222382": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03592411s
STEP: Saw pod success
Jul 20 18:57:37.539: INFO: Pod "pod-configmaps-5dbcae14-3c1b-44c2-bc7f-55e999222382" satisfied condition "success or failure"
Jul 20 18:57:37.549: INFO: Trying to get logs from node 10.123.236.230 pod pod-configmaps-5dbcae14-3c1b-44c2-bc7f-55e999222382 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 20 18:57:37.610: INFO: Waiting for pod pod-configmaps-5dbcae14-3c1b-44c2-bc7f-55e999222382 to disappear
Jul 20 18:57:37.619: INFO: Pod pod-configmaps-5dbcae14-3c1b-44c2-bc7f-55e999222382 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:57:37.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3158" for this suite.
Jul 20 18:57:43.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:57:44.186: INFO: namespace configmap-3158 deletion completed in 6.550671885s

â€¢ [SLOW TEST:10.945 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:57:44.186: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3512
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-3512
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-3512
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3512
Jul 20 18:57:44.465: INFO: Found 0 stateful pods, waiting for 1
Jul 20 18:57:54.477: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul 20 18:57:54.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=statefulset-3512 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 20 18:57:54.804: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 20 18:57:54.804: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 20 18:57:54.804: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 20 18:57:54.813: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 20 18:58:04.825: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 20 18:58:04.826: INFO: Waiting for statefulset status.replicas updated to 0
Jul 20 18:58:04.864: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jul 20 18:58:04.864: INFO: ss-0  10.123.236.230  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:57:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:57:44 +0000 UTC  }]
Jul 20 18:58:04.864: INFO: ss-1                  Pending         []
Jul 20 18:58:04.864: INFO: 
Jul 20 18:58:04.864: INFO: StatefulSet ss has not reached scale 3, at 2
Jul 20 18:58:05.875: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989180014s
Jul 20 18:58:06.886: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.977429792s
Jul 20 18:58:07.898: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.966510424s
Jul 20 18:58:08.908: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.954838877s
Jul 20 18:58:09.918: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.944675178s
Jul 20 18:58:10.930: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.934946008s
Jul 20 18:58:11.942: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.922691169s
Jul 20 18:58:12.954: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.910286654s
Jul 20 18:58:13.968: INFO: Verifying statefulset ss doesn't scale past 3 for another 899.031049ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3512
Jul 20 18:58:14.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=statefulset-3512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 20 18:58:15.217: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 20 18:58:15.217: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 20 18:58:15.217: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 20 18:58:15.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=statefulset-3512 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 20 18:58:15.454: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 20 18:58:15.454: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 20 18:58:15.454: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 20 18:58:15.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=statefulset-3512 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 20 18:58:15.708: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 20 18:58:15.708: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 20 18:58:15.708: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 20 18:58:15.723: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jul 20 18:58:25.736: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 20 18:58:25.736: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 20 18:58:25.736: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul 20 18:58:25.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=statefulset-3512 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 20 18:58:25.982: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 20 18:58:25.983: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 20 18:58:25.983: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 20 18:58:25.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=statefulset-3512 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 20 18:58:26.229: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 20 18:58:26.229: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 20 18:58:26.229: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 20 18:58:26.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=statefulset-3512 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 20 18:58:26.455: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 20 18:58:26.455: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 20 18:58:26.455: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 20 18:58:26.455: INFO: Waiting for statefulset status.replicas updated to 0
Jul 20 18:58:26.464: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul 20 18:58:36.488: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 20 18:58:36.489: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 20 18:58:36.489: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 20 18:58:36.518: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jul 20 18:58:36.518: INFO: ss-0  10.123.236.230  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:57:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:57:44 +0000 UTC  }]
Jul 20 18:58:36.518: INFO: ss-1  10.123.236.220  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:04 +0000 UTC  }]
Jul 20 18:58:36.518: INFO: ss-2  10.123.236.227  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:04 +0000 UTC  }]
Jul 20 18:58:36.518: INFO: 
Jul 20 18:58:36.518: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 20 18:58:37.532: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jul 20 18:58:37.532: INFO: ss-0  10.123.236.230  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:57:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:57:44 +0000 UTC  }]
Jul 20 18:58:37.532: INFO: ss-1  10.123.236.220  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:04 +0000 UTC  }]
Jul 20 18:58:37.532: INFO: ss-2  10.123.236.227  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:04 +0000 UTC  }]
Jul 20 18:58:37.532: INFO: 
Jul 20 18:58:37.532: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 20 18:58:38.543: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jul 20 18:58:38.543: INFO: ss-0  10.123.236.230  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:57:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:57:44 +0000 UTC  }]
Jul 20 18:58:38.543: INFO: ss-1  10.123.236.220  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:04 +0000 UTC  }]
Jul 20 18:58:38.543: INFO: ss-2  10.123.236.227  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:04 +0000 UTC  }]
Jul 20 18:58:38.543: INFO: 
Jul 20 18:58:38.543: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 20 18:58:39.554: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jul 20 18:58:39.554: INFO: ss-0  10.123.236.230  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:57:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:57:44 +0000 UTC  }]
Jul 20 18:58:39.554: INFO: ss-1  10.123.236.220  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:04 +0000 UTC  }]
Jul 20 18:58:39.554: INFO: 
Jul 20 18:58:39.554: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 20 18:58:40.567: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jul 20 18:58:40.567: INFO: ss-0  10.123.236.230  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:57:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:57:44 +0000 UTC  }]
Jul 20 18:58:40.567: INFO: ss-1  10.123.236.220  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:04 +0000 UTC  }]
Jul 20 18:58:40.568: INFO: 
Jul 20 18:58:40.568: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 20 18:58:41.579: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jul 20 18:58:41.579: INFO: ss-1  10.123.236.220  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:04 +0000 UTC  }]
Jul 20 18:58:41.579: INFO: 
Jul 20 18:58:41.579: INFO: StatefulSet ss has not reached scale 0, at 1
Jul 20 18:58:42.593: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jul 20 18:58:42.593: INFO: ss-1  10.123.236.220  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:04 +0000 UTC  }]
Jul 20 18:58:42.593: INFO: 
Jul 20 18:58:42.593: INFO: StatefulSet ss has not reached scale 0, at 1
Jul 20 18:58:43.604: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jul 20 18:58:43.604: INFO: ss-1  10.123.236.220  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:04 +0000 UTC  }]
Jul 20 18:58:43.604: INFO: 
Jul 20 18:58:43.604: INFO: StatefulSet ss has not reached scale 0, at 1
Jul 20 18:58:44.620: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jul 20 18:58:44.620: INFO: ss-1  10.123.236.220  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-20 18:58:04 +0000 UTC  }]
Jul 20 18:58:44.620: INFO: 
Jul 20 18:58:44.620: INFO: StatefulSet ss has not reached scale 0, at 1
Jul 20 18:58:45.630: INFO: Verifying statefulset ss doesn't scale past 0 for another 887.673916ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3512
Jul 20 18:58:46.640: INFO: Scaling statefulset ss to 0
Jul 20 18:58:46.673: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jul 20 18:58:46.682: INFO: Deleting all statefulset in ns statefulset-3512
Jul 20 18:58:46.692: INFO: Scaling statefulset ss to 0
Jul 20 18:58:46.727: INFO: Waiting for statefulset status.replicas updated to 0
Jul 20 18:58:46.737: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:58:46.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3512" for this suite.
Jul 20 18:58:52.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:58:53.404: INFO: namespace statefulset-3512 deletion completed in 6.609622562s

â€¢ [SLOW TEST:69.218 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:58:53.404: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7116
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jul 20 18:58:53.683: INFO: Waiting up to 5m0s for pod "downwardapi-volume-58a5a8e9-6427-48e6-93fd-e6f13224c31d" in namespace "downward-api-7116" to be "success or failure"
Jul 20 18:58:53.693: INFO: Pod "downwardapi-volume-58a5a8e9-6427-48e6-93fd-e6f13224c31d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.472592ms
Jul 20 18:58:55.703: INFO: Pod "downwardapi-volume-58a5a8e9-6427-48e6-93fd-e6f13224c31d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019947531s
STEP: Saw pod success
Jul 20 18:58:55.703: INFO: Pod "downwardapi-volume-58a5a8e9-6427-48e6-93fd-e6f13224c31d" satisfied condition "success or failure"
Jul 20 18:58:55.712: INFO: Trying to get logs from node 10.123.236.227 pod downwardapi-volume-58a5a8e9-6427-48e6-93fd-e6f13224c31d container client-container: <nil>
STEP: delete the pod
Jul 20 18:58:55.805: INFO: Waiting for pod downwardapi-volume-58a5a8e9-6427-48e6-93fd-e6f13224c31d to disappear
Jul 20 18:58:55.814: INFO: Pod downwardapi-volume-58a5a8e9-6427-48e6-93fd-e6f13224c31d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:58:55.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7116" for this suite.
Jul 20 18:59:01.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:59:02.347: INFO: namespace downward-api-7116 deletion completed in 6.512201293s

â€¢ [SLOW TEST:8.943 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:59:02.347: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6417
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 20 18:59:02.598: INFO: Waiting up to 5m0s for pod "pod-6996e05e-cfb0-4042-bb88-3355231fe396" in namespace "emptydir-6417" to be "success or failure"
Jul 20 18:59:02.617: INFO: Pod "pod-6996e05e-cfb0-4042-bb88-3355231fe396": Phase="Pending", Reason="", readiness=false. Elapsed: 18.625216ms
Jul 20 18:59:04.628: INFO: Pod "pod-6996e05e-cfb0-4042-bb88-3355231fe396": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030162127s
Jul 20 18:59:06.640: INFO: Pod "pod-6996e05e-cfb0-4042-bb88-3355231fe396": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041232519s
STEP: Saw pod success
Jul 20 18:59:06.640: INFO: Pod "pod-6996e05e-cfb0-4042-bb88-3355231fe396" satisfied condition "success or failure"
Jul 20 18:59:06.650: INFO: Trying to get logs from node 10.123.236.220 pod pod-6996e05e-cfb0-4042-bb88-3355231fe396 container test-container: <nil>
STEP: delete the pod
Jul 20 18:59:06.754: INFO: Waiting for pod pod-6996e05e-cfb0-4042-bb88-3355231fe396 to disappear
Jul 20 18:59:06.763: INFO: Pod pod-6996e05e-cfb0-4042-bb88-3355231fe396 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 18:59:06.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6417" for this suite.
Jul 20 18:59:13.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 18:59:13.495: INFO: namespace emptydir-6417 deletion completed in 6.715499467s

â€¢ [SLOW TEST:11.148 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 18:59:13.495: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2896
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 18:59:13.923: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul 20 18:59:13.965: INFO: Number of nodes with available pods: 0
Jul 20 18:59:13.965: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 18:59:15.002: INFO: Number of nodes with available pods: 0
Jul 20 18:59:15.003: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 18:59:15.999: INFO: Number of nodes with available pods: 1
Jul 20 18:59:15.999: INFO: Node 10.123.236.227 is running more than one daemon pod
Jul 20 18:59:17.019: INFO: Number of nodes with available pods: 3
Jul 20 18:59:17.019: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul 20 18:59:17.105: INFO: Wrong image for pod: daemon-set-6sx4g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:17.105: INFO: Wrong image for pod: daemon-set-dxxlx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:17.105: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:18.128: INFO: Wrong image for pod: daemon-set-6sx4g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:18.128: INFO: Wrong image for pod: daemon-set-dxxlx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:18.128: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:19.128: INFO: Wrong image for pod: daemon-set-6sx4g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:19.128: INFO: Wrong image for pod: daemon-set-dxxlx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:19.128: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:20.132: INFO: Wrong image for pod: daemon-set-6sx4g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:20.132: INFO: Pod daemon-set-6sx4g is not available
Jul 20 18:59:20.132: INFO: Wrong image for pod: daemon-set-dxxlx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:20.132: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:21.131: INFO: Wrong image for pod: daemon-set-6sx4g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:21.131: INFO: Pod daemon-set-6sx4g is not available
Jul 20 18:59:21.131: INFO: Wrong image for pod: daemon-set-dxxlx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:21.131: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:22.128: INFO: Wrong image for pod: daemon-set-6sx4g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:22.128: INFO: Pod daemon-set-6sx4g is not available
Jul 20 18:59:22.128: INFO: Wrong image for pod: daemon-set-dxxlx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:22.128: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:23.282: INFO: Wrong image for pod: daemon-set-6sx4g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:23.282: INFO: Pod daemon-set-6sx4g is not available
Jul 20 18:59:23.282: INFO: Wrong image for pod: daemon-set-dxxlx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:23.282: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:24.131: INFO: Wrong image for pod: daemon-set-6sx4g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:24.131: INFO: Pod daemon-set-6sx4g is not available
Jul 20 18:59:24.131: INFO: Wrong image for pod: daemon-set-dxxlx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:24.131: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:25.204: INFO: Pod daemon-set-6pg4v is not available
Jul 20 18:59:25.204: INFO: Wrong image for pod: daemon-set-dxxlx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:25.204: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:26.129: INFO: Pod daemon-set-6pg4v is not available
Jul 20 18:59:26.129: INFO: Wrong image for pod: daemon-set-dxxlx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:26.129: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:27.127: INFO: Wrong image for pod: daemon-set-dxxlx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:27.127: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:28.130: INFO: Wrong image for pod: daemon-set-dxxlx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:28.131: INFO: Pod daemon-set-dxxlx is not available
Jul 20 18:59:28.131: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:29.130: INFO: Wrong image for pod: daemon-set-dxxlx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:29.130: INFO: Pod daemon-set-dxxlx is not available
Jul 20 18:59:29.130: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:30.129: INFO: Wrong image for pod: daemon-set-dxxlx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:30.129: INFO: Pod daemon-set-dxxlx is not available
Jul 20 18:59:30.129: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:31.128: INFO: Wrong image for pod: daemon-set-dxxlx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:31.128: INFO: Pod daemon-set-dxxlx is not available
Jul 20 18:59:31.128: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:32.131: INFO: Wrong image for pod: daemon-set-dxxlx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:32.131: INFO: Pod daemon-set-dxxlx is not available
Jul 20 18:59:32.131: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:33.128: INFO: Wrong image for pod: daemon-set-dxxlx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:33.128: INFO: Pod daemon-set-dxxlx is not available
Jul 20 18:59:33.128: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:34.128: INFO: Wrong image for pod: daemon-set-dxxlx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:34.128: INFO: Pod daemon-set-dxxlx is not available
Jul 20 18:59:34.128: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:35.136: INFO: Wrong image for pod: daemon-set-dxxlx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:35.136: INFO: Pod daemon-set-dxxlx is not available
Jul 20 18:59:35.136: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:36.129: INFO: Pod daemon-set-v9sz7 is not available
Jul 20 18:59:36.129: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:37.127: INFO: Pod daemon-set-v9sz7 is not available
Jul 20 18:59:37.127: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:38.128: INFO: Pod daemon-set-v9sz7 is not available
Jul 20 18:59:38.128: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:39.130: INFO: Pod daemon-set-v9sz7 is not available
Jul 20 18:59:39.130: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:40.141: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:41.129: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:41.129: INFO: Pod daemon-set-xjnwm is not available
Jul 20 18:59:42.128: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:42.128: INFO: Pod daemon-set-xjnwm is not available
Jul 20 18:59:43.134: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:43.134: INFO: Pod daemon-set-xjnwm is not available
Jul 20 18:59:44.128: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:44.128: INFO: Pod daemon-set-xjnwm is not available
Jul 20 18:59:45.127: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:45.127: INFO: Pod daemon-set-xjnwm is not available
Jul 20 18:59:46.128: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:46.128: INFO: Pod daemon-set-xjnwm is not available
Jul 20 18:59:47.130: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:47.130: INFO: Pod daemon-set-xjnwm is not available
Jul 20 18:59:48.128: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:48.128: INFO: Pod daemon-set-xjnwm is not available
Jul 20 18:59:49.128: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:49.128: INFO: Pod daemon-set-xjnwm is not available
Jul 20 18:59:50.129: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:50.129: INFO: Pod daemon-set-xjnwm is not available
Jul 20 18:59:51.130: INFO: Wrong image for pod: daemon-set-xjnwm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jul 20 18:59:51.130: INFO: Pod daemon-set-xjnwm is not available
Jul 20 18:59:52.130: INFO: Pod daemon-set-d9zcz is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jul 20 18:59:52.170: INFO: Number of nodes with available pods: 2
Jul 20 18:59:52.170: INFO: Node 10.123.236.230 is running more than one daemon pod
Jul 20 18:59:53.211: INFO: Number of nodes with available pods: 3
Jul 20 18:59:53.211: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2896, will wait for the garbage collector to delete the pods
Jul 20 18:59:53.343: INFO: Deleting DaemonSet.extensions daemon-set took: 24.099094ms
Jul 20 18:59:53.543: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.227464ms
Jul 20 19:00:05.255: INFO: Number of nodes with available pods: 0
Jul 20 19:00:05.255: INFO: Number of running nodes: 0, number of available pods: 0
Jul 20 19:00:05.263: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2896/daemonsets","resourceVersion":"28936"},"items":null}

Jul 20 19:00:05.273: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2896/pods","resourceVersion":"28936"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:00:05.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2896" for this suite.
Jul 20 19:00:13.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:00:13.896: INFO: namespace daemonsets-2896 deletion completed in 8.56251953s

â€¢ [SLOW TEST:60.401 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:00:13.896: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4473
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Jul 20 19:00:14.119: INFO: namespace kubectl-4473
Jul 20 19:00:14.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 create -f - --namespace=kubectl-4473'
Jul 20 19:00:14.450: INFO: stderr: ""
Jul 20 19:00:14.450: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 20 19:00:15.459: INFO: Selector matched 1 pods for map[app:redis]
Jul 20 19:00:15.459: INFO: Found 0 / 1
Jul 20 19:00:16.470: INFO: Selector matched 1 pods for map[app:redis]
Jul 20 19:00:16.470: INFO: Found 1 / 1
Jul 20 19:00:16.470: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 20 19:00:16.479: INFO: Selector matched 1 pods for map[app:redis]
Jul 20 19:00:16.479: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 20 19:00:16.479: INFO: wait on redis-master startup in kubectl-4473 
Jul 20 19:00:16.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 logs redis-master-68gp7 redis-master --namespace=kubectl-4473'
Jul 20 19:00:16.601: INFO: stderr: ""
Jul 20 19:00:16.601: INFO: stdout: "1:C 20 Jul 2020 19:00:15.913 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 20 Jul 2020 19:00:15.913 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 20 Jul 2020 19:00:15.913 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 20 Jul 2020 19:00:15.914 * Running mode=standalone, port=6379.\n1:M 20 Jul 2020 19:00:15.914 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 20 Jul 2020 19:00:15.914 # Server initialized\n1:M 20 Jul 2020 19:00:15.914 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 20 Jul 2020 19:00:15.914 * Ready to accept connections\n"
STEP: exposing RC
Jul 20 19:00:16.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-4473'
Jul 20 19:00:16.742: INFO: stderr: ""
Jul 20 19:00:16.742: INFO: stdout: "service/rm2 exposed\n"
Jul 20 19:00:16.771: INFO: Service rm2 in namespace kubectl-4473 found.
STEP: exposing service
Jul 20 19:00:18.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-4473'
Jul 20 19:00:18.930: INFO: stderr: ""
Jul 20 19:00:18.930: INFO: stdout: "service/rm3 exposed\n"
Jul 20 19:00:18.964: INFO: Service rm3 in namespace kubectl-4473 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:00:21.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4473" for this suite.
Jul 20 19:00:35.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:00:35.631: INFO: namespace kubectl-4473 deletion completed in 14.610851136s

â€¢ [SLOW TEST:21.734 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:00:35.631: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9274
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jul 20 19:00:35.934: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1dc94f13-6a5b-4c20-b933-7677e7892646" in namespace "projected-9274" to be "success or failure"
Jul 20 19:00:35.947: INFO: Pod "downwardapi-volume-1dc94f13-6a5b-4c20-b933-7677e7892646": Phase="Pending", Reason="", readiness=false. Elapsed: 13.854514ms
Jul 20 19:00:37.960: INFO: Pod "downwardapi-volume-1dc94f13-6a5b-4c20-b933-7677e7892646": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026664013s
STEP: Saw pod success
Jul 20 19:00:37.960: INFO: Pod "downwardapi-volume-1dc94f13-6a5b-4c20-b933-7677e7892646" satisfied condition "success or failure"
Jul 20 19:00:37.973: INFO: Trying to get logs from node 10.123.236.220 pod downwardapi-volume-1dc94f13-6a5b-4c20-b933-7677e7892646 container client-container: <nil>
STEP: delete the pod
Jul 20 19:00:38.063: INFO: Waiting for pod downwardapi-volume-1dc94f13-6a5b-4c20-b933-7677e7892646 to disappear
Jul 20 19:00:38.074: INFO: Pod downwardapi-volume-1dc94f13-6a5b-4c20-b933-7677e7892646 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:00:38.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9274" for this suite.
Jul 20 19:00:44.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:00:44.738: INFO: namespace projected-9274 deletion completed in 6.649008628s

â€¢ [SLOW TEST:9.107 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:00:44.738: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5273
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-525624b3-a552-4bf5-8a03-4ba77be840be
STEP: Creating a pod to test consume configMaps
Jul 20 19:00:45.010: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-865492b3-bf11-4077-b398-d462d024c2aa" in namespace "projected-5273" to be "success or failure"
Jul 20 19:00:45.025: INFO: Pod "pod-projected-configmaps-865492b3-bf11-4077-b398-d462d024c2aa": Phase="Pending", Reason="", readiness=false. Elapsed: 14.653675ms
Jul 20 19:00:47.034: INFO: Pod "pod-projected-configmaps-865492b3-bf11-4077-b398-d462d024c2aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024171661s
STEP: Saw pod success
Jul 20 19:00:47.034: INFO: Pod "pod-projected-configmaps-865492b3-bf11-4077-b398-d462d024c2aa" satisfied condition "success or failure"
Jul 20 19:00:47.044: INFO: Trying to get logs from node 10.123.236.220 pod pod-projected-configmaps-865492b3-bf11-4077-b398-d462d024c2aa container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 20 19:00:47.110: INFO: Waiting for pod pod-projected-configmaps-865492b3-bf11-4077-b398-d462d024c2aa to disappear
Jul 20 19:00:47.125: INFO: Pod pod-projected-configmaps-865492b3-bf11-4077-b398-d462d024c2aa no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:00:47.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5273" for this suite.
Jul 20 19:00:53.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:00:53.823: INFO: namespace projected-5273 deletion completed in 6.682968078s

â€¢ [SLOW TEST:9.085 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:00:53.824: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8021
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jul 20 19:00:54.074: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8956ce79-5047-4c7e-999b-e37be45aaf77" in namespace "projected-8021" to be "success or failure"
Jul 20 19:00:54.084: INFO: Pod "downwardapi-volume-8956ce79-5047-4c7e-999b-e37be45aaf77": Phase="Pending", Reason="", readiness=false. Elapsed: 9.916014ms
Jul 20 19:00:56.098: INFO: Pod "downwardapi-volume-8956ce79-5047-4c7e-999b-e37be45aaf77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024173039s
STEP: Saw pod success
Jul 20 19:00:56.098: INFO: Pod "downwardapi-volume-8956ce79-5047-4c7e-999b-e37be45aaf77" satisfied condition "success or failure"
Jul 20 19:00:56.108: INFO: Trying to get logs from node 10.123.236.227 pod downwardapi-volume-8956ce79-5047-4c7e-999b-e37be45aaf77 container client-container: <nil>
STEP: delete the pod
Jul 20 19:00:56.188: INFO: Waiting for pod downwardapi-volume-8956ce79-5047-4c7e-999b-e37be45aaf77 to disappear
Jul 20 19:00:56.197: INFO: Pod downwardapi-volume-8956ce79-5047-4c7e-999b-e37be45aaf77 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:00:56.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8021" for this suite.
Jul 20 19:01:02.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:01:03.061: INFO: namespace projected-8021 deletion completed in 6.846775814s

â€¢ [SLOW TEST:9.238 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:01:03.063: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1412
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-93a62706-b697-47fb-8fe1-f0af44fd3d64
STEP: Creating a pod to test consume configMaps
Jul 20 19:01:03.347: INFO: Waiting up to 5m0s for pod "pod-configmaps-a4295750-bac6-4d1a-a108-dfe0ed407698" in namespace "configmap-1412" to be "success or failure"
Jul 20 19:01:03.361: INFO: Pod "pod-configmaps-a4295750-bac6-4d1a-a108-dfe0ed407698": Phase="Pending", Reason="", readiness=false. Elapsed: 13.387876ms
Jul 20 19:01:05.372: INFO: Pod "pod-configmaps-a4295750-bac6-4d1a-a108-dfe0ed407698": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024384519s
STEP: Saw pod success
Jul 20 19:01:05.372: INFO: Pod "pod-configmaps-a4295750-bac6-4d1a-a108-dfe0ed407698" satisfied condition "success or failure"
Jul 20 19:01:05.384: INFO: Trying to get logs from node 10.123.236.220 pod pod-configmaps-a4295750-bac6-4d1a-a108-dfe0ed407698 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 20 19:01:05.440: INFO: Waiting for pod pod-configmaps-a4295750-bac6-4d1a-a108-dfe0ed407698 to disappear
Jul 20 19:01:05.452: INFO: Pod pod-configmaps-a4295750-bac6-4d1a-a108-dfe0ed407698 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:01:05.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1412" for this suite.
Jul 20 19:01:11.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:01:12.338: INFO: namespace configmap-1412 deletion completed in 6.867545519s

â€¢ [SLOW TEST:9.275 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:01:12.338: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9781
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Jul 20 19:01:12.601: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:01:16.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9781" for this suite.
Jul 20 19:01:24.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:01:24.607: INFO: namespace init-container-9781 deletion completed in 8.576202324s

â€¢ [SLOW TEST:12.269 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:01:24.607: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-70
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-3bc29e6e-a095-40aa-99a0-dbc003587c27
STEP: Creating a pod to test consume secrets
Jul 20 19:01:24.910: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8218b543-722d-454f-85a0-0faa6e899e9b" in namespace "projected-70" to be "success or failure"
Jul 20 19:01:24.923: INFO: Pod "pod-projected-secrets-8218b543-722d-454f-85a0-0faa6e899e9b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.141465ms
Jul 20 19:01:26.934: INFO: Pod "pod-projected-secrets-8218b543-722d-454f-85a0-0faa6e899e9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024051496s
STEP: Saw pod success
Jul 20 19:01:26.935: INFO: Pod "pod-projected-secrets-8218b543-722d-454f-85a0-0faa6e899e9b" satisfied condition "success or failure"
Jul 20 19:01:26.947: INFO: Trying to get logs from node 10.123.236.230 pod pod-projected-secrets-8218b543-722d-454f-85a0-0faa6e899e9b container secret-volume-test: <nil>
STEP: delete the pod
Jul 20 19:01:27.036: INFO: Waiting for pod pod-projected-secrets-8218b543-722d-454f-85a0-0faa6e899e9b to disappear
Jul 20 19:01:27.046: INFO: Pod pod-projected-secrets-8218b543-722d-454f-85a0-0faa6e899e9b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:01:27.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-70" for this suite.
Jul 20 19:01:33.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:01:33.592: INFO: namespace projected-70 deletion completed in 6.532429227s

â€¢ [SLOW TEST:8.985 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:01:33.594: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1283
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jul 20 19:01:33.867: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c865c0f3-2fd5-48ed-b2b1-30dd4073ad30" in namespace "projected-1283" to be "success or failure"
Jul 20 19:01:33.884: INFO: Pod "downwardapi-volume-c865c0f3-2fd5-48ed-b2b1-30dd4073ad30": Phase="Pending", Reason="", readiness=false. Elapsed: 17.220641ms
Jul 20 19:01:35.894: INFO: Pod "downwardapi-volume-c865c0f3-2fd5-48ed-b2b1-30dd4073ad30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027194995s
STEP: Saw pod success
Jul 20 19:01:35.894: INFO: Pod "downwardapi-volume-c865c0f3-2fd5-48ed-b2b1-30dd4073ad30" satisfied condition "success or failure"
Jul 20 19:01:35.923: INFO: Trying to get logs from node 10.123.236.230 pod downwardapi-volume-c865c0f3-2fd5-48ed-b2b1-30dd4073ad30 container client-container: <nil>
STEP: delete the pod
Jul 20 19:01:35.977: INFO: Waiting for pod downwardapi-volume-c865c0f3-2fd5-48ed-b2b1-30dd4073ad30 to disappear
Jul 20 19:01:35.985: INFO: Pod downwardapi-volume-c865c0f3-2fd5-48ed-b2b1-30dd4073ad30 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:01:35.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1283" for this suite.
Jul 20 19:01:42.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:01:42.642: INFO: namespace projected-1283 deletion completed in 6.635535256s

â€¢ [SLOW TEST:9.048 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:01:42.643: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8286
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jul 20 19:01:47.005: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-291352008 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jul 20 19:01:52.177: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:01:52.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8286" for this suite.
Jul 20 19:01:58.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:01:58.820: INFO: namespace pods-8286 deletion completed in 6.599810312s

â€¢ [SLOW TEST:16.177 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:01:58.820: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3578
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-2z77
STEP: Creating a pod to test atomic-volume-subpath
Jul 20 19:01:59.119: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-2z77" in namespace "subpath-3578" to be "success or failure"
Jul 20 19:01:59.132: INFO: Pod "pod-subpath-test-secret-2z77": Phase="Pending", Reason="", readiness=false. Elapsed: 13.371642ms
Jul 20 19:02:01.145: INFO: Pod "pod-subpath-test-secret-2z77": Phase="Running", Reason="", readiness=true. Elapsed: 2.025683983s
Jul 20 19:02:03.155: INFO: Pod "pod-subpath-test-secret-2z77": Phase="Running", Reason="", readiness=true. Elapsed: 4.035776798s
Jul 20 19:02:05.164: INFO: Pod "pod-subpath-test-secret-2z77": Phase="Running", Reason="", readiness=true. Elapsed: 6.044829618s
Jul 20 19:02:07.330: INFO: Pod "pod-subpath-test-secret-2z77": Phase="Running", Reason="", readiness=true. Elapsed: 8.210537103s
Jul 20 19:02:09.339: INFO: Pod "pod-subpath-test-secret-2z77": Phase="Running", Reason="", readiness=true. Elapsed: 10.220445433s
Jul 20 19:02:11.353: INFO: Pod "pod-subpath-test-secret-2z77": Phase="Running", Reason="", readiness=true. Elapsed: 12.233676349s
Jul 20 19:02:13.366: INFO: Pod "pod-subpath-test-secret-2z77": Phase="Running", Reason="", readiness=true. Elapsed: 14.246623923s
Jul 20 19:02:15.379: INFO: Pod "pod-subpath-test-secret-2z77": Phase="Running", Reason="", readiness=true. Elapsed: 16.259731117s
Jul 20 19:02:17.390: INFO: Pod "pod-subpath-test-secret-2z77": Phase="Running", Reason="", readiness=true. Elapsed: 18.270600564s
Jul 20 19:02:19.400: INFO: Pod "pod-subpath-test-secret-2z77": Phase="Running", Reason="", readiness=true. Elapsed: 20.281186757s
Jul 20 19:02:21.412: INFO: Pod "pod-subpath-test-secret-2z77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.293011502s
STEP: Saw pod success
Jul 20 19:02:21.413: INFO: Pod "pod-subpath-test-secret-2z77" satisfied condition "success or failure"
Jul 20 19:02:21.424: INFO: Trying to get logs from node 10.123.236.220 pod pod-subpath-test-secret-2z77 container test-container-subpath-secret-2z77: <nil>
STEP: delete the pod
Jul 20 19:02:21.493: INFO: Waiting for pod pod-subpath-test-secret-2z77 to disappear
Jul 20 19:02:21.502: INFO: Pod pod-subpath-test-secret-2z77 no longer exists
STEP: Deleting pod pod-subpath-test-secret-2z77
Jul 20 19:02:21.502: INFO: Deleting pod "pod-subpath-test-secret-2z77" in namespace "subpath-3578"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:02:21.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3578" for this suite.
Jul 20 19:02:27.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:02:28.114: INFO: namespace subpath-3578 deletion completed in 6.580133608s

â€¢ [SLOW TEST:29.294 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:02:28.114: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-5634
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6180
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3827
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:02:34.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5634" for this suite.
Jul 20 19:02:41.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:02:41.582: INFO: namespace namespaces-5634 deletion completed in 6.621105135s
STEP: Destroying namespace "nsdeletetest-6180" for this suite.
Jul 20 19:02:41.607: INFO: Namespace nsdeletetest-6180 was already deleted
STEP: Destroying namespace "nsdeletetest-3827" for this suite.
Jul 20 19:02:47.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:02:48.227: INFO: namespace nsdeletetest-3827 deletion completed in 6.619992132s

â€¢ [SLOW TEST:20.113 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:02:48.229: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1405
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jul 20 19:02:48.475: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1b663393-d8f7-495d-9fa6-d87bfa72df6c" in namespace "downward-api-1405" to be "success or failure"
Jul 20 19:02:48.488: INFO: Pod "downwardapi-volume-1b663393-d8f7-495d-9fa6-d87bfa72df6c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.466185ms
Jul 20 19:02:50.497: INFO: Pod "downwardapi-volume-1b663393-d8f7-495d-9fa6-d87bfa72df6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022320483s
STEP: Saw pod success
Jul 20 19:02:50.498: INFO: Pod "downwardapi-volume-1b663393-d8f7-495d-9fa6-d87bfa72df6c" satisfied condition "success or failure"
Jul 20 19:02:50.506: INFO: Trying to get logs from node 10.123.236.227 pod downwardapi-volume-1b663393-d8f7-495d-9fa6-d87bfa72df6c container client-container: <nil>
STEP: delete the pod
Jul 20 19:02:50.599: INFO: Waiting for pod downwardapi-volume-1b663393-d8f7-495d-9fa6-d87bfa72df6c to disappear
Jul 20 19:02:50.611: INFO: Pod downwardapi-volume-1b663393-d8f7-495d-9fa6-d87bfa72df6c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:02:50.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1405" for this suite.
Jul 20 19:02:56.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:02:57.179: INFO: namespace downward-api-1405 deletion completed in 6.552034085s

â€¢ [SLOW TEST:8.950 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:02:57.181: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8952
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 19:02:57.536: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul 20 19:02:57.559: INFO: Number of nodes with available pods: 0
Jul 20 19:02:57.560: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jul 20 19:02:57.604: INFO: Number of nodes with available pods: 0
Jul 20 19:02:57.604: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 19:02:58.614: INFO: Number of nodes with available pods: 0
Jul 20 19:02:58.614: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 19:02:59.615: INFO: Number of nodes with available pods: 0
Jul 20 19:02:59.615: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 19:03:00.615: INFO: Number of nodes with available pods: 1
Jul 20 19:03:00.615: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul 20 19:03:00.660: INFO: Number of nodes with available pods: 0
Jul 20 19:03:00.660: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul 20 19:03:00.687: INFO: Number of nodes with available pods: 0
Jul 20 19:03:00.687: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 19:03:01.703: INFO: Number of nodes with available pods: 0
Jul 20 19:03:01.703: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 19:03:02.701: INFO: Number of nodes with available pods: 0
Jul 20 19:03:02.701: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 19:03:03.699: INFO: Number of nodes with available pods: 0
Jul 20 19:03:03.699: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 19:03:04.700: INFO: Number of nodes with available pods: 0
Jul 20 19:03:04.700: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 19:03:05.698: INFO: Number of nodes with available pods: 0
Jul 20 19:03:05.698: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 19:03:06.699: INFO: Number of nodes with available pods: 0
Jul 20 19:03:06.699: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 19:03:07.698: INFO: Number of nodes with available pods: 0
Jul 20 19:03:07.698: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 19:03:08.697: INFO: Number of nodes with available pods: 0
Jul 20 19:03:08.697: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 19:03:09.698: INFO: Number of nodes with available pods: 0
Jul 20 19:03:09.698: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 19:03:10.697: INFO: Number of nodes with available pods: 0
Jul 20 19:03:10.697: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 19:03:11.697: INFO: Number of nodes with available pods: 0
Jul 20 19:03:11.697: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 19:03:12.702: INFO: Number of nodes with available pods: 0
Jul 20 19:03:12.702: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 19:03:13.697: INFO: Number of nodes with available pods: 0
Jul 20 19:03:13.697: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 19:03:14.699: INFO: Number of nodes with available pods: 0
Jul 20 19:03:14.699: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 19:03:15.699: INFO: Number of nodes with available pods: 0
Jul 20 19:03:15.699: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 19:03:16.698: INFO: Number of nodes with available pods: 0
Jul 20 19:03:16.698: INFO: Node 10.123.236.220 is running more than one daemon pod
Jul 20 19:03:17.697: INFO: Number of nodes with available pods: 1
Jul 20 19:03:17.697: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8952, will wait for the garbage collector to delete the pods
Jul 20 19:03:17.796: INFO: Deleting DaemonSet.extensions daemon-set took: 21.294787ms
Jul 20 19:03:17.997: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.482854ms
Jul 20 19:03:21.406: INFO: Number of nodes with available pods: 0
Jul 20 19:03:21.406: INFO: Number of running nodes: 0, number of available pods: 0
Jul 20 19:03:21.414: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8952/daemonsets","resourceVersion":"29798"},"items":null}

Jul 20 19:03:21.427: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8952/pods","resourceVersion":"29798"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:03:21.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8952" for this suite.
Jul 20 19:03:27.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:03:28.266: INFO: namespace daemonsets-8952 deletion completed in 6.763736739s

â€¢ [SLOW TEST:31.085 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:03:28.266: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6055
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 20 19:03:32.632: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 20 19:03:32.647: INFO: Pod pod-with-prestop-http-hook still exists
Jul 20 19:03:34.648: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 20 19:03:34.660: INFO: Pod pod-with-prestop-http-hook still exists
Jul 20 19:03:36.648: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 20 19:03:36.658: INFO: Pod pod-with-prestop-http-hook still exists
Jul 20 19:03:38.648: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 20 19:03:38.698: INFO: Pod pod-with-prestop-http-hook still exists
Jul 20 19:03:40.648: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 20 19:03:40.657: INFO: Pod pod-with-prestop-http-hook still exists
Jul 20 19:03:42.648: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 20 19:03:42.661: INFO: Pod pod-with-prestop-http-hook still exists
Jul 20 19:03:44.648: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 20 19:03:44.659: INFO: Pod pod-with-prestop-http-hook still exists
Jul 20 19:03:46.648: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 20 19:03:46.657: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:03:46.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6055" for this suite.
Jul 20 19:04:12.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:04:13.228: INFO: namespace container-lifecycle-hook-6055 deletion completed in 26.532087107s

â€¢ [SLOW TEST:44.961 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:04:13.229: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6662
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 20 19:04:15.528: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:04:15.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6662" for this suite.
Jul 20 19:04:21.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:04:22.112: INFO: namespace container-runtime-6662 deletion completed in 6.517319146s

â€¢ [SLOW TEST:8.883 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:04:22.113: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3669
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jul 20 19:04:22.430: INFO: Waiting up to 5m0s for pod "downward-api-d8b67f58-56f5-4b65-b292-d0c30b17b5d7" in namespace "downward-api-3669" to be "success or failure"
Jul 20 19:04:22.440: INFO: Pod "downward-api-d8b67f58-56f5-4b65-b292-d0c30b17b5d7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.889195ms
Jul 20 19:04:24.451: INFO: Pod "downward-api-d8b67f58-56f5-4b65-b292-d0c30b17b5d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020871931s
STEP: Saw pod success
Jul 20 19:04:24.451: INFO: Pod "downward-api-d8b67f58-56f5-4b65-b292-d0c30b17b5d7" satisfied condition "success or failure"
Jul 20 19:04:24.460: INFO: Trying to get logs from node 10.123.236.227 pod downward-api-d8b67f58-56f5-4b65-b292-d0c30b17b5d7 container dapi-container: <nil>
STEP: delete the pod
Jul 20 19:04:24.525: INFO: Waiting for pod downward-api-d8b67f58-56f5-4b65-b292-d0c30b17b5d7 to disappear
Jul 20 19:04:24.536: INFO: Pod downward-api-d8b67f58-56f5-4b65-b292-d0c30b17b5d7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:04:24.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3669" for this suite.
Jul 20 19:04:30.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:04:31.280: INFO: namespace downward-api-3669 deletion completed in 6.729792626s

â€¢ [SLOW TEST:9.167 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:04:31.280: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8764
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jul 20 19:04:31.520: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 19:04:35.582: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:04:49.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8764" for this suite.
Jul 20 19:04:55.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:04:56.099: INFO: namespace crd-publish-openapi-8764 deletion completed in 6.564888579s

â€¢ [SLOW TEST:24.819 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:04:56.101: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3314
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 20 19:04:56.362: INFO: Waiting up to 5m0s for pod "pod-89eee47b-4234-44f5-a837-ba0b28049312" in namespace "emptydir-3314" to be "success or failure"
Jul 20 19:04:56.375: INFO: Pod "pod-89eee47b-4234-44f5-a837-ba0b28049312": Phase="Pending", Reason="", readiness=false. Elapsed: 12.126008ms
Jul 20 19:04:58.386: INFO: Pod "pod-89eee47b-4234-44f5-a837-ba0b28049312": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023067365s
Jul 20 19:05:00.396: INFO: Pod "pod-89eee47b-4234-44f5-a837-ba0b28049312": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033218401s
STEP: Saw pod success
Jul 20 19:05:00.396: INFO: Pod "pod-89eee47b-4234-44f5-a837-ba0b28049312" satisfied condition "success or failure"
Jul 20 19:05:00.410: INFO: Trying to get logs from node 10.123.236.220 pod pod-89eee47b-4234-44f5-a837-ba0b28049312 container test-container: <nil>
STEP: delete the pod
Jul 20 19:05:00.505: INFO: Waiting for pod pod-89eee47b-4234-44f5-a837-ba0b28049312 to disappear
Jul 20 19:05:00.514: INFO: Pod pod-89eee47b-4234-44f5-a837-ba0b28049312 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:05:00.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3314" for this suite.
Jul 20 19:05:06.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:05:07.098: INFO: namespace emptydir-3314 deletion completed in 6.558680378s

â€¢ [SLOW TEST:10.997 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:05:07.098: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1882
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-1882
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 20 19:05:07.343: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 20 19:05:29.633: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.106.155 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1882 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 20 19:05:29.633: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 19:05:30.773: INFO: Found all expected endpoints: [netserver-0]
Jul 20 19:05:30.784: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.159.118 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1882 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 20 19:05:30.784: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 19:05:31.934: INFO: Found all expected endpoints: [netserver-1]
Jul 20 19:05:31.945: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.176.2 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1882 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 20 19:05:31.945: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 19:05:33.084: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:05:33.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1882" for this suite.
Jul 20 19:05:45.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:05:45.674: INFO: namespace pod-network-test-1882 deletion completed in 12.576225671s

â€¢ [SLOW TEST:38.576 seconds]
[sig-network] Networking
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:05:45.675: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7826
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jul 20 19:05:45.923: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 20 19:05:45.974: INFO: Waiting for terminating namespaces to be deleted...
Jul 20 19:05:46.000: INFO: 
Logging pods the kubelet thinks is on node 10.123.236.220 before test
Jul 20 19:05:46.035: INFO: coredns-54f55c7c7c-gw4fd from kube-system started at 2020-07-20 17:02:33 +0000 UTC (1 container statuses recorded)
Jul 20 19:05:46.035: INFO: 	Container coredns ready: true, restart count 0
Jul 20 19:05:46.035: INFO: calico-node-6z6jp from kube-system started at 2020-07-20 16:38:01 +0000 UTC (1 container statuses recorded)
Jul 20 19:05:46.035: INFO: 	Container calico-node ready: true, restart count 0
Jul 20 19:05:46.035: INFO: ibm-cloud-provider-ip-149-81-145-66-6c99b7f77d-6xqwt from ibm-system started at 2020-07-20 16:38:20 +0000 UTC (1 container statuses recorded)
Jul 20 19:05:46.035: INFO: 	Container ibm-cloud-provider-ip-149-81-145-66 ready: true, restart count 0
Jul 20 19:05:46.035: INFO: public-crbsas6ivf0c8md2c63apg-alb1-5f96ffbc77-c5tzp from kube-system started at 2020-07-20 16:39:13 +0000 UTC (4 container statuses recorded)
Jul 20 19:05:46.035: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Jul 20 19:05:46.035: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Jul 20 19:05:46.035: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Jul 20 19:05:46.035: INFO: 	Container nginx-ingress ready: true, restart count 0
Jul 20 19:05:46.035: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-07-20 16:40:09 +0000 UTC (1 container statuses recorded)
Jul 20 19:05:46.035: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jul 20 19:05:46.035: INFO: sonobuoy-systemd-logs-daemon-set-2fac7b176f1f4511-zngst from sonobuoy started at 2020-07-20 18:23:20 +0000 UTC (2 container statuses recorded)
Jul 20 19:05:46.035: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 20 19:05:46.035: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 20 19:05:46.035: INFO: ibm-master-proxy-static-10.123.236.220 from kube-system started at 2020-07-20 16:37:55 +0000 UTC (2 container statuses recorded)
Jul 20 19:05:46.035: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jul 20 19:05:46.035: INFO: 	Container pause ready: true, restart count 0
Jul 20 19:05:46.035: INFO: ibm-keepalived-watcher-69dl6 from kube-system started at 2020-07-20 16:38:01 +0000 UTC (1 container statuses recorded)
Jul 20 19:05:46.035: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jul 20 19:05:46.035: INFO: 
Logging pods the kubelet thinks is on node 10.123.236.227 before test
Jul 20 19:05:46.091: INFO: calico-kube-controllers-69c4db5c8d-44fh7 from kube-system started at 2020-07-20 16:37:01 +0000 UTC (1 container statuses recorded)
Jul 20 19:05:46.091: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 20 19:05:46.091: INFO: ibm-file-plugin-586dc4596c-gxcnd from kube-system started at 2020-07-20 16:37:01 +0000 UTC (1 container statuses recorded)
Jul 20 19:05:46.091: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jul 20 19:05:46.091: INFO: olm-operator-787498c9b7-mnb2k from ibm-system started at 2020-07-20 16:37:01 +0000 UTC (1 container statuses recorded)
Jul 20 19:05:46.091: INFO: 	Container olm-operator ready: true, restart count 0
Jul 20 19:05:46.091: INFO: addon-catalog-source-c4rf5 from ibm-system started at 2020-07-20 16:37:18 +0000 UTC (1 container statuses recorded)
Jul 20 19:05:46.091: INFO: 	Container configmap-registry-server ready: true, restart count 0
Jul 20 19:05:46.091: INFO: vpn-f66c45467-f46vm from kube-system started at 2020-07-20 17:02:12 +0000 UTC (1 container statuses recorded)
Jul 20 19:05:46.091: INFO: 	Container vpn ready: true, restart count 0
Jul 20 19:05:46.091: INFO: ibm-master-proxy-static-10.123.236.227 from kube-system started at 2020-07-20 16:36:44 +0000 UTC (2 container statuses recorded)
Jul 20 19:05:46.091: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jul 20 19:05:46.091: INFO: 	Container pause ready: true, restart count 0
Jul 20 19:05:46.091: INFO: ibm-keepalived-watcher-m9bwq from kube-system started at 2020-07-20 16:36:51 +0000 UTC (1 container statuses recorded)
Jul 20 19:05:46.091: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jul 20 19:05:46.091: INFO: calico-node-85dkz from kube-system started at 2020-07-20 16:36:51 +0000 UTC (1 container statuses recorded)
Jul 20 19:05:46.091: INFO: 	Container calico-node ready: true, restart count 0
Jul 20 19:05:46.091: INFO: metrics-server-7d665f557c-wfj45 from kube-system started at 2020-07-20 16:37:17 +0000 UTC (2 container statuses recorded)
Jul 20 19:05:46.091: INFO: 	Container metrics-server ready: true, restart count 0
Jul 20 19:05:46.091: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jul 20 19:05:46.091: INFO: dashboard-metrics-scraper-5789d44f58-mr88k from kube-system started at 2020-07-20 16:37:01 +0000 UTC (1 container statuses recorded)
Jul 20 19:05:46.091: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jul 20 19:05:46.091: INFO: kubernetes-dashboard-984c5c57-cf2cf from kube-system started at 2020-07-20 16:37:01 +0000 UTC (1 container statuses recorded)
Jul 20 19:05:46.091: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul 20 19:05:46.091: INFO: ibm-storage-watcher-5955dd9995-ft6sf from kube-system started at 2020-07-20 16:37:05 +0000 UTC (1 container statuses recorded)
Jul 20 19:05:46.091: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jul 20 19:05:46.091: INFO: coredns-autoscaler-6bc79bb9db-nsbq2 from kube-system started at 2020-07-20 16:37:01 +0000 UTC (1 container statuses recorded)
Jul 20 19:05:46.091: INFO: 	Container autoscaler ready: true, restart count 0
Jul 20 19:05:46.091: INFO: catalog-operator-67646bfcdb-49pmq from ibm-system started at 2020-07-20 16:37:01 +0000 UTC (1 container statuses recorded)
Jul 20 19:05:46.091: INFO: 	Container catalog-operator ready: true, restart count 0
Jul 20 19:05:46.091: INFO: coredns-54f55c7c7c-fr2hn from kube-system started at 2020-07-20 17:02:34 +0000 UTC (1 container statuses recorded)
Jul 20 19:05:46.091: INFO: 	Container coredns ready: true, restart count 0
Jul 20 19:05:46.091: INFO: sonobuoy-systemd-logs-daemon-set-2fac7b176f1f4511-mktvj from sonobuoy started at 2020-07-20 18:23:20 +0000 UTC (2 container statuses recorded)
Jul 20 19:05:46.091: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 20 19:05:46.091: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 20 19:05:46.091: INFO: 
Logging pods the kubelet thinks is on node 10.123.236.230 before test
Jul 20 19:05:46.167: INFO: coredns-54f55c7c7c-xgw9w from kube-system started at 2020-07-20 17:02:33 +0000 UTC (1 container statuses recorded)
Jul 20 19:05:46.167: INFO: 	Container coredns ready: true, restart count 0
Jul 20 19:05:46.167: INFO: sonobuoy from sonobuoy started at 2020-07-20 18:23:14 +0000 UTC (1 container statuses recorded)
Jul 20 19:05:46.167: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 20 19:05:46.167: INFO: sonobuoy-e2e-job-0f6e1d1f061d4a08 from sonobuoy started at 2020-07-20 18:23:20 +0000 UTC (2 container statuses recorded)
Jul 20 19:05:46.167: INFO: 	Container e2e ready: true, restart count 0
Jul 20 19:05:46.167: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 20 19:05:46.167: INFO: sonobuoy-systemd-logs-daemon-set-2fac7b176f1f4511-xh4r9 from sonobuoy started at 2020-07-20 18:23:20 +0000 UTC (2 container statuses recorded)
Jul 20 19:05:46.167: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 20 19:05:46.167: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 20 19:05:46.167: INFO: ibm-cloud-provider-ip-149-81-145-66-6c99b7f77d-4rckp from ibm-system started at 2020-07-20 16:38:20 +0000 UTC (1 container statuses recorded)
Jul 20 19:05:46.167: INFO: 	Container ibm-cloud-provider-ip-149-81-145-66 ready: true, restart count 0
Jul 20 19:05:46.167: INFO: calico-node-wpsss from kube-system started at 2020-07-20 16:38:02 +0000 UTC (1 container statuses recorded)
Jul 20 19:05:46.167: INFO: 	Container calico-node ready: true, restart count 0
Jul 20 19:05:46.167: INFO: ibm-keepalived-watcher-fhgvr from kube-system started at 2020-07-20 16:38:02 +0000 UTC (1 container statuses recorded)
Jul 20 19:05:46.167: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jul 20 19:05:46.167: INFO: public-crbsas6ivf0c8md2c63apg-alb1-5f96ffbc77-lpqp5 from kube-system started at 2020-07-20 16:39:13 +0000 UTC (4 container statuses recorded)
Jul 20 19:05:46.167: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Jul 20 19:05:46.167: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Jul 20 19:05:46.167: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Jul 20 19:05:46.167: INFO: 	Container nginx-ingress ready: true, restart count 0
Jul 20 19:05:46.167: INFO: ibm-master-proxy-static-10.123.236.230 from kube-system started at 2020-07-20 16:38:01 +0000 UTC (2 container statuses recorded)
Jul 20 19:05:46.167: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jul 20 19:05:46.167: INFO: 	Container pause ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16238b620c095c23], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:05:47.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7826" for this suite.
Jul 20 19:05:53.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:05:53.900: INFO: namespace sched-pred-7826 deletion completed in 6.634449119s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:8.226 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:05:53.901: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5129
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Jul 20 19:05:56.234: INFO: Pod pod-hostip-abac0b09-2fcd-4452-9c69-258216221fec has hostIP: 10.123.236.230
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:05:56.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5129" for this suite.
Jul 20 19:06:08.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:06:09.192: INFO: namespace pods-5129 deletion completed in 12.942958195s

â€¢ [SLOW TEST:15.292 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:06:09.193: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4436
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jul 20 19:06:09.447: INFO: Waiting up to 5m0s for pod "downward-api-bfa44a37-4fe9-43f0-a9e8-c10b1f58df0e" in namespace "downward-api-4436" to be "success or failure"
Jul 20 19:06:09.466: INFO: Pod "downward-api-bfa44a37-4fe9-43f0-a9e8-c10b1f58df0e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.620116ms
Jul 20 19:06:11.476: INFO: Pod "downward-api-bfa44a37-4fe9-43f0-a9e8-c10b1f58df0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028643498s
STEP: Saw pod success
Jul 20 19:06:11.476: INFO: Pod "downward-api-bfa44a37-4fe9-43f0-a9e8-c10b1f58df0e" satisfied condition "success or failure"
Jul 20 19:06:11.485: INFO: Trying to get logs from node 10.123.236.230 pod downward-api-bfa44a37-4fe9-43f0-a9e8-c10b1f58df0e container dapi-container: <nil>
STEP: delete the pod
Jul 20 19:06:11.551: INFO: Waiting for pod downward-api-bfa44a37-4fe9-43f0-a9e8-c10b1f58df0e to disappear
Jul 20 19:06:11.560: INFO: Pod downward-api-bfa44a37-4fe9-43f0-a9e8-c10b1f58df0e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:06:11.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4436" for this suite.
Jul 20 19:06:17.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:06:18.105: INFO: namespace downward-api-4436 deletion completed in 6.523191582s

â€¢ [SLOW TEST:8.913 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:06:18.107: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2188
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-c4e590df-ddd3-4578-a5fb-f7faa124476e
STEP: Creating a pod to test consume configMaps
Jul 20 19:06:18.372: INFO: Waiting up to 5m0s for pod "pod-configmaps-967ecf22-d120-4635-9c5d-894b816a2277" in namespace "configmap-2188" to be "success or failure"
Jul 20 19:06:18.383: INFO: Pod "pod-configmaps-967ecf22-d120-4635-9c5d-894b816a2277": Phase="Pending", Reason="", readiness=false. Elapsed: 10.592828ms
Jul 20 19:06:20.394: INFO: Pod "pod-configmaps-967ecf22-d120-4635-9c5d-894b816a2277": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021803145s
STEP: Saw pod success
Jul 20 19:06:20.394: INFO: Pod "pod-configmaps-967ecf22-d120-4635-9c5d-894b816a2277" satisfied condition "success or failure"
Jul 20 19:06:20.406: INFO: Trying to get logs from node 10.123.236.230 pod pod-configmaps-967ecf22-d120-4635-9c5d-894b816a2277 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 20 19:06:20.466: INFO: Waiting for pod pod-configmaps-967ecf22-d120-4635-9c5d-894b816a2277 to disappear
Jul 20 19:06:20.479: INFO: Pod pod-configmaps-967ecf22-d120-4635-9c5d-894b816a2277 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:06:20.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2188" for this suite.
Jul 20 19:06:26.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:06:27.075: INFO: namespace configmap-2188 deletion completed in 6.573030619s

â€¢ [SLOW TEST:8.968 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:06:27.075: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5101
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-5101
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-5101
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5101
Jul 20 19:06:27.353: INFO: Found 0 stateful pods, waiting for 1
Jul 20 19:06:37.366: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul 20 19:06:37.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=statefulset-5101 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 20 19:06:37.612: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 20 19:06:37.612: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 20 19:06:37.612: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 20 19:06:37.623: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 20 19:06:47.642: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 20 19:06:47.642: INFO: Waiting for statefulset status.replicas updated to 0
Jul 20 19:06:47.682: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998476s
Jul 20 19:06:48.694: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.990012256s
Jul 20 19:06:49.706: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.978494952s
Jul 20 19:06:50.718: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.965804912s
Jul 20 19:06:51.730: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.953714371s
Jul 20 19:06:52.743: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.942162719s
Jul 20 19:06:53.758: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.928677588s
Jul 20 19:06:54.775: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.913804147s
Jul 20 19:06:55.787: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.897335191s
Jul 20 19:06:56.806: INFO: Verifying statefulset ss doesn't scale past 1 for another 884.855371ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5101
Jul 20 19:06:57.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=statefulset-5101 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 20 19:06:58.306: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 20 19:06:58.306: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 20 19:06:58.306: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 20 19:06:58.318: INFO: Found 1 stateful pods, waiting for 3
Jul 20 19:07:08.336: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 20 19:07:08.336: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 20 19:07:08.336: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul 20 19:07:08.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=statefulset-5101 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 20 19:07:08.676: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 20 19:07:08.676: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 20 19:07:08.676: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 20 19:07:08.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=statefulset-5101 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 20 19:07:08.966: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 20 19:07:08.966: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 20 19:07:08.966: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 20 19:07:08.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=statefulset-5101 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 20 19:07:09.329: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 20 19:07:09.329: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 20 19:07:09.329: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 20 19:07:09.329: INFO: Waiting for statefulset status.replicas updated to 0
Jul 20 19:07:09.337: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul 20 19:07:19.364: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 20 19:07:19.364: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 20 19:07:19.364: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 20 19:07:19.400: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998706s
Jul 20 19:07:20.415: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986530507s
Jul 20 19:07:21.425: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.971370874s
Jul 20 19:07:22.439: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.961067119s
Jul 20 19:07:23.449: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.947884315s
Jul 20 19:07:24.479: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.937893044s
Jul 20 19:07:25.491: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.907819658s
Jul 20 19:07:26.503: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.895551365s
Jul 20 19:07:27.519: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.882929376s
Jul 20 19:07:28.530: INFO: Verifying statefulset ss doesn't scale past 3 for another 867.808056ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5101
Jul 20 19:07:29.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=statefulset-5101 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 20 19:07:29.802: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 20 19:07:29.802: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 20 19:07:29.802: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 20 19:07:29.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=statefulset-5101 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 20 19:07:30.040: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 20 19:07:30.040: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 20 19:07:30.040: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 20 19:07:30.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=statefulset-5101 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 20 19:07:30.282: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 20 19:07:30.282: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 20 19:07:30.282: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 20 19:07:30.282: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jul 20 19:08:00.334: INFO: Deleting all statefulset in ns statefulset-5101
Jul 20 19:08:00.343: INFO: Scaling statefulset ss to 0
Jul 20 19:08:00.369: INFO: Waiting for statefulset status.replicas updated to 0
Jul 20 19:08:00.380: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:08:00.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5101" for this suite.
Jul 20 19:08:08.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:08:08.981: INFO: namespace statefulset-5101 deletion completed in 8.542277493s

â€¢ [SLOW TEST:101.906 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:08:08.982: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7654
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-472r
STEP: Creating a pod to test atomic-volume-subpath
Jul 20 19:08:09.415: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-472r" in namespace "subpath-7654" to be "success or failure"
Jul 20 19:08:09.425: INFO: Pod "pod-subpath-test-downwardapi-472r": Phase="Pending", Reason="", readiness=false. Elapsed: 9.390309ms
Jul 20 19:08:11.438: INFO: Pod "pod-subpath-test-downwardapi-472r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022154572s
Jul 20 19:08:13.449: INFO: Pod "pod-subpath-test-downwardapi-472r": Phase="Running", Reason="", readiness=true. Elapsed: 4.033138388s
Jul 20 19:08:15.460: INFO: Pod "pod-subpath-test-downwardapi-472r": Phase="Running", Reason="", readiness=true. Elapsed: 6.044353338s
Jul 20 19:08:17.472: INFO: Pod "pod-subpath-test-downwardapi-472r": Phase="Running", Reason="", readiness=true. Elapsed: 8.056666386s
Jul 20 19:08:19.485: INFO: Pod "pod-subpath-test-downwardapi-472r": Phase="Running", Reason="", readiness=true. Elapsed: 10.07004666s
Jul 20 19:08:21.496: INFO: Pod "pod-subpath-test-downwardapi-472r": Phase="Running", Reason="", readiness=true. Elapsed: 12.08102541s
Jul 20 19:08:23.508: INFO: Pod "pod-subpath-test-downwardapi-472r": Phase="Running", Reason="", readiness=true. Elapsed: 14.092697628s
Jul 20 19:08:25.519: INFO: Pod "pod-subpath-test-downwardapi-472r": Phase="Running", Reason="", readiness=true. Elapsed: 16.103142693s
Jul 20 19:08:27.530: INFO: Pod "pod-subpath-test-downwardapi-472r": Phase="Running", Reason="", readiness=true. Elapsed: 18.114276684s
Jul 20 19:08:29.540: INFO: Pod "pod-subpath-test-downwardapi-472r": Phase="Running", Reason="", readiness=true. Elapsed: 20.124110504s
Jul 20 19:08:31.550: INFO: Pod "pod-subpath-test-downwardapi-472r": Phase="Running", Reason="", readiness=true. Elapsed: 22.134240002s
Jul 20 19:08:33.560: INFO: Pod "pod-subpath-test-downwardapi-472r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.144109143s
STEP: Saw pod success
Jul 20 19:08:33.560: INFO: Pod "pod-subpath-test-downwardapi-472r" satisfied condition "success or failure"
Jul 20 19:08:33.573: INFO: Trying to get logs from node 10.123.236.227 pod pod-subpath-test-downwardapi-472r container test-container-subpath-downwardapi-472r: <nil>
STEP: delete the pod
Jul 20 19:08:33.720: INFO: Waiting for pod pod-subpath-test-downwardapi-472r to disappear
Jul 20 19:08:33.728: INFO: Pod pod-subpath-test-downwardapi-472r no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-472r
Jul 20 19:08:33.728: INFO: Deleting pod "pod-subpath-test-downwardapi-472r" in namespace "subpath-7654"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:08:33.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7654" for this suite.
Jul 20 19:08:39.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:08:40.389: INFO: namespace subpath-7654 deletion completed in 6.628397115s

â€¢ [SLOW TEST:31.407 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:08:40.390: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-872
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jul 20 19:08:40.659: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e5fafd06-ec06-4122-8613-ed3066879b0c" in namespace "downward-api-872" to be "success or failure"
Jul 20 19:08:40.669: INFO: Pod "downwardapi-volume-e5fafd06-ec06-4122-8613-ed3066879b0c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.97341ms
Jul 20 19:08:42.680: INFO: Pod "downwardapi-volume-e5fafd06-ec06-4122-8613-ed3066879b0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020710195s
Jul 20 19:08:44.695: INFO: Pod "downwardapi-volume-e5fafd06-ec06-4122-8613-ed3066879b0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036206555s
STEP: Saw pod success
Jul 20 19:08:44.696: INFO: Pod "downwardapi-volume-e5fafd06-ec06-4122-8613-ed3066879b0c" satisfied condition "success or failure"
Jul 20 19:08:44.706: INFO: Trying to get logs from node 10.123.236.220 pod downwardapi-volume-e5fafd06-ec06-4122-8613-ed3066879b0c container client-container: <nil>
STEP: delete the pod
Jul 20 19:08:44.792: INFO: Waiting for pod downwardapi-volume-e5fafd06-ec06-4122-8613-ed3066879b0c to disappear
Jul 20 19:08:44.805: INFO: Pod downwardapi-volume-e5fafd06-ec06-4122-8613-ed3066879b0c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:08:44.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-872" for this suite.
Jul 20 19:08:50.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:08:51.395: INFO: namespace downward-api-872 deletion completed in 6.565580566s

â€¢ [SLOW TEST:11.006 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:08:51.396: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-3516
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul 20 19:08:54.771: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:08:54.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3516" for this suite.
Jul 20 19:09:12.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:09:13.544: INFO: namespace replicaset-3516 deletion completed in 18.681809004s

â€¢ [SLOW TEST:22.148 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:09:13.545: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4344
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-jjrk
STEP: Creating a pod to test atomic-volume-subpath
Jul 20 19:09:13.888: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jjrk" in namespace "subpath-4344" to be "success or failure"
Jul 20 19:09:13.900: INFO: Pod "pod-subpath-test-configmap-jjrk": Phase="Pending", Reason="", readiness=false. Elapsed: 11.320996ms
Jul 20 19:09:15.909: INFO: Pod "pod-subpath-test-configmap-jjrk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020355514s
Jul 20 19:09:17.922: INFO: Pod "pod-subpath-test-configmap-jjrk": Phase="Running", Reason="", readiness=true. Elapsed: 4.033179869s
Jul 20 19:09:19.933: INFO: Pod "pod-subpath-test-configmap-jjrk": Phase="Running", Reason="", readiness=true. Elapsed: 6.044454904s
Jul 20 19:09:21.949: INFO: Pod "pod-subpath-test-configmap-jjrk": Phase="Running", Reason="", readiness=true. Elapsed: 8.060803963s
Jul 20 19:09:23.961: INFO: Pod "pod-subpath-test-configmap-jjrk": Phase="Running", Reason="", readiness=true. Elapsed: 10.072561395s
Jul 20 19:09:25.970: INFO: Pod "pod-subpath-test-configmap-jjrk": Phase="Running", Reason="", readiness=true. Elapsed: 12.081765721s
Jul 20 19:09:27.980: INFO: Pod "pod-subpath-test-configmap-jjrk": Phase="Running", Reason="", readiness=true. Elapsed: 14.091729562s
Jul 20 19:09:29.990: INFO: Pod "pod-subpath-test-configmap-jjrk": Phase="Running", Reason="", readiness=true. Elapsed: 16.101313722s
Jul 20 19:09:31.999: INFO: Pod "pod-subpath-test-configmap-jjrk": Phase="Running", Reason="", readiness=true. Elapsed: 18.11076874s
Jul 20 19:09:34.028: INFO: Pod "pod-subpath-test-configmap-jjrk": Phase="Running", Reason="", readiness=true. Elapsed: 20.139091583s
Jul 20 19:09:36.039: INFO: Pod "pod-subpath-test-configmap-jjrk": Phase="Running", Reason="", readiness=true. Elapsed: 22.15006572s
Jul 20 19:09:38.049: INFO: Pod "pod-subpath-test-configmap-jjrk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.160920054s
STEP: Saw pod success
Jul 20 19:09:38.049: INFO: Pod "pod-subpath-test-configmap-jjrk" satisfied condition "success or failure"
Jul 20 19:09:38.060: INFO: Trying to get logs from node 10.123.236.227 pod pod-subpath-test-configmap-jjrk container test-container-subpath-configmap-jjrk: <nil>
STEP: delete the pod
Jul 20 19:09:38.144: INFO: Waiting for pod pod-subpath-test-configmap-jjrk to disappear
Jul 20 19:09:38.154: INFO: Pod pod-subpath-test-configmap-jjrk no longer exists
STEP: Deleting pod pod-subpath-test-configmap-jjrk
Jul 20 19:09:38.154: INFO: Deleting pod "pod-subpath-test-configmap-jjrk" in namespace "subpath-4344"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:09:38.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4344" for this suite.
Jul 20 19:09:44.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:09:44.834: INFO: namespace subpath-4344 deletion completed in 6.650411276s

â€¢ [SLOW TEST:31.289 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:09:44.834: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6912
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 20 19:09:45.573: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul 20 19:09:47.604: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730868985, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730868985, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730868985, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730868985, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 20 19:09:50.650: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:10:03.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6912" for this suite.
Jul 20 19:10:09.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:10:09.769: INFO: namespace webhook-6912 deletion completed in 6.676313306s
STEP: Destroying namespace "webhook-6912-markers" for this suite.
Jul 20 19:10:15.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:10:16.299: INFO: namespace webhook-6912-markers deletion completed in 6.529970372s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:31.562 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:10:16.396: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7497
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7497
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Jul 20 19:10:16.667: INFO: Found 0 stateful pods, waiting for 3
Jul 20 19:10:26.681: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 20 19:10:26.681: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 20 19:10:26.681: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul 20 19:10:26.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=statefulset-7497 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 20 19:10:27.147: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 20 19:10:27.147: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 20 19:10:27.147: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jul 20 19:10:37.225: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul 20 19:10:47.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=statefulset-7497 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 20 19:10:47.668: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 20 19:10:47.668: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 20 19:10:47.668: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 20 19:10:57.728: INFO: Waiting for StatefulSet statefulset-7497/ss2 to complete update
Jul 20 19:10:57.728: INFO: Waiting for Pod statefulset-7497/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 20 19:10:57.728: INFO: Waiting for Pod statefulset-7497/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 20 19:11:07.751: INFO: Waiting for StatefulSet statefulset-7497/ss2 to complete update
Jul 20 19:11:07.752: INFO: Waiting for Pod statefulset-7497/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Jul 20 19:11:17.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=statefulset-7497 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 20 19:11:18.137: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 20 19:11:18.137: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 20 19:11:18.137: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 20 19:11:28.212: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul 20 19:11:38.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=statefulset-7497 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 20 19:11:38.498: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 20 19:11:38.498: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 20 19:11:38.498: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jul 20 19:11:58.563: INFO: Deleting all statefulset in ns statefulset-7497
Jul 20 19:11:58.571: INFO: Scaling statefulset ss2 to 0
Jul 20 19:12:28.765: INFO: Waiting for statefulset status.replicas updated to 0
Jul 20 19:12:28.774: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:12:28.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7497" for this suite.
Jul 20 19:12:36.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:12:37.391: INFO: namespace statefulset-7497 deletion completed in 8.564363647s

â€¢ [SLOW TEST:140.995 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:12:37.391: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9099
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 20 19:12:38.132: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 20 19:12:40.161: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730869158, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730869158, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730869158, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730869158, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 20 19:12:43.218: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jul 20 19:12:43.285: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:12:43.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9099" for this suite.
Jul 20 19:12:49.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:12:49.972: INFO: namespace webhook-9099 deletion completed in 6.60277719s
STEP: Destroying namespace "webhook-9099-markers" for this suite.
Jul 20 19:12:56.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:12:56.699: INFO: namespace webhook-9099-markers deletion completed in 6.727130543s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:19.467 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:12:56.858: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4473
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 19:12:59.215: INFO: Waiting up to 5m0s for pod "client-envvars-7f97d17e-b486-42e8-ae7c-5e70ea9eae8c" in namespace "pods-4473" to be "success or failure"
Jul 20 19:12:59.229: INFO: Pod "client-envvars-7f97d17e-b486-42e8-ae7c-5e70ea9eae8c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.174387ms
Jul 20 19:13:01.240: INFO: Pod "client-envvars-7f97d17e-b486-42e8-ae7c-5e70ea9eae8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025344785s
STEP: Saw pod success
Jul 20 19:13:01.241: INFO: Pod "client-envvars-7f97d17e-b486-42e8-ae7c-5e70ea9eae8c" satisfied condition "success or failure"
Jul 20 19:13:01.249: INFO: Trying to get logs from node 10.123.236.230 pod client-envvars-7f97d17e-b486-42e8-ae7c-5e70ea9eae8c container env3cont: <nil>
STEP: delete the pod
Jul 20 19:13:01.356: INFO: Waiting for pod client-envvars-7f97d17e-b486-42e8-ae7c-5e70ea9eae8c to disappear
Jul 20 19:13:01.365: INFO: Pod client-envvars-7f97d17e-b486-42e8-ae7c-5e70ea9eae8c no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:13:01.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4473" for this suite.
Jul 20 19:13:29.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:13:29.939: INFO: namespace pods-4473 deletion completed in 28.549962626s

â€¢ [SLOW TEST:33.081 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:13:29.941: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2586
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 19:13:30.189: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:13:30.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2586" for this suite.
Jul 20 19:13:36.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:13:36.920: INFO: namespace custom-resource-definition-2586 deletion completed in 6.520985731s

â€¢ [SLOW TEST:6.980 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:13:36.922: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-3478
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
Jul 20 19:13:37.147: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 20 19:14:37.199: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 19:14:37.218: INFO: Starting informer...
STEP: Starting pod...
Jul 20 19:14:37.462: INFO: Pod is running on 10.123.236.227. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jul 20 19:14:37.510: INFO: Pod wasn't evicted. Proceeding
Jul 20 19:14:37.510: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jul 20 19:15:52.554: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:15:52.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-3478" for this suite.
Jul 20 19:16:20.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:16:21.178: INFO: namespace taint-single-pod-3478 deletion completed in 28.601818393s

â€¢ [SLOW TEST:164.256 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:16:21.178: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4431
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 20 19:16:21.481: INFO: Waiting up to 5m0s for pod "pod-a7980c62-b2fb-40ed-85b1-9fa89e8f2b70" in namespace "emptydir-4431" to be "success or failure"
Jul 20 19:16:21.490: INFO: Pod "pod-a7980c62-b2fb-40ed-85b1-9fa89e8f2b70": Phase="Pending", Reason="", readiness=false. Elapsed: 9.105591ms
Jul 20 19:16:23.507: INFO: Pod "pod-a7980c62-b2fb-40ed-85b1-9fa89e8f2b70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02653s
Jul 20 19:16:25.524: INFO: Pod "pod-a7980c62-b2fb-40ed-85b1-9fa89e8f2b70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043537775s
STEP: Saw pod success
Jul 20 19:16:25.524: INFO: Pod "pod-a7980c62-b2fb-40ed-85b1-9fa89e8f2b70" satisfied condition "success or failure"
Jul 20 19:16:25.538: INFO: Trying to get logs from node 10.123.236.227 pod pod-a7980c62-b2fb-40ed-85b1-9fa89e8f2b70 container test-container: <nil>
STEP: delete the pod
Jul 20 19:16:25.645: INFO: Waiting for pod pod-a7980c62-b2fb-40ed-85b1-9fa89e8f2b70 to disappear
Jul 20 19:16:25.654: INFO: Pod pod-a7980c62-b2fb-40ed-85b1-9fa89e8f2b70 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:16:25.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4431" for this suite.
Jul 20 19:16:31.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:16:32.270: INFO: namespace emptydir-4431 deletion completed in 6.593079004s

â€¢ [SLOW TEST:11.092 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:16:32.270: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8316
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 20 19:16:32.507: INFO: Waiting up to 5m0s for pod "pod-e8b5e306-2156-4724-943f-15619e71f903" in namespace "emptydir-8316" to be "success or failure"
Jul 20 19:16:32.515: INFO: Pod "pod-e8b5e306-2156-4724-943f-15619e71f903": Phase="Pending", Reason="", readiness=false. Elapsed: 8.061504ms
Jul 20 19:16:34.526: INFO: Pod "pod-e8b5e306-2156-4724-943f-15619e71f903": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018613916s
Jul 20 19:16:36.536: INFO: Pod "pod-e8b5e306-2156-4724-943f-15619e71f903": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02855197s
STEP: Saw pod success
Jul 20 19:16:36.536: INFO: Pod "pod-e8b5e306-2156-4724-943f-15619e71f903" satisfied condition "success or failure"
Jul 20 19:16:36.545: INFO: Trying to get logs from node 10.123.236.227 pod pod-e8b5e306-2156-4724-943f-15619e71f903 container test-container: <nil>
STEP: delete the pod
Jul 20 19:16:36.604: INFO: Waiting for pod pod-e8b5e306-2156-4724-943f-15619e71f903 to disappear
Jul 20 19:16:36.619: INFO: Pod pod-e8b5e306-2156-4724-943f-15619e71f903 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:16:36.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8316" for this suite.
Jul 20 19:16:42.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:16:43.234: INFO: namespace emptydir-8316 deletion completed in 6.593348811s

â€¢ [SLOW TEST:10.964 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:16:43.234: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-637
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-3641149a-7982-484d-88fc-2cf84f1f3772
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:16:43.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-637" for this suite.
Jul 20 19:16:49.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:16:50.175: INFO: namespace secrets-637 deletion completed in 6.689634445s

â€¢ [SLOW TEST:6.941 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:16:50.178: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-266
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 20 19:16:50.954: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 20 19:16:52.986: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730869410, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730869410, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730869410, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730869410, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 20 19:16:56.037: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 19:16:56.054: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7723-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:16:57.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-266" for this suite.
Jul 20 19:17:07.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:17:07.850: INFO: namespace webhook-266 deletion completed in 10.824179811s
STEP: Destroying namespace "webhook-266-markers" for this suite.
Jul 20 19:17:14.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:17:14.800: INFO: namespace webhook-266-markers deletion completed in 6.949284442s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:24.749 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:17:14.927: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3804
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-d9d91db2-78a6-473a-9e65-f6cc070320d2
STEP: Creating a pod to test consume secrets
Jul 20 19:17:15.264: INFO: Waiting up to 5m0s for pod "pod-secrets-acfd1178-ce2b-4abb-9c5f-970ddf8bc16e" in namespace "secrets-3804" to be "success or failure"
Jul 20 19:17:15.273: INFO: Pod "pod-secrets-acfd1178-ce2b-4abb-9c5f-970ddf8bc16e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.704262ms
Jul 20 19:17:17.283: INFO: Pod "pod-secrets-acfd1178-ce2b-4abb-9c5f-970ddf8bc16e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018973574s
STEP: Saw pod success
Jul 20 19:17:17.283: INFO: Pod "pod-secrets-acfd1178-ce2b-4abb-9c5f-970ddf8bc16e" satisfied condition "success or failure"
Jul 20 19:17:17.295: INFO: Trying to get logs from node 10.123.236.227 pod pod-secrets-acfd1178-ce2b-4abb-9c5f-970ddf8bc16e container secret-volume-test: <nil>
STEP: delete the pod
Jul 20 19:17:17.354: INFO: Waiting for pod pod-secrets-acfd1178-ce2b-4abb-9c5f-970ddf8bc16e to disappear
Jul 20 19:17:17.369: INFO: Pod pod-secrets-acfd1178-ce2b-4abb-9c5f-970ddf8bc16e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:17:17.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3804" for this suite.
Jul 20 19:17:23.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:17:23.999: INFO: namespace secrets-3804 deletion completed in 6.611637616s

â€¢ [SLOW TEST:9.072 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:17:24.000: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8677
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 19:17:24.237: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:17:27.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8677" for this suite.
Jul 20 19:17:33.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:17:34.577: INFO: namespace custom-resource-definition-8677 deletion completed in 6.685785694s

â€¢ [SLOW TEST:10.577 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:17:34.577: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8388
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 19:17:34.849: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 20 19:17:38.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 --namespace=crd-publish-openapi-8388 create -f -'
Jul 20 19:17:39.227: INFO: stderr: ""
Jul 20 19:17:39.227: INFO: stdout: "e2e-test-crd-publish-openapi-3706-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul 20 19:17:39.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 --namespace=crd-publish-openapi-8388 delete e2e-test-crd-publish-openapi-3706-crds test-cr'
Jul 20 19:17:39.416: INFO: stderr: ""
Jul 20 19:17:39.416: INFO: stdout: "e2e-test-crd-publish-openapi-3706-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jul 20 19:17:39.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 --namespace=crd-publish-openapi-8388 apply -f -'
Jul 20 19:17:39.860: INFO: stderr: ""
Jul 20 19:17:39.860: INFO: stdout: "e2e-test-crd-publish-openapi-3706-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul 20 19:17:39.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 --namespace=crd-publish-openapi-8388 delete e2e-test-crd-publish-openapi-3706-crds test-cr'
Jul 20 19:17:39.958: INFO: stderr: ""
Jul 20 19:17:39.958: INFO: stdout: "e2e-test-crd-publish-openapi-3706-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jul 20 19:17:39.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 explain e2e-test-crd-publish-openapi-3706-crds'
Jul 20 19:17:40.254: INFO: stderr: ""
Jul 20 19:17:40.254: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3706-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:17:44.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8388" for this suite.
Jul 20 19:17:50.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:17:50.955: INFO: namespace crd-publish-openapi-8388 deletion completed in 6.607759219s

â€¢ [SLOW TEST:16.378 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:17:50.955: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5869
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jul 20 19:17:51.239: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8a1c170c-f9b3-4716-aab8-f2b15fe45331" in namespace "downward-api-5869" to be "success or failure"
Jul 20 19:17:51.250: INFO: Pod "downwardapi-volume-8a1c170c-f9b3-4716-aab8-f2b15fe45331": Phase="Pending", Reason="", readiness=false. Elapsed: 10.613253ms
Jul 20 19:17:53.261: INFO: Pod "downwardapi-volume-8a1c170c-f9b3-4716-aab8-f2b15fe45331": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021741028s
Jul 20 19:17:55.274: INFO: Pod "downwardapi-volume-8a1c170c-f9b3-4716-aab8-f2b15fe45331": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034847597s
STEP: Saw pod success
Jul 20 19:17:55.274: INFO: Pod "downwardapi-volume-8a1c170c-f9b3-4716-aab8-f2b15fe45331" satisfied condition "success or failure"
Jul 20 19:17:55.289: INFO: Trying to get logs from node 10.123.236.227 pod downwardapi-volume-8a1c170c-f9b3-4716-aab8-f2b15fe45331 container client-container: <nil>
STEP: delete the pod
Jul 20 19:17:55.356: INFO: Waiting for pod downwardapi-volume-8a1c170c-f9b3-4716-aab8-f2b15fe45331 to disappear
Jul 20 19:17:55.365: INFO: Pod downwardapi-volume-8a1c170c-f9b3-4716-aab8-f2b15fe45331 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:17:55.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5869" for this suite.
Jul 20 19:18:01.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:18:01.986: INFO: namespace downward-api-5869 deletion completed in 6.605099387s

â€¢ [SLOW TEST:11.031 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:18:01.988: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2311
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-908ffced-7a14-4f1f-a222-c31c9ff0461d
STEP: Creating a pod to test consume configMaps
Jul 20 19:18:02.268: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-118427d1-5b05-49a2-98e8-b0286235a31f" in namespace "projected-2311" to be "success or failure"
Jul 20 19:18:02.277: INFO: Pod "pod-projected-configmaps-118427d1-5b05-49a2-98e8-b0286235a31f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.648581ms
Jul 20 19:18:04.292: INFO: Pod "pod-projected-configmaps-118427d1-5b05-49a2-98e8-b0286235a31f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024543851s
Jul 20 19:18:06.304: INFO: Pod "pod-projected-configmaps-118427d1-5b05-49a2-98e8-b0286235a31f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03615877s
STEP: Saw pod success
Jul 20 19:18:06.304: INFO: Pod "pod-projected-configmaps-118427d1-5b05-49a2-98e8-b0286235a31f" satisfied condition "success or failure"
Jul 20 19:18:06.314: INFO: Trying to get logs from node 10.123.236.227 pod pod-projected-configmaps-118427d1-5b05-49a2-98e8-b0286235a31f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 20 19:18:06.381: INFO: Waiting for pod pod-projected-configmaps-118427d1-5b05-49a2-98e8-b0286235a31f to disappear
Jul 20 19:18:06.394: INFO: Pod pod-projected-configmaps-118427d1-5b05-49a2-98e8-b0286235a31f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:18:06.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2311" for this suite.
Jul 20 19:18:12.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:18:13.025: INFO: namespace projected-2311 deletion completed in 6.576795165s

â€¢ [SLOW TEST:11.037 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:18:13.025: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-574
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 20 19:18:13.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-574'
Jul 20 19:18:13.392: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 20 19:18:13.392: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Jul 20 19:18:15.433: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-cw785]
Jul 20 19:18:15.433: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-cw785" in namespace "kubectl-574" to be "running and ready"
Jul 20 19:18:15.442: INFO: Pod "e2e-test-httpd-rc-cw785": Phase="Running", Reason="", readiness=true. Elapsed: 8.600515ms
Jul 20 19:18:15.442: INFO: Pod "e2e-test-httpd-rc-cw785" satisfied condition "running and ready"
Jul 20 19:18:15.442: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-cw785]
Jul 20 19:18:15.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 logs rc/e2e-test-httpd-rc --namespace=kubectl-574'
Jul 20 19:18:15.577: INFO: stderr: ""
Jul 20 19:18:15.577: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.30.106.173. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.30.106.173. Set the 'ServerName' directive globally to suppress this message\n[Mon Jul 20 19:18:14.747350 2020] [mpm_event:notice] [pid 1:tid 140335295695720] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Mon Jul 20 19:18:14.747421 2020] [core:notice] [pid 1:tid 140335295695720] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Jul 20 19:18:15.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 delete rc e2e-test-httpd-rc --namespace=kubectl-574'
Jul 20 19:18:15.687: INFO: stderr: ""
Jul 20 19:18:15.687: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:18:15.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-574" for this suite.
Jul 20 19:18:21.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:18:22.469: INFO: namespace kubectl-574 deletion completed in 6.76197297s

â€¢ [SLOW TEST:9.444 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:18:22.471: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1682
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-c5fb552a-fd7c-41ac-95f8-de7dd97f1758
STEP: Creating a pod to test consume configMaps
Jul 20 19:18:22.757: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-af60cf24-6be6-4fff-be6e-92d480310931" in namespace "projected-1682" to be "success or failure"
Jul 20 19:18:22.767: INFO: Pod "pod-projected-configmaps-af60cf24-6be6-4fff-be6e-92d480310931": Phase="Pending", Reason="", readiness=false. Elapsed: 9.23333ms
Jul 20 19:18:24.783: INFO: Pod "pod-projected-configmaps-af60cf24-6be6-4fff-be6e-92d480310931": Phase="Running", Reason="", readiness=true. Elapsed: 2.025512797s
Jul 20 19:18:26.795: INFO: Pod "pod-projected-configmaps-af60cf24-6be6-4fff-be6e-92d480310931": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037483506s
STEP: Saw pod success
Jul 20 19:18:26.795: INFO: Pod "pod-projected-configmaps-af60cf24-6be6-4fff-be6e-92d480310931" satisfied condition "success or failure"
Jul 20 19:18:26.807: INFO: Trying to get logs from node 10.123.236.227 pod pod-projected-configmaps-af60cf24-6be6-4fff-be6e-92d480310931 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 20 19:18:26.860: INFO: Waiting for pod pod-projected-configmaps-af60cf24-6be6-4fff-be6e-92d480310931 to disappear
Jul 20 19:18:26.873: INFO: Pod pod-projected-configmaps-af60cf24-6be6-4fff-be6e-92d480310931 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:18:26.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1682" for this suite.
Jul 20 19:18:32.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:18:33.458: INFO: namespace projected-1682 deletion completed in 6.569160739s

â€¢ [SLOW TEST:10.988 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:18:33.460: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7564
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Jul 20 19:18:33.686: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-291352008 proxy --unix-socket=/tmp/kubectl-proxy-unix525948071/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:18:33.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7564" for this suite.
Jul 20 19:18:39.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:18:40.315: INFO: namespace kubectl-7564 deletion completed in 6.554021257s

â€¢ [SLOW TEST:6.855 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:18:40.316: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8757
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 20 19:18:40.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-8757'
Jul 20 19:18:40.808: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 20 19:18:40.808: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Jul 20 19:18:40.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 delete deployment e2e-test-httpd-deployment --namespace=kubectl-8757'
Jul 20 19:18:40.903: INFO: stderr: ""
Jul 20 19:18:40.903: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:18:40.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8757" for this suite.
Jul 20 19:19:08.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:19:09.549: INFO: namespace kubectl-8757 deletion completed in 28.632002033s

â€¢ [SLOW TEST:29.233 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:19:09.549: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9369
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:19:33.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9369" for this suite.
Jul 20 19:19:39.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:19:39.995: INFO: namespace container-runtime-9369 deletion completed in 6.520752224s

â€¢ [SLOW TEST:30.446 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:19:40.001: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7478
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 20 19:19:40.889: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 20 19:19:42.923: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730869580, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730869580, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730869580, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730869580, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 20 19:19:45.980: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 19:19:45.991: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1538-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:19:46.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7478" for this suite.
Jul 20 19:19:52.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:19:53.417: INFO: namespace webhook-7478 deletion completed in 6.531574493s
STEP: Destroying namespace "webhook-7478-markers" for this suite.
Jul 20 19:19:59.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:20:00.145: INFO: namespace webhook-7478-markers deletion completed in 6.728322489s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:20.260 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:20:00.261: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6784
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Jul 20 19:20:00.517: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jul 20 19:20:00.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 create -f - --namespace=kubectl-6784'
Jul 20 19:20:00.955: INFO: stderr: ""
Jul 20 19:20:00.955: INFO: stdout: "service/redis-slave created\n"
Jul 20 19:20:00.955: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jul 20 19:20:00.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 create -f - --namespace=kubectl-6784'
Jul 20 19:20:01.336: INFO: stderr: ""
Jul 20 19:20:01.336: INFO: stdout: "service/redis-master created\n"
Jul 20 19:20:01.336: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul 20 19:20:01.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 create -f - --namespace=kubectl-6784'
Jul 20 19:20:01.678: INFO: stderr: ""
Jul 20 19:20:01.678: INFO: stdout: "service/frontend created\n"
Jul 20 19:20:01.679: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jul 20 19:20:01.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 create -f - --namespace=kubectl-6784'
Jul 20 19:20:01.971: INFO: stderr: ""
Jul 20 19:20:01.971: INFO: stdout: "deployment.apps/frontend created\n"
Jul 20 19:20:01.971: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 20 19:20:01.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 create -f - --namespace=kubectl-6784'
Jul 20 19:20:02.291: INFO: stderr: ""
Jul 20 19:20:02.291: INFO: stdout: "deployment.apps/redis-master created\n"
Jul 20 19:20:02.291: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jul 20 19:20:02.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 create -f - --namespace=kubectl-6784'
Jul 20 19:20:02.579: INFO: stderr: ""
Jul 20 19:20:02.579: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jul 20 19:20:02.579: INFO: Waiting for all frontend pods to be Running.
Jul 20 19:20:17.631: INFO: Waiting for frontend to serve content.
Jul 20 19:20:17.668: INFO: Trying to add a new entry to the guestbook.
Jul 20 19:20:17.702: INFO: Verifying that added entry can be retrieved.
Jul 20 19:20:17.760: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 20 19:20:22.797: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 20 19:20:27.836: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 20 19:20:32.874: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 20 19:20:37.929: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 20 19:20:42.978: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 20 19:20:48.030: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 20 19:20:53.065: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 20 19:20:58.103: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 20 19:21:03.288: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Jul 20 19:21:08.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 delete --grace-period=0 --force -f - --namespace=kubectl-6784'
Jul 20 19:21:08.635: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 20 19:21:08.635: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jul 20 19:21:08.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 delete --grace-period=0 --force -f - --namespace=kubectl-6784'
Jul 20 19:21:08.870: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 20 19:21:08.870: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul 20 19:21:08.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 delete --grace-period=0 --force -f - --namespace=kubectl-6784'
Jul 20 19:21:09.157: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 20 19:21:09.157: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 20 19:21:09.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 delete --grace-period=0 --force -f - --namespace=kubectl-6784'
Jul 20 19:21:09.261: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 20 19:21:09.261: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 20 19:21:09.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 delete --grace-period=0 --force -f - --namespace=kubectl-6784'
Jul 20 19:21:09.364: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 20 19:21:09.364: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul 20 19:21:09.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 delete --grace-period=0 --force -f - --namespace=kubectl-6784'
Jul 20 19:21:09.488: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 20 19:21:09.488: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:21:09.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6784" for this suite.
Jul 20 19:21:39.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:21:40.104: INFO: namespace kubectl-6784 deletion completed in 30.594551919s

â€¢ [SLOW TEST:99.843 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:21:40.106: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9795
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-ctgc
STEP: Creating a pod to test atomic-volume-subpath
Jul 20 19:21:40.431: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-ctgc" in namespace "subpath-9795" to be "success or failure"
Jul 20 19:21:40.441: INFO: Pod "pod-subpath-test-projected-ctgc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.239889ms
Jul 20 19:21:42.451: INFO: Pod "pod-subpath-test-projected-ctgc": Phase="Running", Reason="", readiness=true. Elapsed: 2.019361011s
Jul 20 19:21:44.462: INFO: Pod "pod-subpath-test-projected-ctgc": Phase="Running", Reason="", readiness=true. Elapsed: 4.030903864s
Jul 20 19:21:46.475: INFO: Pod "pod-subpath-test-projected-ctgc": Phase="Running", Reason="", readiness=true. Elapsed: 6.043312643s
Jul 20 19:21:48.486: INFO: Pod "pod-subpath-test-projected-ctgc": Phase="Running", Reason="", readiness=true. Elapsed: 8.05424992s
Jul 20 19:21:50.496: INFO: Pod "pod-subpath-test-projected-ctgc": Phase="Running", Reason="", readiness=true. Elapsed: 10.064402494s
Jul 20 19:21:52.506: INFO: Pod "pod-subpath-test-projected-ctgc": Phase="Running", Reason="", readiness=true. Elapsed: 12.075112764s
Jul 20 19:21:54.516: INFO: Pod "pod-subpath-test-projected-ctgc": Phase="Running", Reason="", readiness=true. Elapsed: 14.084396369s
Jul 20 19:21:56.525: INFO: Pod "pod-subpath-test-projected-ctgc": Phase="Running", Reason="", readiness=true. Elapsed: 16.094051044s
Jul 20 19:21:58.537: INFO: Pod "pod-subpath-test-projected-ctgc": Phase="Running", Reason="", readiness=true. Elapsed: 18.105249346s
Jul 20 19:22:00.548: INFO: Pod "pod-subpath-test-projected-ctgc": Phase="Running", Reason="", readiness=true. Elapsed: 20.116585899s
Jul 20 19:22:02.565: INFO: Pod "pod-subpath-test-projected-ctgc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.133186976s
STEP: Saw pod success
Jul 20 19:22:02.565: INFO: Pod "pod-subpath-test-projected-ctgc" satisfied condition "success or failure"
Jul 20 19:22:02.574: INFO: Trying to get logs from node 10.123.236.227 pod pod-subpath-test-projected-ctgc container test-container-subpath-projected-ctgc: <nil>
STEP: delete the pod
Jul 20 19:22:02.751: INFO: Waiting for pod pod-subpath-test-projected-ctgc to disappear
Jul 20 19:22:02.761: INFO: Pod pod-subpath-test-projected-ctgc no longer exists
STEP: Deleting pod pod-subpath-test-projected-ctgc
Jul 20 19:22:02.761: INFO: Deleting pod "pod-subpath-test-projected-ctgc" in namespace "subpath-9795"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:22:02.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9795" for this suite.
Jul 20 19:22:08.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:22:09.367: INFO: namespace subpath-9795 deletion completed in 6.581006203s

â€¢ [SLOW TEST:29.261 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:22:09.367: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7126
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jul 20 19:22:09.654: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f35cdc37-de5e-4190-9905-bc67b71cb678" in namespace "downward-api-7126" to be "success or failure"
Jul 20 19:22:09.664: INFO: Pod "downwardapi-volume-f35cdc37-de5e-4190-9905-bc67b71cb678": Phase="Pending", Reason="", readiness=false. Elapsed: 9.682534ms
Jul 20 19:22:11.676: INFO: Pod "downwardapi-volume-f35cdc37-de5e-4190-9905-bc67b71cb678": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021293518s
Jul 20 19:22:13.692: INFO: Pod "downwardapi-volume-f35cdc37-de5e-4190-9905-bc67b71cb678": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037566415s
STEP: Saw pod success
Jul 20 19:22:13.692: INFO: Pod "downwardapi-volume-f35cdc37-de5e-4190-9905-bc67b71cb678" satisfied condition "success or failure"
Jul 20 19:22:13.709: INFO: Trying to get logs from node 10.123.236.227 pod downwardapi-volume-f35cdc37-de5e-4190-9905-bc67b71cb678 container client-container: <nil>
STEP: delete the pod
Jul 20 19:22:13.797: INFO: Waiting for pod downwardapi-volume-f35cdc37-de5e-4190-9905-bc67b71cb678 to disappear
Jul 20 19:22:13.805: INFO: Pod downwardapi-volume-f35cdc37-de5e-4190-9905-bc67b71cb678 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:22:13.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7126" for this suite.
Jul 20 19:22:19.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:22:20.540: INFO: namespace downward-api-7126 deletion completed in 6.717868675s

â€¢ [SLOW TEST:11.173 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:22:20.541: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2519
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jul 20 19:22:20.811: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jul 20 19:22:27.936: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:22:27.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2519" for this suite.
Jul 20 19:22:34.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:22:34.623: INFO: namespace pods-2519 deletion completed in 6.637107902s

â€¢ [SLOW TEST:14.082 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:22:34.623: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8895
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-09dd7e8b-784f-41f7-99f7-5ab1cffb6946 in namespace container-probe-8895
Jul 20 19:22:36.928: INFO: Started pod busybox-09dd7e8b-784f-41f7-99f7-5ab1cffb6946 in namespace container-probe-8895
STEP: checking the pod's current state and verifying that restartCount is present
Jul 20 19:22:36.937: INFO: Initial restart count of pod busybox-09dd7e8b-784f-41f7-99f7-5ab1cffb6946 is 0
Jul 20 19:23:27.288: INFO: Restart count of pod container-probe-8895/busybox-09dd7e8b-784f-41f7-99f7-5ab1cffb6946 is now 1 (50.351533194s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:23:27.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8895" for this suite.
Jul 20 19:23:33.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:23:34.050: INFO: namespace container-probe-8895 deletion completed in 6.686445827s

â€¢ [SLOW TEST:59.427 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:23:34.051: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8220
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-66401d0e-d8ae-4684-b210-c083ac012927
STEP: Creating a pod to test consume secrets
Jul 20 19:23:34.352: INFO: Waiting up to 5m0s for pod "pod-secrets-90714215-138a-4984-9a6e-d6babffe451d" in namespace "secrets-8220" to be "success or failure"
Jul 20 19:23:34.365: INFO: Pod "pod-secrets-90714215-138a-4984-9a6e-d6babffe451d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.473368ms
Jul 20 19:23:36.377: INFO: Pod "pod-secrets-90714215-138a-4984-9a6e-d6babffe451d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024876869s
Jul 20 19:23:38.395: INFO: Pod "pod-secrets-90714215-138a-4984-9a6e-d6babffe451d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042432043s
STEP: Saw pod success
Jul 20 19:23:38.395: INFO: Pod "pod-secrets-90714215-138a-4984-9a6e-d6babffe451d" satisfied condition "success or failure"
Jul 20 19:23:38.408: INFO: Trying to get logs from node 10.123.236.227 pod pod-secrets-90714215-138a-4984-9a6e-d6babffe451d container secret-volume-test: <nil>
STEP: delete the pod
Jul 20 19:23:38.511: INFO: Waiting for pod pod-secrets-90714215-138a-4984-9a6e-d6babffe451d to disappear
Jul 20 19:23:38.520: INFO: Pod pod-secrets-90714215-138a-4984-9a6e-d6babffe451d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:23:38.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8220" for this suite.
Jul 20 19:23:44.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:23:45.168: INFO: namespace secrets-8220 deletion completed in 6.630467957s

â€¢ [SLOW TEST:11.116 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:23:45.170: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9475
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 20 19:23:45.845: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 20 19:23:47.876: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730869825, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730869825, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730869825, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730869825, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 20 19:23:50.962: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:23:51.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9475" for this suite.
Jul 20 19:23:57.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:23:58.210: INFO: namespace webhook-9475 deletion completed in 6.650817615s
STEP: Destroying namespace "webhook-9475-markers" for this suite.
Jul 20 19:24:04.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:24:04.998: INFO: namespace webhook-9475-markers deletion completed in 6.787222494s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:20.020 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:24:05.190: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-132
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-132
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-132
STEP: Creating statefulset with conflicting port in namespace statefulset-132
STEP: Waiting until pod test-pod will start running in namespace statefulset-132
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-132
Jul 20 19:24:07.580: INFO: Observed stateful pod in namespace: statefulset-132, name: ss-0, uid: 90b7ea57-b2a0-4ea5-9fa8-4bc0c0e1dde6, status phase: Pending. Waiting for statefulset controller to delete.
Jul 20 19:24:07.631: INFO: Observed stateful pod in namespace: statefulset-132, name: ss-0, uid: 90b7ea57-b2a0-4ea5-9fa8-4bc0c0e1dde6, status phase: Failed. Waiting for statefulset controller to delete.
Jul 20 19:24:07.644: INFO: Observed stateful pod in namespace: statefulset-132, name: ss-0, uid: 90b7ea57-b2a0-4ea5-9fa8-4bc0c0e1dde6, status phase: Failed. Waiting for statefulset controller to delete.
Jul 20 19:24:07.651: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-132
STEP: Removing pod with conflicting port in namespace statefulset-132
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-132 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jul 20 19:24:09.711: INFO: Deleting all statefulset in ns statefulset-132
Jul 20 19:24:09.720: INFO: Scaling statefulset ss to 0
Jul 20 19:24:19.763: INFO: Waiting for statefulset status.replicas updated to 0
Jul 20 19:24:19.772: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:24:19.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-132" for this suite.
Jul 20 19:24:27.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:24:28.431: INFO: namespace statefulset-132 deletion completed in 8.588691412s

â€¢ [SLOW TEST:23.241 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:24:28.432: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-9146
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
Jul 20 19:24:28.666: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 20 19:25:28.721: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 19:25:28.737: INFO: Starting informer...
STEP: Starting pods...
Jul 20 19:25:28.985: INFO: Pod1 is running on 10.123.236.227. Tainting Node
Jul 20 19:25:31.247: INFO: Pod2 is running on 10.123.236.227. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jul 20 19:25:44.693: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jul 20 19:26:04.748: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:26:04.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-9146" for this suite.
Jul 20 19:26:10.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:26:11.456: INFO: namespace taint-multiple-pods-9146 deletion completed in 6.65918618s

â€¢ [SLOW TEST:103.024 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:26:11.457: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-162
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:27:11.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-162" for this suite.
Jul 20 19:27:23.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:27:24.387: INFO: namespace container-probe-162 deletion completed in 12.598633726s

â€¢ [SLOW TEST:72.931 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:27:24.388: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-128
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Jul 20 19:27:24.645: INFO: PodSpec: initContainers in spec.initContainers
Jul 20 19:28:11.549: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-5a1879e5-2239-4a18-92a3-0f0c11c81dfa", GenerateName:"", Namespace:"init-container-128", SelfLink:"/api/v1/namespaces/init-container-128/pods/pod-init-5a1879e5-2239-4a18-92a3-0f0c11c81dfa", UID:"f00314fb-e4e4-4944-ba6b-8aa6a80a9b84", ResourceVersion:"35534", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63730870044, loc:(*time.Location)(0x78a7940)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"645089442"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-b4q5k", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0048ba000), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-b4q5k", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-b4q5k", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-b4q5k", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc006d20088), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.123.236.227", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00301e000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006d20110)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006d20130)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc006d20138), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc006d2013c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870044, loc:(*time.Location)(0x78a7940)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870044, loc:(*time.Location)(0x78a7940)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870044, loc:(*time.Location)(0x78a7940)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870044, loc:(*time.Location)(0x78a7940)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.123.236.227", PodIP:"172.30.106.187", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.106.187"}}, StartTime:(*v1.Time)(0xc002884300), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0008fc070)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0008fc0e0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://c634a9c1d23ea475e574629e031bd9f933f4175b36815811395fb440d7985196", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002885aa0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002885a60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc006d201b4)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:28:11.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-128" for this suite.
Jul 20 19:28:39.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:28:40.199: INFO: namespace init-container-128 deletion completed in 28.622914703s

â€¢ [SLOW TEST:75.811 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:28:40.199: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5813
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jul 20 19:29:20.551: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0720 19:29:20.551069      25 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 20 19:29:20.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5813" for this suite.
Jul 20 19:29:28.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:29:29.091: INFO: namespace gc-5813 deletion completed in 8.526508484s

â€¢ [SLOW TEST:48.892 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:29:29.091: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-552
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul 20 19:29:29.344: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-552 /api/v1/namespaces/watch-552/configmaps/e2e-watch-test-watch-closed a1a4326e-912a-4066-8b56-e10326125054 35935 0 2020-07-20 19:29:29 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 20 19:29:29.344: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-552 /api/v1/namespaces/watch-552/configmaps/e2e-watch-test-watch-closed a1a4326e-912a-4066-8b56-e10326125054 35936 0 2020-07-20 19:29:29 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul 20 19:29:29.388: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-552 /api/v1/namespaces/watch-552/configmaps/e2e-watch-test-watch-closed a1a4326e-912a-4066-8b56-e10326125054 35937 0 2020-07-20 19:29:29 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 20 19:29:29.388: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-552 /api/v1/namespaces/watch-552/configmaps/e2e-watch-test-watch-closed a1a4326e-912a-4066-8b56-e10326125054 35938 0 2020-07-20 19:29:29 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:29:29.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-552" for this suite.
Jul 20 19:29:35.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:29:36.019: INFO: namespace watch-552 deletion completed in 6.614736234s

â€¢ [SLOW TEST:6.928 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:29:36.020: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-25
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-7a01085c-b77c-442e-bde6-889f7fbdf72b
STEP: Creating a pod to test consume secrets
Jul 20 19:29:36.302: INFO: Waiting up to 5m0s for pod "pod-secrets-864d300d-d01a-42ed-9e55-36333b782de1" in namespace "secrets-25" to be "success or failure"
Jul 20 19:29:36.318: INFO: Pod "pod-secrets-864d300d-d01a-42ed-9e55-36333b782de1": Phase="Pending", Reason="", readiness=false. Elapsed: 15.752718ms
Jul 20 19:29:38.332: INFO: Pod "pod-secrets-864d300d-d01a-42ed-9e55-36333b782de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029763754s
STEP: Saw pod success
Jul 20 19:29:38.332: INFO: Pod "pod-secrets-864d300d-d01a-42ed-9e55-36333b782de1" satisfied condition "success or failure"
Jul 20 19:29:38.344: INFO: Trying to get logs from node 10.123.236.227 pod pod-secrets-864d300d-d01a-42ed-9e55-36333b782de1 container secret-volume-test: <nil>
STEP: delete the pod
Jul 20 19:29:38.428: INFO: Waiting for pod pod-secrets-864d300d-d01a-42ed-9e55-36333b782de1 to disappear
Jul 20 19:29:38.440: INFO: Pod pod-secrets-864d300d-d01a-42ed-9e55-36333b782de1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:29:38.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-25" for this suite.
Jul 20 19:29:44.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:29:45.065: INFO: namespace secrets-25 deletion completed in 6.606898925s

â€¢ [SLOW TEST:9.046 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:29:45.066: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6697
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-6697
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6697 to expose endpoints map[]
Jul 20 19:29:45.352: INFO: successfully validated that service multi-endpoint-test in namespace services-6697 exposes endpoints map[] (10.610224ms elapsed)
STEP: Creating pod pod1 in namespace services-6697
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6697 to expose endpoints map[pod1:[100]]
Jul 20 19:29:47.474: INFO: successfully validated that service multi-endpoint-test in namespace services-6697 exposes endpoints map[pod1:[100]] (2.086656713s elapsed)
STEP: Creating pod pod2 in namespace services-6697
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6697 to expose endpoints map[pod1:[100] pod2:[101]]
Jul 20 19:29:49.580: INFO: successfully validated that service multi-endpoint-test in namespace services-6697 exposes endpoints map[pod1:[100] pod2:[101]] (2.093435285s elapsed)
STEP: Deleting pod pod1 in namespace services-6697
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6697 to expose endpoints map[pod2:[101]]
Jul 20 19:29:49.617: INFO: successfully validated that service multi-endpoint-test in namespace services-6697 exposes endpoints map[pod2:[101]] (19.062951ms elapsed)
STEP: Deleting pod pod2 in namespace services-6697
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6697 to expose endpoints map[]
Jul 20 19:29:50.665: INFO: successfully validated that service multi-endpoint-test in namespace services-6697 exposes endpoints map[] (1.02467175s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:29:50.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6697" for this suite.
Jul 20 19:29:56.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:29:57.469: INFO: namespace services-6697 deletion completed in 6.67150379s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:12.403 seconds]
[sig-network] Services
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:29:57.470: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5515
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-5515
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 20 19:29:57.709: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 20 19:30:22.040: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.106.152:8080/dial?request=hostName&protocol=udp&host=172.30.176.27&port=8081&tries=1'] Namespace:pod-network-test-5515 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 20 19:30:22.041: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 19:30:22.617: INFO: Waiting for endpoints: map[]
Jul 20 19:30:22.628: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.106.152:8080/dial?request=hostName&protocol=udp&host=172.30.106.131&port=8081&tries=1'] Namespace:pod-network-test-5515 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 20 19:30:22.628: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 19:30:22.769: INFO: Waiting for endpoints: map[]
Jul 20 19:30:22.778: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.106.152:8080/dial?request=hostName&protocol=udp&host=172.30.159.76&port=8081&tries=1'] Namespace:pod-network-test-5515 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 20 19:30:22.778: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 19:30:23.152: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:30:23.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5515" for this suite.
Jul 20 19:30:35.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:30:35.936: INFO: namespace pod-network-test-5515 deletion completed in 12.767932164s

â€¢ [SLOW TEST:38.465 seconds]
[sig-network] Networking
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:30:35.936: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-4906
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:30:38.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4906" for this suite.
Jul 20 19:30:44.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:30:45.144: INFO: namespace emptydir-wrapper-4906 deletion completed in 6.67266342s

â€¢ [SLOW TEST:9.208 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:30:45.144: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-882
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6111
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9822
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:31:01.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-882" for this suite.
Jul 20 19:31:08.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:31:08.639: INFO: namespace namespaces-882 deletion completed in 6.629673826s
STEP: Destroying namespace "nsdeletetest-6111" for this suite.
Jul 20 19:31:08.667: INFO: Namespace nsdeletetest-6111 was already deleted
STEP: Destroying namespace "nsdeletetest-9822" for this suite.
Jul 20 19:31:14.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:31:15.209: INFO: namespace nsdeletetest-9822 deletion completed in 6.541891857s

â€¢ [SLOW TEST:30.064 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:31:15.210: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-6858
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Jul 20 19:31:15.448: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Jul 20 19:31:16.023: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jul 20 19:31:18.168: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870276, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870276, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870276, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870276, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 20 19:31:20.179: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870276, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870276, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870276, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870276, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 20 19:31:22.178: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870276, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870276, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870276, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870276, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 20 19:31:24.177: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870276, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870276, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870276, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870276, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 20 19:31:27.556: INFO: Waited 1.365952642s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:31:28.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-6858" for this suite.
Jul 20 19:31:34.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:31:35.170: INFO: namespace aggregator-6858 deletion completed in 6.573095556s

â€¢ [SLOW TEST:19.960 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:31:35.171: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1509
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:31:52.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1509" for this suite.
Jul 20 19:31:58.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:31:59.505: INFO: namespace resourcequota-1509 deletion completed in 6.749990679s

â€¢ [SLOW TEST:24.333 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:31:59.505: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4988
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jul 20 19:31:59.788: INFO: Waiting up to 5m0s for pod "downwardapi-volume-25bee7b6-89b4-44b8-98ec-b708f77666e7" in namespace "projected-4988" to be "success or failure"
Jul 20 19:31:59.804: INFO: Pod "downwardapi-volume-25bee7b6-89b4-44b8-98ec-b708f77666e7": Phase="Pending", Reason="", readiness=false. Elapsed: 16.16203ms
Jul 20 19:32:01.815: INFO: Pod "downwardapi-volume-25bee7b6-89b4-44b8-98ec-b708f77666e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026534436s
Jul 20 19:32:03.827: INFO: Pod "downwardapi-volume-25bee7b6-89b4-44b8-98ec-b708f77666e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039002807s
STEP: Saw pod success
Jul 20 19:32:03.827: INFO: Pod "downwardapi-volume-25bee7b6-89b4-44b8-98ec-b708f77666e7" satisfied condition "success or failure"
Jul 20 19:32:03.843: INFO: Trying to get logs from node 10.123.236.227 pod downwardapi-volume-25bee7b6-89b4-44b8-98ec-b708f77666e7 container client-container: <nil>
STEP: delete the pod
Jul 20 19:32:03.941: INFO: Waiting for pod downwardapi-volume-25bee7b6-89b4-44b8-98ec-b708f77666e7 to disappear
Jul 20 19:32:03.949: INFO: Pod downwardapi-volume-25bee7b6-89b4-44b8-98ec-b708f77666e7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:32:03.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4988" for this suite.
Jul 20 19:32:10.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:32:10.491: INFO: namespace projected-4988 deletion completed in 6.527604437s

â€¢ [SLOW TEST:10.986 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:32:10.496: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6573
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-b5eca6b3-b1c0-44d0-90ed-73b20aba48ca
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:32:10.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6573" for this suite.
Jul 20 19:32:16.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:32:17.247: INFO: namespace configmap-6573 deletion completed in 6.492886854s

â€¢ [SLOW TEST:6.751 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:32:17.248: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3860
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-4a7121bf-c406-416c-97de-64e1feea7486
STEP: Creating a pod to test consume configMaps
Jul 20 19:32:17.509: INFO: Waiting up to 5m0s for pod "pod-configmaps-3a2da4b9-7a19-49aa-8a29-c77244a66c87" in namespace "configmap-3860" to be "success or failure"
Jul 20 19:32:17.518: INFO: Pod "pod-configmaps-3a2da4b9-7a19-49aa-8a29-c77244a66c87": Phase="Pending", Reason="", readiness=false. Elapsed: 9.323035ms
Jul 20 19:32:19.529: INFO: Pod "pod-configmaps-3a2da4b9-7a19-49aa-8a29-c77244a66c87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020492637s
STEP: Saw pod success
Jul 20 19:32:19.529: INFO: Pod "pod-configmaps-3a2da4b9-7a19-49aa-8a29-c77244a66c87" satisfied condition "success or failure"
Jul 20 19:32:19.541: INFO: Trying to get logs from node 10.123.236.227 pod pod-configmaps-3a2da4b9-7a19-49aa-8a29-c77244a66c87 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 20 19:32:19.602: INFO: Waiting for pod pod-configmaps-3a2da4b9-7a19-49aa-8a29-c77244a66c87 to disappear
Jul 20 19:32:19.613: INFO: Pod pod-configmaps-3a2da4b9-7a19-49aa-8a29-c77244a66c87 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:32:19.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3860" for this suite.
Jul 20 19:32:25.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:32:26.340: INFO: namespace configmap-3860 deletion completed in 6.703563733s

â€¢ [SLOW TEST:9.093 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:32:26.340: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2570
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Jul 20 19:32:26.596: INFO: Waiting up to 5m0s for pod "var-expansion-c5833d37-9f91-458f-9a0a-3e167de8cf91" in namespace "var-expansion-2570" to be "success or failure"
Jul 20 19:32:26.604: INFO: Pod "var-expansion-c5833d37-9f91-458f-9a0a-3e167de8cf91": Phase="Pending", Reason="", readiness=false. Elapsed: 8.620167ms
Jul 20 19:32:28.615: INFO: Pod "var-expansion-c5833d37-9f91-458f-9a0a-3e167de8cf91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019743072s
STEP: Saw pod success
Jul 20 19:32:28.616: INFO: Pod "var-expansion-c5833d37-9f91-458f-9a0a-3e167de8cf91" satisfied condition "success or failure"
Jul 20 19:32:28.625: INFO: Trying to get logs from node 10.123.236.227 pod var-expansion-c5833d37-9f91-458f-9a0a-3e167de8cf91 container dapi-container: <nil>
STEP: delete the pod
Jul 20 19:32:28.703: INFO: Waiting for pod var-expansion-c5833d37-9f91-458f-9a0a-3e167de8cf91 to disappear
Jul 20 19:32:28.714: INFO: Pod var-expansion-c5833d37-9f91-458f-9a0a-3e167de8cf91 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:32:28.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2570" for this suite.
Jul 20 19:32:34.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:32:35.407: INFO: namespace var-expansion-2570 deletion completed in 6.673236034s

â€¢ [SLOW TEST:9.067 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:32:35.413: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2343
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:32:46.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2343" for this suite.
Jul 20 19:32:52.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:32:53.389: INFO: namespace resourcequota-2343 deletion completed in 6.570057314s

â€¢ [SLOW TEST:17.976 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:32:53.390: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3929
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jul 20 19:32:53.649: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 20 19:32:53.697: INFO: Waiting for terminating namespaces to be deleted...
Jul 20 19:32:53.717: INFO: 
Logging pods the kubelet thinks is on node 10.123.236.220 before test
Jul 20 19:32:53.803: INFO: ibm-keepalived-watcher-69dl6 from kube-system started at 2020-07-20 16:38:01 +0000 UTC (1 container statuses recorded)
Jul 20 19:32:53.803: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jul 20 19:32:53.803: INFO: ibm-storage-watcher-5955dd9995-4kmxp from kube-system started at 2020-07-20 19:14:37 +0000 UTC (1 container statuses recorded)
Jul 20 19:32:53.803: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jul 20 19:32:53.803: INFO: metrics-server-7d665f557c-bxtgh from kube-system started at 2020-07-20 19:14:37 +0000 UTC (2 container statuses recorded)
Jul 20 19:32:53.803: INFO: 	Container metrics-server ready: true, restart count 0
Jul 20 19:32:53.803: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jul 20 19:32:53.803: INFO: ibm-master-proxy-static-10.123.236.220 from kube-system started at 2020-07-20 16:37:55 +0000 UTC (2 container statuses recorded)
Jul 20 19:32:53.803: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jul 20 19:32:53.803: INFO: 	Container pause ready: true, restart count 0
Jul 20 19:32:53.803: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-07-20 16:40:09 +0000 UTC (1 container statuses recorded)
Jul 20 19:32:53.803: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jul 20 19:32:53.803: INFO: sonobuoy-systemd-logs-daemon-set-2fac7b176f1f4511-zngst from sonobuoy started at 2020-07-20 18:23:20 +0000 UTC (2 container statuses recorded)
Jul 20 19:32:53.803: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul 20 19:32:53.803: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 20 19:32:53.803: INFO: coredns-54f55c7c7c-gw4fd from kube-system started at 2020-07-20 17:02:33 +0000 UTC (1 container statuses recorded)
Jul 20 19:32:53.803: INFO: 	Container coredns ready: true, restart count 0
Jul 20 19:32:53.803: INFO: public-crbsas6ivf0c8md2c63apg-alb1-5f96ffbc77-c5tzp from kube-system started at 2020-07-20 16:39:13 +0000 UTC (4 container statuses recorded)
Jul 20 19:32:53.803: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Jul 20 19:32:53.803: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Jul 20 19:32:53.803: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Jul 20 19:32:53.803: INFO: 	Container nginx-ingress ready: true, restart count 0
Jul 20 19:32:53.803: INFO: vpn-f66c45467-jtzph from kube-system started at 2020-07-20 19:14:37 +0000 UTC (1 container statuses recorded)
Jul 20 19:32:53.803: INFO: 	Container vpn ready: true, restart count 0
Jul 20 19:32:53.803: INFO: ibm-cloud-provider-ip-149-81-145-66-6c99b7f77d-6xqwt from ibm-system started at 2020-07-20 16:38:20 +0000 UTC (1 container statuses recorded)
Jul 20 19:32:53.803: INFO: 	Container ibm-cloud-provider-ip-149-81-145-66 ready: true, restart count 0
Jul 20 19:32:53.803: INFO: coredns-autoscaler-6bc79bb9db-7vjd7 from kube-system started at 2020-07-20 19:14:37 +0000 UTC (1 container statuses recorded)
Jul 20 19:32:53.803: INFO: 	Container autoscaler ready: true, restart count 0
Jul 20 19:32:53.803: INFO: calico-node-6z6jp from kube-system started at 2020-07-20 16:38:01 +0000 UTC (1 container statuses recorded)
Jul 20 19:32:53.803: INFO: 	Container calico-node ready: true, restart count 0
Jul 20 19:32:53.803: INFO: 
Logging pods the kubelet thinks is on node 10.123.236.227 before test
Jul 20 19:32:53.841: INFO: addon-catalog-source-c4rf5 from ibm-system started at 2020-07-20 16:37:18 +0000 UTC (1 container statuses recorded)
Jul 20 19:32:53.841: INFO: 	Container configmap-registry-server ready: true, restart count 0
Jul 20 19:32:53.841: INFO: ibm-master-proxy-static-10.123.236.227 from kube-system started at 2020-07-20 16:36:44 +0000 UTC (2 container statuses recorded)
Jul 20 19:32:53.841: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jul 20 19:32:53.841: INFO: 	Container pause ready: true, restart count 0
Jul 20 19:32:53.841: INFO: ibm-keepalived-watcher-m9bwq from kube-system started at 2020-07-20 16:36:51 +0000 UTC (1 container statuses recorded)
Jul 20 19:32:53.841: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jul 20 19:32:53.841: INFO: calico-node-85dkz from kube-system started at 2020-07-20 16:36:51 +0000 UTC (1 container statuses recorded)
Jul 20 19:32:53.841: INFO: 	Container calico-node ready: true, restart count 0
Jul 20 19:32:53.841: INFO: sonobuoy-systemd-logs-daemon-set-2fac7b176f1f4511-mktvj from sonobuoy started at 2020-07-20 18:23:20 +0000 UTC (2 container statuses recorded)
Jul 20 19:32:53.841: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul 20 19:32:53.841: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 20 19:32:53.841: INFO: 
Logging pods the kubelet thinks is on node 10.123.236.230 before test
Jul 20 19:32:53.920: INFO: ibm-cloud-provider-ip-149-81-145-66-6c99b7f77d-4rckp from ibm-system started at 2020-07-20 16:38:20 +0000 UTC (1 container statuses recorded)
Jul 20 19:32:53.920: INFO: 	Container ibm-cloud-provider-ip-149-81-145-66 ready: true, restart count 0
Jul 20 19:32:53.920: INFO: sonobuoy from sonobuoy started at 2020-07-20 18:23:14 +0000 UTC (1 container statuses recorded)
Jul 20 19:32:53.920: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 20 19:32:53.920: INFO: sonobuoy-e2e-job-0f6e1d1f061d4a08 from sonobuoy started at 2020-07-20 18:23:20 +0000 UTC (2 container statuses recorded)
Jul 20 19:32:53.920: INFO: 	Container e2e ready: true, restart count 0
Jul 20 19:32:53.920: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 20 19:32:53.920: INFO: sonobuoy-systemd-logs-daemon-set-2fac7b176f1f4511-xh4r9 from sonobuoy started at 2020-07-20 18:23:20 +0000 UTC (2 container statuses recorded)
Jul 20 19:32:53.920: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul 20 19:32:53.920: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 20 19:32:53.920: INFO: ibm-file-plugin-586dc4596c-f2b5n from kube-system started at 2020-07-20 19:14:37 +0000 UTC (1 container statuses recorded)
Jul 20 19:32:53.920: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jul 20 19:32:53.920: INFO: calico-node-wpsss from kube-system started at 2020-07-20 16:38:02 +0000 UTC (1 container statuses recorded)
Jul 20 19:32:53.920: INFO: 	Container calico-node ready: true, restart count 0
Jul 20 19:32:53.920: INFO: ibm-keepalived-watcher-fhgvr from kube-system started at 2020-07-20 16:38:02 +0000 UTC (1 container statuses recorded)
Jul 20 19:32:53.920: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jul 20 19:32:53.920: INFO: public-crbsas6ivf0c8md2c63apg-alb1-5f96ffbc77-lpqp5 from kube-system started at 2020-07-20 16:39:13 +0000 UTC (4 container statuses recorded)
Jul 20 19:32:53.920: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Jul 20 19:32:53.920: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Jul 20 19:32:53.920: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Jul 20 19:32:53.920: INFO: 	Container nginx-ingress ready: true, restart count 0
Jul 20 19:32:53.920: INFO: kubernetes-dashboard-984c5c57-z4jh4 from kube-system started at 2020-07-20 19:25:31 +0000 UTC (1 container statuses recorded)
Jul 20 19:32:53.920: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul 20 19:32:53.920: INFO: coredns-54f55c7c7c-xgw9w from kube-system started at 2020-07-20 17:02:33 +0000 UTC (1 container statuses recorded)
Jul 20 19:32:53.920: INFO: 	Container coredns ready: true, restart count 0
Jul 20 19:32:53.920: INFO: coredns-54f55c7c7c-tp7r8 from kube-system started at 2020-07-20 19:14:37 +0000 UTC (1 container statuses recorded)
Jul 20 19:32:53.920: INFO: 	Container coredns ready: true, restart count 0
Jul 20 19:32:53.920: INFO: olm-operator-787498c9b7-nhx76 from ibm-system started at 2020-07-20 19:14:37 +0000 UTC (1 container statuses recorded)
Jul 20 19:32:53.920: INFO: 	Container olm-operator ready: true, restart count 0
Jul 20 19:32:53.920: INFO: dashboard-metrics-scraper-5789d44f58-kdvwc from kube-system started at 2020-07-20 19:14:37 +0000 UTC (1 container statuses recorded)
Jul 20 19:32:53.920: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jul 20 19:32:53.920: INFO: catalog-operator-67646bfcdb-vdbxg from ibm-system started at 2020-07-20 19:25:31 +0000 UTC (1 container statuses recorded)
Jul 20 19:32:53.920: INFO: 	Container catalog-operator ready: true, restart count 0
Jul 20 19:32:53.920: INFO: ibm-master-proxy-static-10.123.236.230 from kube-system started at 2020-07-20 16:38:01 +0000 UTC (2 container statuses recorded)
Jul 20 19:32:53.920: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jul 20 19:32:53.920: INFO: 	Container pause ready: true, restart count 0
Jul 20 19:32:53.920: INFO: calico-kube-controllers-69c4db5c8d-pff85 from kube-system started at 2020-07-20 19:25:31 +0000 UTC (1 container statuses recorded)
Jul 20 19:32:53.920: INFO: 	Container calico-kube-controllers ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-ea6a3566-fff3-442e-b26e-f2edb7f4961e 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-ea6a3566-fff3-442e-b26e-f2edb7f4961e off the node 10.123.236.227
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ea6a3566-fff3-442e-b26e-f2edb7f4961e
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:33:10.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3929" for this suite.
Jul 20 19:33:28.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:33:28.907: INFO: namespace sched-pred-3929 deletion completed in 18.663803568s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:35.518 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:33:28.907: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2212
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Jul 20 19:33:29.145: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:33:33.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2212" for this suite.
Jul 20 19:33:39.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:33:40.170: INFO: namespace init-container-2212 deletion completed in 6.898699944s

â€¢ [SLOW TEST:11.263 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:33:40.173: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7024
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 19:33:58.516: INFO: Container started at 2020-07-20 19:33:41 +0000 UTC, pod became ready at 2020-07-20 19:33:57 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:33:58.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7024" for this suite.
Jul 20 19:34:26.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:34:27.069: INFO: namespace container-probe-7024 deletion completed in 28.53895099s

â€¢ [SLOW TEST:46.896 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:34:27.070: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9967
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:34:29.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9967" for this suite.
Jul 20 19:35:15.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:35:16.015: INFO: namespace kubelet-test-9967 deletion completed in 46.585539484s

â€¢ [SLOW TEST:48.946 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:35:16.016: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4659
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 19:35:16.324: INFO: (0) /api/v1/nodes/10.123.236.220:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 62.657869ms)
Jul 20 19:35:16.345: INFO: (1) /api/v1/nodes/10.123.236.220:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 20.650304ms)
Jul 20 19:35:16.364: INFO: (2) /api/v1/nodes/10.123.236.220:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 18.914236ms)
Jul 20 19:35:16.383: INFO: (3) /api/v1/nodes/10.123.236.220:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 19.367904ms)
Jul 20 19:35:16.407: INFO: (4) /api/v1/nodes/10.123.236.220:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 23.687092ms)
Jul 20 19:35:16.427: INFO: (5) /api/v1/nodes/10.123.236.220:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 20.150317ms)
Jul 20 19:35:16.446: INFO: (6) /api/v1/nodes/10.123.236.220:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 19.35756ms)
Jul 20 19:35:16.463: INFO: (7) /api/v1/nodes/10.123.236.220:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 16.944188ms)
Jul 20 19:35:16.480: INFO: (8) /api/v1/nodes/10.123.236.220:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 16.736615ms)
Jul 20 19:35:16.499: INFO: (9) /api/v1/nodes/10.123.236.220:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 18.622816ms)
Jul 20 19:35:16.524: INFO: (10) /api/v1/nodes/10.123.236.220:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 24.695799ms)
Jul 20 19:35:16.541: INFO: (11) /api/v1/nodes/10.123.236.220:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 16.809055ms)
Jul 20 19:35:16.558: INFO: (12) /api/v1/nodes/10.123.236.220:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 17.846494ms)
Jul 20 19:35:16.577: INFO: (13) /api/v1/nodes/10.123.236.220:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 18.481244ms)
Jul 20 19:35:16.594: INFO: (14) /api/v1/nodes/10.123.236.220:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 17.072155ms)
Jul 20 19:35:16.615: INFO: (15) /api/v1/nodes/10.123.236.220:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 20.791991ms)
Jul 20 19:35:16.633: INFO: (16) /api/v1/nodes/10.123.236.220:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 17.941006ms)
Jul 20 19:35:16.648: INFO: (17) /api/v1/nodes/10.123.236.220:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 15.350011ms)
Jul 20 19:35:16.670: INFO: (18) /api/v1/nodes/10.123.236.220:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 21.772717ms)
Jul 20 19:35:16.688: INFO: (19) /api/v1/nodes/10.123.236.220:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 17.628463ms)
[AfterEach] version v1
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:35:16.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4659" for this suite.
Jul 20 19:35:22.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:35:23.355: INFO: namespace proxy-4659 deletion completed in 6.653736109s

â€¢ [SLOW TEST:7.340 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:35:23.356: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1704
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jul 20 19:35:23.625: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e7ce610-6da4-4f60-9a57-368bdcc61f7f" in namespace "downward-api-1704" to be "success or failure"
Jul 20 19:35:23.635: INFO: Pod "downwardapi-volume-4e7ce610-6da4-4f60-9a57-368bdcc61f7f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.504578ms
Jul 20 19:35:25.646: INFO: Pod "downwardapi-volume-4e7ce610-6da4-4f60-9a57-368bdcc61f7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020835395s
STEP: Saw pod success
Jul 20 19:35:25.646: INFO: Pod "downwardapi-volume-4e7ce610-6da4-4f60-9a57-368bdcc61f7f" satisfied condition "success or failure"
Jul 20 19:35:25.656: INFO: Trying to get logs from node 10.123.236.227 pod downwardapi-volume-4e7ce610-6da4-4f60-9a57-368bdcc61f7f container client-container: <nil>
STEP: delete the pod
Jul 20 19:35:25.721: INFO: Waiting for pod downwardapi-volume-4e7ce610-6da4-4f60-9a57-368bdcc61f7f to disappear
Jul 20 19:35:25.731: INFO: Pod downwardapi-volume-4e7ce610-6da4-4f60-9a57-368bdcc61f7f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:35:25.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1704" for this suite.
Jul 20 19:35:31.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:35:32.300: INFO: namespace downward-api-1704 deletion completed in 6.550622391s

â€¢ [SLOW TEST:8.945 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:35:32.302: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9323
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jul 20 19:35:38.655: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0720 19:35:38.655683      25 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:35:38.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9323" for this suite.
Jul 20 19:35:46.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:35:47.224: INFO: namespace gc-9323 deletion completed in 8.549958848s

â€¢ [SLOW TEST:14.922 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:35:47.224: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4303
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Jul 20 19:35:47.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 create -f - --namespace=kubectl-4303'
Jul 20 19:35:47.855: INFO: stderr: ""
Jul 20 19:35:47.855: INFO: stdout: "pod/pause created\n"
Jul 20 19:35:47.855: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul 20 19:35:47.855: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4303" to be "running and ready"
Jul 20 19:35:47.866: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 10.672049ms
Jul 20 19:35:49.875: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.020264647s
Jul 20 19:35:49.875: INFO: Pod "pause" satisfied condition "running and ready"
Jul 20 19:35:49.875: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Jul 20 19:35:49.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 label pods pause testing-label=testing-label-value --namespace=kubectl-4303'
Jul 20 19:35:50.046: INFO: stderr: ""
Jul 20 19:35:50.046: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul 20 19:35:50.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pod pause -L testing-label --namespace=kubectl-4303'
Jul 20 19:35:50.121: INFO: stderr: ""
Jul 20 19:35:50.121: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul 20 19:35:50.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 label pods pause testing-label- --namespace=kubectl-4303'
Jul 20 19:35:50.209: INFO: stderr: ""
Jul 20 19:35:50.209: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul 20 19:35:50.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pod pause -L testing-label --namespace=kubectl-4303'
Jul 20 19:35:50.311: INFO: stderr: ""
Jul 20 19:35:50.311: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Jul 20 19:35:50.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 delete --grace-period=0 --force -f - --namespace=kubectl-4303'
Jul 20 19:35:50.402: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 20 19:35:50.402: INFO: stdout: "pod \"pause\" force deleted\n"
Jul 20 19:35:50.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get rc,svc -l name=pause --no-headers --namespace=kubectl-4303'
Jul 20 19:35:50.508: INFO: stderr: "No resources found in kubectl-4303 namespace.\n"
Jul 20 19:35:50.508: INFO: stdout: ""
Jul 20 19:35:50.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods -l name=pause --namespace=kubectl-4303 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 20 19:35:50.581: INFO: stderr: ""
Jul 20 19:35:50.581: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:35:50.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4303" for this suite.
Jul 20 19:35:56.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:35:57.344: INFO: namespace kubectl-4303 deletion completed in 6.746364961s

â€¢ [SLOW TEST:10.120 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:35:57.345: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4778
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-f7856b11-b6dd-4a22-8eb4-4b97a0e42b72
STEP: Creating secret with name secret-projected-all-test-volume-b0055291-f7e3-4f14-88c3-39e696fded50
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul 20 19:35:57.770: INFO: Waiting up to 5m0s for pod "projected-volume-093c1860-b3b1-4695-a6b9-435e62bd92dc" in namespace "projected-4778" to be "success or failure"
Jul 20 19:35:57.782: INFO: Pod "projected-volume-093c1860-b3b1-4695-a6b9-435e62bd92dc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.33315ms
Jul 20 19:35:59.832: INFO: Pod "projected-volume-093c1860-b3b1-4695-a6b9-435e62bd92dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.062094227s
STEP: Saw pod success
Jul 20 19:35:59.832: INFO: Pod "projected-volume-093c1860-b3b1-4695-a6b9-435e62bd92dc" satisfied condition "success or failure"
Jul 20 19:36:00.012: INFO: Trying to get logs from node 10.123.236.227 pod projected-volume-093c1860-b3b1-4695-a6b9-435e62bd92dc container projected-all-volume-test: <nil>
STEP: delete the pod
Jul 20 19:36:00.364: INFO: Waiting for pod projected-volume-093c1860-b3b1-4695-a6b9-435e62bd92dc to disappear
Jul 20 19:36:00.391: INFO: Pod projected-volume-093c1860-b3b1-4695-a6b9-435e62bd92dc no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:36:00.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4778" for this suite.
Jul 20 19:36:06.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:36:07.129: INFO: namespace projected-4778 deletion completed in 6.699734064s

â€¢ [SLOW TEST:9.785 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:36:07.130: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-428
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 20 19:36:08.638: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 20 19:36:10.682: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870568, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870568, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870568, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870568, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 20 19:36:13.728: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jul 20 19:36:15.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 attach --namespace=webhook-428 to-be-attached-pod -i -c=container1'
Jul 20 19:36:15.993: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:36:16.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-428" for this suite.
Jul 20 19:36:46.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:36:46.635: INFO: namespace webhook-428 deletion completed in 30.541150944s
STEP: Destroying namespace "webhook-428-markers" for this suite.
Jul 20 19:36:52.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:36:53.362: INFO: namespace webhook-428-markers deletion completed in 6.726873283s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:46.386 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:36:53.517: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-4959
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 19:36:53.776: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-66511658-05df-41fd-b9de-981c2d60c7e8" in namespace "security-context-test-4959" to be "success or failure"
Jul 20 19:36:53.786: INFO: Pod "busybox-privileged-false-66511658-05df-41fd-b9de-981c2d60c7e8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.656589ms
Jul 20 19:36:55.795: INFO: Pod "busybox-privileged-false-66511658-05df-41fd-b9de-981c2d60c7e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018931355s
Jul 20 19:36:55.795: INFO: Pod "busybox-privileged-false-66511658-05df-41fd-b9de-981c2d60c7e8" satisfied condition "success or failure"
Jul 20 19:36:55.850: INFO: Got logs for pod "busybox-privileged-false-66511658-05df-41fd-b9de-981c2d60c7e8": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:36:55.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4959" for this suite.
Jul 20 19:37:01.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:37:02.471: INFO: namespace security-context-test-4959 deletion completed in 6.600673569s

â€¢ [SLOW TEST:8.954 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:37:02.471: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4899
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-jkzk
STEP: Creating a pod to test atomic-volume-subpath
Jul 20 19:37:02.840: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jkzk" in namespace "subpath-4899" to be "success or failure"
Jul 20 19:37:02.850: INFO: Pod "pod-subpath-test-configmap-jkzk": Phase="Pending", Reason="", readiness=false. Elapsed: 10.301361ms
Jul 20 19:37:04.870: INFO: Pod "pod-subpath-test-configmap-jkzk": Phase="Running", Reason="", readiness=true. Elapsed: 2.030554578s
Jul 20 19:37:06.895: INFO: Pod "pod-subpath-test-configmap-jkzk": Phase="Running", Reason="", readiness=true. Elapsed: 4.055696173s
Jul 20 19:37:08.912: INFO: Pod "pod-subpath-test-configmap-jkzk": Phase="Running", Reason="", readiness=true. Elapsed: 6.072679428s
Jul 20 19:37:10.927: INFO: Pod "pod-subpath-test-configmap-jkzk": Phase="Running", Reason="", readiness=true. Elapsed: 8.087555662s
Jul 20 19:37:12.942: INFO: Pod "pod-subpath-test-configmap-jkzk": Phase="Running", Reason="", readiness=true. Elapsed: 10.101762192s
Jul 20 19:37:14.951: INFO: Pod "pod-subpath-test-configmap-jkzk": Phase="Running", Reason="", readiness=true. Elapsed: 12.111634785s
Jul 20 19:37:16.962: INFO: Pod "pod-subpath-test-configmap-jkzk": Phase="Running", Reason="", readiness=true. Elapsed: 14.122352262s
Jul 20 19:37:18.973: INFO: Pod "pod-subpath-test-configmap-jkzk": Phase="Running", Reason="", readiness=true. Elapsed: 16.133647651s
Jul 20 19:37:20.986: INFO: Pod "pod-subpath-test-configmap-jkzk": Phase="Running", Reason="", readiness=true. Elapsed: 18.145875925s
Jul 20 19:37:22.995: INFO: Pod "pod-subpath-test-configmap-jkzk": Phase="Running", Reason="", readiness=true. Elapsed: 20.155658217s
Jul 20 19:37:25.006: INFO: Pod "pod-subpath-test-configmap-jkzk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.166537102s
STEP: Saw pod success
Jul 20 19:37:25.007: INFO: Pod "pod-subpath-test-configmap-jkzk" satisfied condition "success or failure"
Jul 20 19:37:25.016: INFO: Trying to get logs from node 10.123.236.227 pod pod-subpath-test-configmap-jkzk container test-container-subpath-configmap-jkzk: <nil>
STEP: delete the pod
Jul 20 19:37:25.093: INFO: Waiting for pod pod-subpath-test-configmap-jkzk to disappear
Jul 20 19:37:25.102: INFO: Pod pod-subpath-test-configmap-jkzk no longer exists
STEP: Deleting pod pod-subpath-test-configmap-jkzk
Jul 20 19:37:25.102: INFO: Deleting pod "pod-subpath-test-configmap-jkzk" in namespace "subpath-4899"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:37:25.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4899" for this suite.
Jul 20 19:37:31.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:37:31.746: INFO: namespace subpath-4899 deletion completed in 6.609347496s

â€¢ [SLOW TEST:29.275 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:37:31.747: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8258
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 20 19:37:32.032: INFO: Waiting up to 5m0s for pod "pod-3fabff55-5241-47b2-b36e-8d30fa9dde9e" in namespace "emptydir-8258" to be "success or failure"
Jul 20 19:37:32.043: INFO: Pod "pod-3fabff55-5241-47b2-b36e-8d30fa9dde9e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.886641ms
Jul 20 19:37:34.054: INFO: Pod "pod-3fabff55-5241-47b2-b36e-8d30fa9dde9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021344551s
STEP: Saw pod success
Jul 20 19:37:34.054: INFO: Pod "pod-3fabff55-5241-47b2-b36e-8d30fa9dde9e" satisfied condition "success or failure"
Jul 20 19:37:34.065: INFO: Trying to get logs from node 10.123.236.227 pod pod-3fabff55-5241-47b2-b36e-8d30fa9dde9e container test-container: <nil>
STEP: delete the pod
Jul 20 19:37:34.140: INFO: Waiting for pod pod-3fabff55-5241-47b2-b36e-8d30fa9dde9e to disappear
Jul 20 19:37:34.150: INFO: Pod pod-3fabff55-5241-47b2-b36e-8d30fa9dde9e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:37:34.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8258" for this suite.
Jul 20 19:37:40.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:37:40.887: INFO: namespace emptydir-8258 deletion completed in 6.718107034s

â€¢ [SLOW TEST:9.141 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:37:40.888: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2115
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 20 19:37:43.766: INFO: Successfully updated pod "pod-update-f9fd3a83-e9e7-4adc-840f-7d25a721a435"
STEP: verifying the updated pod is in kubernetes
Jul 20 19:37:43.807: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:37:43.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2115" for this suite.
Jul 20 19:37:55.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:37:56.521: INFO: namespace pods-2115 deletion completed in 12.682396274s

â€¢ [SLOW TEST:15.633 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:37:56.522: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2157
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 20 19:37:57.577: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 20 19:37:59.609: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870677, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870677, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870677, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730870677, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 20 19:38:02.658: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:38:02.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2157" for this suite.
Jul 20 19:38:08.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:38:09.401: INFO: namespace webhook-2157 deletion completed in 6.603992922s
STEP: Destroying namespace "webhook-2157-markers" for this suite.
Jul 20 19:38:15.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:38:16.165: INFO: namespace webhook-2157-markers deletion completed in 6.763810017s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:19.810 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:38:16.332: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6300
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6300.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6300.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6300.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6300.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6300.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6300.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6300.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6300.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6300.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6300.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6300.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6300.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6300.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 70.219.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.219.70_udp@PTR;check="$$(dig +tcp +noall +answer +search 70.219.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.219.70_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6300.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6300.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6300.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6300.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6300.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6300.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6300.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6300.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6300.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6300.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6300.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6300.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6300.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 70.219.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.219.70_udp@PTR;check="$$(dig +tcp +noall +answer +search 70.219.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.219.70_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 20 19:38:20.699: INFO: Unable to read wheezy_udp@dns-test-service.dns-6300.svc.cluster.local from pod dns-6300/dns-test-743b5395-cd98-47f0-be19-a0cb121f1fd1: the server could not find the requested resource (get pods dns-test-743b5395-cd98-47f0-be19-a0cb121f1fd1)
Jul 20 19:38:20.932: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6300.svc.cluster.local from pod dns-6300/dns-test-743b5395-cd98-47f0-be19-a0cb121f1fd1: the server could not find the requested resource (get pods dns-test-743b5395-cd98-47f0-be19-a0cb121f1fd1)
Jul 20 19:38:20.951: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6300.svc.cluster.local from pod dns-6300/dns-test-743b5395-cd98-47f0-be19-a0cb121f1fd1: the server could not find the requested resource (get pods dns-test-743b5395-cd98-47f0-be19-a0cb121f1fd1)
Jul 20 19:38:21.051: INFO: Lookups using dns-6300/dns-test-743b5395-cd98-47f0-be19-a0cb121f1fd1 failed for: [wheezy_udp@dns-test-service.dns-6300.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6300.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6300.svc.cluster.local]

Jul 20 19:38:26.451: INFO: DNS probes using dns-6300/dns-test-743b5395-cd98-47f0-be19-a0cb121f1fd1 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:38:26.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6300" for this suite.
Jul 20 19:38:32.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:38:33.371: INFO: namespace dns-6300 deletion completed in 6.687206882s

â€¢ [SLOW TEST:17.039 seconds]
[sig-network] DNS
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:38:33.372: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4444
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:38:40.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4444" for this suite.
Jul 20 19:38:46.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:38:47.436: INFO: namespace resourcequota-4444 deletion completed in 6.679964313s

â€¢ [SLOW TEST:14.065 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:38:47.437: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4020
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-3173b197-3554-4425-8857-f31ce5061ad8 in namespace container-probe-4020
Jul 20 19:38:51.743: INFO: Started pod liveness-3173b197-3554-4425-8857-f31ce5061ad8 in namespace container-probe-4020
STEP: checking the pod's current state and verifying that restartCount is present
Jul 20 19:38:51.756: INFO: Initial restart count of pod liveness-3173b197-3554-4425-8857-f31ce5061ad8 is 0
Jul 20 19:39:01.831: INFO: Restart count of pod container-probe-4020/liveness-3173b197-3554-4425-8857-f31ce5061ad8 is now 1 (10.075088872s elapsed)
Jul 20 19:39:21.984: INFO: Restart count of pod container-probe-4020/liveness-3173b197-3554-4425-8857-f31ce5061ad8 is now 2 (30.228501541s elapsed)
Jul 20 19:39:42.104: INFO: Restart count of pod container-probe-4020/liveness-3173b197-3554-4425-8857-f31ce5061ad8 is now 3 (50.347989213s elapsed)
Jul 20 19:40:02.238: INFO: Restart count of pod container-probe-4020/liveness-3173b197-3554-4425-8857-f31ce5061ad8 is now 4 (1m10.482467804s elapsed)
Jul 20 19:41:04.684: INFO: Restart count of pod container-probe-4020/liveness-3173b197-3554-4425-8857-f31ce5061ad8 is now 5 (2m12.928490674s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:41:04.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4020" for this suite.
Jul 20 19:41:10.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:41:11.501: INFO: namespace container-probe-4020 deletion completed in 6.763661327s

â€¢ [SLOW TEST:144.065 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:41:11.502: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9810
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 20 19:41:14.852: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:41:14.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9810" for this suite.
Jul 20 19:41:21.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:41:21.798: INFO: namespace container-runtime-9810 deletion completed in 6.876710149s

â€¢ [SLOW TEST:10.296 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:41:21.799: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2720
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Jul 20 19:41:26.737: INFO: Successfully updated pod "annotationupdateaed6d935-14c5-4900-9075-e50c47aa42fe"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:41:28.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2720" for this suite.
Jul 20 19:41:56.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:41:57.563: INFO: namespace downward-api-2720 deletion completed in 28.739745373s

â€¢ [SLOW TEST:35.764 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:41:57.563: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6831
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:42:14.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6831" for this suite.
Jul 20 19:42:20.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:42:20.935: INFO: namespace resourcequota-6831 deletion completed in 6.759161074s

â€¢ [SLOW TEST:23.372 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:42:20.935: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-514
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-514
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 20 19:42:21.199: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 20 19:42:39.496: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.106.176:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-514 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 20 19:42:39.496: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 19:42:39.619: INFO: Found all expected endpoints: [netserver-0]
Jul 20 19:42:39.631: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.159.78:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-514 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 20 19:42:39.631: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 19:42:39.784: INFO: Found all expected endpoints: [netserver-1]
Jul 20 19:42:39.795: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.176.30:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-514 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 20 19:42:39.795: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 19:42:40.301: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:42:40.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-514" for this suite.
Jul 20 19:42:52.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:42:52.832: INFO: namespace pod-network-test-514 deletion completed in 12.512600216s

â€¢ [SLOW TEST:31.897 seconds]
[sig-network] Networking
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:42:52.833: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-603
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:43:04.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-603" for this suite.
Jul 20 19:43:10.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:43:11.049: INFO: namespace resourcequota-603 deletion completed in 6.828634751s

â€¢ [SLOW TEST:18.216 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:43:11.050: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8876
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 19:43:11.308: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 20 19:43:15.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 --namespace=crd-publish-openapi-8876 create -f -'
Jul 20 19:43:15.975: INFO: stderr: ""
Jul 20 19:43:15.975: INFO: stdout: "e2e-test-crd-publish-openapi-8937-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul 20 19:43:15.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 --namespace=crd-publish-openapi-8876 delete e2e-test-crd-publish-openapi-8937-crds test-cr'
Jul 20 19:43:16.168: INFO: stderr: ""
Jul 20 19:43:16.168: INFO: stdout: "e2e-test-crd-publish-openapi-8937-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jul 20 19:43:16.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 --namespace=crd-publish-openapi-8876 apply -f -'
Jul 20 19:43:16.625: INFO: stderr: ""
Jul 20 19:43:16.625: INFO: stdout: "e2e-test-crd-publish-openapi-8937-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul 20 19:43:16.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 --namespace=crd-publish-openapi-8876 delete e2e-test-crd-publish-openapi-8937-crds test-cr'
Jul 20 19:43:16.751: INFO: stderr: ""
Jul 20 19:43:16.751: INFO: stdout: "e2e-test-crd-publish-openapi-8937-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jul 20 19:43:16.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 explain e2e-test-crd-publish-openapi-8937-crds'
Jul 20 19:43:17.025: INFO: stderr: ""
Jul 20 19:43:17.025: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8937-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:43:21.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8876" for this suite.
Jul 20 19:43:27.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:43:27.912: INFO: namespace crd-publish-openapi-8876 deletion completed in 6.765426032s

â€¢ [SLOW TEST:16.862 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:43:27.914: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8027
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8027.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8027.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8027.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8027.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8027.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8027.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 20 19:43:32.430: INFO: DNS probes using dns-8027/dns-test-fe17a971-c3b0-4f5b-883e-602b0986cc67 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:43:32.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8027" for this suite.
Jul 20 19:43:38.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:43:39.556: INFO: namespace dns-8027 deletion completed in 6.742174972s

â€¢ [SLOW TEST:11.642 seconds]
[sig-network] DNS
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:43:39.556: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4814
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 20 19:43:39.831: INFO: Waiting up to 5m0s for pod "pod-dbd36bb4-0e9b-44ff-9a25-3c054541af9d" in namespace "emptydir-4814" to be "success or failure"
Jul 20 19:43:39.848: INFO: Pod "pod-dbd36bb4-0e9b-44ff-9a25-3c054541af9d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.609033ms
Jul 20 19:43:41.860: INFO: Pod "pod-dbd36bb4-0e9b-44ff-9a25-3c054541af9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028763042s
STEP: Saw pod success
Jul 20 19:43:41.860: INFO: Pod "pod-dbd36bb4-0e9b-44ff-9a25-3c054541af9d" satisfied condition "success or failure"
Jul 20 19:43:41.874: INFO: Trying to get logs from node 10.123.236.227 pod pod-dbd36bb4-0e9b-44ff-9a25-3c054541af9d container test-container: <nil>
STEP: delete the pod
Jul 20 19:43:41.977: INFO: Waiting for pod pod-dbd36bb4-0e9b-44ff-9a25-3c054541af9d to disappear
Jul 20 19:43:41.989: INFO: Pod pod-dbd36bb4-0e9b-44ff-9a25-3c054541af9d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:43:41.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4814" for this suite.
Jul 20 19:43:48.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:43:48.593: INFO: namespace emptydir-4814 deletion completed in 6.590299189s

â€¢ [SLOW TEST:9.037 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:43:48.593: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5538
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Jul 20 19:43:48.835: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:43:52.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5538" for this suite.
Jul 20 19:44:06.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:44:07.180: INFO: namespace init-container-5538 deletion completed in 14.604792115s

â€¢ [SLOW TEST:18.586 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:44:07.180: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1347
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-1347
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1347 to expose endpoints map[]
Jul 20 19:44:07.577: INFO: successfully validated that service endpoint-test2 in namespace services-1347 exposes endpoints map[] (10.113468ms elapsed)
STEP: Creating pod pod1 in namespace services-1347
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1347 to expose endpoints map[pod1:[80]]
Jul 20 19:44:09.674: INFO: successfully validated that service endpoint-test2 in namespace services-1347 exposes endpoints map[pod1:[80]] (2.069595534s elapsed)
STEP: Creating pod pod2 in namespace services-1347
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1347 to expose endpoints map[pod1:[80] pod2:[80]]
Jul 20 19:44:12.833: INFO: successfully validated that service endpoint-test2 in namespace services-1347 exposes endpoints map[pod1:[80] pod2:[80]] (3.145440994s elapsed)
STEP: Deleting pod pod1 in namespace services-1347
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1347 to expose endpoints map[pod2:[80]]
Jul 20 19:44:13.908: INFO: successfully validated that service endpoint-test2 in namespace services-1347 exposes endpoints map[pod2:[80]] (1.044924253s elapsed)
STEP: Deleting pod pod2 in namespace services-1347
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1347 to expose endpoints map[]
Jul 20 19:44:14.949: INFO: successfully validated that service endpoint-test2 in namespace services-1347 exposes endpoints map[] (1.021925041s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:44:15.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1347" for this suite.
Jul 20 19:44:27.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:44:27.772: INFO: namespace services-1347 deletion completed in 12.712438373s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:20.593 seconds]
[sig-network] Services
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:44:27.773: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8128
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-57c60f23-b264-450b-8777-0eef748c0f0d
STEP: Creating a pod to test consume secrets
Jul 20 19:44:28.090: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-99c89d11-ef12-4df7-ab1c-bedae4fc1960" in namespace "projected-8128" to be "success or failure"
Jul 20 19:44:28.098: INFO: Pod "pod-projected-secrets-99c89d11-ef12-4df7-ab1c-bedae4fc1960": Phase="Pending", Reason="", readiness=false. Elapsed: 8.22347ms
Jul 20 19:44:30.113: INFO: Pod "pod-projected-secrets-99c89d11-ef12-4df7-ab1c-bedae4fc1960": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023474504s
STEP: Saw pod success
Jul 20 19:44:30.114: INFO: Pod "pod-projected-secrets-99c89d11-ef12-4df7-ab1c-bedae4fc1960" satisfied condition "success or failure"
Jul 20 19:44:30.123: INFO: Trying to get logs from node 10.123.236.227 pod pod-projected-secrets-99c89d11-ef12-4df7-ab1c-bedae4fc1960 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 20 19:44:30.177: INFO: Waiting for pod pod-projected-secrets-99c89d11-ef12-4df7-ab1c-bedae4fc1960 to disappear
Jul 20 19:44:30.300: INFO: Pod pod-projected-secrets-99c89d11-ef12-4df7-ab1c-bedae4fc1960 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:44:30.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8128" for this suite.
Jul 20 19:44:36.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:44:37.099: INFO: namespace projected-8128 deletion completed in 6.782963093s

â€¢ [SLOW TEST:9.327 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:44:37.100: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-517
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul 20 19:44:37.431: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-517 /api/v1/namespaces/watch-517/configmaps/e2e-watch-test-label-changed 17d3ccf9-c210-4107-be52-768f1de9a75d 39456 0 2020-07-20 19:44:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 20 19:44:37.431: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-517 /api/v1/namespaces/watch-517/configmaps/e2e-watch-test-label-changed 17d3ccf9-c210-4107-be52-768f1de9a75d 39457 0 2020-07-20 19:44:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul 20 19:44:37.431: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-517 /api/v1/namespaces/watch-517/configmaps/e2e-watch-test-label-changed 17d3ccf9-c210-4107-be52-768f1de9a75d 39458 0 2020-07-20 19:44:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul 20 19:44:47.516: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-517 /api/v1/namespaces/watch-517/configmaps/e2e-watch-test-label-changed 17d3ccf9-c210-4107-be52-768f1de9a75d 39474 0 2020-07-20 19:44:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 20 19:44:47.517: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-517 /api/v1/namespaces/watch-517/configmaps/e2e-watch-test-label-changed 17d3ccf9-c210-4107-be52-768f1de9a75d 39475 0 2020-07-20 19:44:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jul 20 19:44:47.517: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-517 /api/v1/namespaces/watch-517/configmaps/e2e-watch-test-label-changed 17d3ccf9-c210-4107-be52-768f1de9a75d 39476 0 2020-07-20 19:44:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:44:47.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-517" for this suite.
Jul 20 19:44:53.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:44:54.267: INFO: namespace watch-517 deletion completed in 6.72428608s

â€¢ [SLOW TEST:17.167 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:44:54.267: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2571
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 19:44:54.524: INFO: Creating deployment "webserver-deployment"
Jul 20 19:44:54.535: INFO: Waiting for observed generation 1
Jul 20 19:44:56.561: INFO: Waiting for all required pods to come up
Jul 20 19:44:56.578: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul 20 19:44:58.608: INFO: Waiting for deployment "webserver-deployment" to complete
Jul 20 19:44:58.625: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jul 20 19:44:58.645: INFO: Updating deployment webserver-deployment
Jul 20 19:44:58.645: INFO: Waiting for observed generation 2
Jul 20 19:45:00.662: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul 20 19:45:00.673: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul 20 19:45:00.683: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul 20 19:45:00.728: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul 20 19:45:00.728: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul 20 19:45:00.739: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul 20 19:45:00.762: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jul 20 19:45:00.762: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jul 20 19:45:00.783: INFO: Updating deployment webserver-deployment
Jul 20 19:45:00.783: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jul 20 19:45:00.808: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul 20 19:45:02.841: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 20 19:45:02.866: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2571 /apis/apps/v1/namespaces/deployment-2571/deployments/webserver-deployment a8b7f456-8201-497e-b01c-f8d063686ac9 39935 3 2020-07-20 19:44:54 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0022c6b68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:10,UnavailableReplicas:23,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-07-20 19:45:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-07-20 19:45:02 +0000 UTC,LastTransitionTime:2020-07-20 19:44:54 +0000 UTC,},},ReadyReplicas:10,CollisionCount:nil,},}

Jul 20 19:45:02.877: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-2571 /apis/apps/v1/namespaces/deployment-2571/replicasets/webserver-deployment-c7997dcc8 5594ad64-cefe-4237-87f8-7d7f1aefad69 39776 3 2020-07-20 19:44:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment a8b7f456-8201-497e-b01c-f8d063686ac9 0xc003a35c37 0xc003a35c38}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003a35cb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 20 19:45:02.877: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jul 20 19:45:02.877: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-2571 /apis/apps/v1/namespaces/deployment-2571/replicasets/webserver-deployment-595b5b9587 a3697c63-00c5-49b0-a76e-94d7c03ddf57 39933 3 2020-07-20 19:44:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment a8b7f456-8201-497e-b01c-f8d063686ac9 0xc003a35b77 0xc003a35b78}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003a35bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:10,AvailableReplicas:10,Conditions:[]ReplicaSetCondition{},},}
Jul 20 19:45:02.894: INFO: Pod "webserver-deployment-595b5b9587-2jvbg" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2jvbg webserver-deployment-595b5b9587- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-595b5b9587-2jvbg ddc1e69f-0c75-47a6-94d7-84ac2a4bc680 39817 0 2020-07-20 19:45:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a3697c63-00c5-49b0-a76e-94d7c03ddf57 0xc0004a3cf7 0xc0004a3cf8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.230,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.230,PodIP:,StartTime:2020-07-20 19:45:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.894: INFO: Pod "webserver-deployment-595b5b9587-6st8z" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-6st8z webserver-deployment-595b5b9587- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-595b5b9587-6st8z 2f16e986-caa7-40bf-b472-26d8a0a75808 39848 0 2020-07-20 19:45:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a3697c63-00c5-49b0-a76e-94d7c03ddf57 0xc000052bb7 0xc000052bb8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.220,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.220,PodIP:,StartTime:2020-07-20 19:45:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.894: INFO: Pod "webserver-deployment-595b5b9587-7hbh8" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-7hbh8 webserver-deployment-595b5b9587- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-595b5b9587-7hbh8 72096a38-4ea1-4738-a8bb-2a02f144fc52 39827 0 2020-07-20 19:45:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a3697c63-00c5-49b0-a76e-94d7c03ddf57 0xc000053eb7 0xc000053eb8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.230,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.230,PodIP:,StartTime:2020-07-20 19:45:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.895: INFO: Pod "webserver-deployment-595b5b9587-89qhn" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-89qhn webserver-deployment-595b5b9587- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-595b5b9587-89qhn e861abed-bf7a-4ab8-9a14-1ade2efdbd43 39611 0 2020-07-20 19:44:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a3697c63-00c5-49b0-a76e-94d7c03ddf57 0xc0027220f7 0xc0027220f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.230,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.230,PodIP:172.30.176.33,StartTime:2020-07-20 19:44:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-20 19:44:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://234adb790b7240bee14a701c686f19cf9aad3ac13e0bedba6f08394a87b3d672,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.176.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.895: INFO: Pod "webserver-deployment-595b5b9587-9fmqr" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-9fmqr webserver-deployment-595b5b9587- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-595b5b9587-9fmqr 833597e4-af78-43ba-b2a9-eaaeb51fd8be 39802 0 2020-07-20 19:45:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a3697c63-00c5-49b0-a76e-94d7c03ddf57 0xc002722277 0xc002722278}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.230,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.230,PodIP:,StartTime:2020-07-20 19:45:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.895: INFO: Pod "webserver-deployment-595b5b9587-9lx89" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-9lx89 webserver-deployment-595b5b9587- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-595b5b9587-9lx89 760d9118-fc9b-44ac-af28-c789f81de801 39636 0 2020-07-20 19:44:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a3697c63-00c5-49b0-a76e-94d7c03ddf57 0xc002722417 0xc002722418}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.220,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.220,PodIP:172.30.159.82,StartTime:2020-07-20 19:44:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-20 19:44:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://2484eb0b7fa8bd1f0c01435f728115a294f9cb5069988a0bfb71621d164bc625,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.159.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.895: INFO: Pod "webserver-deployment-595b5b9587-cmh87" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-cmh87 webserver-deployment-595b5b9587- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-595b5b9587-cmh87 1e5fe016-068c-4890-bc65-5dcabf440cfe 39639 0 2020-07-20 19:44:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a3697c63-00c5-49b0-a76e-94d7c03ddf57 0xc002722667 0xc002722668}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.220,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.220,PodIP:172.30.159.81,StartTime:2020-07-20 19:44:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-20 19:44:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://15972f754e762e76ccdd5ffc3501bd0f1e9c7de994b1b87660020e977a05acc3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.159.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.895: INFO: Pod "webserver-deployment-595b5b9587-grmcx" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-grmcx webserver-deployment-595b5b9587- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-595b5b9587-grmcx 64ca8da5-412d-4ec6-b2cf-e77b1bbc3c00 39831 0 2020-07-20 19:45:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a3697c63-00c5-49b0-a76e-94d7c03ddf57 0xc0027229b7 0xc0027229b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.220,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.220,PodIP:,StartTime:2020-07-20 19:45:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.896: INFO: Pod "webserver-deployment-595b5b9587-kkrvw" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-kkrvw webserver-deployment-595b5b9587- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-595b5b9587-kkrvw 9f9b6232-9d49-4773-b2e3-3957b355f67e 39834 0 2020-07-20 19:45:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a3697c63-00c5-49b0-a76e-94d7c03ddf57 0xc002722f77 0xc002722f78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.220,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.220,PodIP:,StartTime:2020-07-20 19:45:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.896: INFO: Pod "webserver-deployment-595b5b9587-lvd89" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lvd89 webserver-deployment-595b5b9587- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-595b5b9587-lvd89 e4030a12-a43e-4346-998c-c94b1ba0f6be 39627 0 2020-07-20 19:44:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a3697c63-00c5-49b0-a76e-94d7c03ddf57 0xc0027230f7 0xc0027230f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.227,PodIP:172.30.106.184,StartTime:2020-07-20 19:44:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-20 19:44:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://54c8754800bc0a407a8e131676a9b02faa34c3bfb667c4c48fc727bb79e86f93,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.106.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.896: INFO: Pod "webserver-deployment-595b5b9587-mgtnk" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mgtnk webserver-deployment-595b5b9587- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-595b5b9587-mgtnk 0e3f5d52-1729-41ec-bf2a-d3e610760833 39630 0 2020-07-20 19:44:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a3697c63-00c5-49b0-a76e-94d7c03ddf57 0xc002723277 0xc002723278}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.227,PodIP:172.30.106.186,StartTime:2020-07-20 19:44:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-20 19:44:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://bf6c2d500d7e99f768e21a77bc41e272b1e7bf19491d9c1b64d5e60adfa6bcc6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.106.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.896: INFO: Pod "webserver-deployment-595b5b9587-qwpgs" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qwpgs webserver-deployment-595b5b9587- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-595b5b9587-qwpgs c5fffc05-bc92-4ae3-95d3-16d34c9f4f95 39846 0 2020-07-20 19:45:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a3697c63-00c5-49b0-a76e-94d7c03ddf57 0xc0027233f7 0xc0027233f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.227,PodIP:,StartTime:2020-07-20 19:45:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.896: INFO: Pod "webserver-deployment-595b5b9587-r2d5x" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-r2d5x webserver-deployment-595b5b9587- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-595b5b9587-r2d5x 5d672661-1551-4eae-a8d3-06659b32383c 39836 0 2020-07-20 19:45:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a3697c63-00c5-49b0-a76e-94d7c03ddf57 0xc002723557 0xc002723558}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.227,PodIP:,StartTime:2020-07-20 19:45:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.897: INFO: Pod "webserver-deployment-595b5b9587-rs9g2" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rs9g2 webserver-deployment-595b5b9587- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-595b5b9587-rs9g2 6d68eaab-10de-4b30-8c3a-18d185935fe3 39929 0 2020-07-20 19:45:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a3697c63-00c5-49b0-a76e-94d7c03ddf57 0xc002723e97 0xc002723e98}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.227,PodIP:172.30.106.190,StartTime:2020-07-20 19:45:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-20 19:45:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://d99ef0212a9092ca50e3568f52cc58cceeecc847af2b6ae769a79b320d8fb2ea,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.106.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.899: INFO: Pod "webserver-deployment-595b5b9587-stnhq" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-stnhq webserver-deployment-595b5b9587- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-595b5b9587-stnhq ca824c00-6f39-44bb-8d29-ba04840c108d 39932 0 2020-07-20 19:45:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a3697c63-00c5-49b0-a76e-94d7c03ddf57 0xc002ea8097 0xc002ea8098}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.227,PodIP:172.30.106.133,StartTime:2020-07-20 19:45:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-20 19:45:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://c497b6d1c88f4705b85df13863155bb16423441190c6878259f074d8aa5d49ef,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.106.133,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.899: INFO: Pod "webserver-deployment-595b5b9587-szwff" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-szwff webserver-deployment-595b5b9587- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-595b5b9587-szwff 10f16963-ac59-4536-8dfa-980fe49e8122 39785 0 2020-07-20 19:45:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a3697c63-00c5-49b0-a76e-94d7c03ddf57 0xc002ea82f7 0xc002ea82f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.227,PodIP:,StartTime:2020-07-20 19:45:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.903: INFO: Pod "webserver-deployment-595b5b9587-twbbm" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-twbbm webserver-deployment-595b5b9587- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-595b5b9587-twbbm b130b76a-bd65-435f-b8ff-00441771528c 39850 0 2020-07-20 19:45:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a3697c63-00c5-49b0-a76e-94d7c03ddf57 0xc002ea8457 0xc002ea8458}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.227,PodIP:,StartTime:2020-07-20 19:45:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.904: INFO: Pod "webserver-deployment-595b5b9587-v2ffz" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-v2ffz webserver-deployment-595b5b9587- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-595b5b9587-v2ffz 9e1ace77-034f-43ae-ac8c-6db26f4113d5 39633 0 2020-07-20 19:44:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a3697c63-00c5-49b0-a76e-94d7c03ddf57 0xc002ea85c7 0xc002ea85c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.227,PodIP:172.30.106.185,StartTime:2020-07-20 19:44:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-20 19:44:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://1413a7229775816fb40be188119f5cd4a09d997b6e36e94f10dcd05901ce6802,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.106.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.904: INFO: Pod "webserver-deployment-595b5b9587-vfdrl" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-vfdrl webserver-deployment-595b5b9587- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-595b5b9587-vfdrl 95b41302-3054-4894-b615-a79ce3c461f5 39615 0 2020-07-20 19:44:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a3697c63-00c5-49b0-a76e-94d7c03ddf57 0xc002ea8757 0xc002ea8758}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.230,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.230,PodIP:172.30.176.31,StartTime:2020-07-20 19:44:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-20 19:44:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://27608c22646aa94345e79008656e15ba09afaed84eb4fe4081f76bf75cd417b2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.176.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.904: INFO: Pod "webserver-deployment-595b5b9587-zvf4x" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-zvf4x webserver-deployment-595b5b9587- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-595b5b9587-zvf4x df3662f1-175e-4b3e-96c7-de8c17ceed23 39642 0 2020-07-20 19:44:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a3697c63-00c5-49b0-a76e-94d7c03ddf57 0xc002ea9127 0xc002ea9128}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.220,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.220,PodIP:172.30.159.83,StartTime:2020-07-20 19:44:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-20 19:44:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://15c1ec4cdb6487060708c8329b49f4af0e537893c08767d40a453279861a9d23,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.159.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.904: INFO: Pod "webserver-deployment-c7997dcc8-29gfm" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-29gfm webserver-deployment-c7997dcc8- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-c7997dcc8-29gfm fa126d9f-e939-44ed-b3dc-0dd5ff0893f3 39812 0 2020-07-20 19:45:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5594ad64-cefe-4237-87f8-7d7f1aefad69 0xc002ea9ba7 0xc002ea9ba8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.230,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.230,PodIP:,StartTime:2020-07-20 19:45:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.904: INFO: Pod "webserver-deployment-c7997dcc8-2jgsr" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-2jgsr webserver-deployment-c7997dcc8- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-c7997dcc8-2jgsr f8583ca4-a3a5-4998-9638-61e6298f03e9 39779 0 2020-07-20 19:45:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5594ad64-cefe-4237-87f8-7d7f1aefad69 0xc002f2a027 0xc002f2a028}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.230,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.230,PodIP:,StartTime:2020-07-20 19:45:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.905: INFO: Pod "webserver-deployment-c7997dcc8-49nkm" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-49nkm webserver-deployment-c7997dcc8- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-c7997dcc8-49nkm 48752625-46e1-4bdf-acef-24d3eaec8e29 39782 0 2020-07-20 19:45:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5594ad64-cefe-4237-87f8-7d7f1aefad69 0xc002f2a1a7 0xc002f2a1a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.220,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.220,PodIP:,StartTime:2020-07-20 19:45:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.905: INFO: Pod "webserver-deployment-c7997dcc8-78xgb" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-78xgb webserver-deployment-c7997dcc8- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-c7997dcc8-78xgb 73633d4d-bbc5-49bd-bd16-47e66e95205d 39708 0 2020-07-20 19:44:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5594ad64-cefe-4237-87f8-7d7f1aefad69 0xc002f2a597 0xc002f2a598}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.227,PodIP:,StartTime:2020-07-20 19:44:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.905: INFO: Pod "webserver-deployment-c7997dcc8-8qc4l" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-8qc4l webserver-deployment-c7997dcc8- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-c7997dcc8-8qc4l 435d076b-9c33-4d93-b5de-135ecc012e4e 39860 0 2020-07-20 19:44:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5594ad64-cefe-4237-87f8-7d7f1aefad69 0xc002f2acc7 0xc002f2acc8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.227,PodIP:172.30.106.189,StartTime:2020-07-20 19:44:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.106.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.905: INFO: Pod "webserver-deployment-c7997dcc8-l2rdp" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-l2rdp webserver-deployment-c7997dcc8- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-c7997dcc8-l2rdp f9f33412-7dc1-41c5-8eaf-c56b50519f32 39707 0 2020-07-20 19:44:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5594ad64-cefe-4237-87f8-7d7f1aefad69 0xc002f2ba47 0xc002f2ba48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.227,PodIP:,StartTime:2020-07-20 19:44:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.906: INFO: Pod "webserver-deployment-c7997dcc8-n76lz" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-n76lz webserver-deployment-c7997dcc8- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-c7997dcc8-n76lz 98da3bcc-b333-4b81-baee-528c3757f68a 39833 0 2020-07-20 19:45:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5594ad64-cefe-4237-87f8-7d7f1aefad69 0xc002f2beb7 0xc002f2beb8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.227,PodIP:,StartTime:2020-07-20 19:45:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.906: INFO: Pod "webserver-deployment-c7997dcc8-q8jhc" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-q8jhc webserver-deployment-c7997dcc8- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-c7997dcc8-q8jhc c4706d47-7654-47cb-9819-ff663f9f0208 39840 0 2020-07-20 19:45:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5594ad64-cefe-4237-87f8-7d7f1aefad69 0xc00366c587 0xc00366c588}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.227,PodIP:,StartTime:2020-07-20 19:45:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.906: INFO: Pod "webserver-deployment-c7997dcc8-v9pzz" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-v9pzz webserver-deployment-c7997dcc8- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-c7997dcc8-v9pzz 7e2619a5-5937-4d02-83c3-6bb0e9c7ec80 39842 0 2020-07-20 19:45:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5594ad64-cefe-4237-87f8-7d7f1aefad69 0xc00366cb77 0xc00366cb78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.220,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.220,PodIP:,StartTime:2020-07-20 19:45:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.906: INFO: Pod "webserver-deployment-c7997dcc8-vhxcl" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-vhxcl webserver-deployment-c7997dcc8- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-c7997dcc8-vhxcl 44845949-1bf8-4c90-92c4-0627cec4e7cc 39837 0 2020-07-20 19:45:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5594ad64-cefe-4237-87f8-7d7f1aefad69 0xc00366d027 0xc00366d028}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.220,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.220,PodIP:,StartTime:2020-07-20 19:45:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.906: INFO: Pod "webserver-deployment-c7997dcc8-wtzgt" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-wtzgt webserver-deployment-c7997dcc8- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-c7997dcc8-wtzgt 68808987-52cd-4ac5-aaa5-cab4f7bd70e2 39822 0 2020-07-20 19:45:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5594ad64-cefe-4237-87f8-7d7f1aefad69 0xc00366d407 0xc00366d408}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.230,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:45:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.230,PodIP:,StartTime:2020-07-20 19:45:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.907: INFO: Pod "webserver-deployment-c7997dcc8-wxmxt" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-wxmxt webserver-deployment-c7997dcc8- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-c7997dcc8-wxmxt a8f3bfa3-6219-4acc-8a76-c516bbca0ed2 39890 0 2020-07-20 19:44:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5594ad64-cefe-4237-87f8-7d7f1aefad69 0xc00366da17 0xc00366da18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.230,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.230,PodIP:172.30.176.35,StartTime:2020-07-20 19:44:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.176.35,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 20 19:45:02.907: INFO: Pod "webserver-deployment-c7997dcc8-x47gj" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-x47gj webserver-deployment-c7997dcc8- deployment-2571 /api/v1/namespaces/deployment-2571/pods/webserver-deployment-c7997dcc8-x47gj 648280f2-f1c8-4705-ae54-d56a52e7e7c2 39865 0 2020-07-20 19:44:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5594ad64-cefe-4237-87f8-7d7f1aefad69 0xc00366dd47 0xc00366dd48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbvqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.220,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:44:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.220,PodIP:172.30.159.79,StartTime:2020-07-20 19:44:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.159.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:45:02.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2571" for this suite.
Jul 20 19:45:11.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:45:11.554: INFO: namespace deployment-2571 deletion completed in 8.62921377s

â€¢ [SLOW TEST:17.287 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:45:11.554: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9219
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-e29a2384-f1fc-46f4-b5af-e1b942dbf6be
STEP: Creating a pod to test consume secrets
Jul 20 19:45:11.953: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-73ac00e1-d720-4237-9c88-99674d8cc2fd" in namespace "projected-9219" to be "success or failure"
Jul 20 19:45:11.963: INFO: Pod "pod-projected-secrets-73ac00e1-d720-4237-9c88-99674d8cc2fd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.41766ms
Jul 20 19:45:13.975: INFO: Pod "pod-projected-secrets-73ac00e1-d720-4237-9c88-99674d8cc2fd": Phase="Running", Reason="", readiness=true. Elapsed: 2.021504268s
Jul 20 19:45:15.988: INFO: Pod "pod-projected-secrets-73ac00e1-d720-4237-9c88-99674d8cc2fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034546535s
STEP: Saw pod success
Jul 20 19:45:15.988: INFO: Pod "pod-projected-secrets-73ac00e1-d720-4237-9c88-99674d8cc2fd" satisfied condition "success or failure"
Jul 20 19:45:16.000: INFO: Trying to get logs from node 10.123.236.227 pod pod-projected-secrets-73ac00e1-d720-4237-9c88-99674d8cc2fd container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 20 19:45:16.085: INFO: Waiting for pod pod-projected-secrets-73ac00e1-d720-4237-9c88-99674d8cc2fd to disappear
Jul 20 19:45:16.096: INFO: Pod pod-projected-secrets-73ac00e1-d720-4237-9c88-99674d8cc2fd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:45:16.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9219" for this suite.
Jul 20 19:45:22.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:45:22.802: INFO: namespace projected-9219 deletion completed in 6.682476653s

â€¢ [SLOW TEST:11.248 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:45:22.802: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8209
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jul 20 19:45:23.152: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d2c88af7-dd44-45fd-a1fb-6be651c8de2d" in namespace "downward-api-8209" to be "success or failure"
Jul 20 19:45:23.161: INFO: Pod "downwardapi-volume-d2c88af7-dd44-45fd-a1fb-6be651c8de2d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.730404ms
Jul 20 19:45:25.172: INFO: Pod "downwardapi-volume-d2c88af7-dd44-45fd-a1fb-6be651c8de2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020167341s
STEP: Saw pod success
Jul 20 19:45:25.172: INFO: Pod "downwardapi-volume-d2c88af7-dd44-45fd-a1fb-6be651c8de2d" satisfied condition "success or failure"
Jul 20 19:45:25.183: INFO: Trying to get logs from node 10.123.236.227 pod downwardapi-volume-d2c88af7-dd44-45fd-a1fb-6be651c8de2d container client-container: <nil>
STEP: delete the pod
Jul 20 19:45:25.256: INFO: Waiting for pod downwardapi-volume-d2c88af7-dd44-45fd-a1fb-6be651c8de2d to disappear
Jul 20 19:45:25.269: INFO: Pod downwardapi-volume-d2c88af7-dd44-45fd-a1fb-6be651c8de2d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:45:25.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8209" for this suite.
Jul 20 19:45:31.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:45:31.973: INFO: namespace downward-api-8209 deletion completed in 6.685824213s

â€¢ [SLOW TEST:9.171 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:45:31.973: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-6474
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul 20 19:45:32.912: INFO: Pod name wrapped-volume-race-85526819-0e5e-48cf-947c-9e29458c661f: Found 0 pods out of 5
Jul 20 19:45:37.931: INFO: Pod name wrapped-volume-race-85526819-0e5e-48cf-947c-9e29458c661f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-85526819-0e5e-48cf-947c-9e29458c661f in namespace emptydir-wrapper-6474, will wait for the garbage collector to delete the pods
Jul 20 19:45:38.076: INFO: Deleting ReplicationController wrapped-volume-race-85526819-0e5e-48cf-947c-9e29458c661f took: 23.789193ms
Jul 20 19:45:38.277: INFO: Terminating ReplicationController wrapped-volume-race-85526819-0e5e-48cf-947c-9e29458c661f pods took: 200.31881ms
STEP: Creating RC which spawns configmap-volume pods
Jul 20 19:46:15.941: INFO: Pod name wrapped-volume-race-e9d31703-5f88-49e0-9756-6d28904d998c: Found 0 pods out of 5
Jul 20 19:46:20.959: INFO: Pod name wrapped-volume-race-e9d31703-5f88-49e0-9756-6d28904d998c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e9d31703-5f88-49e0-9756-6d28904d998c in namespace emptydir-wrapper-6474, will wait for the garbage collector to delete the pods
Jul 20 19:46:21.104: INFO: Deleting ReplicationController wrapped-volume-race-e9d31703-5f88-49e0-9756-6d28904d998c took: 30.564236ms
Jul 20 19:46:21.305: INFO: Terminating ReplicationController wrapped-volume-race-e9d31703-5f88-49e0-9756-6d28904d998c pods took: 200.327872ms
STEP: Creating RC which spawns configmap-volume pods
Jul 20 19:47:05.265: INFO: Pod name wrapped-volume-race-297f2a4f-453f-4b93-bb96-05690e857b93: Found 0 pods out of 5
Jul 20 19:47:10.285: INFO: Pod name wrapped-volume-race-297f2a4f-453f-4b93-bb96-05690e857b93: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-297f2a4f-453f-4b93-bb96-05690e857b93 in namespace emptydir-wrapper-6474, will wait for the garbage collector to delete the pods
Jul 20 19:47:10.442: INFO: Deleting ReplicationController wrapped-volume-race-297f2a4f-453f-4b93-bb96-05690e857b93 took: 29.888307ms
Jul 20 19:47:10.642: INFO: Terminating ReplicationController wrapped-volume-race-297f2a4f-453f-4b93-bb96-05690e857b93 pods took: 200.466832ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:47:56.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6474" for this suite.
Jul 20 19:48:04.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:48:05.057: INFO: namespace emptydir-wrapper-6474 deletion completed in 8.731916654s

â€¢ [SLOW TEST:153.084 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:48:05.061: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-7131
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jul 20 19:48:09.931: INFO: Successfully updated pod "adopt-release-5qxdq"
STEP: Checking that the Job readopts the Pod
Jul 20 19:48:09.931: INFO: Waiting up to 15m0s for pod "adopt-release-5qxdq" in namespace "job-7131" to be "adopted"
Jul 20 19:48:09.939: INFO: Pod "adopt-release-5qxdq": Phase="Running", Reason="", readiness=true. Elapsed: 8.475952ms
Jul 20 19:48:11.948: INFO: Pod "adopt-release-5qxdq": Phase="Running", Reason="", readiness=true. Elapsed: 2.017327694s
Jul 20 19:48:11.948: INFO: Pod "adopt-release-5qxdq" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jul 20 19:48:12.485: INFO: Successfully updated pod "adopt-release-5qxdq"
STEP: Checking that the Job releases the Pod
Jul 20 19:48:12.485: INFO: Waiting up to 15m0s for pod "adopt-release-5qxdq" in namespace "job-7131" to be "released"
Jul 20 19:48:12.496: INFO: Pod "adopt-release-5qxdq": Phase="Running", Reason="", readiness=true. Elapsed: 10.150278ms
Jul 20 19:48:12.496: INFO: Pod "adopt-release-5qxdq" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:48:12.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7131" for this suite.
Jul 20 19:48:58.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:48:59.105: INFO: namespace job-7131 deletion completed in 46.594811277s

â€¢ [SLOW TEST:54.045 seconds]
[sig-apps] Job
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:48:59.105: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5186
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-5186
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5186
STEP: creating replication controller externalsvc in namespace services-5186
I0720 19:48:59.455124      25 runners.go:184] Created replication controller with name: externalsvc, namespace: services-5186, replica count: 2
I0720 19:49:02.505610      25 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jul 20 19:49:02.644: INFO: Creating new exec pod
Jul 20 19:49:04.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=services-5186 execpodncgvx -- /bin/sh -x -c nslookup nodeport-service'
Jul 20 19:49:04.962: INFO: stderr: "+ nslookup nodeport-service\n"
Jul 20 19:49:04.962: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-5186.svc.cluster.local\tcanonical name = externalsvc.services-5186.svc.cluster.local.\nName:\texternalsvc.services-5186.svc.cluster.local\nAddress: 172.21.20.3\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5186, will wait for the garbage collector to delete the pods
Jul 20 19:49:05.075: INFO: Deleting ReplicationController externalsvc took: 26.250264ms
Jul 20 19:49:05.276: INFO: Terminating ReplicationController externalsvc pods took: 200.293425ms
Jul 20 19:49:15.373: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:49:15.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5186" for this suite.
Jul 20 19:49:21.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:49:22.122: INFO: namespace services-5186 deletion completed in 6.635976782s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:23.016 seconds]
[sig-network] Services
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:49:22.123: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2225
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul 20 19:49:22.432: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2225 /api/v1/namespaces/watch-2225/configmaps/e2e-watch-test-configmap-a 5704f2ef-b72b-484e-9a34-e5d56aa833a7 41486 0 2020-07-20 19:49:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 20 19:49:22.432: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2225 /api/v1/namespaces/watch-2225/configmaps/e2e-watch-test-configmap-a 5704f2ef-b72b-484e-9a34-e5d56aa833a7 41486 0 2020-07-20 19:49:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul 20 19:49:32.453: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2225 /api/v1/namespaces/watch-2225/configmaps/e2e-watch-test-configmap-a 5704f2ef-b72b-484e-9a34-e5d56aa833a7 41499 0 2020-07-20 19:49:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul 20 19:49:32.453: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2225 /api/v1/namespaces/watch-2225/configmaps/e2e-watch-test-configmap-a 5704f2ef-b72b-484e-9a34-e5d56aa833a7 41499 0 2020-07-20 19:49:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul 20 19:49:42.475: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2225 /api/v1/namespaces/watch-2225/configmaps/e2e-watch-test-configmap-a 5704f2ef-b72b-484e-9a34-e5d56aa833a7 41514 0 2020-07-20 19:49:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 20 19:49:42.475: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2225 /api/v1/namespaces/watch-2225/configmaps/e2e-watch-test-configmap-a 5704f2ef-b72b-484e-9a34-e5d56aa833a7 41514 0 2020-07-20 19:49:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul 20 19:49:52.497: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2225 /api/v1/namespaces/watch-2225/configmaps/e2e-watch-test-configmap-a 5704f2ef-b72b-484e-9a34-e5d56aa833a7 41528 0 2020-07-20 19:49:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 20 19:49:52.497: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2225 /api/v1/namespaces/watch-2225/configmaps/e2e-watch-test-configmap-a 5704f2ef-b72b-484e-9a34-e5d56aa833a7 41528 0 2020-07-20 19:49:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul 20 19:50:02.519: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2225 /api/v1/namespaces/watch-2225/configmaps/e2e-watch-test-configmap-b b81ba446-f5dd-473e-9156-59f20fe10aed 41542 0 2020-07-20 19:50:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 20 19:50:02.519: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2225 /api/v1/namespaces/watch-2225/configmaps/e2e-watch-test-configmap-b b81ba446-f5dd-473e-9156-59f20fe10aed 41542 0 2020-07-20 19:50:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul 20 19:50:12.542: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2225 /api/v1/namespaces/watch-2225/configmaps/e2e-watch-test-configmap-b b81ba446-f5dd-473e-9156-59f20fe10aed 41556 0 2020-07-20 19:50:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 20 19:50:12.542: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2225 /api/v1/namespaces/watch-2225/configmaps/e2e-watch-test-configmap-b b81ba446-f5dd-473e-9156-59f20fe10aed 41556 0 2020-07-20 19:50:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:50:22.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2225" for this suite.
Jul 20 19:50:28.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:50:29.334: INFO: namespace watch-2225 deletion completed in 6.774654893s

â€¢ [SLOW TEST:67.211 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:50:29.334: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2899
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:50:33.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2899" for this suite.
Jul 20 19:51:19.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:51:20.352: INFO: namespace kubelet-test-2899 deletion completed in 46.59767367s

â€¢ [SLOW TEST:51.018 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:51:20.353: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2812
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-6f1573b4-1a61-46ce-adae-489057b3fb14
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-6f1573b4-1a61-46ce-adae-489057b3fb14
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:51:24.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2812" for this suite.
Jul 20 19:51:36.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:51:37.384: INFO: namespace configmap-2812 deletion completed in 12.601223691s

â€¢ [SLOW TEST:17.032 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:51:37.385: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5337
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 20 19:51:39.724: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:51:39.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5337" for this suite.
Jul 20 19:51:45.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:51:46.433: INFO: namespace container-runtime-5337 deletion completed in 6.639548928s

â€¢ [SLOW TEST:9.049 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:51:46.435: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-166
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Jul 20 19:51:46.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 create -f - --namespace=kubectl-166'
Jul 20 19:51:46.919: INFO: stderr: ""
Jul 20 19:51:46.919: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 20 19:51:47.929: INFO: Selector matched 1 pods for map[app:redis]
Jul 20 19:51:47.929: INFO: Found 0 / 1
Jul 20 19:51:48.933: INFO: Selector matched 1 pods for map[app:redis]
Jul 20 19:51:48.933: INFO: Found 1 / 1
Jul 20 19:51:48.933: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul 20 19:51:48.943: INFO: Selector matched 1 pods for map[app:redis]
Jul 20 19:51:48.944: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 20 19:51:48.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 patch pod redis-master-b5n9r --namespace=kubectl-166 -p {"metadata":{"annotations":{"x":"y"}}}'
Jul 20 19:51:49.061: INFO: stderr: ""
Jul 20 19:51:49.061: INFO: stdout: "pod/redis-master-b5n9r patched\n"
STEP: checking annotations
Jul 20 19:51:49.072: INFO: Selector matched 1 pods for map[app:redis]
Jul 20 19:51:49.072: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:51:49.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-166" for this suite.
Jul 20 19:52:17.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:52:17.711: INFO: namespace kubectl-166 deletion completed in 28.621659363s

â€¢ [SLOW TEST:31.276 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:52:17.716: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-2bec9a4f-1303-4c64-9310-059be4eb5231
STEP: Creating a pod to test consume secrets
Jul 20 19:52:18.016: INFO: Waiting up to 5m0s for pod "pod-secrets-e8758411-cff5-4bc0-8b36-e52ffb5409fa" in namespace "secrets-9" to be "success or failure"
Jul 20 19:52:18.024: INFO: Pod "pod-secrets-e8758411-cff5-4bc0-8b36-e52ffb5409fa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.73251ms
Jul 20 19:52:20.040: INFO: Pod "pod-secrets-e8758411-cff5-4bc0-8b36-e52ffb5409fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024611186s
STEP: Saw pod success
Jul 20 19:52:20.041: INFO: Pod "pod-secrets-e8758411-cff5-4bc0-8b36-e52ffb5409fa" satisfied condition "success or failure"
Jul 20 19:52:20.054: INFO: Trying to get logs from node 10.123.236.227 pod pod-secrets-e8758411-cff5-4bc0-8b36-e52ffb5409fa container secret-volume-test: <nil>
STEP: delete the pod
Jul 20 19:52:20.118: INFO: Waiting for pod pod-secrets-e8758411-cff5-4bc0-8b36-e52ffb5409fa to disappear
Jul 20 19:52:20.132: INFO: Pod pod-secrets-e8758411-cff5-4bc0-8b36-e52ffb5409fa no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:52:20.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9" for this suite.
Jul 20 19:52:26.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:52:26.855: INFO: namespace secrets-9 deletion completed in 6.705341973s

â€¢ [SLOW TEST:9.140 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:52:26.856: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9481
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 20 19:52:27.852: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 20 19:52:29.882: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871547, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871547, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871547, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871547, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 20 19:52:31.892: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871547, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871547, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871547, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871547, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 20 19:52:34.926: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:52:34.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9481" for this suite.
Jul 20 19:52:41.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:52:41.762: INFO: namespace webhook-9481 deletion completed in 6.798768675s
STEP: Destroying namespace "webhook-9481-markers" for this suite.
Jul 20 19:52:47.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:52:48.560: INFO: namespace webhook-9481-markers deletion completed in 6.797971654s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:21.960 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:52:48.817: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6872
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 19:52:49.097: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul 20 19:52:49.127: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 20 19:52:54.139: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 20 19:52:54.139: INFO: Creating deployment "test-rolling-update-deployment"
Jul 20 19:52:54.151: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul 20 19:52:54.169: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jul 20 19:52:56.193: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul 20 19:52:56.202: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 20 19:52:56.231: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6872 /apis/apps/v1/namespaces/deployment-6872/deployments/test-rolling-update-deployment b9ef213a-605a-4792-b2fa-5a79c9a902cf 42108 1 2020-07-20 19:52:54 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0072ef9e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-07-20 19:52:54 +0000 UTC,LastTransitionTime:2020-07-20 19:52:54 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2020-07-20 19:52:55 +0000 UTC,LastTransitionTime:2020-07-20 19:52:54 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 20 19:52:56.242: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-6872 /apis/apps/v1/namespaces/deployment-6872/replicasets/test-rolling-update-deployment-55d946486 92bc5dba-e5f0-4c68-8d1d-e2718cf8b2b6 42098 1 2020-07-20 19:52:54 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment b9ef213a-605a-4792-b2fa-5a79c9a902cf 0xc003115000 0xc003115001}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003115068 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 20 19:52:56.242: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul 20 19:52:56.242: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6872 /apis/apps/v1/namespaces/deployment-6872/replicasets/test-rolling-update-controller f56e9522-40a1-4d30-bab0-13b1e537f175 42107 2 2020-07-20 19:52:49 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment b9ef213a-605a-4792-b2fa-5a79c9a902cf 0xc003114dc7 0xc003114dc8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003114e28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 20 19:52:56.253: INFO: Pod "test-rolling-update-deployment-55d946486-bwtkv" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-bwtkv test-rolling-update-deployment-55d946486- deployment-6872 /api/v1/namespaces/deployment-6872/pods/test-rolling-update-deployment-55d946486-bwtkv 0b3ca139-3263-4d6d-aa86-3352e8f4fcdb 42097 0 2020-07-20 19:52:54 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 92bc5dba-e5f0-4c68-8d1d-e2718cf8b2b6 0xc0031157f0 0xc0031157f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bzbxw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bzbxw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bzbxw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:52:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:52:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:52:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:52:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.227,PodIP:172.30.106.159,StartTime:2020-07-20 19:52:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-20 19:52:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:containerd://6cd39055b91ba4cea754a25de17763f0e970dc42e68d100c2c3a3da3b633a389,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.106.159,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:52:56.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6872" for this suite.
Jul 20 19:53:04.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:53:04.881: INFO: namespace deployment-6872 deletion completed in 8.612060595s

â€¢ [SLOW TEST:16.063 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:53:04.881: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2578
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Jul 20 19:53:05.155: INFO: Waiting up to 5m0s for pod "client-containers-f27cae8c-1404-4e0e-b631-96c9b0c00895" in namespace "containers-2578" to be "success or failure"
Jul 20 19:53:05.174: INFO: Pod "client-containers-f27cae8c-1404-4e0e-b631-96c9b0c00895": Phase="Pending", Reason="", readiness=false. Elapsed: 19.052765ms
Jul 20 19:53:07.184: INFO: Pod "client-containers-f27cae8c-1404-4e0e-b631-96c9b0c00895": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028975589s
STEP: Saw pod success
Jul 20 19:53:07.184: INFO: Pod "client-containers-f27cae8c-1404-4e0e-b631-96c9b0c00895" satisfied condition "success or failure"
Jul 20 19:53:07.195: INFO: Trying to get logs from node 10.123.236.227 pod client-containers-f27cae8c-1404-4e0e-b631-96c9b0c00895 container test-container: <nil>
STEP: delete the pod
Jul 20 19:53:07.258: INFO: Waiting for pod client-containers-f27cae8c-1404-4e0e-b631-96c9b0c00895 to disappear
Jul 20 19:53:07.269: INFO: Pod client-containers-f27cae8c-1404-4e0e-b631-96c9b0c00895 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:53:07.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2578" for this suite.
Jul 20 19:53:13.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:53:13.985: INFO: namespace containers-2578 deletion completed in 6.697537316s

â€¢ [SLOW TEST:9.104 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:53:13.985: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-3954
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jul 20 19:53:14.753: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jul 20 19:53:16.783: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871594, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871594, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871594, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871594, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 20 19:53:19.831: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 19:53:19.844: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:53:21.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3954" for this suite.
Jul 20 19:53:27.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:53:27.969: INFO: namespace crd-webhook-3954 deletion completed in 6.733746011s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:14.204 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:53:28.190: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7522
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 20 19:53:28.514: INFO: Waiting up to 5m0s for pod "pod-d9f7962f-dab8-4af3-ae65-f15428ff5ace" in namespace "emptydir-7522" to be "success or failure"
Jul 20 19:53:28.531: INFO: Pod "pod-d9f7962f-dab8-4af3-ae65-f15428ff5ace": Phase="Pending", Reason="", readiness=false. Elapsed: 17.203248ms
Jul 20 19:53:30.542: INFO: Pod "pod-d9f7962f-dab8-4af3-ae65-f15428ff5ace": Phase="Running", Reason="", readiness=true. Elapsed: 2.028605558s
Jul 20 19:53:32.553: INFO: Pod "pod-d9f7962f-dab8-4af3-ae65-f15428ff5ace": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039083202s
STEP: Saw pod success
Jul 20 19:53:32.553: INFO: Pod "pod-d9f7962f-dab8-4af3-ae65-f15428ff5ace" satisfied condition "success or failure"
Jul 20 19:53:32.567: INFO: Trying to get logs from node 10.123.236.227 pod pod-d9f7962f-dab8-4af3-ae65-f15428ff5ace container test-container: <nil>
STEP: delete the pod
Jul 20 19:53:32.841: INFO: Waiting for pod pod-d9f7962f-dab8-4af3-ae65-f15428ff5ace to disappear
Jul 20 19:53:32.851: INFO: Pod pod-d9f7962f-dab8-4af3-ae65-f15428ff5ace no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:53:32.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7522" for this suite.
Jul 20 19:53:38.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:53:39.447: INFO: namespace emptydir-7522 deletion completed in 6.577498666s

â€¢ [SLOW TEST:11.257 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:53:39.447: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9058
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jul 20 19:53:39.701: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e86f366-41f2-4c98-9dbd-6eb74d1e7763" in namespace "downward-api-9058" to be "success or failure"
Jul 20 19:53:39.709: INFO: Pod "downwardapi-volume-1e86f366-41f2-4c98-9dbd-6eb74d1e7763": Phase="Pending", Reason="", readiness=false. Elapsed: 8.497731ms
Jul 20 19:53:41.725: INFO: Pod "downwardapi-volume-1e86f366-41f2-4c98-9dbd-6eb74d1e7763": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023758012s
Jul 20 19:53:43.742: INFO: Pod "downwardapi-volume-1e86f366-41f2-4c98-9dbd-6eb74d1e7763": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041015032s
STEP: Saw pod success
Jul 20 19:53:43.742: INFO: Pod "downwardapi-volume-1e86f366-41f2-4c98-9dbd-6eb74d1e7763" satisfied condition "success or failure"
Jul 20 19:53:43.753: INFO: Trying to get logs from node 10.123.236.227 pod downwardapi-volume-1e86f366-41f2-4c98-9dbd-6eb74d1e7763 container client-container: <nil>
STEP: delete the pod
Jul 20 19:53:43.814: INFO: Waiting for pod downwardapi-volume-1e86f366-41f2-4c98-9dbd-6eb74d1e7763 to disappear
Jul 20 19:53:43.831: INFO: Pod downwardapi-volume-1e86f366-41f2-4c98-9dbd-6eb74d1e7763 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:53:43.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9058" for this suite.
Jul 20 19:53:49.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:53:50.802: INFO: namespace downward-api-9058 deletion completed in 6.951567488s

â€¢ [SLOW TEST:11.355 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:53:50.803: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7911
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 20 19:53:51.104: INFO: Waiting up to 5m0s for pod "pod-55aa55f8-db29-40d9-a759-bd3965fb28cc" in namespace "emptydir-7911" to be "success or failure"
Jul 20 19:53:51.114: INFO: Pod "pod-55aa55f8-db29-40d9-a759-bd3965fb28cc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008514ms
Jul 20 19:53:53.135: INFO: Pod "pod-55aa55f8-db29-40d9-a759-bd3965fb28cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030431354s
STEP: Saw pod success
Jul 20 19:53:53.135: INFO: Pod "pod-55aa55f8-db29-40d9-a759-bd3965fb28cc" satisfied condition "success or failure"
Jul 20 19:53:53.145: INFO: Trying to get logs from node 10.123.236.227 pod pod-55aa55f8-db29-40d9-a759-bd3965fb28cc container test-container: <nil>
STEP: delete the pod
Jul 20 19:53:53.223: INFO: Waiting for pod pod-55aa55f8-db29-40d9-a759-bd3965fb28cc to disappear
Jul 20 19:53:53.240: INFO: Pod pod-55aa55f8-db29-40d9-a759-bd3965fb28cc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:53:53.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7911" for this suite.
Jul 20 19:53:59.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:53:59.971: INFO: namespace emptydir-7911 deletion completed in 6.710411446s

â€¢ [SLOW TEST:9.168 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:53:59.972: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4850
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 20 19:54:00.937: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 20 19:54:02.966: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871640, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871640, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871640, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871640, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 20 19:54:06.021: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:54:06.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4850" for this suite.
Jul 20 19:54:36.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:54:37.148: INFO: namespace webhook-4850 deletion completed in 30.962764722s
STEP: Destroying namespace "webhook-4850-markers" for this suite.
Jul 20 19:54:43.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:54:43.649: INFO: namespace webhook-4850-markers deletion completed in 6.500793599s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:43.772 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:54:43.744: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8588
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 19:54:44.018: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jul 20 19:54:49.034: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 20 19:54:49.034: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul 20 19:54:51.046: INFO: Creating deployment "test-rollover-deployment"
Jul 20 19:54:51.071: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul 20 19:54:53.091: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul 20 19:54:53.111: INFO: Ensure that both replica sets have 1 created replica
Jul 20 19:54:53.132: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul 20 19:54:53.154: INFO: Updating deployment test-rollover-deployment
Jul 20 19:54:53.154: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul 20 19:54:55.174: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul 20 19:54:55.193: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul 20 19:54:55.213: INFO: all replica sets need to contain the pod-template-hash label
Jul 20 19:54:55.214: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871691, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871691, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871694, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871691, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 20 19:54:57.236: INFO: all replica sets need to contain the pod-template-hash label
Jul 20 19:54:57.236: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871691, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871691, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871694, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871691, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 20 19:54:59.240: INFO: all replica sets need to contain the pod-template-hash label
Jul 20 19:54:59.240: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871691, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871691, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871694, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871691, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 20 19:55:01.235: INFO: all replica sets need to contain the pod-template-hash label
Jul 20 19:55:01.235: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871691, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871691, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871694, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871691, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 20 19:55:03.235: INFO: all replica sets need to contain the pod-template-hash label
Jul 20 19:55:03.235: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871691, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871691, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871694, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871691, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 20 19:55:05.237: INFO: 
Jul 20 19:55:05.237: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 20 19:55:05.268: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8588 /apis/apps/v1/namespaces/deployment-8588/deployments/test-rollover-deployment 4ba9019d-6888-40ad-b3c7-a1d19c473b14 42728 2 2020-07-20 19:54:51 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc000b41d18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-07-20 19:54:51 +0000 UTC,LastTransitionTime:2020-07-20 19:54:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2020-07-20 19:55:04 +0000 UTC,LastTransitionTime:2020-07-20 19:54:51 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 20 19:55:05.281: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-8588 /apis/apps/v1/namespaces/deployment-8588/replicasets/test-rollover-deployment-7d7dc6548c 3d2a8896-45bd-4507-a9df-3ac3f0b276d5 42718 2 2020-07-20 19:54:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 4ba9019d-6888-40ad-b3c7-a1d19c473b14 0xc003e581d7 0xc003e581d8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003e58238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 20 19:55:05.281: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul 20 19:55:05.281: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8588 /apis/apps/v1/namespaces/deployment-8588/replicasets/test-rollover-controller 1149b131-044b-4c04-a824-de0bd6419492 42727 2 2020-07-20 19:54:43 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 4ba9019d-6888-40ad-b3c7-a1d19c473b14 0xc003e58107 0xc003e58108}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003e58168 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 20 19:55:05.281: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-8588 /apis/apps/v1/namespaces/deployment-8588/replicasets/test-rollover-deployment-f6c94f66c d0aa7074-5cd4-40d0-85fe-61afc1cc89ac 42682 2 2020-07-20 19:54:51 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 4ba9019d-6888-40ad-b3c7-a1d19c473b14 0xc003e582a0 0xc003e582a1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003e58318 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 20 19:55:05.294: INFO: Pod "test-rollover-deployment-7d7dc6548c-k556t" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-k556t test-rollover-deployment-7d7dc6548c- deployment-8588 /api/v1/namespaces/deployment-8588/pods/test-rollover-deployment-7d7dc6548c-k556t 3e15644c-30ff-4902-89c9-459f9f7ca7dd 42703 0 2020-07-20 19:54:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c 3d2a8896-45bd-4507-a9df-3ac3f0b276d5 0xc000795367 0xc000795368}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fggvq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fggvq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fggvq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.236.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:54:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:54:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:54:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-20 19:54:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.236.227,PodIP:172.30.106.132,StartTime:2020-07-20 19:54:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-20 19:54:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:containerd://75b3e08688536e4833baab2298c7c7fa9923d243c440f1887304c954bdc8059a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.106.132,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:55:05.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8588" for this suite.
Jul 20 19:55:11.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:55:12.057: INFO: namespace deployment-8588 deletion completed in 6.732885211s

â€¢ [SLOW TEST:28.313 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:55:12.057: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9751
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-199ab312-650b-48a1-8ca6-e267a52b3081
STEP: Creating a pod to test consume secrets
Jul 20 19:55:12.421: INFO: Waiting up to 5m0s for pod "pod-secrets-a1ead181-244f-45d8-aac2-0879fa6cd548" in namespace "secrets-9751" to be "success or failure"
Jul 20 19:55:12.447: INFO: Pod "pod-secrets-a1ead181-244f-45d8-aac2-0879fa6cd548": Phase="Pending", Reason="", readiness=false. Elapsed: 25.609849ms
Jul 20 19:55:14.462: INFO: Pod "pod-secrets-a1ead181-244f-45d8-aac2-0879fa6cd548": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041187727s
STEP: Saw pod success
Jul 20 19:55:14.462: INFO: Pod "pod-secrets-a1ead181-244f-45d8-aac2-0879fa6cd548" satisfied condition "success or failure"
Jul 20 19:55:14.473: INFO: Trying to get logs from node 10.123.236.227 pod pod-secrets-a1ead181-244f-45d8-aac2-0879fa6cd548 container secret-env-test: <nil>
STEP: delete the pod
Jul 20 19:55:14.545: INFO: Waiting for pod pod-secrets-a1ead181-244f-45d8-aac2-0879fa6cd548 to disappear
Jul 20 19:55:14.555: INFO: Pod pod-secrets-a1ead181-244f-45d8-aac2-0879fa6cd548 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:55:14.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9751" for this suite.
Jul 20 19:55:20.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:55:21.361: INFO: namespace secrets-9751 deletion completed in 6.787705785s

â€¢ [SLOW TEST:9.304 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:55:21.361: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6381
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 20 19:55:22.309: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 20 19:55:24.343: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871722, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871722, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871722, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871722, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 20 19:55:27.386: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:55:27.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6381" for this suite.
Jul 20 19:55:33.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:55:34.125: INFO: namespace webhook-6381 deletion completed in 6.533996773s
STEP: Destroying namespace "webhook-6381-markers" for this suite.
Jul 20 19:55:40.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:55:41.007: INFO: namespace webhook-6381-markers deletion completed in 6.881356093s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:19.732 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:55:41.094: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2237
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 20 19:55:41.817: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 20 19:55:43.848: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871741, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871741, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871741, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730871741, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 20 19:55:46.897: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:55:57.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2237" for this suite.
Jul 20 19:56:03.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:56:04.438: INFO: namespace webhook-2237 deletion completed in 6.922721557s
STEP: Destroying namespace "webhook-2237-markers" for this suite.
Jul 20 19:56:10.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:56:11.263: INFO: namespace webhook-2237-markers deletion completed in 6.824584289s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:30.507 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:56:11.608: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-4966
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Jul 20 19:56:11.933: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-4966" to be "success or failure"
Jul 20 19:56:11.951: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 18.619812ms
Jul 20 19:56:13.961: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028551238s
Jul 20 19:56:15.971: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038888471s
STEP: Saw pod success
Jul 20 19:56:15.972: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jul 20 19:56:15.981: INFO: Trying to get logs from node 10.123.236.227 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jul 20 19:56:16.042: INFO: Waiting for pod pod-host-path-test to disappear
Jul 20 19:56:16.052: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:56:16.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-4966" for this suite.
Jul 20 19:56:22.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:56:22.856: INFO: namespace hostpath-4966 deletion completed in 6.787027146s

â€¢ [SLOW TEST:11.248 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:56:22.861: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-426
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jul 20 19:56:23.144: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c86a3cb5-9bf2-4928-bcc2-5733fcd911c0" in namespace "projected-426" to be "success or failure"
Jul 20 19:56:23.155: INFO: Pod "downwardapi-volume-c86a3cb5-9bf2-4928-bcc2-5733fcd911c0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.458359ms
Jul 20 19:56:25.170: INFO: Pod "downwardapi-volume-c86a3cb5-9bf2-4928-bcc2-5733fcd911c0": Phase="Running", Reason="", readiness=true. Elapsed: 2.02615545s
Jul 20 19:56:27.181: INFO: Pod "downwardapi-volume-c86a3cb5-9bf2-4928-bcc2-5733fcd911c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036930325s
STEP: Saw pod success
Jul 20 19:56:27.181: INFO: Pod "downwardapi-volume-c86a3cb5-9bf2-4928-bcc2-5733fcd911c0" satisfied condition "success or failure"
Jul 20 19:56:27.190: INFO: Trying to get logs from node 10.123.236.227 pod downwardapi-volume-c86a3cb5-9bf2-4928-bcc2-5733fcd911c0 container client-container: <nil>
STEP: delete the pod
Jul 20 19:56:27.272: INFO: Waiting for pod downwardapi-volume-c86a3cb5-9bf2-4928-bcc2-5733fcd911c0 to disappear
Jul 20 19:56:27.281: INFO: Pod downwardapi-volume-c86a3cb5-9bf2-4928-bcc2-5733fcd911c0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:56:27.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-426" for this suite.
Jul 20 19:56:33.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:56:33.868: INFO: namespace projected-426 deletion completed in 6.568870207s

â€¢ [SLOW TEST:11.007 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:56:33.868: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4235
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4235.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4235.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 20 19:56:36.373: INFO: DNS probes using dns-4235/dns-test-67338050-33d8-4ac8-ace1-123ad2cc738f succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:56:36.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4235" for this suite.
Jul 20 19:56:42.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:56:43.095: INFO: namespace dns-4235 deletion completed in 6.604473074s

â€¢ [SLOW TEST:9.227 seconds]
[sig-network] DNS
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:56:43.100: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-205
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jul 20 19:56:53.471: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0720 19:56:53.471177      25 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:56:53.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-205" for this suite.
Jul 20 19:56:59.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:57:00.191: INFO: namespace gc-205 deletion completed in 6.704809418s

â€¢ [SLOW TEST:17.091 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:57:00.192: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1903
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 19:57:00.476: INFO: Creating ReplicaSet my-hostname-basic-6af99ced-04f3-455f-aaf3-727761934d4a
Jul 20 19:57:00.499: INFO: Pod name my-hostname-basic-6af99ced-04f3-455f-aaf3-727761934d4a: Found 0 pods out of 1
Jul 20 19:57:05.512: INFO: Pod name my-hostname-basic-6af99ced-04f3-455f-aaf3-727761934d4a: Found 1 pods out of 1
Jul 20 19:57:05.512: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-6af99ced-04f3-455f-aaf3-727761934d4a" is running
Jul 20 19:57:05.524: INFO: Pod "my-hostname-basic-6af99ced-04f3-455f-aaf3-727761934d4a-277nw" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-20 19:57:00 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-20 19:57:03 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-20 19:57:03 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-20 19:57:00 +0000 UTC Reason: Message:}])
Jul 20 19:57:05.524: INFO: Trying to dial the pod
Jul 20 19:57:10.587: INFO: Controller my-hostname-basic-6af99ced-04f3-455f-aaf3-727761934d4a: Got expected result from replica 1 [my-hostname-basic-6af99ced-04f3-455f-aaf3-727761934d4a-277nw]: "my-hostname-basic-6af99ced-04f3-455f-aaf3-727761934d4a-277nw", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:57:10.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1903" for this suite.
Jul 20 19:57:16.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:57:17.167: INFO: namespace replicaset-1903 deletion completed in 6.545518717s

â€¢ [SLOW TEST:16.975 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:57:17.167: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2780
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 20 19:57:21.600: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 20 19:57:21.610: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 20 19:57:23.610: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 20 19:57:23.621: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 20 19:57:25.610: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 20 19:57:25.622: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:57:25.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2780" for this suite.
Jul 20 19:57:37.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:57:38.362: INFO: namespace container-lifecycle-hook-2780 deletion completed in 12.697896092s

â€¢ [SLOW TEST:21.195 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:57:38.362: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-171
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:57:38.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-171" for this suite.
Jul 20 19:57:44.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:57:45.586: INFO: namespace services-171 deletion completed in 6.877858587s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:7.223 seconds]
[sig-network] Services
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:57:45.587: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8099
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Jul 20 19:57:45.868: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:58:06.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8099" for this suite.
Jul 20 19:58:12.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:58:13.333: INFO: namespace crd-publish-openapi-8099 deletion completed in 6.966913079s

â€¢ [SLOW TEST:27.747 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:58:13.334: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6070
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Jul 20 19:58:13.602: INFO: Waiting up to 5m0s for pod "client-containers-d3345a0b-02b9-456f-8ecb-eb8b78c5b49d" in namespace "containers-6070" to be "success or failure"
Jul 20 19:58:13.611: INFO: Pod "client-containers-d3345a0b-02b9-456f-8ecb-eb8b78c5b49d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.701105ms
Jul 20 19:58:15.625: INFO: Pod "client-containers-d3345a0b-02b9-456f-8ecb-eb8b78c5b49d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022215592s
STEP: Saw pod success
Jul 20 19:58:15.626: INFO: Pod "client-containers-d3345a0b-02b9-456f-8ecb-eb8b78c5b49d" satisfied condition "success or failure"
Jul 20 19:58:15.643: INFO: Trying to get logs from node 10.123.236.227 pod client-containers-d3345a0b-02b9-456f-8ecb-eb8b78c5b49d container test-container: <nil>
STEP: delete the pod
Jul 20 19:58:15.704: INFO: Waiting for pod client-containers-d3345a0b-02b9-456f-8ecb-eb8b78c5b49d to disappear
Jul 20 19:58:15.714: INFO: Pod client-containers-d3345a0b-02b9-456f-8ecb-eb8b78c5b49d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:58:15.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6070" for this suite.
Jul 20 19:58:21.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:58:22.377: INFO: namespace containers-6070 deletion completed in 6.646265376s

â€¢ [SLOW TEST:9.043 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:58:22.377: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4575
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-4575, will wait for the garbage collector to delete the pods
Jul 20 19:58:27.002: INFO: Deleting Job.batch foo took: 25.605563ms
Jul 20 19:58:27.202: INFO: Terminating Job.batch foo pods took: 200.30499ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:59:05.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4575" for this suite.
Jul 20 19:59:11.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:59:11.758: INFO: namespace job-4575 deletion completed in 6.515593008s

â€¢ [SLOW TEST:49.381 seconds]
[sig-apps] Job
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:59:11.759: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7934
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7934
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-7934
Jul 20 19:59:12.041: INFO: Found 0 stateful pods, waiting for 1
Jul 20 19:59:22.053: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jul 20 19:59:22.106: INFO: Deleting all statefulset in ns statefulset-7934
Jul 20 19:59:22.117: INFO: Scaling statefulset ss to 0
Jul 20 19:59:42.159: INFO: Waiting for statefulset status.replicas updated to 0
Jul 20 19:59:42.169: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:59:42.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7934" for this suite.
Jul 20 19:59:48.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:59:48.864: INFO: namespace statefulset-7934 deletion completed in 6.629653035s

â€¢ [SLOW TEST:37.106 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:59:48.864: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7713
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Jul 20 19:59:51.745: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7713 pod-service-account-c72bd4e5-57fb-45a9-95bc-0587da7d53d5 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jul 20 19:59:52.095: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7713 pod-service-account-c72bd4e5-57fb-45a9-95bc-0587da7d53d5 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jul 20 19:59:52.332: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7713 pod-service-account-c72bd4e5-57fb-45a9-95bc-0587da7d53d5 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 19:59:52.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7713" for this suite.
Jul 20 19:59:58.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 19:59:59.383: INFO: namespace svcaccounts-7713 deletion completed in 6.712447633s

â€¢ [SLOW TEST:10.519 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 19:59:59.384: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1796
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jul 20 19:59:59.691: INFO: Waiting up to 5m0s for pod "downward-api-9f2bb0e6-baa2-4769-8c81-a5ecedbc2459" in namespace "downward-api-1796" to be "success or failure"
Jul 20 19:59:59.705: INFO: Pod "downward-api-9f2bb0e6-baa2-4769-8c81-a5ecedbc2459": Phase="Pending", Reason="", readiness=false. Elapsed: 14.172004ms
Jul 20 20:00:01.717: INFO: Pod "downward-api-9f2bb0e6-baa2-4769-8c81-a5ecedbc2459": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025623064s
STEP: Saw pod success
Jul 20 20:00:01.717: INFO: Pod "downward-api-9f2bb0e6-baa2-4769-8c81-a5ecedbc2459" satisfied condition "success or failure"
Jul 20 20:00:01.727: INFO: Trying to get logs from node 10.123.236.227 pod downward-api-9f2bb0e6-baa2-4769-8c81-a5ecedbc2459 container dapi-container: <nil>
STEP: delete the pod
Jul 20 20:00:01.829: INFO: Waiting for pod downward-api-9f2bb0e6-baa2-4769-8c81-a5ecedbc2459 to disappear
Jul 20 20:00:01.840: INFO: Pod downward-api-9f2bb0e6-baa2-4769-8c81-a5ecedbc2459 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:00:01.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1796" for this suite.
Jul 20 20:00:10.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:00:10.879: INFO: namespace downward-api-1796 deletion completed in 9.015544458s

â€¢ [SLOW TEST:11.495 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:00:10.881: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9933
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Jul 20 20:00:11.204: INFO: Waiting up to 5m0s for pod "var-expansion-c2fce9c1-20bc-4637-9725-c73c1e5a83bb" in namespace "var-expansion-9933" to be "success or failure"
Jul 20 20:00:11.217: INFO: Pod "var-expansion-c2fce9c1-20bc-4637-9725-c73c1e5a83bb": Phase="Pending", Reason="", readiness=false. Elapsed: 13.277878ms
Jul 20 20:00:13.244: INFO: Pod "var-expansion-c2fce9c1-20bc-4637-9725-c73c1e5a83bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03975737s
Jul 20 20:00:15.253: INFO: Pod "var-expansion-c2fce9c1-20bc-4637-9725-c73c1e5a83bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049003357s
STEP: Saw pod success
Jul 20 20:00:15.253: INFO: Pod "var-expansion-c2fce9c1-20bc-4637-9725-c73c1e5a83bb" satisfied condition "success or failure"
Jul 20 20:00:15.262: INFO: Trying to get logs from node 10.123.236.227 pod var-expansion-c2fce9c1-20bc-4637-9725-c73c1e5a83bb container dapi-container: <nil>
STEP: delete the pod
Jul 20 20:00:15.353: INFO: Waiting for pod var-expansion-c2fce9c1-20bc-4637-9725-c73c1e5a83bb to disappear
Jul 20 20:00:15.376: INFO: Pod var-expansion-c2fce9c1-20bc-4637-9725-c73c1e5a83bb no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:00:15.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9933" for this suite.
Jul 20 20:00:21.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:00:22.020: INFO: namespace var-expansion-9933 deletion completed in 6.622287759s

â€¢ [SLOW TEST:11.139 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:00:22.021: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9969
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-2309636f-e8a6-4c5b-81e6-8876e85e0557
STEP: Creating a pod to test consume secrets
Jul 20 20:00:22.587: INFO: Waiting up to 5m0s for pod "pod-secrets-b991ea5e-b88f-4691-8be4-6bf5af39d85a" in namespace "secrets-9969" to be "success or failure"
Jul 20 20:00:22.604: INFO: Pod "pod-secrets-b991ea5e-b88f-4691-8be4-6bf5af39d85a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.796246ms
Jul 20 20:00:24.616: INFO: Pod "pod-secrets-b991ea5e-b88f-4691-8be4-6bf5af39d85a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028940033s
STEP: Saw pod success
Jul 20 20:00:24.616: INFO: Pod "pod-secrets-b991ea5e-b88f-4691-8be4-6bf5af39d85a" satisfied condition "success or failure"
Jul 20 20:00:24.628: INFO: Trying to get logs from node 10.123.236.227 pod pod-secrets-b991ea5e-b88f-4691-8be4-6bf5af39d85a container secret-volume-test: <nil>
STEP: delete the pod
Jul 20 20:00:24.719: INFO: Waiting for pod pod-secrets-b991ea5e-b88f-4691-8be4-6bf5af39d85a to disappear
Jul 20 20:00:24.727: INFO: Pod pod-secrets-b991ea5e-b88f-4691-8be4-6bf5af39d85a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:00:24.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9969" for this suite.
Jul 20 20:00:30.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:00:31.459: INFO: namespace secrets-9969 deletion completed in 6.716144887s

â€¢ [SLOW TEST:9.438 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:00:31.459: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9853
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:00:48.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9853" for this suite.
Jul 20 20:00:54.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:00:54.779: INFO: namespace resourcequota-9853 deletion completed in 6.643264202s

â€¢ [SLOW TEST:23.320 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:00:54.783: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6580
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Jul 20 20:00:55.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 cluster-info'
Jul 20 20:00:55.165: INFO: stderr: ""
Jul 20 20:00:55.165: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\x1b[0;32mNodeLocalDNS\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/node-local-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:00:55.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6580" for this suite.
Jul 20 20:01:01.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:01:01.894: INFO: namespace kubectl-6580 deletion completed in 6.712863848s

â€¢ [SLOW TEST:7.111 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:01:01.895: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9706
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Jul 20 20:01:06.883: INFO: Successfully updated pod "labelsupdateb57a783d-8332-4dd7-a9cc-686c11555d20"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:01:08.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9706" for this suite.
Jul 20 20:01:21.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:01:22.091: INFO: namespace downward-api-9706 deletion completed in 13.11226833s

â€¢ [SLOW TEST:20.196 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:01:22.091: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5196
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 20:01:22.396: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 20 20:01:25.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 --namespace=crd-publish-openapi-5196 create -f -'
Jul 20 20:01:26.491: INFO: stderr: ""
Jul 20 20:01:26.491: INFO: stdout: "e2e-test-crd-publish-openapi-9438-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul 20 20:01:26.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 --namespace=crd-publish-openapi-5196 delete e2e-test-crd-publish-openapi-9438-crds test-cr'
Jul 20 20:01:26.677: INFO: stderr: ""
Jul 20 20:01:26.677: INFO: stdout: "e2e-test-crd-publish-openapi-9438-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jul 20 20:01:26.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 --namespace=crd-publish-openapi-5196 apply -f -'
Jul 20 20:01:27.385: INFO: stderr: ""
Jul 20 20:01:27.385: INFO: stdout: "e2e-test-crd-publish-openapi-9438-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul 20 20:01:27.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 --namespace=crd-publish-openapi-5196 delete e2e-test-crd-publish-openapi-9438-crds test-cr'
Jul 20 20:01:27.586: INFO: stderr: ""
Jul 20 20:01:27.586: INFO: stdout: "e2e-test-crd-publish-openapi-9438-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jul 20 20:01:27.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 explain e2e-test-crd-publish-openapi-9438-crds'
Jul 20 20:01:27.907: INFO: stderr: ""
Jul 20 20:01:27.907: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9438-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:01:31.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5196" for this suite.
Jul 20 20:01:37.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:01:38.150: INFO: namespace crd-publish-openapi-5196 deletion completed in 6.612051646s

â€¢ [SLOW TEST:16.059 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:01:38.150: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7223
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 20:01:38.403: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:01:42.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7223" for this suite.
Jul 20 20:02:28.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:02:29.450: INFO: namespace pods-7223 deletion completed in 46.875031176s

â€¢ [SLOW TEST:51.300 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:02:29.451: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7105
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 20 20:02:29.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-7105'
Jul 20 20:02:29.819: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 20 20:02:29.819: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Jul 20 20:02:29.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 delete jobs e2e-test-httpd-job --namespace=kubectl-7105'
Jul 20 20:02:29.959: INFO: stderr: ""
Jul 20 20:02:29.959: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:02:29.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7105" for this suite.
Jul 20 20:02:36.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:02:36.878: INFO: namespace kubectl-7105 deletion completed in 6.905917677s

â€¢ [SLOW TEST:7.428 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:02:36.879: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3584
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-6680c95c-ff8a-40a9-a625-4256a5f13284
STEP: Creating a pod to test consume secrets
Jul 20 20:02:37.283: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-67e4e15e-9620-42da-aa8f-6f3cfefbb4d9" in namespace "projected-3584" to be "success or failure"
Jul 20 20:02:37.299: INFO: Pod "pod-projected-secrets-67e4e15e-9620-42da-aa8f-6f3cfefbb4d9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.983518ms
Jul 20 20:02:39.312: INFO: Pod "pod-projected-secrets-67e4e15e-9620-42da-aa8f-6f3cfefbb4d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027800559s
Jul 20 20:02:41.323: INFO: Pod "pod-projected-secrets-67e4e15e-9620-42da-aa8f-6f3cfefbb4d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039624792s
STEP: Saw pod success
Jul 20 20:02:41.323: INFO: Pod "pod-projected-secrets-67e4e15e-9620-42da-aa8f-6f3cfefbb4d9" satisfied condition "success or failure"
Jul 20 20:02:41.337: INFO: Trying to get logs from node 10.123.236.227 pod pod-projected-secrets-67e4e15e-9620-42da-aa8f-6f3cfefbb4d9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 20 20:02:41.441: INFO: Waiting for pod pod-projected-secrets-67e4e15e-9620-42da-aa8f-6f3cfefbb4d9 to disappear
Jul 20 20:02:41.459: INFO: Pod pod-projected-secrets-67e4e15e-9620-42da-aa8f-6f3cfefbb4d9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:02:41.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3584" for this suite.
Jul 20 20:02:47.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:02:48.197: INFO: namespace projected-3584 deletion completed in 6.714707999s

â€¢ [SLOW TEST:11.318 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:02:48.199: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-541
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 20 20:02:48.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-541'
Jul 20 20:02:48.619: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 20 20:02:48.620: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: rolling-update to same image controller
Jul 20 20:02:48.641: INFO: scanned /root for discovery docs: <nil>
Jul 20 20:02:48.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-541'
Jul 20 20:03:04.882: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul 20 20:03:04.882: INFO: stdout: "Created e2e-test-httpd-rc-c61f51b76482d346aed0e5db793505bb\nScaling up e2e-test-httpd-rc-c61f51b76482d346aed0e5db793505bb from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-c61f51b76482d346aed0e5db793505bb up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-c61f51b76482d346aed0e5db793505bb to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Jul 20 20:03:04.882: INFO: stdout: "Created e2e-test-httpd-rc-c61f51b76482d346aed0e5db793505bb\nScaling up e2e-test-httpd-rc-c61f51b76482d346aed0e5db793505bb from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-c61f51b76482d346aed0e5db793505bb up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-c61f51b76482d346aed0e5db793505bb to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Jul 20 20:03:04.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-541'
Jul 20 20:03:05.206: INFO: stderr: ""
Jul 20 20:03:05.206: INFO: stdout: "e2e-test-httpd-rc-c61f51b76482d346aed0e5db793505bb-z2p7l "
Jul 20 20:03:05.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods e2e-test-httpd-rc-c61f51b76482d346aed0e5db793505bb-z2p7l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-541'
Jul 20 20:03:05.286: INFO: stderr: ""
Jul 20 20:03:05.286: INFO: stdout: "true"
Jul 20 20:03:05.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 get pods e2e-test-httpd-rc-c61f51b76482d346aed0e5db793505bb-z2p7l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-541'
Jul 20 20:03:05.368: INFO: stderr: ""
Jul 20 20:03:05.368: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Jul 20 20:03:05.368: INFO: e2e-test-httpd-rc-c61f51b76482d346aed0e5db793505bb-z2p7l is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Jul 20 20:03:05.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 delete rc e2e-test-httpd-rc --namespace=kubectl-541'
Jul 20 20:03:05.503: INFO: stderr: ""
Jul 20 20:03:05.503: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:03:05.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-541" for this suite.
Jul 20 20:03:17.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:03:18.205: INFO: namespace kubectl-541 deletion completed in 12.682491256s

â€¢ [SLOW TEST:30.007 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:03:18.208: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5690
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-5690
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 20 20:03:18.476: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 20 20:03:40.883: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.106.187:8080/dial?request=hostName&protocol=http&host=172.30.176.40&port=8080&tries=1'] Namespace:pod-network-test-5690 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 20 20:03:40.883: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 20:03:41.040: INFO: Waiting for endpoints: map[]
Jul 20 20:03:41.053: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.106.187:8080/dial?request=hostName&protocol=http&host=172.30.106.186&port=8080&tries=1'] Namespace:pod-network-test-5690 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 20 20:03:41.053: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 20:03:41.220: INFO: Waiting for endpoints: map[]
Jul 20 20:03:41.255: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.106.187:8080/dial?request=hostName&protocol=http&host=172.30.159.103&port=8080&tries=1'] Namespace:pod-network-test-5690 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 20 20:03:41.255: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 20:03:41.396: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:03:41.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5690" for this suite.
Jul 20 20:03:53.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:03:54.438: INFO: namespace pod-network-test-5690 deletion completed in 13.02402174s

â€¢ [SLOW TEST:36.230 seconds]
[sig-network] Networking
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:03:54.439: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7698
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-031a7b5f-a429-424c-b9ef-bbb838530d29 in namespace container-probe-7698
Jul 20 20:03:58.726: INFO: Started pod test-webserver-031a7b5f-a429-424c-b9ef-bbb838530d29 in namespace container-probe-7698
STEP: checking the pod's current state and verifying that restartCount is present
Jul 20 20:03:58.738: INFO: Initial restart count of pod test-webserver-031a7b5f-a429-424c-b9ef-bbb838530d29 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:08:00.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7698" for this suite.
Jul 20 20:08:06.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:08:07.153: INFO: namespace container-probe-7698 deletion completed in 6.534837337s

â€¢ [SLOW TEST:252.714 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:08:07.153: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5258
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with configMap that has name projected-configmap-test-upd-c0dc5697-285a-4c76-bb1f-ff0d5b527afe
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-c0dc5697-285a-4c76-bb1f-ff0d5b527afe
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:09:27.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5258" for this suite.
Jul 20 20:09:55.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:09:55.771: INFO: namespace projected-5258 deletion completed in 28.627869773s

â€¢ [SLOW TEST:108.617 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:09:55.771: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2944
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 20 20:09:56.031: INFO: Waiting up to 5m0s for pod "pod-56adcf33-fe71-4e18-a017-94f846fae10e" in namespace "emptydir-2944" to be "success or failure"
Jul 20 20:09:56.043: INFO: Pod "pod-56adcf33-fe71-4e18-a017-94f846fae10e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.37184ms
Jul 20 20:09:58.064: INFO: Pod "pod-56adcf33-fe71-4e18-a017-94f846fae10e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032713769s
STEP: Saw pod success
Jul 20 20:09:58.064: INFO: Pod "pod-56adcf33-fe71-4e18-a017-94f846fae10e" satisfied condition "success or failure"
Jul 20 20:09:58.078: INFO: Trying to get logs from node 10.123.236.227 pod pod-56adcf33-fe71-4e18-a017-94f846fae10e container test-container: <nil>
STEP: delete the pod
Jul 20 20:09:58.173: INFO: Waiting for pod pod-56adcf33-fe71-4e18-a017-94f846fae10e to disappear
Jul 20 20:09:58.186: INFO: Pod pod-56adcf33-fe71-4e18-a017-94f846fae10e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:09:58.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2944" for this suite.
Jul 20 20:10:04.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:10:04.787: INFO: namespace emptydir-2944 deletion completed in 6.585479394s

â€¢ [SLOW TEST:9.015 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:10:04.787: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-8651
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:10:05.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-8651" for this suite.
Jul 20 20:10:11.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:10:11.740: INFO: namespace tables-8651 deletion completed in 6.687383524s

â€¢ [SLOW TEST:6.953 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:10:11.743: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8797
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 20 20:10:16.260: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 20 20:10:16.269: INFO: Pod pod-with-poststart-http-hook still exists
Jul 20 20:10:18.269: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 20 20:10:18.281: INFO: Pod pod-with-poststart-http-hook still exists
Jul 20 20:10:20.269: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 20 20:10:20.281: INFO: Pod pod-with-poststart-http-hook still exists
Jul 20 20:10:22.269: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 20 20:10:22.280: INFO: Pod pod-with-poststart-http-hook still exists
Jul 20 20:10:24.269: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 20 20:10:24.278: INFO: Pod pod-with-poststart-http-hook still exists
Jul 20 20:10:26.269: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 20 20:10:26.285: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:10:26.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8797" for this suite.
Jul 20 20:10:54.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:10:54.916: INFO: namespace container-lifecycle-hook-8797 deletion completed in 28.61401423s

â€¢ [SLOW TEST:43.173 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:10:54.916: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7707
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-362a003c-1aae-4dd6-b324-b6d6a3716d3f
STEP: Creating secret with name s-test-opt-upd-def938b5-06f8-4df4-b3b8-697c752be467
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-362a003c-1aae-4dd6-b324-b6d6a3716d3f
STEP: Updating secret s-test-opt-upd-def938b5-06f8-4df4-b3b8-697c752be467
STEP: Creating secret with name s-test-opt-create-cc30de67-ad83-498c-a9ac-f83ea1ae5360
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:12:03.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7707" for this suite.
Jul 20 20:12:15.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:12:15.653: INFO: namespace secrets-7707 deletion completed in 12.567440759s

â€¢ [SLOW TEST:80.737 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:12:15.654: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3112
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 20 20:12:16.637: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul 20 20:12:18.668: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730872736, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730872736, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730872736, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730872736, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 20 20:12:21.718: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 20:12:21.727: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:12:22.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3112" for this suite.
Jul 20 20:12:28.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:12:29.394: INFO: namespace webhook-3112 deletion completed in 6.636048403s
STEP: Destroying namespace "webhook-3112-markers" for this suite.
Jul 20 20:12:35.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:12:35.941: INFO: namespace webhook-3112-markers deletion completed in 6.547120974s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:20.381 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:12:36.035: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8404
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0720 20:12:36.923703      25 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 20 20:12:36.924: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:12:36.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8404" for this suite.
Jul 20 20:12:43.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:12:43.544: INFO: namespace gc-8404 deletion completed in 6.603167127s

â€¢ [SLOW TEST:7.509 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:12:43.545: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-773
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 20:12:43.794: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jul 20 20:12:44.972: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:12:45.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-773" for this suite.
Jul 20 20:12:54.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:12:54.624: INFO: namespace replication-controller-773 deletion completed in 8.604652621s

â€¢ [SLOW TEST:11.079 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:12:54.625: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7741
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jul 20 20:12:54.917: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5d9ea1a1-9439-4397-92d8-3d4b8199d8c2" in namespace "projected-7741" to be "success or failure"
Jul 20 20:12:54.933: INFO: Pod "downwardapi-volume-5d9ea1a1-9439-4397-92d8-3d4b8199d8c2": Phase="Pending", Reason="", readiness=false. Elapsed: 15.669648ms
Jul 20 20:12:56.945: INFO: Pod "downwardapi-volume-5d9ea1a1-9439-4397-92d8-3d4b8199d8c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028470058s
Jul 20 20:12:58.963: INFO: Pod "downwardapi-volume-5d9ea1a1-9439-4397-92d8-3d4b8199d8c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045869487s
STEP: Saw pod success
Jul 20 20:12:58.963: INFO: Pod "downwardapi-volume-5d9ea1a1-9439-4397-92d8-3d4b8199d8c2" satisfied condition "success or failure"
Jul 20 20:12:58.974: INFO: Trying to get logs from node 10.123.236.227 pod downwardapi-volume-5d9ea1a1-9439-4397-92d8-3d4b8199d8c2 container client-container: <nil>
STEP: delete the pod
Jul 20 20:12:59.045: INFO: Waiting for pod downwardapi-volume-5d9ea1a1-9439-4397-92d8-3d4b8199d8c2 to disappear
Jul 20 20:12:59.053: INFO: Pod downwardapi-volume-5d9ea1a1-9439-4397-92d8-3d4b8199d8c2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:12:59.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7741" for this suite.
Jul 20 20:13:05.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:13:05.744: INFO: namespace projected-7741 deletion completed in 6.676849003s

â€¢ [SLOW TEST:11.119 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:13:05.745: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5686
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-d01ad282-ee14-4876-854e-482f39326563
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:13:08.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5686" for this suite.
Jul 20 20:13:36.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:13:36.995: INFO: namespace configmap-5686 deletion completed in 28.780029063s

â€¢ [SLOW TEST:31.250 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:13:36.995: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6995
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:13:50.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6995" for this suite.
Jul 20 20:13:56.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:13:57.189: INFO: namespace resourcequota-6995 deletion completed in 6.722564394s

â€¢ [SLOW TEST:20.194 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:13:57.190: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-3255
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:14:03.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3255" for this suite.
Jul 20 20:14:09.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:14:10.062: INFO: namespace job-3255 deletion completed in 6.546031538s

â€¢ [SLOW TEST:12.873 seconds]
[sig-apps] Job
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:14:10.063: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6300
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-c874b5da-27bf-4509-a584-d173ec5e5510
STEP: Creating a pod to test consume configMaps
Jul 20 20:14:10.323: INFO: Waiting up to 5m0s for pod "pod-configmaps-ed0d1988-cff1-4c9c-b3c9-93ed6524497f" in namespace "configmap-6300" to be "success or failure"
Jul 20 20:14:10.334: INFO: Pod "pod-configmaps-ed0d1988-cff1-4c9c-b3c9-93ed6524497f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.945125ms
Jul 20 20:14:12.344: INFO: Pod "pod-configmaps-ed0d1988-cff1-4c9c-b3c9-93ed6524497f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020390383s
STEP: Saw pod success
Jul 20 20:14:12.344: INFO: Pod "pod-configmaps-ed0d1988-cff1-4c9c-b3c9-93ed6524497f" satisfied condition "success or failure"
Jul 20 20:14:12.353: INFO: Trying to get logs from node 10.123.236.227 pod pod-configmaps-ed0d1988-cff1-4c9c-b3c9-93ed6524497f container configmap-volume-test: <nil>
STEP: delete the pod
Jul 20 20:14:12.411: INFO: Waiting for pod pod-configmaps-ed0d1988-cff1-4c9c-b3c9-93ed6524497f to disappear
Jul 20 20:14:12.420: INFO: Pod pod-configmaps-ed0d1988-cff1-4c9c-b3c9-93ed6524497f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:14:12.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6300" for this suite.
Jul 20 20:14:18.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:14:19.073: INFO: namespace configmap-6300 deletion completed in 6.634744918s

â€¢ [SLOW TEST:9.010 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:14:19.074: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8019
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-71b54394-4021-4e7e-9848-41abaf70c72a
STEP: Creating a pod to test consume configMaps
Jul 20 20:14:19.358: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7ebbde7d-a198-43fd-ab7d-6a9bc7a24b14" in namespace "projected-8019" to be "success or failure"
Jul 20 20:14:19.369: INFO: Pod "pod-projected-configmaps-7ebbde7d-a198-43fd-ab7d-6a9bc7a24b14": Phase="Pending", Reason="", readiness=false. Elapsed: 10.413078ms
Jul 20 20:14:21.379: INFO: Pod "pod-projected-configmaps-7ebbde7d-a198-43fd-ab7d-6a9bc7a24b14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020713804s
STEP: Saw pod success
Jul 20 20:14:21.379: INFO: Pod "pod-projected-configmaps-7ebbde7d-a198-43fd-ab7d-6a9bc7a24b14" satisfied condition "success or failure"
Jul 20 20:14:21.395: INFO: Trying to get logs from node 10.123.236.227 pod pod-projected-configmaps-7ebbde7d-a198-43fd-ab7d-6a9bc7a24b14 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 20 20:14:21.461: INFO: Waiting for pod pod-projected-configmaps-7ebbde7d-a198-43fd-ab7d-6a9bc7a24b14 to disappear
Jul 20 20:14:21.475: INFO: Pod pod-projected-configmaps-7ebbde7d-a198-43fd-ab7d-6a9bc7a24b14 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:14:21.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8019" for this suite.
Jul 20 20:14:27.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:14:28.199: INFO: namespace projected-8019 deletion completed in 6.704823616s

â€¢ [SLOW TEST:9.125 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:14:28.199: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3968
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3968.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3968.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3968.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3968.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 20 20:14:30.676: INFO: DNS probes using dns-test-a54b00fb-6004-4a84-9a73-4eb9beb5d6f3 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3968.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3968.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3968.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3968.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 20 20:14:34.885: INFO: File wheezy_udp@dns-test-service-3.dns-3968.svc.cluster.local from pod  dns-3968/dns-test-465a5d18-d2ce-42a2-9595-ade16de46f48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 20 20:14:34.903: INFO: File jessie_udp@dns-test-service-3.dns-3968.svc.cluster.local from pod  dns-3968/dns-test-465a5d18-d2ce-42a2-9595-ade16de46f48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 20 20:14:34.903: INFO: Lookups using dns-3968/dns-test-465a5d18-d2ce-42a2-9595-ade16de46f48 failed for: [wheezy_udp@dns-test-service-3.dns-3968.svc.cluster.local jessie_udp@dns-test-service-3.dns-3968.svc.cluster.local]

Jul 20 20:14:39.927: INFO: File wheezy_udp@dns-test-service-3.dns-3968.svc.cluster.local from pod  dns-3968/dns-test-465a5d18-d2ce-42a2-9595-ade16de46f48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 20 20:14:39.942: INFO: File jessie_udp@dns-test-service-3.dns-3968.svc.cluster.local from pod  dns-3968/dns-test-465a5d18-d2ce-42a2-9595-ade16de46f48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 20 20:14:39.942: INFO: Lookups using dns-3968/dns-test-465a5d18-d2ce-42a2-9595-ade16de46f48 failed for: [wheezy_udp@dns-test-service-3.dns-3968.svc.cluster.local jessie_udp@dns-test-service-3.dns-3968.svc.cluster.local]

Jul 20 20:14:44.934: INFO: File jessie_udp@dns-test-service-3.dns-3968.svc.cluster.local from pod  dns-3968/dns-test-465a5d18-d2ce-42a2-9595-ade16de46f48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 20 20:14:44.934: INFO: Lookups using dns-3968/dns-test-465a5d18-d2ce-42a2-9595-ade16de46f48 failed for: [jessie_udp@dns-test-service-3.dns-3968.svc.cluster.local]

Jul 20 20:14:49.919: INFO: File wheezy_udp@dns-test-service-3.dns-3968.svc.cluster.local from pod  dns-3968/dns-test-465a5d18-d2ce-42a2-9595-ade16de46f48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 20 20:14:49.936: INFO: Lookups using dns-3968/dns-test-465a5d18-d2ce-42a2-9595-ade16de46f48 failed for: [wheezy_udp@dns-test-service-3.dns-3968.svc.cluster.local]

Jul 20 20:14:54.920: INFO: File wheezy_udp@dns-test-service-3.dns-3968.svc.cluster.local from pod  dns-3968/dns-test-465a5d18-d2ce-42a2-9595-ade16de46f48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 20 20:14:54.936: INFO: Lookups using dns-3968/dns-test-465a5d18-d2ce-42a2-9595-ade16de46f48 failed for: [wheezy_udp@dns-test-service-3.dns-3968.svc.cluster.local]

Jul 20 20:14:59.919: INFO: File wheezy_udp@dns-test-service-3.dns-3968.svc.cluster.local from pod  dns-3968/dns-test-465a5d18-d2ce-42a2-9595-ade16de46f48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 20 20:14:59.947: INFO: Lookups using dns-3968/dns-test-465a5d18-d2ce-42a2-9595-ade16de46f48 failed for: [wheezy_udp@dns-test-service-3.dns-3968.svc.cluster.local]

Jul 20 20:15:04.943: INFO: DNS probes using dns-test-465a5d18-d2ce-42a2-9595-ade16de46f48 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3968.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3968.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3968.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3968.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 20 20:15:09.219: INFO: DNS probes using dns-test-66543855-c98a-4f57-93e0-c5413fb13930 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:15:09.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3968" for this suite.
Jul 20 20:15:17.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:15:18.112: INFO: namespace dns-3968 deletion completed in 8.737970244s

â€¢ [SLOW TEST:49.913 seconds]
[sig-network] DNS
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:15:18.113: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3813
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-3813/configmap-test-1b4ebea6-cae4-487d-bed9-38c16c985916
STEP: Creating a pod to test consume configMaps
Jul 20 20:15:18.444: INFO: Waiting up to 5m0s for pod "pod-configmaps-6ef49ddc-d5fa-4ef6-9f84-90fb9fcbeac1" in namespace "configmap-3813" to be "success or failure"
Jul 20 20:15:18.491: INFO: Pod "pod-configmaps-6ef49ddc-d5fa-4ef6-9f84-90fb9fcbeac1": Phase="Pending", Reason="", readiness=false. Elapsed: 47.227388ms
Jul 20 20:15:20.502: INFO: Pod "pod-configmaps-6ef49ddc-d5fa-4ef6-9f84-90fb9fcbeac1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.057572306s
STEP: Saw pod success
Jul 20 20:15:20.502: INFO: Pod "pod-configmaps-6ef49ddc-d5fa-4ef6-9f84-90fb9fcbeac1" satisfied condition "success or failure"
Jul 20 20:15:20.512: INFO: Trying to get logs from node 10.123.236.227 pod pod-configmaps-6ef49ddc-d5fa-4ef6-9f84-90fb9fcbeac1 container env-test: <nil>
STEP: delete the pod
Jul 20 20:15:20.575: INFO: Waiting for pod pod-configmaps-6ef49ddc-d5fa-4ef6-9f84-90fb9fcbeac1 to disappear
Jul 20 20:15:20.584: INFO: Pod pod-configmaps-6ef49ddc-d5fa-4ef6-9f84-90fb9fcbeac1 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:15:20.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3813" for this suite.
Jul 20 20:15:26.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:15:27.101: INFO: namespace configmap-3813 deletion completed in 6.503001519s

â€¢ [SLOW TEST:8.989 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:15:27.102: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9565
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 20 20:15:27.879: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 20 20:15:29.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730872927, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730872927, loc:(*time.Location)(0x78a7940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730872927, loc:(*time.Location)(0x78a7940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730872927, loc:(*time.Location)(0x78a7940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 20 20:15:32.961: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 20:15:32.973: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-860-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:15:34.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9565" for this suite.
Jul 20 20:15:40.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:15:41.283: INFO: namespace webhook-9565 deletion completed in 7.040300071s
STEP: Destroying namespace "webhook-9565-markers" for this suite.
Jul 20 20:15:47.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:15:47.947: INFO: namespace webhook-9565-markers deletion completed in 6.664150647s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:21.248 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:15:48.351: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3058
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3058.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3058.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3058.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3058.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3058.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3058.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3058.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3058.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3058.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3058.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 20 20:15:50.712: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:15:50.766: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:15:50.787: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:15:50.803: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:15:50.864: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:15:50.879: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:15:50.894: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:15:50.911: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:15:50.952: INFO: Lookups using dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3058.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3058.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local jessie_udp@dns-test-service-2.dns-3058.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3058.svc.cluster.local]

Jul 20 20:15:55.970: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:15:55.989: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:15:56.080: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:15:56.098: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:15:56.136: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:15:56.168: INFO: Lookups using dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3058.svc.cluster.local]

Jul 20 20:16:00.971: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:16:00.992: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:16:01.072: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:16:01.087: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:16:01.121: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:16:01.163: INFO: Lookups using dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3058.svc.cluster.local]

Jul 20 20:16:05.976: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:16:05.994: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:16:06.110: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:16:06.127: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:16:06.195: INFO: Lookups using dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local]

Jul 20 20:16:10.972: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:16:10.991: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:16:11.127: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:16:11.147: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:16:11.217: INFO: Lookups using dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local]

Jul 20 20:16:15.969: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:16:15.984: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:16:16.083: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:16:16.099: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local from pod dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d: the server could not find the requested resource (get pods dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d)
Jul 20 20:16:16.171: INFO: Lookups using dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3058.svc.cluster.local]

Jul 20 20:16:21.164: INFO: DNS probes using dns-3058/dns-test-1ff98b74-0edf-4504-a9a0-24fb80b0e00d succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:16:21.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3058" for this suite.
Jul 20 20:16:27.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:16:28.410: INFO: namespace dns-3058 deletion completed in 7.020379972s

â€¢ [SLOW TEST:40.059 seconds]
[sig-network] DNS
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:16:28.410: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9120
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jul 20 20:16:28.682: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 20 20:16:28.733: INFO: Waiting for terminating namespaces to be deleted...
Jul 20 20:16:28.766: INFO: 
Logging pods the kubelet thinks is on node 10.123.236.220 before test
Jul 20 20:16:28.833: INFO: public-crbsas6ivf0c8md2c63apg-alb1-5f96ffbc77-c5tzp from kube-system started at 2020-07-20 16:39:13 +0000 UTC (4 container statuses recorded)
Jul 20 20:16:28.834: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Jul 20 20:16:28.834: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Jul 20 20:16:28.834: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Jul 20 20:16:28.834: INFO: 	Container nginx-ingress ready: true, restart count 0
Jul 20 20:16:28.834: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-07-20 16:40:09 +0000 UTC (1 container statuses recorded)
Jul 20 20:16:28.834: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jul 20 20:16:28.834: INFO: sonobuoy-systemd-logs-daemon-set-2fac7b176f1f4511-zngst from sonobuoy started at 2020-07-20 18:23:20 +0000 UTC (2 container statuses recorded)
Jul 20 20:16:28.834: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul 20 20:16:28.834: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 20 20:16:28.834: INFO: coredns-54f55c7c7c-gw4fd from kube-system started at 2020-07-20 17:02:33 +0000 UTC (1 container statuses recorded)
Jul 20 20:16:28.834: INFO: 	Container coredns ready: true, restart count 0
Jul 20 20:16:28.834: INFO: vpn-f66c45467-jtzph from kube-system started at 2020-07-20 19:14:37 +0000 UTC (1 container statuses recorded)
Jul 20 20:16:28.834: INFO: 	Container vpn ready: true, restart count 0
Jul 20 20:16:28.834: INFO: calico-node-6z6jp from kube-system started at 2020-07-20 16:38:01 +0000 UTC (1 container statuses recorded)
Jul 20 20:16:28.834: INFO: 	Container calico-node ready: true, restart count 0
Jul 20 20:16:28.834: INFO: ibm-cloud-provider-ip-149-81-145-66-6c99b7f77d-6xqwt from ibm-system started at 2020-07-20 16:38:20 +0000 UTC (1 container statuses recorded)
Jul 20 20:16:28.834: INFO: 	Container ibm-cloud-provider-ip-149-81-145-66 ready: true, restart count 0
Jul 20 20:16:28.834: INFO: coredns-autoscaler-6bc79bb9db-7vjd7 from kube-system started at 2020-07-20 19:14:37 +0000 UTC (1 container statuses recorded)
Jul 20 20:16:28.834: INFO: 	Container autoscaler ready: true, restart count 0
Jul 20 20:16:28.834: INFO: ibm-master-proxy-static-10.123.236.220 from kube-system started at 2020-07-20 16:37:55 +0000 UTC (2 container statuses recorded)
Jul 20 20:16:28.834: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jul 20 20:16:28.834: INFO: 	Container pause ready: true, restart count 0
Jul 20 20:16:28.834: INFO: ibm-keepalived-watcher-69dl6 from kube-system started at 2020-07-20 16:38:01 +0000 UTC (1 container statuses recorded)
Jul 20 20:16:28.834: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jul 20 20:16:28.834: INFO: ibm-storage-watcher-5955dd9995-4kmxp from kube-system started at 2020-07-20 19:14:37 +0000 UTC (1 container statuses recorded)
Jul 20 20:16:28.834: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jul 20 20:16:28.834: INFO: metrics-server-7d665f557c-bxtgh from kube-system started at 2020-07-20 19:14:37 +0000 UTC (2 container statuses recorded)
Jul 20 20:16:28.834: INFO: 	Container metrics-server ready: true, restart count 0
Jul 20 20:16:28.834: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jul 20 20:16:28.834: INFO: 
Logging pods the kubelet thinks is on node 10.123.236.227 before test
Jul 20 20:16:28.863: INFO: sonobuoy-systemd-logs-daemon-set-2fac7b176f1f4511-mktvj from sonobuoy started at 2020-07-20 18:23:20 +0000 UTC (2 container statuses recorded)
Jul 20 20:16:28.863: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul 20 20:16:28.863: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 20 20:16:28.863: INFO: addon-catalog-source-c4rf5 from ibm-system started at 2020-07-20 16:37:18 +0000 UTC (1 container statuses recorded)
Jul 20 20:16:28.863: INFO: 	Container configmap-registry-server ready: true, restart count 0
Jul 20 20:16:28.863: INFO: ibm-master-proxy-static-10.123.236.227 from kube-system started at 2020-07-20 16:36:44 +0000 UTC (2 container statuses recorded)
Jul 20 20:16:28.863: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jul 20 20:16:28.863: INFO: 	Container pause ready: true, restart count 0
Jul 20 20:16:28.863: INFO: ibm-keepalived-watcher-m9bwq from kube-system started at 2020-07-20 16:36:51 +0000 UTC (1 container statuses recorded)
Jul 20 20:16:28.863: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jul 20 20:16:28.863: INFO: calico-node-85dkz from kube-system started at 2020-07-20 16:36:51 +0000 UTC (1 container statuses recorded)
Jul 20 20:16:28.863: INFO: 	Container calico-node ready: true, restart count 0
Jul 20 20:16:28.863: INFO: 
Logging pods the kubelet thinks is on node 10.123.236.230 before test
Jul 20 20:16:28.937: INFO: dashboard-metrics-scraper-5789d44f58-kdvwc from kube-system started at 2020-07-20 19:14:37 +0000 UTC (1 container statuses recorded)
Jul 20 20:16:28.937: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jul 20 20:16:28.937: INFO: catalog-operator-67646bfcdb-vdbxg from ibm-system started at 2020-07-20 19:25:31 +0000 UTC (1 container statuses recorded)
Jul 20 20:16:28.937: INFO: 	Container catalog-operator ready: true, restart count 0
Jul 20 20:16:28.937: INFO: coredns-54f55c7c7c-xgw9w from kube-system started at 2020-07-20 17:02:33 +0000 UTC (1 container statuses recorded)
Jul 20 20:16:28.937: INFO: 	Container coredns ready: true, restart count 0
Jul 20 20:16:28.937: INFO: coredns-54f55c7c7c-tp7r8 from kube-system started at 2020-07-20 19:14:37 +0000 UTC (1 container statuses recorded)
Jul 20 20:16:28.937: INFO: 	Container coredns ready: true, restart count 0
Jul 20 20:16:28.937: INFO: olm-operator-787498c9b7-nhx76 from ibm-system started at 2020-07-20 19:14:37 +0000 UTC (1 container statuses recorded)
Jul 20 20:16:28.937: INFO: 	Container olm-operator ready: true, restart count 0
Jul 20 20:16:28.937: INFO: ibm-master-proxy-static-10.123.236.230 from kube-system started at 2020-07-20 16:38:01 +0000 UTC (2 container statuses recorded)
Jul 20 20:16:28.937: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jul 20 20:16:28.937: INFO: 	Container pause ready: true, restart count 0
Jul 20 20:16:28.937: INFO: calico-kube-controllers-69c4db5c8d-pff85 from kube-system started at 2020-07-20 19:25:31 +0000 UTC (1 container statuses recorded)
Jul 20 20:16:28.937: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 20 20:16:28.937: INFO: sonobuoy-systemd-logs-daemon-set-2fac7b176f1f4511-xh4r9 from sonobuoy started at 2020-07-20 18:23:20 +0000 UTC (2 container statuses recorded)
Jul 20 20:16:28.937: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul 20 20:16:28.937: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 20 20:16:28.937: INFO: ibm-file-plugin-586dc4596c-f2b5n from kube-system started at 2020-07-20 19:14:37 +0000 UTC (1 container statuses recorded)
Jul 20 20:16:28.937: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jul 20 20:16:28.937: INFO: ibm-cloud-provider-ip-149-81-145-66-6c99b7f77d-4rckp from ibm-system started at 2020-07-20 16:38:20 +0000 UTC (1 container statuses recorded)
Jul 20 20:16:28.937: INFO: 	Container ibm-cloud-provider-ip-149-81-145-66 ready: true, restart count 0
Jul 20 20:16:28.937: INFO: sonobuoy from sonobuoy started at 2020-07-20 18:23:14 +0000 UTC (1 container statuses recorded)
Jul 20 20:16:28.938: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 20 20:16:28.938: INFO: sonobuoy-e2e-job-0f6e1d1f061d4a08 from sonobuoy started at 2020-07-20 18:23:20 +0000 UTC (2 container statuses recorded)
Jul 20 20:16:28.938: INFO: 	Container e2e ready: true, restart count 0
Jul 20 20:16:28.938: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 20 20:16:28.938: INFO: kubernetes-dashboard-984c5c57-z4jh4 from kube-system started at 2020-07-20 19:25:31 +0000 UTC (1 container statuses recorded)
Jul 20 20:16:28.938: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul 20 20:16:28.938: INFO: calico-node-wpsss from kube-system started at 2020-07-20 16:38:02 +0000 UTC (1 container statuses recorded)
Jul 20 20:16:28.938: INFO: 	Container calico-node ready: true, restart count 0
Jul 20 20:16:28.938: INFO: ibm-keepalived-watcher-fhgvr from kube-system started at 2020-07-20 16:38:02 +0000 UTC (1 container statuses recorded)
Jul 20 20:16:28.938: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jul 20 20:16:28.938: INFO: public-crbsas6ivf0c8md2c63apg-alb1-5f96ffbc77-lpqp5 from kube-system started at 2020-07-20 16:39:13 +0000 UTC (4 container statuses recorded)
Jul 20 20:16:28.938: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Jul 20 20:16:28.938: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Jul 20 20:16:28.938: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Jul 20 20:16:28.938: INFO: 	Container nginx-ingress ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c7294ad2-fc42-42cd-9c73-7ec06b6ae63e 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-c7294ad2-fc42-42cd-9c73-7ec06b6ae63e off the node 10.123.236.227
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c7294ad2-fc42-42cd-9c73-7ec06b6ae63e
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:21:33.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9120" for this suite.
Jul 20 20:21:43.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:21:44.164: INFO: namespace sched-pred-9120 deletion completed in 10.919584701s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:315.755 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:21:44.165: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9796
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 20 20:21:47.034: INFO: Successfully updated pod "pod-update-activedeadlineseconds-d6420613-7b85-4adf-896d-b97da470c456"
Jul 20 20:21:47.034: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d6420613-7b85-4adf-896d-b97da470c456" in namespace "pods-9796" to be "terminated due to deadline exceeded"
Jul 20 20:21:47.044: INFO: Pod "pod-update-activedeadlineseconds-d6420613-7b85-4adf-896d-b97da470c456": Phase="Running", Reason="", readiness=true. Elapsed: 10.192649ms
Jul 20 20:21:49.059: INFO: Pod "pod-update-activedeadlineseconds-d6420613-7b85-4adf-896d-b97da470c456": Phase="Running", Reason="", readiness=true. Elapsed: 2.024483525s
Jul 20 20:21:51.075: INFO: Pod "pod-update-activedeadlineseconds-d6420613-7b85-4adf-896d-b97da470c456": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.040915088s
Jul 20 20:21:51.075: INFO: Pod "pod-update-activedeadlineseconds-d6420613-7b85-4adf-896d-b97da470c456" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:21:51.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9796" for this suite.
Jul 20 20:21:57.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:21:58.000: INFO: namespace pods-9796 deletion completed in 6.91024916s

â€¢ [SLOW TEST:13.835 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:21:58.000: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5372
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-8e6bd851-7bb6-4878-b1db-24aada17f014
STEP: Creating a pod to test consume secrets
Jul 20 20:21:58.365: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6b3b1bf6-4185-4ea0-9ced-6018137c4ce3" in namespace "projected-5372" to be "success or failure"
Jul 20 20:21:58.429: INFO: Pod "pod-projected-secrets-6b3b1bf6-4185-4ea0-9ced-6018137c4ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 63.346019ms
Jul 20 20:22:00.440: INFO: Pod "pod-projected-secrets-6b3b1bf6-4185-4ea0-9ced-6018137c4ce3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.074221157s
STEP: Saw pod success
Jul 20 20:22:00.440: INFO: Pod "pod-projected-secrets-6b3b1bf6-4185-4ea0-9ced-6018137c4ce3" satisfied condition "success or failure"
Jul 20 20:22:00.452: INFO: Trying to get logs from node 10.123.236.227 pod pod-projected-secrets-6b3b1bf6-4185-4ea0-9ced-6018137c4ce3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 20 20:22:00.558: INFO: Waiting for pod pod-projected-secrets-6b3b1bf6-4185-4ea0-9ced-6018137c4ce3 to disappear
Jul 20 20:22:00.572: INFO: Pod pod-projected-secrets-6b3b1bf6-4185-4ea0-9ced-6018137c4ce3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:22:00.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5372" for this suite.
Jul 20 20:22:06.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:22:07.333: INFO: namespace projected-5372 deletion completed in 6.721390254s

â€¢ [SLOW TEST:9.333 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:22:07.333: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5345
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-5345
STEP: creating replication controller nodeport-test in namespace services-5345
I0720 20:22:07.685878      25 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-5345, replica count: 2
I0720 20:22:10.736454      25 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 20 20:22:10.736: INFO: Creating new exec pod
Jul 20 20:22:13.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=services-5345 execpods2rsz -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Jul 20 20:22:14.110: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jul 20 20:22:14.110: INFO: stdout: ""
Jul 20 20:22:14.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=services-5345 execpods2rsz -- /bin/sh -x -c nc -zv -t -w 2 172.21.235.249 80'
Jul 20 20:22:14.427: INFO: stderr: "+ nc -zv -t -w 2 172.21.235.249 80\nConnection to 172.21.235.249 80 port [tcp/http] succeeded!\n"
Jul 20 20:22:14.427: INFO: stdout: ""
Jul 20 20:22:14.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=services-5345 execpods2rsz -- /bin/sh -x -c nc -zv -t -w 2 10.123.236.220 30198'
Jul 20 20:22:14.660: INFO: stderr: "+ nc -zv -t -w 2 10.123.236.220 30198\nConnection to 10.123.236.220 30198 port [tcp/30198] succeeded!\n"
Jul 20 20:22:14.660: INFO: stdout: ""
Jul 20 20:22:14.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=services-5345 execpods2rsz -- /bin/sh -x -c nc -zv -t -w 2 10.123.236.227 30198'
Jul 20 20:22:15.103: INFO: stderr: "+ nc -zv -t -w 2 10.123.236.227 30198\nConnection to 10.123.236.227 30198 port [tcp/30198] succeeded!\n"
Jul 20 20:22:15.103: INFO: stdout: ""
Jul 20 20:22:15.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=services-5345 execpods2rsz -- /bin/sh -x -c nc -zv -t -w 2 149.81.110.243 30198'
Jul 20 20:22:15.759: INFO: stderr: "+ nc -zv -t -w 2 149.81.110.243 30198\nConnection to 149.81.110.243 30198 port [tcp/30198] succeeded!\n"
Jul 20 20:22:15.759: INFO: stdout: ""
Jul 20 20:22:15.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec --namespace=services-5345 execpods2rsz -- /bin/sh -x -c nc -zv -t -w 2 149.81.110.250 30198'
Jul 20 20:22:15.969: INFO: stderr: "+ nc -zv -t -w 2 149.81.110.250 30198\nConnection to 149.81.110.250 30198 port [tcp/30198] succeeded!\n"
Jul 20 20:22:15.969: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:22:15.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5345" for this suite.
Jul 20 20:22:24.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:22:24.637: INFO: namespace services-5345 deletion completed in 8.649485954s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:17.304 seconds]
[sig-network] Services
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:22:24.637: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-8647
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-8647
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-8647
STEP: Deleting pre-stop pod
Jul 20 20:22:36.066: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:22:36.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-8647" for this suite.
Jul 20 20:23:22.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:23:22.668: INFO: namespace prestop-8647 deletion completed in 46.562406895s

â€¢ [SLOW TEST:58.031 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:23:22.668: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1828
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 20:23:22.970: INFO: (0) /api/v1/nodes/10.123.236.220/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 47.498468ms)
Jul 20 20:23:22.989: INFO: (1) /api/v1/nodes/10.123.236.220/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 19.385734ms)
Jul 20 20:23:23.005: INFO: (2) /api/v1/nodes/10.123.236.220/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 15.766007ms)
Jul 20 20:23:23.025: INFO: (3) /api/v1/nodes/10.123.236.220/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 20.106637ms)
Jul 20 20:23:23.042: INFO: (4) /api/v1/nodes/10.123.236.220/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 16.252426ms)
Jul 20 20:23:23.059: INFO: (5) /api/v1/nodes/10.123.236.220/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 17.442157ms)
Jul 20 20:23:23.077: INFO: (6) /api/v1/nodes/10.123.236.220/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 17.116271ms)
Jul 20 20:23:23.096: INFO: (7) /api/v1/nodes/10.123.236.220/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 18.984943ms)
Jul 20 20:23:23.113: INFO: (8) /api/v1/nodes/10.123.236.220/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 17.695337ms)
Jul 20 20:23:23.131: INFO: (9) /api/v1/nodes/10.123.236.220/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 17.700083ms)
Jul 20 20:23:23.149: INFO: (10) /api/v1/nodes/10.123.236.220/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 18.169664ms)
Jul 20 20:23:23.166: INFO: (11) /api/v1/nodes/10.123.236.220/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 16.708236ms)
Jul 20 20:23:23.184: INFO: (12) /api/v1/nodes/10.123.236.220/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 17.320007ms)
Jul 20 20:23:23.200: INFO: (13) /api/v1/nodes/10.123.236.220/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 16.54556ms)
Jul 20 20:23:23.223: INFO: (14) /api/v1/nodes/10.123.236.220/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 23.115184ms)
Jul 20 20:23:23.246: INFO: (15) /api/v1/nodes/10.123.236.220/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 22.255372ms)
Jul 20 20:23:23.262: INFO: (16) /api/v1/nodes/10.123.236.220/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 16.063983ms)
Jul 20 20:23:23.279: INFO: (17) /api/v1/nodes/10.123.236.220/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 16.944586ms)
Jul 20 20:23:23.311: INFO: (18) /api/v1/nodes/10.123.236.220/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 31.975541ms)
Jul 20 20:23:23.340: INFO: (19) /api/v1/nodes/10.123.236.220/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 28.684316ms)
[AfterEach] version v1
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:23:23.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1828" for this suite.
Jul 20 20:23:29.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:23:29.925: INFO: namespace proxy-1828 deletion completed in 6.570030791s

â€¢ [SLOW TEST:7.257 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:23:29.925: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2251
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jul 20 20:23:30.213: INFO: Waiting up to 5m0s for pod "downwardapi-volume-899074b9-a15d-4303-9a94-f3e48dd9e6f6" in namespace "downward-api-2251" to be "success or failure"
Jul 20 20:23:30.222: INFO: Pod "downwardapi-volume-899074b9-a15d-4303-9a94-f3e48dd9e6f6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.349642ms
Jul 20 20:23:32.231: INFO: Pod "downwardapi-volume-899074b9-a15d-4303-9a94-f3e48dd9e6f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018439688s
Jul 20 20:23:34.241: INFO: Pod "downwardapi-volume-899074b9-a15d-4303-9a94-f3e48dd9e6f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028764614s
STEP: Saw pod success
Jul 20 20:23:34.241: INFO: Pod "downwardapi-volume-899074b9-a15d-4303-9a94-f3e48dd9e6f6" satisfied condition "success or failure"
Jul 20 20:23:34.251: INFO: Trying to get logs from node 10.123.236.227 pod downwardapi-volume-899074b9-a15d-4303-9a94-f3e48dd9e6f6 container client-container: <nil>
STEP: delete the pod
Jul 20 20:23:34.333: INFO: Waiting for pod downwardapi-volume-899074b9-a15d-4303-9a94-f3e48dd9e6f6 to disappear
Jul 20 20:23:34.341: INFO: Pod downwardapi-volume-899074b9-a15d-4303-9a94-f3e48dd9e6f6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:23:34.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2251" for this suite.
Jul 20 20:23:40.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:23:40.985: INFO: namespace downward-api-2251 deletion completed in 6.628468109s

â€¢ [SLOW TEST:11.060 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:23:40.985: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5187
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:23:45.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5187" for this suite.
Jul 20 20:24:29.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:24:30.284: INFO: namespace kubelet-test-5187 deletion completed in 44.864810374s

â€¢ [SLOW TEST:49.299 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:24:30.287: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5722
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul 20 20:24:30.570: INFO: Waiting up to 5m0s for pod "pod-28215bd7-2267-46cf-a1b2-488c1819148d" in namespace "emptydir-5722" to be "success or failure"
Jul 20 20:24:30.598: INFO: Pod "pod-28215bd7-2267-46cf-a1b2-488c1819148d": Phase="Pending", Reason="", readiness=false. Elapsed: 27.816571ms
Jul 20 20:24:32.617: INFO: Pod "pod-28215bd7-2267-46cf-a1b2-488c1819148d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046750531s
STEP: Saw pod success
Jul 20 20:24:32.617: INFO: Pod "pod-28215bd7-2267-46cf-a1b2-488c1819148d" satisfied condition "success or failure"
Jul 20 20:24:32.639: INFO: Trying to get logs from node 10.123.236.227 pod pod-28215bd7-2267-46cf-a1b2-488c1819148d container test-container: <nil>
STEP: delete the pod
Jul 20 20:24:32.759: INFO: Waiting for pod pod-28215bd7-2267-46cf-a1b2-488c1819148d to disappear
Jul 20 20:24:32.769: INFO: Pod pod-28215bd7-2267-46cf-a1b2-488c1819148d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:24:32.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5722" for this suite.
Jul 20 20:24:38.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:24:39.366: INFO: namespace emptydir-5722 deletion completed in 6.578067408s

â€¢ [SLOW TEST:9.079 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:24:39.368: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6819
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 20:24:39.630: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-9d90983b-0c2d-49c7-98e6-58feebbc2285" in namespace "security-context-test-6819" to be "success or failure"
Jul 20 20:24:39.641: INFO: Pod "alpine-nnp-false-9d90983b-0c2d-49c7-98e6-58feebbc2285": Phase="Pending", Reason="", readiness=false. Elapsed: 10.138873ms
Jul 20 20:24:41.650: INFO: Pod "alpine-nnp-false-9d90983b-0c2d-49c7-98e6-58feebbc2285": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019639445s
Jul 20 20:24:43.662: INFO: Pod "alpine-nnp-false-9d90983b-0c2d-49c7-98e6-58feebbc2285": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031461999s
Jul 20 20:24:43.662: INFO: Pod "alpine-nnp-false-9d90983b-0c2d-49c7-98e6-58feebbc2285" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:24:43.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6819" for this suite.
Jul 20 20:24:49.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:24:50.283: INFO: namespace security-context-test-6819 deletion completed in 6.548837911s

â€¢ [SLOW TEST:10.915 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:24:50.284: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7689
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-7689/configmap-test-ed7e64d8-9aac-4c42-afc3-4264d691c1e8
STEP: Creating a pod to test consume configMaps
Jul 20 20:24:50.563: INFO: Waiting up to 5m0s for pod "pod-configmaps-283a29c8-655e-4b01-bfb9-cd0bef03878e" in namespace "configmap-7689" to be "success or failure"
Jul 20 20:24:50.579: INFO: Pod "pod-configmaps-283a29c8-655e-4b01-bfb9-cd0bef03878e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.226659ms
Jul 20 20:24:52.799: INFO: Pod "pod-configmaps-283a29c8-655e-4b01-bfb9-cd0bef03878e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.23563489s
Jul 20 20:24:54.810: INFO: Pod "pod-configmaps-283a29c8-655e-4b01-bfb9-cd0bef03878e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.247103804s
STEP: Saw pod success
Jul 20 20:24:54.810: INFO: Pod "pod-configmaps-283a29c8-655e-4b01-bfb9-cd0bef03878e" satisfied condition "success or failure"
Jul 20 20:24:54.820: INFO: Trying to get logs from node 10.123.236.227 pod pod-configmaps-283a29c8-655e-4b01-bfb9-cd0bef03878e container env-test: <nil>
STEP: delete the pod
Jul 20 20:24:54.877: INFO: Waiting for pod pod-configmaps-283a29c8-655e-4b01-bfb9-cd0bef03878e to disappear
Jul 20 20:24:54.886: INFO: Pod pod-configmaps-283a29c8-655e-4b01-bfb9-cd0bef03878e no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:24:54.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7689" for this suite.
Jul 20 20:25:01.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:25:01.684: INFO: namespace configmap-7689 deletion completed in 6.782779853s

â€¢ [SLOW TEST:11.401 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:25:01.684: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6287
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jul 20 20:25:04.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 exec pod-sharedvolume-9ed06ffe-411f-4bd5-b801-94606e12715b -c busybox-main-container --namespace=emptydir-6287 -- cat /usr/share/volumeshare/shareddata.txt'
Jul 20 20:25:04.876: INFO: stderr: ""
Jul 20 20:25:04.876: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:25:04.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6287" for this suite.
Jul 20 20:25:10.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:25:11.582: INFO: namespace emptydir-6287 deletion completed in 6.689496956s

â€¢ [SLOW TEST:9.898 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:25:11.584: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4516
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Jul 20 20:25:11.855: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:25:30.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4516" for this suite.
Jul 20 20:25:36.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:25:36.989: INFO: namespace crd-publish-openapi-4516 deletion completed in 6.614969788s

â€¢ [SLOW TEST:25.406 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:25:36.989: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-199
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:25:53.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-199" for this suite.
Jul 20 20:25:59.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:25:59.998: INFO: namespace resourcequota-199 deletion completed in 6.602029313s

â€¢ [SLOW TEST:23.009 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:25:59.999: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5477
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 20 20:26:00.293: INFO: Waiting up to 5m0s for pod "pod-1d643e83-f9e2-4c4d-ba0e-57c95ce587be" in namespace "emptydir-5477" to be "success or failure"
Jul 20 20:26:00.304: INFO: Pod "pod-1d643e83-f9e2-4c4d-ba0e-57c95ce587be": Phase="Pending", Reason="", readiness=false. Elapsed: 11.349723ms
Jul 20 20:26:02.334: INFO: Pod "pod-1d643e83-f9e2-4c4d-ba0e-57c95ce587be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041696959s
Jul 20 20:26:04.349: INFO: Pod "pod-1d643e83-f9e2-4c4d-ba0e-57c95ce587be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056212198s
STEP: Saw pod success
Jul 20 20:26:04.349: INFO: Pod "pod-1d643e83-f9e2-4c4d-ba0e-57c95ce587be" satisfied condition "success or failure"
Jul 20 20:26:04.361: INFO: Trying to get logs from node 10.123.236.227 pod pod-1d643e83-f9e2-4c4d-ba0e-57c95ce587be container test-container: <nil>
STEP: delete the pod
Jul 20 20:26:04.424: INFO: Waiting for pod pod-1d643e83-f9e2-4c4d-ba0e-57c95ce587be to disappear
Jul 20 20:26:04.433: INFO: Pod pod-1d643e83-f9e2-4c4d-ba0e-57c95ce587be no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:26:04.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5477" for this suite.
Jul 20 20:26:10.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:26:11.198: INFO: namespace emptydir-5477 deletion completed in 6.74520584s

â€¢ [SLOW TEST:11.200 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:26:11.200: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-480
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jul 20 20:26:11.467: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jul 20 20:26:25.330: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 20:26:28.431: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:26:42.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-480" for this suite.
Jul 20 20:26:48.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:26:49.004: INFO: namespace crd-publish-openapi-480 deletion completed in 6.633708422s

â€¢ [SLOW TEST:37.805 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:26:49.008: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9434
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 20:26:49.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 version'
Jul 20 20:26:49.369: INFO: stderr: ""
Jul 20 20:26:49.369: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.13\", GitCommit:\"39a145ca3413079bcb9c80846488786fed5fe1cb\", GitTreeState:\"clean\", BuildDate:\"2020-07-15T16:18:19Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.13+IKS\", GitCommit:\"ab90f016957ae913df1f5f1429668032af382053\", GitTreeState:\"clean\", BuildDate:\"2020-07-15T21:56:15Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:26:49.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9434" for this suite.
Jul 20 20:26:55.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:26:56.122: INFO: namespace kubectl-9434 deletion completed in 6.7198018s

â€¢ [SLOW TEST:7.114 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:26:56.122: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6275
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-b0585448-de0b-4840-86ec-f18fdc18134d
STEP: Creating a pod to test consume configMaps
Jul 20 20:26:56.432: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-02986528-0358-4dfb-a8a3-028a462f997d" in namespace "projected-6275" to be "success or failure"
Jul 20 20:26:56.442: INFO: Pod "pod-projected-configmaps-02986528-0358-4dfb-a8a3-028a462f997d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.415765ms
Jul 20 20:26:58.461: INFO: Pod "pod-projected-configmaps-02986528-0358-4dfb-a8a3-028a462f997d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029294623s
STEP: Saw pod success
Jul 20 20:26:58.461: INFO: Pod "pod-projected-configmaps-02986528-0358-4dfb-a8a3-028a462f997d" satisfied condition "success or failure"
Jul 20 20:26:58.472: INFO: Trying to get logs from node 10.123.236.227 pod pod-projected-configmaps-02986528-0358-4dfb-a8a3-028a462f997d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 20 20:26:58.541: INFO: Waiting for pod pod-projected-configmaps-02986528-0358-4dfb-a8a3-028a462f997d to disappear
Jul 20 20:26:58.561: INFO: Pod pod-projected-configmaps-02986528-0358-4dfb-a8a3-028a462f997d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:26:58.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6275" for this suite.
Jul 20 20:27:04.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:27:05.142: INFO: namespace projected-6275 deletion completed in 6.563267719s

â€¢ [SLOW TEST:9.020 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:27:05.143: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3916
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 20:27:05.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 create -f - --namespace=kubectl-3916'
Jul 20 20:27:05.732: INFO: stderr: ""
Jul 20 20:27:05.732: INFO: stdout: "replicationcontroller/redis-master created\n"
Jul 20 20:27:05.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 create -f - --namespace=kubectl-3916'
Jul 20 20:27:06.088: INFO: stderr: ""
Jul 20 20:27:06.088: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 20 20:27:07.100: INFO: Selector matched 1 pods for map[app:redis]
Jul 20 20:27:07.100: INFO: Found 0 / 1
Jul 20 20:27:08.100: INFO: Selector matched 1 pods for map[app:redis]
Jul 20 20:27:08.100: INFO: Found 1 / 1
Jul 20 20:27:08.100: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 20 20:27:08.110: INFO: Selector matched 1 pods for map[app:redis]
Jul 20 20:27:08.110: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 20 20:27:08.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 describe pod redis-master-gv7cv --namespace=kubectl-3916'
Jul 20 20:27:08.215: INFO: stderr: ""
Jul 20 20:27:08.215: INFO: stdout: "Name:         redis-master-gv7cv\nNamespace:    kubectl-3916\nPriority:     0\nNode:         10.123.236.227/10.123.236.227\nStart Time:   Mon, 20 Jul 2020 20:27:05 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           172.30.106.167\nIPs:\n  IP:           172.30.106.167\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   containerd://cf242ff999e5ce7968a94afd657fa3a9eaa4df78d96d98a03c70c0499e7bd781\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 20 Jul 2020 20:27:07 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-wzlbx (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-wzlbx:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-wzlbx\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 600s\n                 node.kubernetes.io/unreachable:NoExecute for 600s\nEvents:\n  Type    Reason     Age   From                     Message\n  ----    ------     ----  ----                     -------\n  Normal  Scheduled  3s    default-scheduler        Successfully assigned kubectl-3916/redis-master-gv7cv to 10.123.236.227\n  Normal  Pulled     2s    kubelet, 10.123.236.227  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    2s    kubelet, 10.123.236.227  Created container redis-master\n  Normal  Started    1s    kubelet, 10.123.236.227  Started container redis-master\n"
Jul 20 20:27:08.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 describe rc redis-master --namespace=kubectl-3916'
Jul 20 20:27:08.365: INFO: stderr: ""
Jul 20 20:27:08.365: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-3916\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-gv7cv\n"
Jul 20 20:27:08.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 describe service redis-master --namespace=kubectl-3916'
Jul 20 20:27:08.793: INFO: stderr: ""
Jul 20 20:27:08.793: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-3916\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.21.137.54\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.30.106.167:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul 20 20:27:08.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 describe node 10.123.236.220'
Jul 20 20:27:09.105: INFO: stderr: ""
Jul 20 20:27:09.105: INFO: stdout: "Name:               10.123.236.220\nRoles:              <none>\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-de\n                    failure-domain.beta.kubernetes.io/zone=fra05\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=149.81.110.243\n                    ibm-cloud.kubernetes.io/ha-worker=true\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.123.236.220\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=UBUNTU_18_64\n                    ibm-cloud.kubernetes.io/region=eu-de\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-bsas6ivf0c8md2c63apg-kubee2epvgj-default-000001f7\n                    ibm-cloud.kubernetes.io/worker-pool-id=bsas6ivf0c8md2c63apg-c0b6a01\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=1.16.13_1539\n                    ibm-cloud.kubernetes.io/zone=fra05\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.123.236.220\n                    kubernetes.io/os=linux\n                    privateVLAN=2723042\n                    publicVLAN=2723040\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 20 Jul 2020 16:38:01 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 20 Jul 2020 20:26:43 +0000   Mon, 20 Jul 2020 16:38:01 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 20 Jul 2020 20:26:43 +0000   Mon, 20 Jul 2020 16:38:01 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 20 Jul 2020 20:26:43 +0000   Mon, 20 Jul 2020 16:38:01 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 20 Jul 2020 20:26:43 +0000   Mon, 20 Jul 2020 16:38:11 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.123.236.220\n  ExternalIP:  149.81.110.243\n  Hostname:    10.123.236.220\nCapacity:\n cpu:                4\n ephemeral-storage:  102685624Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16419668Ki\n pods:               110\nAllocatable:\n cpu:                3910m\n ephemeral-storage:  99892574949\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             13627220Ki\n pods:               110\nSystem Info:\n Machine ID:                 e93efd7d485b44db8d1b98e4227a7161\n System UUID:                7491E5EB-2316-2F78-D33C-26C70F7CA6AA\n Boot ID:                    9c3539c4-5c2e-4fd8-9e53-3b822b531677\n Kernel Version:             4.15.0-111-generic\n OS Image:                   Ubuntu 18.04.4 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  containerd://1.3.4\n Kubelet Version:            v1.16.13+IKS\n Kube-Proxy Version:         v1.16.13+IKS\nProviderID:                  ibm://fee034388aa6435883a1f720010ab3a2///bsas6ivf0c8md2c63apg/kube-bsas6ivf0c8md2c63apg-kubee2epvgj-default-000001f7\nNon-terminated Pods:         (12 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  default                    test-k8s-e2e-pvg-master-verification                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h47m\n  ibm-system                 ibm-cloud-provider-ip-149-81-145-66-6c99b7f77d-6xqwt       5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         3h48m\n  kube-system                calico-node-6z6jp                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         3h49m\n  kube-system                coredns-54f55c7c7c-gw4fd                                   100m (2%)     0 (0%)      70Mi (0%)        400Mi (3%)     3h24m\n  kube-system                coredns-autoscaler-6bc79bb9db-7vjd7                        20m (0%)      0 (0%)      10Mi (0%)        0 (0%)         72m\n  kube-system                ibm-keepalived-watcher-69dl6                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         3h49m\n  kube-system                ibm-master-proxy-static-10.123.236.220                     25m (0%)      300m (7%)   32M (0%)         512M (3%)      3h47m\n  kube-system                ibm-storage-watcher-5955dd9995-4kmxp                       50m (1%)      200m (5%)   100Mi (0%)       200Mi (1%)     72m\n  kube-system                metrics-server-7d665f557c-bxtgh                            121m (3%)     216m (5%)   186Mi (1%)       436Mi (3%)     72m\n  kube-system                public-crbsas6ivf0c8md2c63apg-alb1-5f96ffbc77-c5tzp        10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         3h47m\n  kube-system                vpn-f66c45467-jtzph                                        5m (0%)       0 (0%)      5Mi (0%)         0 (0%)         72m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-2fac7b176f1f4511-zngst    0 (0%)        0 (0%)      0 (0%)           0 (0%)         123m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests       Limits\n  --------           --------       ------\n  cpu                591m (15%)     716m (18%)\n  memory             615954Ki (4%)  1560864Ki (11%)\n  ephemeral-storage  0 (0%)         0 (0%)\nEvents:              <none>\n"
Jul 20 20:27:09.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 describe namespace kubectl-3916'
Jul 20 20:27:09.239: INFO: stderr: ""
Jul 20 20:27:09.239: INFO: stdout: "Name:         kubectl-3916\nLabels:       e2e-framework=kubectl\n              e2e-run=f223d469-6a1f-47ea-9bdf-d0495435c689\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:27:09.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3916" for this suite.
Jul 20 20:27:21.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:27:22.067: INFO: namespace kubectl-3916 deletion completed in 12.813284196s

â€¢ [SLOW TEST:16.923 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:27:22.067: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6595
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jul 20 20:27:22.364: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
Jul 20 20:27:25.988: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:27:39.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6595" for this suite.
Jul 20 20:27:45.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:27:46.538: INFO: namespace crd-publish-openapi-6595 deletion completed in 6.673760107s

â€¢ [SLOW TEST:24.471 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:27:46.539: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5246
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 20 20:27:46.803: INFO: Waiting up to 5m0s for pod "pod-a2261154-c0e8-4015-aded-3ac2314f3f5c" in namespace "emptydir-5246" to be "success or failure"
Jul 20 20:27:46.812: INFO: Pod "pod-a2261154-c0e8-4015-aded-3ac2314f3f5c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.67848ms
Jul 20 20:27:48.824: INFO: Pod "pod-a2261154-c0e8-4015-aded-3ac2314f3f5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02094783s
Jul 20 20:27:50.835: INFO: Pod "pod-a2261154-c0e8-4015-aded-3ac2314f3f5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032041201s
STEP: Saw pod success
Jul 20 20:27:50.835: INFO: Pod "pod-a2261154-c0e8-4015-aded-3ac2314f3f5c" satisfied condition "success or failure"
Jul 20 20:27:50.848: INFO: Trying to get logs from node 10.123.236.227 pod pod-a2261154-c0e8-4015-aded-3ac2314f3f5c container test-container: <nil>
STEP: delete the pod
Jul 20 20:27:50.934: INFO: Waiting for pod pod-a2261154-c0e8-4015-aded-3ac2314f3f5c to disappear
Jul 20 20:27:50.948: INFO: Pod pod-a2261154-c0e8-4015-aded-3ac2314f3f5c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:27:50.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5246" for this suite.
Jul 20 20:27:57.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:27:57.779: INFO: namespace emptydir-5246 deletion completed in 6.812595734s

â€¢ [SLOW TEST:11.240 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:27:57.779: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7674
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Jul 20 20:27:58.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-7674 -- logs-generator --log-lines-total 100 --run-duration 20s'
Jul 20 20:27:58.256: INFO: stderr: ""
Jul 20 20:27:58.256: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Jul 20 20:27:58.256: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jul 20 20:27:58.256: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7674" to be "running and ready, or succeeded"
Jul 20 20:27:58.268: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 11.585867ms
Jul 20 20:28:00.278: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.021193058s
Jul 20 20:28:00.278: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jul 20 20:28:00.278: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jul 20 20:28:00.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 logs logs-generator logs-generator --namespace=kubectl-7674'
Jul 20 20:28:00.484: INFO: stderr: ""
Jul 20 20:28:00.484: INFO: stdout: "I0720 20:27:59.476301       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/pnp8 277\nI0720 20:27:59.676608       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/vdkf 541\nI0720 20:27:59.876533       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/68nd 471\nI0720 20:28:00.076556       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/scx 395\nI0720 20:28:00.276527       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/5jl 224\n"
STEP: limiting log lines
Jul 20 20:28:00.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 logs logs-generator logs-generator --namespace=kubectl-7674 --tail=1'
Jul 20 20:28:00.638: INFO: stderr: ""
Jul 20 20:28:00.639: INFO: stdout: "I0720 20:28:00.476485       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/nzm 567\n"
STEP: limiting log bytes
Jul 20 20:28:00.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 logs logs-generator logs-generator --namespace=kubectl-7674 --limit-bytes=1'
Jul 20 20:28:00.762: INFO: stderr: ""
Jul 20 20:28:00.762: INFO: stdout: "I"
STEP: exposing timestamps
Jul 20 20:28:00.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 logs logs-generator logs-generator --namespace=kubectl-7674 --tail=1 --timestamps'
Jul 20 20:28:00.948: INFO: stderr: ""
Jul 20 20:28:00.948: INFO: stdout: "2020-07-20T20:28:00.878103025Z I0720 20:28:00.877080       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/brp 332\n"
STEP: restricting to a time range
Jul 20 20:28:03.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 logs logs-generator logs-generator --namespace=kubectl-7674 --since=1s'
Jul 20 20:28:03.595: INFO: stderr: ""
Jul 20 20:28:03.595: INFO: stdout: "I0720 20:28:02.676523       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/pn76 343\nI0720 20:28:02.876579       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/ps62 406\nI0720 20:28:03.076480       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/8pqc 343\nI0720 20:28:03.276551       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/c2c 229\nI0720 20:28:03.476567       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/rfb4 344\n"
Jul 20 20:28:03.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 logs logs-generator logs-generator --namespace=kubectl-7674 --since=24h'
Jul 20 20:28:03.718: INFO: stderr: ""
Jul 20 20:28:03.718: INFO: stdout: "I0720 20:27:59.476301       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/pnp8 277\nI0720 20:27:59.676608       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/vdkf 541\nI0720 20:27:59.876533       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/68nd 471\nI0720 20:28:00.076556       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/scx 395\nI0720 20:28:00.276527       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/5jl 224\nI0720 20:28:00.476485       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/nzm 567\nI0720 20:28:00.676549       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/tgf 324\nI0720 20:28:00.877080       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/brp 332\nI0720 20:28:01.076553       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/pncb 463\nI0720 20:28:01.276535       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/wwtr 396\nI0720 20:28:01.476525       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/s2c 434\nI0720 20:28:01.676562       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/dvgr 356\nI0720 20:28:01.876499       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/kfw 436\nI0720 20:28:02.076494       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/drqw 599\nI0720 20:28:02.276474       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/7vr 394\nI0720 20:28:02.476467       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/qqn 565\nI0720 20:28:02.676523       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/pn76 343\nI0720 20:28:02.876579       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/ps62 406\nI0720 20:28:03.076480       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/8pqc 343\nI0720 20:28:03.276551       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/c2c 229\nI0720 20:28:03.476567       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/rfb4 344\nI0720 20:28:03.676551       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/r55 362\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Jul 20 20:28:03.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 delete pod logs-generator --namespace=kubectl-7674'
Jul 20 20:28:14.737: INFO: stderr: ""
Jul 20 20:28:14.737: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:28:14.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7674" for this suite.
Jul 20 20:28:20.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:28:21.442: INFO: namespace kubectl-7674 deletion completed in 6.690727006s

â€¢ [SLOW TEST:23.663 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 20 20:28:21.444: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8021
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jul 20 20:28:21.720: INFO: >>> kubeConfig: /tmp/kubeconfig-291352008
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jul 20 20:28:25.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 --namespace=crd-publish-openapi-8021 create -f -'
Jul 20 20:28:25.726: INFO: stderr: ""
Jul 20 20:28:25.726: INFO: stdout: "e2e-test-crd-publish-openapi-4829-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul 20 20:28:25.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 --namespace=crd-publish-openapi-8021 delete e2e-test-crd-publish-openapi-4829-crds test-foo'
Jul 20 20:28:25.953: INFO: stderr: ""
Jul 20 20:28:25.953: INFO: stdout: "e2e-test-crd-publish-openapi-4829-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jul 20 20:28:25.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 --namespace=crd-publish-openapi-8021 apply -f -'
Jul 20 20:28:26.310: INFO: stderr: ""
Jul 20 20:28:26.310: INFO: stdout: "e2e-test-crd-publish-openapi-4829-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul 20 20:28:26.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 --namespace=crd-publish-openapi-8021 delete e2e-test-crd-publish-openapi-4829-crds test-foo'
Jul 20 20:28:31.525: INFO: stderr: ""
Jul 20 20:28:31.526: INFO: stdout: "e2e-test-crd-publish-openapi-4829-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jul 20 20:28:31.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 --namespace=crd-publish-openapi-8021 create -f -'
Jul 20 20:28:31.687: INFO: rc: 1
Jul 20 20:28:31.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 --namespace=crd-publish-openapi-8021 apply -f -'
Jul 20 20:28:31.833: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jul 20 20:28:31.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 --namespace=crd-publish-openapi-8021 create -f -'
Jul 20 20:28:31.980: INFO: rc: 1
Jul 20 20:28:31.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 --namespace=crd-publish-openapi-8021 apply -f -'
Jul 20 20:28:32.259: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jul 20 20:28:32.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 explain e2e-test-crd-publish-openapi-4829-crds'
Jul 20 20:28:32.501: INFO: stderr: ""
Jul 20 20:28:32.501: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4829-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jul 20 20:28:32.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 explain e2e-test-crd-publish-openapi-4829-crds.metadata'
Jul 20 20:28:32.776: INFO: stderr: ""
Jul 20 20:28:32.776: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4829-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jul 20 20:28:32.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 explain e2e-test-crd-publish-openapi-4829-crds.spec'
Jul 20 20:28:32.964: INFO: stderr: ""
Jul 20 20:28:32.964: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4829-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jul 20 20:28:32.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 explain e2e-test-crd-publish-openapi-4829-crds.spec.bars'
Jul 20 20:28:33.196: INFO: stderr: ""
Jul 20 20:28:33.196: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4829-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jul 20 20:28:33.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-291352008 explain e2e-test-crd-publish-openapi-4829-crds.spec.bars2'
Jul 20 20:28:33.528: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 20 20:28:37.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8021" for this suite.
Jul 20 20:28:43.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 20 20:28:43.671: INFO: namespace crd-publish-openapi-8021 deletion completed in 6.563947164s

â€¢ [SLOW TEST:22.228 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.13-rc.0.25+dda9914de448ab/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSJul 20 20:28:43.672: INFO: Running AfterSuite actions on all nodes
Jul 20 20:28:43.672: INFO: Running AfterSuite actions on node 1
Jul 20 20:28:43.672: INFO: Skipping dumping logs from cluster

Ran 276 of 4847 Specs in 7497.868 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4571 Skipped
PASS

Ginkgo ran 1 suite in 2h4m59.03376932s
Test Suite Passed
