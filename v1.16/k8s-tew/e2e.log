I0223 17:36:37.154958      26 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-952870286
I0223 17:36:37.155021      26 e2e.go:92] Starting e2e run "35e2b7b8-edf0-48b8-a136-1c3777f7c450" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1582479395 - Will randomize all specs
Will run 276 of 4731 specs

Feb 23 17:36:37.162: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
Feb 23 17:36:37.164: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 23 17:36:37.673: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 23 17:36:37.707: INFO: 16 / 16 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 23 17:36:37.707: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 23 17:36:37.707: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 23 17:36:37.724: INFO: e2e test version: v1.16.7
Feb 23 17:36:37.725: INFO: kube-apiserver version: v1.16.7
Feb 23 17:36:37.725: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
Feb 23 17:36:37.736: INFO: Cluster IP family: ipv4
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:36:37.736: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename init-container
Feb 23 17:36:37.765: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Feb 23 17:36:37.766: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:36:44.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4521" for this suite.
Feb 23 17:36:50.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:36:50.208: INFO: namespace init-container-4521 deletion completed in 6.100495828s

• [SLOW TEST:12.472 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:36:50.208: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Feb 23 17:36:52.173: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0223 17:36:52.173101      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:36:52.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6774" for this suite.
Feb 23 17:36:58.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:36:58.270: INFO: namespace gc-6774 deletion completed in 6.095079848s

• [SLOW TEST:8.062 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:36:58.271: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 17:36:58.300: INFO: Creating deployment "test-recreate-deployment"
Feb 23 17:36:58.307: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 23 17:36:58.316: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Feb 23 17:37:00.326: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 23 17:37:00.329: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718076218, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718076218, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718076218, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718076218, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 17:37:02.334: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718076218, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718076218, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718076218, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718076218, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 17:37:04.333: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718076218, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718076218, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718076218, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718076218, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 17:37:06.337: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 23 17:37:06.347: INFO: Updating deployment test-recreate-deployment
Feb 23 17:37:06.347: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Feb 23 17:37:06.944: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-7648 /apis/apps/v1/namespaces/deployment-7648/deployments/test-recreate-deployment bb4be596-a73d-4272-a2ab-cfec976ec7e4 3708 2 2020-02-23 17:36:58 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0030a9588 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-02-23 17:37:06 +0000 UTC,LastTransitionTime:2020-02-23 17:37:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-02-23 17:37:06 +0000 UTC,LastTransitionTime:2020-02-23 17:36:58 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Feb 23 17:37:06.947: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-7648 /apis/apps/v1/namespaces/deployment-7648/replicasets/test-recreate-deployment-5f94c574ff 4d888e4a-663f-4570-a900-937ecb42d0fd 3704 1 2020-02-23 17:37:06 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment bb4be596-a73d-4272-a2ab-cfec976ec7e4 0xc00313b677 0xc00313b678}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00313b6d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 23 17:37:06.947: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 23 17:37:06.948: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-7648 /apis/apps/v1/namespaces/deployment-7648/replicasets/test-recreate-deployment-68fc85c7bb b2c0c253-24fa-4b03-a29e-2094dd8db652 3696 2 2020-02-23 17:36:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment bb4be596-a73d-4272-a2ab-cfec976ec7e4 0xc00313b737 0xc00313b738}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00313b798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 23 17:37:06.951: INFO: Pod "test-recreate-deployment-5f94c574ff-7mssj" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-7mssj test-recreate-deployment-5f94c574ff- deployment-7648 /api/v1/namespaces/deployment-7648/pods/test-recreate-deployment-5f94c574ff-7mssj 7fb40c9f-3e1e-4937-8d13-4b0e0f51b89e 3707 0 2020-02-23 17:37:06 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 4d888e4a-663f-4570-a900-937ecb42d0fd 0xc0030a9947 0xc0030a9948}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dtftx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dtftx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dtftx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 17:37:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 17:37:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 17:37:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 17:37:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:,StartTime:2020-02-23 17:37:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:37:06.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7648" for this suite.
Feb 23 17:37:12.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:37:13.054: INFO: namespace deployment-7648 deletion completed in 6.097888897s

• [SLOW TEST:14.782 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:37:13.055: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Feb 23 17:37:15.122: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-952870286 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb 23 17:37:25.198: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:37:25.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3859" for this suite.
Feb 23 17:37:31.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:37:31.309: INFO: namespace pods-3859 deletion completed in 6.104814032s

• [SLOW TEST:18.254 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:37:31.313: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 17:37:31.409: INFO: (0) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.512063ms)
Feb 23 17:37:31.413: INFO: (1) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.020634ms)
Feb 23 17:37:31.418: INFO: (2) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.332389ms)
Feb 23 17:37:31.421: INFO: (3) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.64272ms)
Feb 23 17:37:31.448: INFO: (4) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 26.759895ms)
Feb 23 17:37:31.453: INFO: (5) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.879196ms)
Feb 23 17:37:31.457: INFO: (6) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.208813ms)
Feb 23 17:37:31.460: INFO: (7) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.702964ms)
Feb 23 17:37:31.463: INFO: (8) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.077899ms)
Feb 23 17:37:31.466: INFO: (9) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.437918ms)
Feb 23 17:37:31.469: INFO: (10) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.190482ms)
Feb 23 17:37:31.472: INFO: (11) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.852407ms)
Feb 23 17:37:31.477: INFO: (12) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.289683ms)
Feb 23 17:37:31.480: INFO: (13) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.699784ms)
Feb 23 17:37:31.486: INFO: (14) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.663798ms)
Feb 23 17:37:31.489: INFO: (15) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.774113ms)
Feb 23 17:37:31.492: INFO: (16) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.156398ms)
Feb 23 17:37:31.496: INFO: (17) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.667466ms)
Feb 23 17:37:31.499: INFO: (18) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.452725ms)
Feb 23 17:37:31.502: INFO: (19) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.744735ms)
[AfterEach] version v1
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:37:31.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1776" for this suite.
Feb 23 17:37:44.900: INFO: Error while waiting for namespace to be terminated: etcdserver: request timed out
Feb 23 17:37:45.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:37:45.659: INFO: namespace proxy-1776 deletion completed in 14.153151283s

• [SLOW TEST:14.346 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:37:45.659: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6638.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6638.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6638.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6638.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6638.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6638.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6638.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6638.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6638.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6638.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6638.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6638.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6638.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 3.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.3_udp@PTR;check="$$(dig +tcp +noall +answer +search 3.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.3_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6638.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6638.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6638.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6638.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6638.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6638.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6638.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6638.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6638.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6638.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6638.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6638.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6638.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 3.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.3_udp@PTR;check="$$(dig +tcp +noall +answer +search 3.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.3_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 23 17:38:13.873: INFO: Unable to read wheezy_udp@dns-test-service.dns-6638.svc.cluster.local from pod dns-6638/dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935: the server could not find the requested resource (get pods dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935)
Feb 23 17:38:13.878: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6638.svc.cluster.local from pod dns-6638/dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935: the server could not find the requested resource (get pods dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935)
Feb 23 17:38:13.881: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6638.svc.cluster.local from pod dns-6638/dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935: the server could not find the requested resource (get pods dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935)
Feb 23 17:38:13.885: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6638.svc.cluster.local from pod dns-6638/dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935: the server could not find the requested resource (get pods dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935)
Feb 23 17:38:13.907: INFO: Unable to read jessie_udp@dns-test-service.dns-6638.svc.cluster.local from pod dns-6638/dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935: the server could not find the requested resource (get pods dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935)
Feb 23 17:38:13.910: INFO: Unable to read jessie_tcp@dns-test-service.dns-6638.svc.cluster.local from pod dns-6638/dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935: the server could not find the requested resource (get pods dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935)
Feb 23 17:38:13.913: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6638.svc.cluster.local from pod dns-6638/dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935: the server could not find the requested resource (get pods dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935)
Feb 23 17:38:13.917: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6638.svc.cluster.local from pod dns-6638/dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935: the server could not find the requested resource (get pods dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935)
Feb 23 17:38:13.936: INFO: Lookups using dns-6638/dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935 failed for: [wheezy_udp@dns-test-service.dns-6638.svc.cluster.local wheezy_tcp@dns-test-service.dns-6638.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6638.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6638.svc.cluster.local jessie_udp@dns-test-service.dns-6638.svc.cluster.local jessie_tcp@dns-test-service.dns-6638.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6638.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6638.svc.cluster.local]

Feb 23 17:38:18.943: INFO: Unable to read wheezy_udp@dns-test-service.dns-6638.svc.cluster.local from pod dns-6638/dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935: the server could not find the requested resource (get pods dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935)
Feb 23 17:38:18.948: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6638.svc.cluster.local from pod dns-6638/dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935: the server could not find the requested resource (get pods dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935)
Feb 23 17:38:18.952: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6638.svc.cluster.local from pod dns-6638/dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935: the server could not find the requested resource (get pods dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935)
Feb 23 17:38:18.956: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6638.svc.cluster.local from pod dns-6638/dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935: the server could not find the requested resource (get pods dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935)
Feb 23 17:38:18.981: INFO: Unable to read jessie_udp@dns-test-service.dns-6638.svc.cluster.local from pod dns-6638/dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935: the server could not find the requested resource (get pods dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935)
Feb 23 17:38:18.985: INFO: Unable to read jessie_tcp@dns-test-service.dns-6638.svc.cluster.local from pod dns-6638/dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935: the server could not find the requested resource (get pods dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935)
Feb 23 17:38:18.988: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6638.svc.cluster.local from pod dns-6638/dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935: the server could not find the requested resource (get pods dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935)
Feb 23 17:38:18.991: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6638.svc.cluster.local from pod dns-6638/dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935: the server could not find the requested resource (get pods dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935)
Feb 23 17:38:19.009: INFO: Lookups using dns-6638/dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935 failed for: [wheezy_udp@dns-test-service.dns-6638.svc.cluster.local wheezy_tcp@dns-test-service.dns-6638.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6638.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6638.svc.cluster.local jessie_udp@dns-test-service.dns-6638.svc.cluster.local jessie_tcp@dns-test-service.dns-6638.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6638.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6638.svc.cluster.local]

Feb 23 17:38:24.003: INFO: DNS probes using dns-6638/dns-test-7844c365-dd6d-47bb-bc40-69e4f1054935 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:38:24.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6638" for this suite.
Feb 23 17:38:30.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:38:30.740: INFO: namespace dns-6638 deletion completed in 6.620240234s

• [SLOW TEST:45.081 seconds]
[sig-network] DNS
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:38:30.740: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 23 17:38:30.775: INFO: Waiting up to 5m0s for pod "pod-06a9b4a1-6b8b-4390-819e-63f61c828779" in namespace "emptydir-9005" to be "success or failure"
Feb 23 17:38:30.778: INFO: Pod "pod-06a9b4a1-6b8b-4390-819e-63f61c828779": Phase="Pending", Reason="", readiness=false. Elapsed: 2.653033ms
Feb 23 17:38:32.784: INFO: Pod "pod-06a9b4a1-6b8b-4390-819e-63f61c828779": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008806795s
Feb 23 17:38:34.789: INFO: Pod "pod-06a9b4a1-6b8b-4390-819e-63f61c828779": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013987044s
STEP: Saw pod success
Feb 23 17:38:34.789: INFO: Pod "pod-06a9b4a1-6b8b-4390-819e-63f61c828779" satisfied condition "success or failure"
Feb 23 17:38:34.794: INFO: Trying to get logs from node worker00 pod pod-06a9b4a1-6b8b-4390-819e-63f61c828779 container test-container: <nil>
STEP: delete the pod
Feb 23 17:38:34.818: INFO: Waiting for pod pod-06a9b4a1-6b8b-4390-819e-63f61c828779 to disappear
Feb 23 17:38:34.821: INFO: Pod pod-06a9b4a1-6b8b-4390-819e-63f61c828779 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:38:34.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9005" for this suite.
Feb 23 17:38:40.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:38:40.924: INFO: namespace emptydir-9005 deletion completed in 6.099817437s

• [SLOW TEST:10.184 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:38:40.924: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:39:40.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5767" for this suite.
Feb 23 17:40:08.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:40:09.073: INFO: namespace container-probe-5767 deletion completed in 28.104316619s

• [SLOW TEST:88.149 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:40:09.079: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb 23 17:40:09.558: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Feb 23 17:40:11.570: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718076409, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718076409, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718076409, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718076409, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 17:40:13.578: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718076409, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718076409, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718076409, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718076409, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 17:40:15.591: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718076409, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718076409, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718076409, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718076409, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 17:40:18.584: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 17:40:18.588: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:40:24.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-693" for this suite.
Feb 23 17:40:30.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:40:30.819: INFO: namespace crd-webhook-693 deletion completed in 6.088587354s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:21.753 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:40:30.833: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-c31e039e-d202-43a7-b047-bb5d584ed83d
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:40:30.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4916" for this suite.
Feb 23 17:40:36.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:40:36.971: INFO: namespace secrets-4916 deletion completed in 6.102791472s

• [SLOW TEST:6.139 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:40:36.971: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 23 17:40:37.017: INFO: Waiting up to 5m0s for pod "downwardapi-volume-11d1babb-de59-458b-929d-e05153184652" in namespace "projected-2843" to be "success or failure"
Feb 23 17:40:37.019: INFO: Pod "downwardapi-volume-11d1babb-de59-458b-929d-e05153184652": Phase="Pending", Reason="", readiness=false. Elapsed: 2.670099ms
Feb 23 17:40:39.024: INFO: Pod "downwardapi-volume-11d1babb-de59-458b-929d-e05153184652": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007473312s
STEP: Saw pod success
Feb 23 17:40:39.024: INFO: Pod "downwardapi-volume-11d1babb-de59-458b-929d-e05153184652" satisfied condition "success or failure"
Feb 23 17:40:39.028: INFO: Trying to get logs from node worker00 pod downwardapi-volume-11d1babb-de59-458b-929d-e05153184652 container client-container: <nil>
STEP: delete the pod
Feb 23 17:40:39.052: INFO: Waiting for pod downwardapi-volume-11d1babb-de59-458b-929d-e05153184652 to disappear
Feb 23 17:40:39.057: INFO: Pod downwardapi-volume-11d1babb-de59-458b-929d-e05153184652 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:40:39.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2843" for this suite.
Feb 23 17:40:45.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:40:45.658: INFO: namespace projected-2843 deletion completed in 6.598767651s

• [SLOW TEST:8.687 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:40:45.659: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Feb 23 17:40:45.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 api-versions'
Feb 23 17:40:45.765: INFO: stderr: ""
Feb 23 17:40:45.765: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauditregistration.k8s.io/v1alpha1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1alpha1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1alpha1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1alpha1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1alpha1\nscheduling.k8s.io/v1beta1\nsettings.k8s.io/v1alpha1\nsnapshot.storage.k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1alpha1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:40:45.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8143" for this suite.
Feb 23 17:40:51.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:40:51.867: INFO: namespace kubectl-8143 deletion completed in 6.097566856s

• [SLOW TEST:6.208 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:40:51.875: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 17:40:52.395: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 17:40:55.419: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:40:55.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5495" for this suite.
Feb 23 17:41:01.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:41:01.690: INFO: namespace webhook-5495 deletion completed in 6.098983645s
STEP: Destroying namespace "webhook-5495-markers" for this suite.
Feb 23 17:41:07.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:41:07.795: INFO: namespace webhook-5495-markers deletion completed in 6.105477849s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:15.932 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:41:07.807: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Feb 23 17:41:08.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 --namespace=kubectl-6363 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb 23 17:41:10.698: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb 23 17:41:10.698: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:41:12.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6363" for this suite.
Feb 23 17:41:26.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:41:26.806: INFO: namespace kubectl-6363 deletion completed in 14.100188631s

• [SLOW TEST:18.999 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:41:26.806: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-5090
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 23 17:41:26.838: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 23 17:41:50.924: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.131.148:8080/dial?request=hostName&protocol=udp&host=10.200.5.14&port=8081&tries=1'] Namespace:pod-network-test-5090 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 17:41:50.924: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
Feb 23 17:41:50.989: INFO: Waiting for endpoints: map[]
Feb 23 17:41:50.992: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.131.148:8080/dial?request=hostName&protocol=udp&host=10.200.131.147&port=8081&tries=1'] Namespace:pod-network-test-5090 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 17:41:50.992: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
Feb 23 17:41:51.055: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:41:51.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5090" for this suite.
Feb 23 17:42:03.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:42:03.150: INFO: namespace pod-network-test-5090 deletion completed in 12.093154155s

• [SLOW TEST:36.345 seconds]
[sig-network] Networking
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:42:03.151: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-2bf91fec-953b-41b4-8714-33fcb9d15924 in namespace container-probe-9011
Feb 23 17:42:05.209: INFO: Started pod busybox-2bf91fec-953b-41b4-8714-33fcb9d15924 in namespace container-probe-9011
STEP: checking the pod's current state and verifying that restartCount is present
Feb 23 17:42:05.212: INFO: Initial restart count of pod busybox-2bf91fec-953b-41b4-8714-33fcb9d15924 is 0
Feb 23 17:42:51.564: INFO: Restart count of pod container-probe-9011/busybox-2bf91fec-953b-41b4-8714-33fcb9d15924 is now 1 (46.351917222s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:42:51.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9011" for this suite.
Feb 23 17:42:57.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:42:57.986: INFO: namespace container-probe-9011 deletion completed in 6.39255624s

• [SLOW TEST:54.836 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:42:57.987: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 23 17:43:04.073: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9305 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 17:43:04.073: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
Feb 23 17:43:04.137: INFO: Exec stderr: ""
Feb 23 17:43:04.138: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9305 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 17:43:04.138: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
Feb 23 17:43:04.201: INFO: Exec stderr: ""
Feb 23 17:43:04.201: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9305 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 17:43:04.201: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
Feb 23 17:43:04.268: INFO: Exec stderr: ""
Feb 23 17:43:04.268: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9305 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 17:43:04.268: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
Feb 23 17:43:04.323: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 23 17:43:04.323: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9305 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 17:43:04.323: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
Feb 23 17:43:04.392: INFO: Exec stderr: ""
Feb 23 17:43:04.392: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9305 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 17:43:04.392: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
Feb 23 17:43:04.469: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 23 17:43:04.469: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9305 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 17:43:04.469: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
Feb 23 17:43:04.535: INFO: Exec stderr: ""
Feb 23 17:43:04.535: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9305 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 17:43:04.535: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
Feb 23 17:43:04.594: INFO: Exec stderr: ""
Feb 23 17:43:04.594: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9305 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 17:43:04.594: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
Feb 23 17:43:04.665: INFO: Exec stderr: ""
Feb 23 17:43:04.665: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9305 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 17:43:04.665: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
Feb 23 17:43:04.731: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:43:04.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9305" for this suite.
Feb 23 17:43:48.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:43:48.838: INFO: namespace e2e-kubelet-etc-hosts-9305 deletion completed in 44.104798822s

• [SLOW TEST:50.852 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:43:48.839: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 23 17:43:48.877: INFO: Waiting up to 5m0s for pod "pod-7705496c-ea6f-49db-bd3a-faa39838cf67" in namespace "emptydir-8887" to be "success or failure"
Feb 23 17:43:48.880: INFO: Pod "pod-7705496c-ea6f-49db-bd3a-faa39838cf67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.918607ms
Feb 23 17:43:50.886: INFO: Pod "pod-7705496c-ea6f-49db-bd3a-faa39838cf67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00863649s
Feb 23 17:43:52.890: INFO: Pod "pod-7705496c-ea6f-49db-bd3a-faa39838cf67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012306594s
STEP: Saw pod success
Feb 23 17:43:52.890: INFO: Pod "pod-7705496c-ea6f-49db-bd3a-faa39838cf67" satisfied condition "success or failure"
Feb 23 17:43:52.894: INFO: Trying to get logs from node worker00 pod pod-7705496c-ea6f-49db-bd3a-faa39838cf67 container test-container: <nil>
STEP: delete the pod
Feb 23 17:43:52.927: INFO: Waiting for pod pod-7705496c-ea6f-49db-bd3a-faa39838cf67 to disappear
Feb 23 17:43:52.931: INFO: Pod pod-7705496c-ea6f-49db-bd3a-faa39838cf67 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:43:52.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8887" for this suite.
Feb 23 17:43:58.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:43:59.030: INFO: namespace emptydir-8887 deletion completed in 6.096145552s

• [SLOW TEST:10.191 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:43:59.030: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Feb 23 17:43:59.061: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Feb 23 17:44:15.562: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
Feb 23 17:44:23.406: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:44:38.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9739" for this suite.
Feb 23 17:44:44.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:44:44.669: INFO: namespace crd-publish-openapi-9739 deletion completed in 6.089307122s

• [SLOW TEST:45.639 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:44:44.669: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4045
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4045
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4045
Feb 23 17:44:44.721: INFO: Found 0 stateful pods, waiting for 1
Feb 23 17:44:54.730: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 23 17:44:54.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=statefulset-4045 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 23 17:44:54.857: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 23 17:44:54.857: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 23 17:44:54.857: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 23 17:44:54.862: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 23 17:45:04.869: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 23 17:45:04.869: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 17:45:04.885: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999649s
Feb 23 17:45:05.889: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996471779s
Feb 23 17:45:06.894: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992967722s
Feb 23 17:45:07.905: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987455733s
Feb 23 17:45:08.910: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.976276656s
Feb 23 17:45:09.918: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.971819167s
Feb 23 17:45:10.923: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.962863609s
Feb 23 17:45:11.930: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.95910543s
Feb 23 17:45:12.934: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.952386747s
Feb 23 17:45:13.943: INFO: Verifying statefulset ss doesn't scale past 1 for another 947.426232ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4045
Feb 23 17:45:14.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=statefulset-4045 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 17:45:15.082: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 23 17:45:15.082: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 23 17:45:15.082: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 23 17:45:15.085: INFO: Found 1 stateful pods, waiting for 3
Feb 23 17:45:25.087: INFO: Found 2 stateful pods, waiting for 3
Feb 23 17:45:35.092: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 17:45:35.092: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 17:45:35.092: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 23 17:45:35.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=statefulset-4045 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 23 17:45:35.229: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 23 17:45:35.229: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 23 17:45:35.229: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 23 17:45:35.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=statefulset-4045 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 23 17:45:35.370: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 23 17:45:35.370: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 23 17:45:35.370: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 23 17:45:35.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=statefulset-4045 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 23 17:45:35.516: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 23 17:45:35.516: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 23 17:45:35.516: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 23 17:45:35.516: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 17:45:35.527: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb 23 17:45:45.536: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 23 17:45:45.536: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 23 17:45:45.536: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 23 17:45:45.553: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999593s
Feb 23 17:45:46.559: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993086957s
Feb 23 17:45:47.563: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986397023s
Feb 23 17:45:48.570: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982180756s
Feb 23 17:45:49.580: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.972964367s
Feb 23 17:45:50.589: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.965326885s
Feb 23 17:45:51.593: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.9571024s
Feb 23 17:45:52.598: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.951958556s
Feb 23 17:45:53.606: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.945792162s
Feb 23 17:45:54.610: INFO: Verifying statefulset ss doesn't scale past 3 for another 940.073928ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4045
Feb 23 17:45:55.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=statefulset-4045 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 17:45:55.727: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 23 17:45:55.727: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 23 17:45:55.727: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 23 17:45:55.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=statefulset-4045 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 17:45:55.837: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 23 17:45:55.838: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 23 17:45:55.838: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 23 17:45:55.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=statefulset-4045 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 17:45:56.483: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 23 17:45:56.483: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 23 17:45:56.483: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 23 17:45:56.483: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Feb 23 17:46:16.496: INFO: Deleting all statefulset in ns statefulset-4045
Feb 23 17:46:16.501: INFO: Scaling statefulset ss to 0
Feb 23 17:46:16.508: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 17:46:16.510: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:46:16.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4045" for this suite.
Feb 23 17:46:22.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:46:22.628: INFO: namespace statefulset-4045 deletion completed in 6.09885497s

• [SLOW TEST:97.959 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:46:22.629: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:46:24.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-541" for this suite.
Feb 23 17:47:06.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:47:06.807: INFO: namespace kubelet-test-541 deletion completed in 42.109357607s

• [SLOW TEST:44.179 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:47:06.807: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-6226
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Feb 23 17:47:06.857: INFO: Found 0 stateful pods, waiting for 3
Feb 23 17:47:16.868: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 17:47:16.868: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 17:47:16.868: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 17:47:16.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=statefulset-6226 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 23 17:47:17.017: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 23 17:47:17.017: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 23 17:47:17.017: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Feb 23 17:47:27.051: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 23 17:47:37.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=statefulset-6226 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 17:47:37.185: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 23 17:47:37.185: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 23 17:47:37.185: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 23 17:47:47.205: INFO: Waiting for StatefulSet statefulset-6226/ss2 to complete update
Feb 23 17:47:47.205: INFO: Waiting for Pod statefulset-6226/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 23 17:47:47.205: INFO: Waiting for Pod statefulset-6226/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 23 17:47:57.214: INFO: Waiting for StatefulSet statefulset-6226/ss2 to complete update
Feb 23 17:47:57.214: INFO: Waiting for Pod statefulset-6226/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 23 17:47:57.214: INFO: Waiting for Pod statefulset-6226/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 23 17:48:07.218: INFO: Waiting for StatefulSet statefulset-6226/ss2 to complete update
Feb 23 17:48:07.218: INFO: Waiting for Pod statefulset-6226/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 23 17:48:17.210: INFO: Waiting for StatefulSet statefulset-6226/ss2 to complete update
Feb 23 17:48:17.210: INFO: Waiting for Pod statefulset-6226/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 23 17:48:27.216: INFO: Waiting for StatefulSet statefulset-6226/ss2 to complete update
Feb 23 17:48:27.216: INFO: Waiting for Pod statefulset-6226/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Feb 23 17:48:37.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=statefulset-6226 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 23 17:48:37.339: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 23 17:48:37.339: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 23 17:48:37.339: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 23 17:48:47.386: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 23 17:48:57.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=statefulset-6226 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 17:48:57.536: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 23 17:48:57.536: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 23 17:48:57.536: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Feb 23 17:49:17.561: INFO: Deleting all statefulset in ns statefulset-6226
Feb 23 17:49:17.565: INFO: Scaling statefulset ss2 to 0
Feb 23 17:49:37.579: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 17:49:37.582: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:49:37.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6226" for this suite.
Feb 23 17:49:45.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:49:45.698: INFO: namespace statefulset-6226 deletion completed in 8.097683377s

• [SLOW TEST:158.891 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:49:45.698: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb 23 17:49:45.966: INFO: Pod name wrapped-volume-race-d20e6d7a-c549-452a-9c07-ccb9d02f5995: Found 3 pods out of 5
Feb 23 17:49:50.972: INFO: Pod name wrapped-volume-race-d20e6d7a-c549-452a-9c07-ccb9d02f5995: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d20e6d7a-c549-452a-9c07-ccb9d02f5995 in namespace emptydir-wrapper-7473, will wait for the garbage collector to delete the pods
Feb 23 17:49:59.461: INFO: Deleting ReplicationController wrapped-volume-race-d20e6d7a-c549-452a-9c07-ccb9d02f5995 took: 8.583009ms
Feb 23 17:50:00.362: INFO: Terminating ReplicationController wrapped-volume-race-d20e6d7a-c549-452a-9c07-ccb9d02f5995 pods took: 901.74322ms
STEP: Creating RC which spawns configmap-volume pods
Feb 23 17:50:43.576: INFO: Pod name wrapped-volume-race-5bd344fa-830c-4f32-bbb6-6c48f0e50355: Found 0 pods out of 5
Feb 23 17:50:48.581: INFO: Pod name wrapped-volume-race-5bd344fa-830c-4f32-bbb6-6c48f0e50355: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-5bd344fa-830c-4f32-bbb6-6c48f0e50355 in namespace emptydir-wrapper-7473, will wait for the garbage collector to delete the pods
Feb 23 17:50:58.667: INFO: Deleting ReplicationController wrapped-volume-race-5bd344fa-830c-4f32-bbb6-6c48f0e50355 took: 11.80386ms
Feb 23 17:50:59.567: INFO: Terminating ReplicationController wrapped-volume-race-5bd344fa-830c-4f32-bbb6-6c48f0e50355 pods took: 900.28765ms
STEP: Creating RC which spawns configmap-volume pods
Feb 23 17:51:43.581: INFO: Pod name wrapped-volume-race-d9976c74-20c5-4fd1-a57b-01369b8a42be: Found 0 pods out of 5
Feb 23 17:51:48.591: INFO: Pod name wrapped-volume-race-d9976c74-20c5-4fd1-a57b-01369b8a42be: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d9976c74-20c5-4fd1-a57b-01369b8a42be in namespace emptydir-wrapper-7473, will wait for the garbage collector to delete the pods
Feb 23 17:51:58.673: INFO: Deleting ReplicationController wrapped-volume-race-d9976c74-20c5-4fd1-a57b-01369b8a42be took: 12.102626ms
Feb 23 17:51:59.974: INFO: Terminating ReplicationController wrapped-volume-race-d9976c74-20c5-4fd1-a57b-01369b8a42be pods took: 1.300242383s
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:52:44.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7473" for this suite.
Feb 23 17:52:52.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:52:52.392: INFO: namespace emptydir-wrapper-7473 deletion completed in 8.102559031s

• [SLOW TEST:186.694 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:52:52.392: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 17:52:52.433: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-53cad685-3c76-4930-a68d-dc38b91d2914" in namespace "security-context-test-1247" to be "success or failure"
Feb 23 17:52:52.439: INFO: Pod "alpine-nnp-false-53cad685-3c76-4930-a68d-dc38b91d2914": Phase="Pending", Reason="", readiness=false. Elapsed: 4.858534ms
Feb 23 17:52:54.444: INFO: Pod "alpine-nnp-false-53cad685-3c76-4930-a68d-dc38b91d2914": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009713787s
Feb 23 17:52:56.450: INFO: Pod "alpine-nnp-false-53cad685-3c76-4930-a68d-dc38b91d2914": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016667636s
Feb 23 17:52:56.451: INFO: Pod "alpine-nnp-false-53cad685-3c76-4930-a68d-dc38b91d2914" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:52:56.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1247" for this suite.
Feb 23 17:53:02.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:53:02.555: INFO: namespace security-context-test-1247 deletion completed in 6.089917723s

• [SLOW TEST:10.162 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:53:02.556: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:53:04.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2074" for this suite.
Feb 23 17:53:10.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:53:10.773: INFO: namespace emptydir-wrapper-2074 deletion completed in 6.107012881s

• [SLOW TEST:8.218 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:53:10.773: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-9361652f-0521-418b-aeb4-fd6a62e05cff
STEP: Creating a pod to test consume configMaps
Feb 23 17:53:10.817: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c06fbfd0-e164-41ed-bf11-e5cdf1fe9650" in namespace "projected-3268" to be "success or failure"
Feb 23 17:53:10.822: INFO: Pod "pod-projected-configmaps-c06fbfd0-e164-41ed-bf11-e5cdf1fe9650": Phase="Pending", Reason="", readiness=false. Elapsed: 4.354333ms
Feb 23 17:53:12.825: INFO: Pod "pod-projected-configmaps-c06fbfd0-e164-41ed-bf11-e5cdf1fe9650": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00761197s
STEP: Saw pod success
Feb 23 17:53:12.825: INFO: Pod "pod-projected-configmaps-c06fbfd0-e164-41ed-bf11-e5cdf1fe9650" satisfied condition "success or failure"
Feb 23 17:53:12.829: INFO: Trying to get logs from node worker00 pod pod-projected-configmaps-c06fbfd0-e164-41ed-bf11-e5cdf1fe9650 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 17:53:12.849: INFO: Waiting for pod pod-projected-configmaps-c06fbfd0-e164-41ed-bf11-e5cdf1fe9650 to disappear
Feb 23 17:53:12.852: INFO: Pod pod-projected-configmaps-c06fbfd0-e164-41ed-bf11-e5cdf1fe9650 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:53:12.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3268" for this suite.
Feb 23 17:53:18.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:53:18.950: INFO: namespace projected-3268 deletion completed in 6.095403469s

• [SLOW TEST:8.177 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:53:18.950: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Feb 23 17:53:18.987: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:53:22.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7506" for this suite.
Feb 23 17:53:28.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:53:28.717: INFO: namespace init-container-7506 deletion completed in 6.097362794s

• [SLOW TEST:9.768 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:53:28.725: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:53:28.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9717" for this suite.
Feb 23 17:53:34.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:53:34.882: INFO: namespace resourcequota-9717 deletion completed in 6.097516656s

• [SLOW TEST:6.159 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:53:34.882: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Feb 23 17:53:34.922: INFO: Waiting up to 5m0s for pod "downward-api-fb8496a1-9add-46db-8787-9f92332ce889" in namespace "downward-api-9849" to be "success or failure"
Feb 23 17:53:34.927: INFO: Pod "downward-api-fb8496a1-9add-46db-8787-9f92332ce889": Phase="Pending", Reason="", readiness=false. Elapsed: 5.121044ms
Feb 23 17:53:36.935: INFO: Pod "downward-api-fb8496a1-9add-46db-8787-9f92332ce889": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012508263s
Feb 23 17:53:38.941: INFO: Pod "downward-api-fb8496a1-9add-46db-8787-9f92332ce889": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019160875s
STEP: Saw pod success
Feb 23 17:53:38.941: INFO: Pod "downward-api-fb8496a1-9add-46db-8787-9f92332ce889" satisfied condition "success or failure"
Feb 23 17:53:38.945: INFO: Trying to get logs from node worker00 pod downward-api-fb8496a1-9add-46db-8787-9f92332ce889 container dapi-container: <nil>
STEP: delete the pod
Feb 23 17:53:38.961: INFO: Waiting for pod downward-api-fb8496a1-9add-46db-8787-9f92332ce889 to disappear
Feb 23 17:53:38.964: INFO: Pod downward-api-fb8496a1-9add-46db-8787-9f92332ce889 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:53:38.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9849" for this suite.
Feb 23 17:53:45.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:53:45.615: INFO: namespace downward-api-9849 deletion completed in 6.648622078s

• [SLOW TEST:10.733 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:53:45.615: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Feb 23 17:53:45.653: INFO: Waiting up to 5m0s for pod "client-containers-c24e3ffc-434a-47fa-a4a9-3bc25d504df0" in namespace "containers-8514" to be "success or failure"
Feb 23 17:53:45.658: INFO: Pod "client-containers-c24e3ffc-434a-47fa-a4a9-3bc25d504df0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.731912ms
Feb 23 17:53:47.662: INFO: Pod "client-containers-c24e3ffc-434a-47fa-a4a9-3bc25d504df0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009070319s
STEP: Saw pod success
Feb 23 17:53:47.662: INFO: Pod "client-containers-c24e3ffc-434a-47fa-a4a9-3bc25d504df0" satisfied condition "success or failure"
Feb 23 17:53:47.666: INFO: Trying to get logs from node worker00 pod client-containers-c24e3ffc-434a-47fa-a4a9-3bc25d504df0 container test-container: <nil>
STEP: delete the pod
Feb 23 17:53:47.686: INFO: Waiting for pod client-containers-c24e3ffc-434a-47fa-a4a9-3bc25d504df0 to disappear
Feb 23 17:53:47.693: INFO: Pod client-containers-c24e3ffc-434a-47fa-a4a9-3bc25d504df0 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:53:47.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8514" for this suite.
Feb 23 17:53:53.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:53:53.781: INFO: namespace containers-8514 deletion completed in 6.085093086s

• [SLOW TEST:8.166 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:53:53.783: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-33787edb-3c2a-43b6-9ded-d5749c9378da
STEP: Creating a pod to test consume configMaps
Feb 23 17:53:53.827: INFO: Waiting up to 5m0s for pod "pod-configmaps-c61b9af5-9712-411e-a1e0-b261078bebd3" in namespace "configmap-3947" to be "success or failure"
Feb 23 17:53:53.831: INFO: Pod "pod-configmaps-c61b9af5-9712-411e-a1e0-b261078bebd3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.691784ms
Feb 23 17:53:55.835: INFO: Pod "pod-configmaps-c61b9af5-9712-411e-a1e0-b261078bebd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007134678s
STEP: Saw pod success
Feb 23 17:53:55.835: INFO: Pod "pod-configmaps-c61b9af5-9712-411e-a1e0-b261078bebd3" satisfied condition "success or failure"
Feb 23 17:53:55.838: INFO: Trying to get logs from node worker00 pod pod-configmaps-c61b9af5-9712-411e-a1e0-b261078bebd3 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 17:53:55.855: INFO: Waiting for pod pod-configmaps-c61b9af5-9712-411e-a1e0-b261078bebd3 to disappear
Feb 23 17:53:55.858: INFO: Pod pod-configmaps-c61b9af5-9712-411e-a1e0-b261078bebd3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:53:55.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3947" for this suite.
Feb 23 17:54:01.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:54:01.955: INFO: namespace configmap-3947 deletion completed in 6.094654025s

• [SLOW TEST:8.172 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:54:01.955: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:54:06.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-435" for this suite.
Feb 23 17:54:58.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:54:58.116: INFO: namespace kubelet-test-435 deletion completed in 52.095448011s

• [SLOW TEST:56.160 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:54:58.118: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 23 17:54:58.155: INFO: Waiting up to 5m0s for pod "downwardapi-volume-37ff7568-b907-4bde-aebd-b8540b3ca1b7" in namespace "downward-api-4787" to be "success or failure"
Feb 23 17:54:58.159: INFO: Pod "downwardapi-volume-37ff7568-b907-4bde-aebd-b8540b3ca1b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.524795ms
Feb 23 17:55:00.166: INFO: Pod "downwardapi-volume-37ff7568-b907-4bde-aebd-b8540b3ca1b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010967159s
STEP: Saw pod success
Feb 23 17:55:00.166: INFO: Pod "downwardapi-volume-37ff7568-b907-4bde-aebd-b8540b3ca1b7" satisfied condition "success or failure"
Feb 23 17:55:00.169: INFO: Trying to get logs from node worker00 pod downwardapi-volume-37ff7568-b907-4bde-aebd-b8540b3ca1b7 container client-container: <nil>
STEP: delete the pod
Feb 23 17:55:00.193: INFO: Waiting for pod downwardapi-volume-37ff7568-b907-4bde-aebd-b8540b3ca1b7 to disappear
Feb 23 17:55:00.199: INFO: Pod downwardapi-volume-37ff7568-b907-4bde-aebd-b8540b3ca1b7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:55:00.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4787" for this suite.
Feb 23 17:55:06.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:55:06.311: INFO: namespace downward-api-4787 deletion completed in 6.108007587s

• [SLOW TEST:8.194 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:55:06.313: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 17:55:08.391: INFO: Waiting up to 5m0s for pod "client-envvars-87183404-3a50-44e4-9224-73109c748b59" in namespace "pods-6482" to be "success or failure"
Feb 23 17:55:08.398: INFO: Pod "client-envvars-87183404-3a50-44e4-9224-73109c748b59": Phase="Pending", Reason="", readiness=false. Elapsed: 6.28119ms
Feb 23 17:55:10.402: INFO: Pod "client-envvars-87183404-3a50-44e4-9224-73109c748b59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01055696s
STEP: Saw pod success
Feb 23 17:55:10.402: INFO: Pod "client-envvars-87183404-3a50-44e4-9224-73109c748b59" satisfied condition "success or failure"
Feb 23 17:55:10.407: INFO: Trying to get logs from node worker00 pod client-envvars-87183404-3a50-44e4-9224-73109c748b59 container env3cont: <nil>
STEP: delete the pod
Feb 23 17:55:10.427: INFO: Waiting for pod client-envvars-87183404-3a50-44e4-9224-73109c748b59 to disappear
Feb 23 17:55:10.430: INFO: Pod client-envvars-87183404-3a50-44e4-9224-73109c748b59 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:55:10.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6482" for this suite.
Feb 23 17:55:22.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:55:22.536: INFO: namespace pods-6482 deletion completed in 12.101584345s

• [SLOW TEST:16.223 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:55:22.538: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-8bc1981a-8edf-46b7-b6bd-cfb0baf6cb77 in namespace container-probe-3912
Feb 23 17:55:26.587: INFO: Started pod liveness-8bc1981a-8edf-46b7-b6bd-cfb0baf6cb77 in namespace container-probe-3912
STEP: checking the pod's current state and verifying that restartCount is present
Feb 23 17:55:26.592: INFO: Initial restart count of pod liveness-8bc1981a-8edf-46b7-b6bd-cfb0baf6cb77 is 0
Feb 23 17:55:42.641: INFO: Restart count of pod container-probe-3912/liveness-8bc1981a-8edf-46b7-b6bd-cfb0baf6cb77 is now 1 (16.048757297s elapsed)
Feb 23 17:56:03.321: INFO: Restart count of pod container-probe-3912/liveness-8bc1981a-8edf-46b7-b6bd-cfb0baf6cb77 is now 2 (36.729212087s elapsed)
Feb 23 17:56:21.371: INFO: Restart count of pod container-probe-3912/liveness-8bc1981a-8edf-46b7-b6bd-cfb0baf6cb77 is now 3 (54.779493765s elapsed)
Feb 23 17:56:41.671: INFO: Restart count of pod container-probe-3912/liveness-8bc1981a-8edf-46b7-b6bd-cfb0baf6cb77 is now 4 (1m15.079308047s elapsed)
Feb 23 17:57:46.383: INFO: Restart count of pod container-probe-3912/liveness-8bc1981a-8edf-46b7-b6bd-cfb0baf6cb77 is now 5 (2m19.791536371s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:57:46.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3912" for this suite.
Feb 23 17:57:52.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:57:52.515: INFO: namespace container-probe-3912 deletion completed in 6.095375918s

• [SLOW TEST:149.978 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:57:52.517: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-9db4dfcd-9f4b-49d1-ab24-3cd558592177
STEP: Creating a pod to test consume secrets
Feb 23 17:57:52.560: INFO: Waiting up to 5m0s for pod "pod-secrets-9987f4f8-cd11-4500-89f2-86a3dd9c2a58" in namespace "secrets-2651" to be "success or failure"
Feb 23 17:57:52.563: INFO: Pod "pod-secrets-9987f4f8-cd11-4500-89f2-86a3dd9c2a58": Phase="Pending", Reason="", readiness=false. Elapsed: 3.587844ms
Feb 23 17:57:54.575: INFO: Pod "pod-secrets-9987f4f8-cd11-4500-89f2-86a3dd9c2a58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015789046s
STEP: Saw pod success
Feb 23 17:57:54.576: INFO: Pod "pod-secrets-9987f4f8-cd11-4500-89f2-86a3dd9c2a58" satisfied condition "success or failure"
Feb 23 17:57:54.579: INFO: Trying to get logs from node worker00 pod pod-secrets-9987f4f8-cd11-4500-89f2-86a3dd9c2a58 container secret-volume-test: <nil>
STEP: delete the pod
Feb 23 17:57:54.611: INFO: Waiting for pod pod-secrets-9987f4f8-cd11-4500-89f2-86a3dd9c2a58 to disappear
Feb 23 17:57:54.613: INFO: Pod pod-secrets-9987f4f8-cd11-4500-89f2-86a3dd9c2a58 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:57:54.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2651" for this suite.
Feb 23 17:58:00.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:58:00.716: INFO: namespace secrets-2651 deletion completed in 6.10016329s

• [SLOW TEST:8.199 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:58:00.716: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:58:00.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-9125" for this suite.
Feb 23 17:58:06.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:58:06.856: INFO: namespace tables-9125 deletion completed in 6.104539436s

• [SLOW TEST:6.140 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:58:06.856: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:58:23.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6258" for this suite.
Feb 23 17:58:29.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:58:29.685: INFO: namespace resourcequota-6258 deletion completed in 6.101227219s

• [SLOW TEST:22.829 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:58:29.685: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-5632
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-5632
Feb 23 17:58:29.797: INFO: Found 0 stateful pods, waiting for 1
Feb 23 17:58:39.800: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Feb 23 17:58:39.825: INFO: Deleting all statefulset in ns statefulset-5632
Feb 23 17:58:39.828: INFO: Scaling statefulset ss to 0
Feb 23 17:58:59.867: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 17:58:59.870: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:58:59.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5632" for this suite.
Feb 23 17:59:05.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:59:05.987: INFO: namespace statefulset-5632 deletion completed in 6.100193052s

• [SLOW TEST:36.302 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:59:05.987: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Feb 23 17:59:10.572: INFO: Successfully updated pod "labelsupdateedcfb8ab-76ec-49a9-9d4c-e73fc36c8e54"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:59:12.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6625" for this suite.
Feb 23 17:59:24.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:59:24.691: INFO: namespace projected-6625 deletion completed in 12.098711653s

• [SLOW TEST:18.704 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:59:24.691: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 17:59:25.128: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 17:59:28.156: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 17:59:38.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7690" for this suite.
Feb 23 17:59:44.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:59:44.465: INFO: namespace webhook-7690 deletion completed in 6.095911652s
STEP: Destroying namespace "webhook-7690-markers" for this suite.
Feb 23 17:59:50.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 17:59:50.565: INFO: namespace webhook-7690-markers deletion completed in 6.099818185s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:25.887 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 17:59:50.580: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-3485
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 23 17:59:50.610: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 23 18:00:14.690: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.131.136:8080/dial?request=hostName&protocol=http&host=10.200.131.135&port=8080&tries=1'] Namespace:pod-network-test-3485 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 18:00:14.690: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
Feb 23 18:00:14.762: INFO: Waiting for endpoints: map[]
Feb 23 18:00:14.766: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.131.136:8080/dial?request=hostName&protocol=http&host=10.200.5.19&port=8080&tries=1'] Namespace:pod-network-test-3485 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 18:00:14.766: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
Feb 23 18:00:14.836: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:00:14.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3485" for this suite.
Feb 23 18:00:26.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:00:26.939: INFO: namespace pod-network-test-3485 deletion completed in 12.101010158s

• [SLOW TEST:36.359 seconds]
[sig-network] Networking
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:00:26.942: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 23 18:00:31.021: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 18:00:31.031: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 18:00:33.033: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 18:00:33.038: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 18:00:35.033: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 18:00:35.037: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 18:00:37.031: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 18:00:37.034: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 18:00:39.031: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 18:00:39.040: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 18:00:41.034: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 18:00:41.041: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 18:00:43.032: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 18:00:43.041: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 18:00:45.031: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 18:00:45.036: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:00:45.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-689" for this suite.
Feb 23 18:00:57.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:00:57.145: INFO: namespace container-lifecycle-hook-689 deletion completed in 12.095991968s

• [SLOW TEST:30.203 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:00:57.145: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-c3553cb5-01e7-466e-ab58-3d29ef9c3b18
STEP: Creating a pod to test consume secrets
Feb 23 18:00:57.185: INFO: Waiting up to 5m0s for pod "pod-secrets-ca493935-6915-4b54-ac79-8152bd14d27a" in namespace "secrets-2765" to be "success or failure"
Feb 23 18:00:57.189: INFO: Pod "pod-secrets-ca493935-6915-4b54-ac79-8152bd14d27a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.368452ms
Feb 23 18:00:59.191: INFO: Pod "pod-secrets-ca493935-6915-4b54-ac79-8152bd14d27a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005871571s
STEP: Saw pod success
Feb 23 18:00:59.191: INFO: Pod "pod-secrets-ca493935-6915-4b54-ac79-8152bd14d27a" satisfied condition "success or failure"
Feb 23 18:00:59.194: INFO: Trying to get logs from node worker00 pod pod-secrets-ca493935-6915-4b54-ac79-8152bd14d27a container secret-volume-test: <nil>
STEP: delete the pod
Feb 23 18:00:59.211: INFO: Waiting for pod pod-secrets-ca493935-6915-4b54-ac79-8152bd14d27a to disappear
Feb 23 18:00:59.215: INFO: Pod pod-secrets-ca493935-6915-4b54-ac79-8152bd14d27a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:00:59.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2765" for this suite.
Feb 23 18:01:05.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:01:05.312: INFO: namespace secrets-2765 deletion completed in 6.094000358s

• [SLOW TEST:8.166 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:01:05.316: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Feb 23 18:01:07.907: INFO: Successfully updated pod "annotationupdate4453d545-53e6-4d8f-92fb-11e7ba561762"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:01:11.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7712" for this suite.
Feb 23 18:01:23.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:01:24.055: INFO: namespace downward-api-7712 deletion completed in 12.103376029s

• [SLOW TEST:18.739 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:01:24.055: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 18:01:24.330: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 23 18:01:26.345: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718077684, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718077684, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718077684, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718077684, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 18:01:29.359: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:01:29.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-40" for this suite.
Feb 23 18:01:41.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:01:41.515: INFO: namespace webhook-40 deletion completed in 12.105880578s
STEP: Destroying namespace "webhook-40-markers" for this suite.
Feb 23 18:01:47.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:01:47.610: INFO: namespace webhook-40-markers deletion completed in 6.095132731s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:23.571 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:01:47.627: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2946
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-2946
I0223 18:01:47.683380      26 runners.go:184] Created replication controller with name: externalname-service, namespace: services-2946, replica count: 2
I0223 18:01:50.736017      26 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 23 18:01:50.736: INFO: Creating new exec pod
Feb 23 18:01:53.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=services-2946 execpod4ffm7 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Feb 23 18:01:54.617: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 23 18:01:54.617: INFO: stdout: ""
Feb 23 18:01:54.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=services-2946 execpod4ffm7 -- /bin/sh -x -c nc -zv -t -w 2 10.32.0.248 80'
Feb 23 18:01:54.734: INFO: stderr: "+ nc -zv -t -w 2 10.32.0.248 80\nConnection to 10.32.0.248 80 port [tcp/http] succeeded!\n"
Feb 23 18:01:54.734: INFO: stdout: ""
Feb 23 18:01:54.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=services-2946 execpod4ffm7 -- /bin/sh -x -c nc -zv -t -w 2 192.168.180.100 31170'
Feb 23 18:01:54.852: INFO: stderr: "+ nc -zv -t -w 2 192.168.180.100 31170\nConnection to 192.168.180.100 31170 port [tcp/31170] succeeded!\n"
Feb 23 18:01:54.852: INFO: stdout: ""
Feb 23 18:01:54.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=services-2946 execpod4ffm7 -- /bin/sh -x -c nc -zv -t -w 2 192.168.180.101 31170'
Feb 23 18:01:54.972: INFO: stderr: "+ nc -zv -t -w 2 192.168.180.101 31170\nConnection to 192.168.180.101 31170 port [tcp/31170] succeeded!\n"
Feb 23 18:01:54.972: INFO: stdout: ""
Feb 23 18:01:54.972: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:01:55.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2946" for this suite.
Feb 23 18:02:01.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:02:01.101: INFO: namespace services-2946 deletion completed in 6.094592938s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:13.475 seconds]
[sig-network] Services
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:02:01.101: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5484.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5484.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5484.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5484.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5484.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5484.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 23 18:02:03.196: INFO: DNS probes using dns-5484/dns-test-fefd1eb1-cd3c-440c-80b7-cd4cd0f02972 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:02:03.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5484" for this suite.
Feb 23 18:02:09.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:02:09.311: INFO: namespace dns-5484 deletion completed in 6.085294657s

• [SLOW TEST:8.210 seconds]
[sig-network] DNS
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:02:09.312: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 18:02:10.018: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 23 18:02:12.033: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718077729, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718077729, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718077729, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718077729, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 18:02:15.049: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 18:02:15.053: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:02:21.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8291" for this suite.
Feb 23 18:02:27.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:02:27.278: INFO: namespace webhook-8291 deletion completed in 6.090179816s
STEP: Destroying namespace "webhook-8291-markers" for this suite.
Feb 23 18:02:33.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:02:33.367: INFO: namespace webhook-8291-markers deletion completed in 6.089362159s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:24.070 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:02:33.383: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 23 18:02:33.419: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6873 /api/v1/namespaces/watch-6873/configmaps/e2e-watch-test-configmap-a 7de0ebe8-382d-4623-bb2d-635a87c2c1c5 10446 0 2020-02-23 18:02:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 23 18:02:33.419: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6873 /api/v1/namespaces/watch-6873/configmaps/e2e-watch-test-configmap-a 7de0ebe8-382d-4623-bb2d-635a87c2c1c5 10446 0 2020-02-23 18:02:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 23 18:02:43.706: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6873 /api/v1/namespaces/watch-6873/configmaps/e2e-watch-test-configmap-a 7de0ebe8-382d-4623-bb2d-635a87c2c1c5 10470 0 2020-02-23 18:02:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 23 18:02:43.706: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6873 /api/v1/namespaces/watch-6873/configmaps/e2e-watch-test-configmap-a 7de0ebe8-382d-4623-bb2d-635a87c2c1c5 10470 0 2020-02-23 18:02:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 23 18:02:53.714: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6873 /api/v1/namespaces/watch-6873/configmaps/e2e-watch-test-configmap-a 7de0ebe8-382d-4623-bb2d-635a87c2c1c5 10491 0 2020-02-23 18:02:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 23 18:02:53.714: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6873 /api/v1/namespaces/watch-6873/configmaps/e2e-watch-test-configmap-a 7de0ebe8-382d-4623-bb2d-635a87c2c1c5 10491 0 2020-02-23 18:02:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 23 18:03:03.728: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6873 /api/v1/namespaces/watch-6873/configmaps/e2e-watch-test-configmap-a 7de0ebe8-382d-4623-bb2d-635a87c2c1c5 10512 0 2020-02-23 18:02:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 23 18:03:03.728: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6873 /api/v1/namespaces/watch-6873/configmaps/e2e-watch-test-configmap-a 7de0ebe8-382d-4623-bb2d-635a87c2c1c5 10512 0 2020-02-23 18:02:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 23 18:03:13.744: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6873 /api/v1/namespaces/watch-6873/configmaps/e2e-watch-test-configmap-b 9d62a955-1541-46dd-85dd-de507cd318f4 10533 0 2020-02-23 18:03:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 23 18:03:13.744: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6873 /api/v1/namespaces/watch-6873/configmaps/e2e-watch-test-configmap-b 9d62a955-1541-46dd-85dd-de507cd318f4 10533 0 2020-02-23 18:03:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 23 18:03:23.756: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6873 /api/v1/namespaces/watch-6873/configmaps/e2e-watch-test-configmap-b 9d62a955-1541-46dd-85dd-de507cd318f4 10555 0 2020-02-23 18:03:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 23 18:03:23.756: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6873 /api/v1/namespaces/watch-6873/configmaps/e2e-watch-test-configmap-b 9d62a955-1541-46dd-85dd-de507cd318f4 10555 0 2020-02-23 18:03:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:03:33.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6873" for this suite.
Feb 23 18:03:39.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:03:39.892: INFO: namespace watch-6873 deletion completed in 6.115722266s

• [SLOW TEST:66.510 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:03:39.893: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-89
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-89 to expose endpoints map[]
Feb 23 18:03:39.947: INFO: Get endpoints failed (6.024709ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Feb 23 18:03:40.952: INFO: successfully validated that service endpoint-test2 in namespace services-89 exposes endpoints map[] (1.010917308s elapsed)
STEP: Creating pod pod1 in namespace services-89
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-89 to expose endpoints map[pod1:[80]]
Feb 23 18:03:44.186: INFO: successfully validated that service endpoint-test2 in namespace services-89 exposes endpoints map[pod1:[80]] (3.219365051s elapsed)
STEP: Creating pod pod2 in namespace services-89
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-89 to expose endpoints map[pod1:[80] pod2:[80]]
Feb 23 18:03:46.238: INFO: successfully validated that service endpoint-test2 in namespace services-89 exposes endpoints map[pod1:[80] pod2:[80]] (2.045680485s elapsed)
STEP: Deleting pod pod1 in namespace services-89
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-89 to expose endpoints map[pod2:[80]]
Feb 23 18:03:47.264: INFO: successfully validated that service endpoint-test2 in namespace services-89 exposes endpoints map[pod2:[80]] (1.020828756s elapsed)
STEP: Deleting pod pod2 in namespace services-89
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-89 to expose endpoints map[]
Feb 23 18:03:47.284: INFO: successfully validated that service endpoint-test2 in namespace services-89 exposes endpoints map[] (14.167528ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:03:47.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-89" for this suite.
Feb 23 18:03:53.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:03:53.413: INFO: namespace services-89 deletion completed in 6.099040713s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:13.521 seconds]
[sig-network] Services
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:03:53.413: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 18:03:54.528: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 18:03:57.549: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Feb 23 18:03:57.568: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:03:57.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4318" for this suite.
Feb 23 18:04:03.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:04:03.683: INFO: namespace webhook-4318 deletion completed in 6.100475353s
STEP: Destroying namespace "webhook-4318-markers" for this suite.
Feb 23 18:04:09.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:04:09.772: INFO: namespace webhook-4318-markers deletion completed in 6.088829032s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:16.375 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:04:09.789: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:04:15.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5981" for this suite.
Feb 23 18:04:21.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:04:22.003: INFO: namespace namespaces-5981 deletion completed in 6.093537435s
STEP: Destroying namespace "nsdeletetest-3975" for this suite.
Feb 23 18:04:22.006: INFO: Namespace nsdeletetest-3975 was already deleted
STEP: Destroying namespace "nsdeletetest-2288" for this suite.
Feb 23 18:04:28.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:04:28.112: INFO: namespace nsdeletetest-2288 deletion completed in 6.105286023s

• [SLOW TEST:18.323 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:04:28.120: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Feb 23 18:04:28.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 create -f - --namespace=kubectl-1690'
Feb 23 18:04:28.379: INFO: stderr: ""
Feb 23 18:04:28.379: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 23 18:04:29.706: INFO: Selector matched 1 pods for map[app:redis]
Feb 23 18:04:29.706: INFO: Found 0 / 1
Feb 23 18:04:30.386: INFO: Selector matched 1 pods for map[app:redis]
Feb 23 18:04:30.386: INFO: Found 0 / 1
Feb 23 18:04:31.385: INFO: Selector matched 1 pods for map[app:redis]
Feb 23 18:04:31.385: INFO: Found 1 / 1
Feb 23 18:04:31.385: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 23 18:04:31.388: INFO: Selector matched 1 pods for map[app:redis]
Feb 23 18:04:31.388: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 23 18:04:31.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 patch pod redis-master-f7v4b --namespace=kubectl-1690 -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 23 18:04:31.459: INFO: stderr: ""
Feb 23 18:04:31.459: INFO: stdout: "pod/redis-master-f7v4b patched\n"
STEP: checking annotations
Feb 23 18:04:31.464: INFO: Selector matched 1 pods for map[app:redis]
Feb 23 18:04:31.464: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:04:31.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1690" for this suite.
Feb 23 18:04:59.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:04:59.567: INFO: namespace kubectl-1690 deletion completed in 28.100720434s

• [SLOW TEST:31.448 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:04:59.567: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:04:59.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8069" for this suite.
Feb 23 18:05:05.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:05:05.718: INFO: namespace services-8069 deletion completed in 6.113003663s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:6.151 seconds]
[sig-network] Services
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:05:05.718: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 23 18:05:08.282: INFO: Successfully updated pod "pod-update-8494dd33-a313-4fae-ad75-777a5b2036bb"
STEP: verifying the updated pod is in kubernetes
Feb 23 18:05:08.290: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:05:08.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-69" for this suite.
Feb 23 18:05:20.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:05:20.388: INFO: namespace pods-69 deletion completed in 12.094356831s

• [SLOW TEST:14.670 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:05:20.388: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 23 18:05:20.428: INFO: Waiting up to 5m0s for pod "pod-6aa3e065-19d2-4031-b19e-1e7f2eb447c4" in namespace "emptydir-110" to be "success or failure"
Feb 23 18:05:20.431: INFO: Pod "pod-6aa3e065-19d2-4031-b19e-1e7f2eb447c4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.906987ms
Feb 23 18:05:22.437: INFO: Pod "pod-6aa3e065-19d2-4031-b19e-1e7f2eb447c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00943273s
STEP: Saw pod success
Feb 23 18:05:22.437: INFO: Pod "pod-6aa3e065-19d2-4031-b19e-1e7f2eb447c4" satisfied condition "success or failure"
Feb 23 18:05:22.442: INFO: Trying to get logs from node worker00 pod pod-6aa3e065-19d2-4031-b19e-1e7f2eb447c4 container test-container: <nil>
STEP: delete the pod
Feb 23 18:05:22.466: INFO: Waiting for pod pod-6aa3e065-19d2-4031-b19e-1e7f2eb447c4 to disappear
Feb 23 18:05:22.468: INFO: Pod pod-6aa3e065-19d2-4031-b19e-1e7f2eb447c4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:05:22.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-110" for this suite.
Feb 23 18:05:28.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:05:28.558: INFO: namespace emptydir-110 deletion completed in 6.08638234s

• [SLOW TEST:8.171 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:05:28.563: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 18:05:29.023: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 18:05:32.049: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:05:32.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3960" for this suite.
Feb 23 18:05:38.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:05:38.192: INFO: namespace webhook-3960 deletion completed in 6.084806484s
STEP: Destroying namespace "webhook-3960-markers" for this suite.
Feb 23 18:05:44.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:05:44.282: INFO: namespace webhook-3960-markers deletion completed in 6.089792562s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:15.731 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:05:44.294: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-7b73312b-816f-4364-983e-2dd5982d0607
STEP: Creating a pod to test consume secrets
Feb 23 18:05:44.336: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-808c4705-05f4-4772-aab3-8a42236f0175" in namespace "projected-3045" to be "success or failure"
Feb 23 18:05:44.344: INFO: Pod "pod-projected-secrets-808c4705-05f4-4772-aab3-8a42236f0175": Phase="Pending", Reason="", readiness=false. Elapsed: 8.304822ms
Feb 23 18:05:46.347: INFO: Pod "pod-projected-secrets-808c4705-05f4-4772-aab3-8a42236f0175": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011635013s
STEP: Saw pod success
Feb 23 18:05:46.347: INFO: Pod "pod-projected-secrets-808c4705-05f4-4772-aab3-8a42236f0175" satisfied condition "success or failure"
Feb 23 18:05:46.350: INFO: Trying to get logs from node worker00 pod pod-projected-secrets-808c4705-05f4-4772-aab3-8a42236f0175 container secret-volume-test: <nil>
STEP: delete the pod
Feb 23 18:05:46.370: INFO: Waiting for pod pod-projected-secrets-808c4705-05f4-4772-aab3-8a42236f0175 to disappear
Feb 23 18:05:46.372: INFO: Pod pod-projected-secrets-808c4705-05f4-4772-aab3-8a42236f0175 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:05:46.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3045" for this suite.
Feb 23 18:05:52.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:05:52.470: INFO: namespace projected-3045 deletion completed in 6.095767883s

• [SLOW TEST:8.176 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:05:52.471: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 23 18:05:52.511: INFO: Waiting up to 5m0s for pod "pod-c51ec0a1-d6b3-4051-a5d9-92326184ec8f" in namespace "emptydir-7775" to be "success or failure"
Feb 23 18:05:52.516: INFO: Pod "pod-c51ec0a1-d6b3-4051-a5d9-92326184ec8f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.113501ms
Feb 23 18:05:54.521: INFO: Pod "pod-c51ec0a1-d6b3-4051-a5d9-92326184ec8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009534614s
Feb 23 18:05:56.525: INFO: Pod "pod-c51ec0a1-d6b3-4051-a5d9-92326184ec8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013736604s
STEP: Saw pod success
Feb 23 18:05:56.525: INFO: Pod "pod-c51ec0a1-d6b3-4051-a5d9-92326184ec8f" satisfied condition "success or failure"
Feb 23 18:05:56.528: INFO: Trying to get logs from node worker00 pod pod-c51ec0a1-d6b3-4051-a5d9-92326184ec8f container test-container: <nil>
STEP: delete the pod
Feb 23 18:05:56.547: INFO: Waiting for pod pod-c51ec0a1-d6b3-4051-a5d9-92326184ec8f to disappear
Feb 23 18:05:56.550: INFO: Pod pod-c51ec0a1-d6b3-4051-a5d9-92326184ec8f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:05:56.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7775" for this suite.
Feb 23 18:06:02.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:06:02.638: INFO: namespace emptydir-7775 deletion completed in 6.085045072s

• [SLOW TEST:10.167 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:06:02.638: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Feb 23 18:06:03.195: INFO: created pod pod-service-account-defaultsa
Feb 23 18:06:03.195: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 23 18:06:03.210: INFO: created pod pod-service-account-mountsa
Feb 23 18:06:03.210: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 23 18:06:03.223: INFO: created pod pod-service-account-nomountsa
Feb 23 18:06:03.223: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 23 18:06:03.242: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 23 18:06:03.242: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 23 18:06:03.248: INFO: created pod pod-service-account-mountsa-mountspec
Feb 23 18:06:03.252: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 23 18:06:03.261: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 23 18:06:03.261: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 23 18:06:03.267: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 23 18:06:03.267: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 23 18:06:03.275: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 23 18:06:03.275: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 23 18:06:03.286: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 23 18:06:03.286: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:06:03.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9868" for this suite.
Feb 23 18:06:15.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:06:15.406: INFO: namespace svcaccounts-9868 deletion completed in 12.114816012s

• [SLOW TEST:12.768 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:06:15.406: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 23 18:06:15.461: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ad13b612-81c7-4051-9972-e23101ccc432" in namespace "downward-api-4650" to be "success or failure"
Feb 23 18:06:15.465: INFO: Pod "downwardapi-volume-ad13b612-81c7-4051-9972-e23101ccc432": Phase="Pending", Reason="", readiness=false. Elapsed: 4.320941ms
Feb 23 18:06:17.469: INFO: Pod "downwardapi-volume-ad13b612-81c7-4051-9972-e23101ccc432": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007588459s
STEP: Saw pod success
Feb 23 18:06:17.469: INFO: Pod "downwardapi-volume-ad13b612-81c7-4051-9972-e23101ccc432" satisfied condition "success or failure"
Feb 23 18:06:17.472: INFO: Trying to get logs from node worker00 pod downwardapi-volume-ad13b612-81c7-4051-9972-e23101ccc432 container client-container: <nil>
STEP: delete the pod
Feb 23 18:06:17.490: INFO: Waiting for pod downwardapi-volume-ad13b612-81c7-4051-9972-e23101ccc432 to disappear
Feb 23 18:06:17.492: INFO: Pod downwardapi-volume-ad13b612-81c7-4051-9972-e23101ccc432 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:06:17.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4650" for this suite.
Feb 23 18:06:23.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:06:23.587: INFO: namespace downward-api-4650 deletion completed in 6.090213643s

• [SLOW TEST:8.181 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:06:23.587: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-3503690a-b65c-4824-8056-49359bbad518
STEP: Creating a pod to test consume configMaps
Feb 23 18:06:23.628: INFO: Waiting up to 5m0s for pod "pod-configmaps-d6709418-b435-4ae9-ae8f-66b572a6a673" in namespace "configmap-1619" to be "success or failure"
Feb 23 18:06:23.631: INFO: Pod "pod-configmaps-d6709418-b435-4ae9-ae8f-66b572a6a673": Phase="Pending", Reason="", readiness=false. Elapsed: 2.597837ms
Feb 23 18:06:25.633: INFO: Pod "pod-configmaps-d6709418-b435-4ae9-ae8f-66b572a6a673": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00531358s
STEP: Saw pod success
Feb 23 18:06:25.633: INFO: Pod "pod-configmaps-d6709418-b435-4ae9-ae8f-66b572a6a673" satisfied condition "success or failure"
Feb 23 18:06:25.636: INFO: Trying to get logs from node worker00 pod pod-configmaps-d6709418-b435-4ae9-ae8f-66b572a6a673 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 18:06:25.651: INFO: Waiting for pod pod-configmaps-d6709418-b435-4ae9-ae8f-66b572a6a673 to disappear
Feb 23 18:06:25.656: INFO: Pod pod-configmaps-d6709418-b435-4ae9-ae8f-66b572a6a673 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:06:25.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1619" for this suite.
Feb 23 18:06:31.674: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:06:31.759: INFO: namespace configmap-1619 deletion completed in 6.100235014s

• [SLOW TEST:8.172 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:06:31.759: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 23 18:06:36.080: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 23 18:06:36.087: INFO: Pod pod-with-prestop-http-hook still exists
Feb 23 18:06:38.087: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 23 18:06:38.090: INFO: Pod pod-with-prestop-http-hook still exists
Feb 23 18:06:40.087: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 23 18:06:40.090: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:06:40.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2989" for this suite.
Feb 23 18:07:06.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:07:06.187: INFO: namespace container-lifecycle-hook-2989 deletion completed in 26.089058815s

• [SLOW TEST:34.428 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:07:06.187: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 18:07:06.227: INFO: (0) /api/v1/nodes/worker00:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.152424ms)
Feb 23 18:07:06.238: INFO: (1) /api/v1/nodes/worker00:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.120453ms)
Feb 23 18:07:06.242: INFO: (2) /api/v1/nodes/worker00:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.936737ms)
Feb 23 18:07:06.248: INFO: (3) /api/v1/nodes/worker00:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.748136ms)
Feb 23 18:07:06.757: INFO: (4) /api/v1/nodes/worker00:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 509.184545ms)
Feb 23 18:07:06.765: INFO: (5) /api/v1/nodes/worker00:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.807862ms)
Feb 23 18:07:06.768: INFO: (6) /api/v1/nodes/worker00:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.472537ms)
Feb 23 18:07:06.774: INFO: (7) /api/v1/nodes/worker00:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.267926ms)
Feb 23 18:07:06.779: INFO: (8) /api/v1/nodes/worker00:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.69122ms)
Feb 23 18:07:06.784: INFO: (9) /api/v1/nodes/worker00:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.570916ms)
Feb 23 18:07:06.787: INFO: (10) /api/v1/nodes/worker00:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.346446ms)
Feb 23 18:07:06.791: INFO: (11) /api/v1/nodes/worker00:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.055058ms)
Feb 23 18:07:06.794: INFO: (12) /api/v1/nodes/worker00:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.849703ms)
Feb 23 18:07:06.799: INFO: (13) /api/v1/nodes/worker00:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.304061ms)
Feb 23 18:07:06.802: INFO: (14) /api/v1/nodes/worker00:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.830096ms)
Feb 23 18:07:06.806: INFO: (15) /api/v1/nodes/worker00:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.941287ms)
Feb 23 18:07:06.810: INFO: (16) /api/v1/nodes/worker00:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.542341ms)
Feb 23 18:07:06.814: INFO: (17) /api/v1/nodes/worker00:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.480344ms)
Feb 23 18:07:06.817: INFO: (18) /api/v1/nodes/worker00:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.140534ms)
Feb 23 18:07:06.820: INFO: (19) /api/v1/nodes/worker00:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.523059ms)
[AfterEach] version v1
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:07:06.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8372" for this suite.
Feb 23 18:07:12.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:07:12.927: INFO: namespace proxy-8372 deletion completed in 6.102873755s

• [SLOW TEST:6.740 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:07:12.929: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-eb3e62ba-dae6-4dc0-baf5-3d3304d031fc
STEP: Creating a pod to test consume configMaps
Feb 23 18:07:12.967: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-374b285d-3237-4254-adc4-097e7d936ecc" in namespace "projected-546" to be "success or failure"
Feb 23 18:07:12.975: INFO: Pod "pod-projected-configmaps-374b285d-3237-4254-adc4-097e7d936ecc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.108174ms
Feb 23 18:07:14.981: INFO: Pod "pod-projected-configmaps-374b285d-3237-4254-adc4-097e7d936ecc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013603316s
STEP: Saw pod success
Feb 23 18:07:14.981: INFO: Pod "pod-projected-configmaps-374b285d-3237-4254-adc4-097e7d936ecc" satisfied condition "success or failure"
Feb 23 18:07:14.985: INFO: Trying to get logs from node worker00 pod pod-projected-configmaps-374b285d-3237-4254-adc4-097e7d936ecc container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 18:07:15.008: INFO: Waiting for pod pod-projected-configmaps-374b285d-3237-4254-adc4-097e7d936ecc to disappear
Feb 23 18:07:15.011: INFO: Pod pod-projected-configmaps-374b285d-3237-4254-adc4-097e7d936ecc no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:07:15.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-546" for this suite.
Feb 23 18:07:21.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:07:21.112: INFO: namespace projected-546 deletion completed in 6.098503863s

• [SLOW TEST:8.183 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:07:21.112: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Feb 23 18:07:21.157: INFO: Waiting up to 5m0s for pod "downward-api-5e007634-c828-44db-b52c-b481020c63a7" in namespace "downward-api-5563" to be "success or failure"
Feb 23 18:07:21.160: INFO: Pod "downward-api-5e007634-c828-44db-b52c-b481020c63a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.646234ms
Feb 23 18:07:23.163: INFO: Pod "downward-api-5e007634-c828-44db-b52c-b481020c63a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005730556s
STEP: Saw pod success
Feb 23 18:07:23.163: INFO: Pod "downward-api-5e007634-c828-44db-b52c-b481020c63a7" satisfied condition "success or failure"
Feb 23 18:07:23.166: INFO: Trying to get logs from node worker00 pod downward-api-5e007634-c828-44db-b52c-b481020c63a7 container dapi-container: <nil>
STEP: delete the pod
Feb 23 18:07:23.189: INFO: Waiting for pod downward-api-5e007634-c828-44db-b52c-b481020c63a7 to disappear
Feb 23 18:07:23.194: INFO: Pod downward-api-5e007634-c828-44db-b52c-b481020c63a7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:07:23.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5563" for this suite.
Feb 23 18:07:29.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:07:29.292: INFO: namespace downward-api-5563 deletion completed in 6.093599136s

• [SLOW TEST:8.180 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:07:29.292: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Feb 23 18:07:29.331: INFO: Waiting up to 5m0s for pod "var-expansion-785979f8-4e3c-4d07-aebd-d57ad0bff61d" in namespace "var-expansion-8037" to be "success or failure"
Feb 23 18:07:29.335: INFO: Pod "var-expansion-785979f8-4e3c-4d07-aebd-d57ad0bff61d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.341209ms
Feb 23 18:07:31.338: INFO: Pod "var-expansion-785979f8-4e3c-4d07-aebd-d57ad0bff61d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006196223s
STEP: Saw pod success
Feb 23 18:07:31.338: INFO: Pod "var-expansion-785979f8-4e3c-4d07-aebd-d57ad0bff61d" satisfied condition "success or failure"
Feb 23 18:07:31.358: INFO: Trying to get logs from node worker00 pod var-expansion-785979f8-4e3c-4d07-aebd-d57ad0bff61d container dapi-container: <nil>
STEP: delete the pod
Feb 23 18:07:31.408: INFO: Waiting for pod var-expansion-785979f8-4e3c-4d07-aebd-d57ad0bff61d to disappear
Feb 23 18:07:31.413: INFO: Pod var-expansion-785979f8-4e3c-4d07-aebd-d57ad0bff61d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:07:31.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8037" for this suite.
Feb 23 18:07:37.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:07:37.542: INFO: namespace var-expansion-8037 deletion completed in 6.124882749s

• [SLOW TEST:8.250 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:07:37.543: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 23 18:07:37.593: INFO: Waiting up to 5m0s for pod "pod-de64b94f-5899-482c-9d18-189feee23f63" in namespace "emptydir-5443" to be "success or failure"
Feb 23 18:07:37.604: INFO: Pod "pod-de64b94f-5899-482c-9d18-189feee23f63": Phase="Pending", Reason="", readiness=false. Elapsed: 10.445601ms
Feb 23 18:07:39.613: INFO: Pod "pod-de64b94f-5899-482c-9d18-189feee23f63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019436425s
STEP: Saw pod success
Feb 23 18:07:39.613: INFO: Pod "pod-de64b94f-5899-482c-9d18-189feee23f63" satisfied condition "success or failure"
Feb 23 18:07:39.617: INFO: Trying to get logs from node worker00 pod pod-de64b94f-5899-482c-9d18-189feee23f63 container test-container: <nil>
STEP: delete the pod
Feb 23 18:07:39.639: INFO: Waiting for pod pod-de64b94f-5899-482c-9d18-189feee23f63 to disappear
Feb 23 18:07:39.641: INFO: Pod pod-de64b94f-5899-482c-9d18-189feee23f63 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:07:39.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5443" for this suite.
Feb 23 18:07:45.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:07:45.734: INFO: namespace emptydir-5443 deletion completed in 6.089523453s

• [SLOW TEST:8.191 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:07:45.734: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-e5be75a1-bc3c-4cc5-b123-0648b68bf819
STEP: Creating a pod to test consume secrets
Feb 23 18:07:45.777: INFO: Waiting up to 5m0s for pod "pod-secrets-ba2d2ffd-f5ab-4f56-bc43-4e49cd3fe197" in namespace "secrets-6838" to be "success or failure"
Feb 23 18:07:45.783: INFO: Pod "pod-secrets-ba2d2ffd-f5ab-4f56-bc43-4e49cd3fe197": Phase="Pending", Reason="", readiness=false. Elapsed: 5.914809ms
Feb 23 18:07:47.786: INFO: Pod "pod-secrets-ba2d2ffd-f5ab-4f56-bc43-4e49cd3fe197": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00896791s
STEP: Saw pod success
Feb 23 18:07:47.786: INFO: Pod "pod-secrets-ba2d2ffd-f5ab-4f56-bc43-4e49cd3fe197" satisfied condition "success or failure"
Feb 23 18:07:47.792: INFO: Trying to get logs from node worker00 pod pod-secrets-ba2d2ffd-f5ab-4f56-bc43-4e49cd3fe197 container secret-volume-test: <nil>
STEP: delete the pod
Feb 23 18:07:47.814: INFO: Waiting for pod pod-secrets-ba2d2ffd-f5ab-4f56-bc43-4e49cd3fe197 to disappear
Feb 23 18:07:47.817: INFO: Pod pod-secrets-ba2d2ffd-f5ab-4f56-bc43-4e49cd3fe197 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:07:47.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6838" for this suite.
Feb 23 18:07:53.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:07:53.922: INFO: namespace secrets-6838 deletion completed in 6.100931008s

• [SLOW TEST:8.188 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:07:53.922: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 18:07:53.949: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 23 18:08:01.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 --namespace=crd-publish-openapi-4056 create -f -'
Feb 23 18:08:02.651: INFO: stderr: ""
Feb 23 18:08:02.651: INFO: stdout: "e2e-test-crd-publish-openapi-6554-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 23 18:08:02.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 --namespace=crd-publish-openapi-4056 delete e2e-test-crd-publish-openapi-6554-crds test-cr'
Feb 23 18:08:02.708: INFO: stderr: ""
Feb 23 18:08:02.708: INFO: stdout: "e2e-test-crd-publish-openapi-6554-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Feb 23 18:08:02.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 --namespace=crd-publish-openapi-4056 apply -f -'
Feb 23 18:08:02.925: INFO: stderr: ""
Feb 23 18:08:02.925: INFO: stdout: "e2e-test-crd-publish-openapi-6554-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 23 18:08:02.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 --namespace=crd-publish-openapi-4056 delete e2e-test-crd-publish-openapi-6554-crds test-cr'
Feb 23 18:08:02.982: INFO: stderr: ""
Feb 23 18:08:02.983: INFO: stdout: "e2e-test-crd-publish-openapi-6554-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Feb 23 18:08:02.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 explain e2e-test-crd-publish-openapi-6554-crds'
Feb 23 18:08:03.154: INFO: stderr: ""
Feb 23 18:08:03.154: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6554-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:08:08.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4056" for this suite.
Feb 23 18:08:14.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:08:14.570: INFO: namespace crd-publish-openapi-4056 deletion completed in 6.089638957s

• [SLOW TEST:20.648 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:08:14.574: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-530c8929-7ae0-4635-9f48-b0c9c60eccb0
STEP: Creating a pod to test consume configMaps
Feb 23 18:08:14.616: INFO: Waiting up to 5m0s for pod "pod-configmaps-8f482a1a-9ac1-404a-9b98-a5b7e83b5c74" in namespace "configmap-6129" to be "success or failure"
Feb 23 18:08:14.620: INFO: Pod "pod-configmaps-8f482a1a-9ac1-404a-9b98-a5b7e83b5c74": Phase="Pending", Reason="", readiness=false. Elapsed: 3.994876ms
Feb 23 18:08:16.623: INFO: Pod "pod-configmaps-8f482a1a-9ac1-404a-9b98-a5b7e83b5c74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006641811s
STEP: Saw pod success
Feb 23 18:08:16.623: INFO: Pod "pod-configmaps-8f482a1a-9ac1-404a-9b98-a5b7e83b5c74" satisfied condition "success or failure"
Feb 23 18:08:16.626: INFO: Trying to get logs from node worker00 pod pod-configmaps-8f482a1a-9ac1-404a-9b98-a5b7e83b5c74 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 18:08:16.643: INFO: Waiting for pod pod-configmaps-8f482a1a-9ac1-404a-9b98-a5b7e83b5c74 to disappear
Feb 23 18:08:16.648: INFO: Pod pod-configmaps-8f482a1a-9ac1-404a-9b98-a5b7e83b5c74 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:08:16.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6129" for this suite.
Feb 23 18:08:22.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:08:22.755: INFO: namespace configmap-6129 deletion completed in 6.102441474s

• [SLOW TEST:8.181 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:08:22.755: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 18:08:23.105: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 23 18:08:25.121: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078103, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078103, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078103, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078103, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 18:08:28.134: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:08:28.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8615" for this suite.
Feb 23 18:08:34.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:08:34.291: INFO: namespace webhook-8615 deletion completed in 6.090148687s
STEP: Destroying namespace "webhook-8615-markers" for this suite.
Feb 23 18:08:40.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:08:40.390: INFO: namespace webhook-8615-markers deletion completed in 6.098572528s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.647 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:08:40.403: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-66c18ec9-820e-4be3-95df-38b4af5ef193
STEP: Creating a pod to test consume secrets
Feb 23 18:08:40.446: INFO: Waiting up to 5m0s for pod "pod-secrets-09406b73-906b-4d13-9bb8-0d62813575e8" in namespace "secrets-6172" to be "success or failure"
Feb 23 18:08:40.456: INFO: Pod "pod-secrets-09406b73-906b-4d13-9bb8-0d62813575e8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.457609ms
Feb 23 18:08:42.462: INFO: Pod "pod-secrets-09406b73-906b-4d13-9bb8-0d62813575e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015988186s
STEP: Saw pod success
Feb 23 18:08:42.462: INFO: Pod "pod-secrets-09406b73-906b-4d13-9bb8-0d62813575e8" satisfied condition "success or failure"
Feb 23 18:08:42.466: INFO: Trying to get logs from node worker00 pod pod-secrets-09406b73-906b-4d13-9bb8-0d62813575e8 container secret-volume-test: <nil>
STEP: delete the pod
Feb 23 18:08:42.484: INFO: Waiting for pod pod-secrets-09406b73-906b-4d13-9bb8-0d62813575e8 to disappear
Feb 23 18:08:42.488: INFO: Pod pod-secrets-09406b73-906b-4d13-9bb8-0d62813575e8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:08:42.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6172" for this suite.
Feb 23 18:08:48.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:08:48.605: INFO: namespace secrets-6172 deletion completed in 6.115085053s

• [SLOW TEST:8.204 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:08:48.606: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-181bd121-2d1a-4db3-961a-bd1063a41882 in namespace container-probe-2491
Feb 23 18:08:50.655: INFO: Started pod test-webserver-181bd121-2d1a-4db3-961a-bd1063a41882 in namespace container-probe-2491
STEP: checking the pod's current state and verifying that restartCount is present
Feb 23 18:08:50.661: INFO: Initial restart count of pod test-webserver-181bd121-2d1a-4db3-961a-bd1063a41882 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:12:52.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2491" for this suite.
Feb 23 18:12:58.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:12:58.700: INFO: namespace container-probe-2491 deletion completed in 6.07979313s

• [SLOW TEST:250.094 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:12:58.700: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 18:12:59.230: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 23 18:13:01.237: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078379, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078379, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078379, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078379, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 18:13:04.747: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Feb 23 18:13:06.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 attach --namespace=webhook-2879 to-be-attached-pod -i -c=container1'
Feb 23 18:13:06.847: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:13:06.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2879" for this suite.
Feb 23 18:13:18.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:13:18.935: INFO: namespace webhook-2879 deletion completed in 12.078056917s
STEP: Destroying namespace "webhook-2879-markers" for this suite.
Feb 23 18:13:24.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:13:25.011: INFO: namespace webhook-2879-markers deletion completed in 6.076075403s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.324 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:13:25.028: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 18:13:25.056: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:13:30.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6616" for this suite.
Feb 23 18:13:36.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:13:36.174: INFO: namespace custom-resource-definition-6616 deletion completed in 6.073083629s

• [SLOW TEST:11.146 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:13:36.174: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-f558f967-e2c9-425c-9797-63632feb70b4
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:13:38.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4974" for this suite.
Feb 23 18:13:56.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:13:56.334: INFO: namespace configmap-4974 deletion completed in 18.08137024s

• [SLOW TEST:20.161 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:13:56.339: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 18:13:56.363: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:14:02.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6203" for this suite.
Feb 23 18:14:08.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:14:08.477: INFO: namespace custom-resource-definition-6203 deletion completed in 6.077369049s

• [SLOW TEST:12.138 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:14:08.477: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-bad6343b-9ad0-4188-8042-bdab47174e58 in namespace container-probe-1279
Feb 23 18:14:12.542: INFO: Started pod busybox-bad6343b-9ad0-4188-8042-bdab47174e58 in namespace container-probe-1279
STEP: checking the pod's current state and verifying that restartCount is present
Feb 23 18:14:12.545: INFO: Initial restart count of pod busybox-bad6343b-9ad0-4188-8042-bdab47174e58 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:18:13.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1279" for this suite.
Feb 23 18:18:19.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:18:19.887: INFO: namespace container-probe-1279 deletion completed in 6.082264704s

• [SLOW TEST:251.410 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:18:19.887: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-5b72efc5-8e99-4cd1-9843-4e81e852c86b in namespace container-probe-3471
Feb 23 18:18:21.930: INFO: Started pod liveness-5b72efc5-8e99-4cd1-9843-4e81e852c86b in namespace container-probe-3471
STEP: checking the pod's current state and verifying that restartCount is present
Feb 23 18:18:21.932: INFO: Initial restart count of pod liveness-5b72efc5-8e99-4cd1-9843-4e81e852c86b is 0
Feb 23 18:18:41.968: INFO: Restart count of pod container-probe-3471/liveness-5b72efc5-8e99-4cd1-9843-4e81e852c86b is now 1 (20.036211388s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:18:41.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3471" for this suite.
Feb 23 18:18:48.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:18:48.073: INFO: namespace container-probe-3471 deletion completed in 6.078195594s

• [SLOW TEST:28.186 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:18:48.074: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-3847ddcb-ca37-4cea-b9ec-609825f7933d
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:18:48.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6235" for this suite.
Feb 23 18:18:54.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:18:54.185: INFO: namespace configmap-6235 deletion completed in 6.081475549s

• [SLOW TEST:6.111 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:18:54.185: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Feb 23 18:18:54.219: INFO: Waiting up to 5m0s for pod "client-containers-17543328-d468-4702-98c5-745b6eb33fc5" in namespace "containers-5871" to be "success or failure"
Feb 23 18:18:54.224: INFO: Pod "client-containers-17543328-d468-4702-98c5-745b6eb33fc5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.995806ms
Feb 23 18:18:56.228: INFO: Pod "client-containers-17543328-d468-4702-98c5-745b6eb33fc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008195547s
STEP: Saw pod success
Feb 23 18:18:56.228: INFO: Pod "client-containers-17543328-d468-4702-98c5-745b6eb33fc5" satisfied condition "success or failure"
Feb 23 18:18:56.230: INFO: Trying to get logs from node worker00 pod client-containers-17543328-d468-4702-98c5-745b6eb33fc5 container test-container: <nil>
STEP: delete the pod
Feb 23 18:18:56.262: INFO: Waiting for pod client-containers-17543328-d468-4702-98c5-745b6eb33fc5 to disappear
Feb 23 18:18:56.265: INFO: Pod client-containers-17543328-d468-4702-98c5-745b6eb33fc5 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:18:56.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5871" for this suite.
Feb 23 18:19:02.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:19:02.342: INFO: namespace containers-5871 deletion completed in 6.07386591s

• [SLOW TEST:8.157 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:19:02.343: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 18:19:02.381: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 23 18:19:07.386: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 23 18:19:07.386: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 23 18:19:09.391: INFO: Creating deployment "test-rollover-deployment"
Feb 23 18:19:09.397: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 23 18:19:11.403: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 23 18:19:11.409: INFO: Ensure that both replica sets have 1 created replica
Feb 23 18:19:11.413: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 23 18:19:11.419: INFO: Updating deployment test-rollover-deployment
Feb 23 18:19:11.419: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 23 18:19:13.426: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 23 18:19:13.431: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 23 18:19:13.436: INFO: all replica sets need to contain the pod-template-hash label
Feb 23 18:19:13.436: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078749, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078749, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078751, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078749, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 18:19:15.443: INFO: all replica sets need to contain the pod-template-hash label
Feb 23 18:19:15.444: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078749, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078749, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078753, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078749, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 18:19:17.441: INFO: all replica sets need to contain the pod-template-hash label
Feb 23 18:19:17.441: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078749, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078749, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078753, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078749, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 18:19:19.441: INFO: all replica sets need to contain the pod-template-hash label
Feb 23 18:19:19.441: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078749, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078749, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078753, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078749, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 18:19:21.441: INFO: all replica sets need to contain the pod-template-hash label
Feb 23 18:19:21.441: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078749, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078749, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078753, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078749, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 18:19:23.440: INFO: all replica sets need to contain the pod-template-hash label
Feb 23 18:19:23.440: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078749, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078749, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078753, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718078749, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 18:19:25.441: INFO: 
Feb 23 18:19:25.441: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Feb 23 18:19:25.451: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-1744 /apis/apps/v1/namespaces/deployment-1744/deployments/test-rollover-deployment 5e156b4c-0f34-485c-9d94-5d2f18345682 13932 2 2020-02-23 18:19:09 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00563f6d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-02-23 18:19:09 +0000 UTC,LastTransitionTime:2020-02-23 18:19:09 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2020-02-23 18:19:24 +0000 UTC,LastTransitionTime:2020-02-23 18:19:09 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 23 18:19:25.454: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-1744 /apis/apps/v1/namespaces/deployment-1744/replicasets/test-rollover-deployment-7d7dc6548c 2a6e7b73-d45a-48bb-80e6-0a197b2aa6c9 13921 2 2020-02-23 18:19:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 5e156b4c-0f34-485c-9d94-5d2f18345682 0xc00563fb87 0xc00563fb88}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00563fbe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 23 18:19:25.455: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 23 18:19:25.455: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1744 /apis/apps/v1/namespaces/deployment-1744/replicasets/test-rollover-controller 1a66b121-69c8-4179-8553-2c8c524dbc10 13931 2 2020-02-23 18:19:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 5e156b4c-0f34-485c-9d94-5d2f18345682 0xc00563fab7 0xc00563fab8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00563fb18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 23 18:19:25.455: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-1744 /apis/apps/v1/namespaces/deployment-1744/replicasets/test-rollover-deployment-f6c94f66c 6975f307-29e6-46eb-a4f3-f9b84d355075 13877 2 2020-02-23 18:19:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 5e156b4c-0f34-485c-9d94-5d2f18345682 0xc00563fc50 0xc00563fc51}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00563fcc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 23 18:19:25.459: INFO: Pod "test-rollover-deployment-7d7dc6548c-vrkdd" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-vrkdd test-rollover-deployment-7d7dc6548c- deployment-1744 /api/v1/namespaces/deployment-1744/pods/test-rollover-deployment-7d7dc6548c-vrkdd e12d21b4-c235-4246-8182-7cae9ce8e517 13898 0 2020-02-23 18:19:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[cni.projectcalico.org/podIP:10.200.131.186/32] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c 2a6e7b73-d45a-48bb-80e6-0a197b2aa6c9 0xc005ba4217 0xc005ba4218}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v5k48,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v5k48,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v5k48,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:19:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:19:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:10.200.131.186,StartTime:2020-02-23 18:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 18:19:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:containerd://0fac36558a7dbefb280b65c6dae1355e9e1b0a24590ba49b85347030bd5e2d92,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.131.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:19:25.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1744" for this suite.
Feb 23 18:19:31.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:19:31.551: INFO: namespace deployment-1744 deletion completed in 6.087705999s

• [SLOW TEST:29.209 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:19:31.556: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:19:44.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5902" for this suite.
Feb 23 18:19:50.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:19:50.722: INFO: namespace resourcequota-5902 deletion completed in 6.086678984s

• [SLOW TEST:19.166 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:19:50.724: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 18:19:50.749: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Feb 23 18:19:52.780: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:19:52.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6623" for this suite.
Feb 23 18:19:58.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:19:58.887: INFO: namespace replication-controller-6623 deletion completed in 6.100979027s

• [SLOW TEST:8.163 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:19:58.887: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-c63cc0cb-6b5c-4bec-b4c6-e167339c697d
STEP: Creating a pod to test consume configMaps
Feb 23 18:19:58.945: INFO: Waiting up to 5m0s for pod "pod-configmaps-9401beb0-d1c7-4a5b-8636-5fdfebd41bf6" in namespace "configmap-3050" to be "success or failure"
Feb 23 18:19:58.952: INFO: Pod "pod-configmaps-9401beb0-d1c7-4a5b-8636-5fdfebd41bf6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.547953ms
Feb 23 18:20:00.956: INFO: Pod "pod-configmaps-9401beb0-d1c7-4a5b-8636-5fdfebd41bf6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011626302s
STEP: Saw pod success
Feb 23 18:20:00.956: INFO: Pod "pod-configmaps-9401beb0-d1c7-4a5b-8636-5fdfebd41bf6" satisfied condition "success or failure"
Feb 23 18:20:00.959: INFO: Trying to get logs from node worker00 pod pod-configmaps-9401beb0-d1c7-4a5b-8636-5fdfebd41bf6 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 18:20:00.973: INFO: Waiting for pod pod-configmaps-9401beb0-d1c7-4a5b-8636-5fdfebd41bf6 to disappear
Feb 23 18:20:00.976: INFO: Pod pod-configmaps-9401beb0-d1c7-4a5b-8636-5fdfebd41bf6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:20:00.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3050" for this suite.
Feb 23 18:20:06.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:20:07.069: INFO: namespace configmap-3050 deletion completed in 6.09064372s

• [SLOW TEST:8.182 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:20:07.071: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-5b727df5-43fc-46e9-a54e-dd03f4b7febf
STEP: Creating a pod to test consume secrets
Feb 23 18:20:07.106: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-17967ac3-2ee8-45b4-91cf-49567b3d782b" in namespace "projected-3553" to be "success or failure"
Feb 23 18:20:07.109: INFO: Pod "pod-projected-secrets-17967ac3-2ee8-45b4-91cf-49567b3d782b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.951405ms
Feb 23 18:20:09.118: INFO: Pod "pod-projected-secrets-17967ac3-2ee8-45b4-91cf-49567b3d782b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011301248s
STEP: Saw pod success
Feb 23 18:20:09.118: INFO: Pod "pod-projected-secrets-17967ac3-2ee8-45b4-91cf-49567b3d782b" satisfied condition "success or failure"
Feb 23 18:20:09.123: INFO: Trying to get logs from node worker00 pod pod-projected-secrets-17967ac3-2ee8-45b4-91cf-49567b3d782b container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 23 18:20:09.144: INFO: Waiting for pod pod-projected-secrets-17967ac3-2ee8-45b4-91cf-49567b3d782b to disappear
Feb 23 18:20:09.147: INFO: Pod pod-projected-secrets-17967ac3-2ee8-45b4-91cf-49567b3d782b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:20:09.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3553" for this suite.
Feb 23 18:20:15.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:20:15.229: INFO: namespace projected-3553 deletion completed in 6.079682905s

• [SLOW TEST:8.159 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:20:15.230: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-862e9b61-520a-4667-ab5d-6ddbb38ff6f4
STEP: Creating a pod to test consume configMaps
Feb 23 18:20:15.270: INFO: Waiting up to 5m0s for pod "pod-configmaps-13efea0f-721f-42c7-b1ef-34d9bfea4c6e" in namespace "configmap-7994" to be "success or failure"
Feb 23 18:20:15.272: INFO: Pod "pod-configmaps-13efea0f-721f-42c7-b1ef-34d9bfea4c6e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.285632ms
Feb 23 18:20:17.276: INFO: Pod "pod-configmaps-13efea0f-721f-42c7-b1ef-34d9bfea4c6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005806693s
STEP: Saw pod success
Feb 23 18:20:17.276: INFO: Pod "pod-configmaps-13efea0f-721f-42c7-b1ef-34d9bfea4c6e" satisfied condition "success or failure"
Feb 23 18:20:17.278: INFO: Trying to get logs from node worker00 pod pod-configmaps-13efea0f-721f-42c7-b1ef-34d9bfea4c6e container configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 18:20:17.297: INFO: Waiting for pod pod-configmaps-13efea0f-721f-42c7-b1ef-34d9bfea4c6e to disappear
Feb 23 18:20:17.300: INFO: Pod pod-configmaps-13efea0f-721f-42c7-b1ef-34d9bfea4c6e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:20:17.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7994" for this suite.
Feb 23 18:20:23.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:20:23.376: INFO: namespace configmap-7994 deletion completed in 6.073436199s

• [SLOW TEST:8.146 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:20:23.376: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 23 18:20:23.889: INFO: Waiting up to 5m0s for pod "pod-e14793a2-6e69-4fbe-9710-9e65c9014042" in namespace "emptydir-4019" to be "success or failure"
Feb 23 18:20:23.895: INFO: Pod "pod-e14793a2-6e69-4fbe-9710-9e65c9014042": Phase="Pending", Reason="", readiness=false. Elapsed: 6.175303ms
Feb 23 18:20:25.898: INFO: Pod "pod-e14793a2-6e69-4fbe-9710-9e65c9014042": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008756582s
STEP: Saw pod success
Feb 23 18:20:25.898: INFO: Pod "pod-e14793a2-6e69-4fbe-9710-9e65c9014042" satisfied condition "success or failure"
Feb 23 18:20:25.900: INFO: Trying to get logs from node worker00 pod pod-e14793a2-6e69-4fbe-9710-9e65c9014042 container test-container: <nil>
STEP: delete the pod
Feb 23 18:20:25.914: INFO: Waiting for pod pod-e14793a2-6e69-4fbe-9710-9e65c9014042 to disappear
Feb 23 18:20:25.919: INFO: Pod pod-e14793a2-6e69-4fbe-9710-9e65c9014042 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:20:25.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4019" for this suite.
Feb 23 18:20:31.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:20:32.001: INFO: namespace emptydir-4019 deletion completed in 6.078978461s

• [SLOW TEST:8.626 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:20:32.005: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:20:39.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1019" for this suite.
Feb 23 18:20:45.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:20:45.128: INFO: namespace resourcequota-1019 deletion completed in 6.077675695s

• [SLOW TEST:13.123 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:20:45.128: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-c49bb4c6-1bbe-4c0e-9be7-d023702f4330
STEP: Creating a pod to test consume configMaps
Feb 23 18:20:45.167: INFO: Waiting up to 5m0s for pod "pod-configmaps-073c96ac-1cc8-4419-90b4-0e3f05c982e0" in namespace "configmap-5544" to be "success or failure"
Feb 23 18:20:45.169: INFO: Pod "pod-configmaps-073c96ac-1cc8-4419-90b4-0e3f05c982e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.176488ms
Feb 23 18:20:47.172: INFO: Pod "pod-configmaps-073c96ac-1cc8-4419-90b4-0e3f05c982e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005249565s
STEP: Saw pod success
Feb 23 18:20:47.172: INFO: Pod "pod-configmaps-073c96ac-1cc8-4419-90b4-0e3f05c982e0" satisfied condition "success or failure"
Feb 23 18:20:47.175: INFO: Trying to get logs from node worker00 pod pod-configmaps-073c96ac-1cc8-4419-90b4-0e3f05c982e0 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 18:20:47.189: INFO: Waiting for pod pod-configmaps-073c96ac-1cc8-4419-90b4-0e3f05c982e0 to disappear
Feb 23 18:20:47.192: INFO: Pod pod-configmaps-073c96ac-1cc8-4419-90b4-0e3f05c982e0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:20:47.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5544" for this suite.
Feb 23 18:20:53.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:20:53.290: INFO: namespace configmap-5544 deletion completed in 6.0955333s

• [SLOW TEST:8.162 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:20:53.290: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 23 18:20:53.326: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bc7493d9-5243-47f9-b091-4e9d131b1aa6" in namespace "projected-786" to be "success or failure"
Feb 23 18:20:53.330: INFO: Pod "downwardapi-volume-bc7493d9-5243-47f9-b091-4e9d131b1aa6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.985353ms
Feb 23 18:20:55.333: INFO: Pod "downwardapi-volume-bc7493d9-5243-47f9-b091-4e9d131b1aa6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007067207s
STEP: Saw pod success
Feb 23 18:20:55.333: INFO: Pod "downwardapi-volume-bc7493d9-5243-47f9-b091-4e9d131b1aa6" satisfied condition "success or failure"
Feb 23 18:20:55.335: INFO: Trying to get logs from node worker00 pod downwardapi-volume-bc7493d9-5243-47f9-b091-4e9d131b1aa6 container client-container: <nil>
STEP: delete the pod
Feb 23 18:20:55.353: INFO: Waiting for pod downwardapi-volume-bc7493d9-5243-47f9-b091-4e9d131b1aa6 to disappear
Feb 23 18:20:55.355: INFO: Pod downwardapi-volume-bc7493d9-5243-47f9-b091-4e9d131b1aa6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:20:55.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-786" for this suite.
Feb 23 18:21:01.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:21:01.441: INFO: namespace projected-786 deletion completed in 6.083006701s

• [SLOW TEST:8.151 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:21:01.443: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1681.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-1681.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1681.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-1681.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1681.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1681.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-1681.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1681.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-1681.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1681.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 23 18:21:03.504: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:03.507: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:03.510: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:03.514: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:03.524: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:03.528: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:03.531: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:03.535: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:03.541: INFO: Lookups using dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1681.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1681.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local jessie_udp@dns-test-service-2.dns-1681.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1681.svc.cluster.local]

Feb 23 18:21:08.547: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:08.550: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:08.553: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:08.556: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:08.564: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:08.566: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:08.571: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:08.574: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:08.579: INFO: Lookups using dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1681.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1681.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local jessie_udp@dns-test-service-2.dns-1681.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1681.svc.cluster.local]

Feb 23 18:21:13.545: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:13.554: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:13.559: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:13.563: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:13.575: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:13.578: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:13.584: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:13.588: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:13.595: INFO: Lookups using dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1681.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1681.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local jessie_udp@dns-test-service-2.dns-1681.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1681.svc.cluster.local]

Feb 23 18:21:18.545: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:18.547: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:18.552: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:18.554: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:18.562: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:18.564: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:18.566: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:18.571: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:18.576: INFO: Lookups using dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1681.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1681.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local jessie_udp@dns-test-service-2.dns-1681.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1681.svc.cluster.local]

Feb 23 18:21:23.951: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:23.954: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:23.957: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:23.959: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:23.967: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:23.972: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:23.976: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:23.980: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:23.987: INFO: Lookups using dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1681.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1681.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local jessie_udp@dns-test-service-2.dns-1681.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1681.svc.cluster.local]

Feb 23 18:21:28.804: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:28.808: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:28.812: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:28.815: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:28.827: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:28.831: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:28.835: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:28.842: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1681.svc.cluster.local from pod dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda: the server could not find the requested resource (get pods dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda)
Feb 23 18:21:28.848: INFO: Lookups using dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1681.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1681.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1681.svc.cluster.local jessie_udp@dns-test-service-2.dns-1681.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1681.svc.cluster.local]

Feb 23 18:21:33.569: INFO: DNS probes using dns-1681/dns-test-25f6c949-b27e-4de3-8963-6ec0de5e6bda succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:21:33.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1681" for this suite.
Feb 23 18:21:39.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:21:39.708: INFO: namespace dns-1681 deletion completed in 6.079948176s

• [SLOW TEST:38.265 seconds]
[sig-network] DNS
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:21:39.712: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:21:50.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7877" for this suite.
Feb 23 18:21:56.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:21:56.861: INFO: namespace resourcequota-7877 deletion completed in 6.08410487s

• [SLOW TEST:17.150 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:21:56.862: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-d3731cc9-ff7c-48f7-80d9-9d3f30a2f787
STEP: Creating configMap with name cm-test-opt-upd-ae9004b2-7ba3-4e9b-ab30-ae1747082c62
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-d3731cc9-ff7c-48f7-80d9-9d3f30a2f787
STEP: Updating configmap cm-test-opt-upd-ae9004b2-7ba3-4e9b-ab30-ae1747082c62
STEP: Creating configMap with name cm-test-opt-create-476134e7-1868-4575-9cc6-74d5fd17ebde
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:22:01.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1593" for this suite.
Feb 23 18:22:13.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:22:13.570: INFO: namespace projected-1593 deletion completed in 12.074157732s

• [SLOW TEST:16.708 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:22:13.570: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 18:22:13.618: INFO: Waiting up to 5m0s for pod "busybox-user-65534-7d0327dd-fe2b-459b-8ea7-f99f552b8b25" in namespace "security-context-test-6167" to be "success or failure"
Feb 23 18:22:13.622: INFO: Pod "busybox-user-65534-7d0327dd-fe2b-459b-8ea7-f99f552b8b25": Phase="Pending", Reason="", readiness=false. Elapsed: 4.203648ms
Feb 23 18:22:15.625: INFO: Pod "busybox-user-65534-7d0327dd-fe2b-459b-8ea7-f99f552b8b25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006903226s
Feb 23 18:22:15.625: INFO: Pod "busybox-user-65534-7d0327dd-fe2b-459b-8ea7-f99f552b8b25" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:22:15.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6167" for this suite.
Feb 23 18:22:21.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:22:21.697: INFO: namespace security-context-test-6167 deletion completed in 6.069014031s

• [SLOW TEST:8.127 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:22:21.701: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 18:22:22.146: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 18:22:25.163: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:22:25.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1777" for this suite.
Feb 23 18:22:31.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:22:31.407: INFO: namespace webhook-1777 deletion completed in 6.083400863s
STEP: Destroying namespace "webhook-1777-markers" for this suite.
Feb 23 18:22:37.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:22:37.479: INFO: namespace webhook-1777-markers deletion completed in 6.071865753s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:15.791 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:22:37.494: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Feb 23 18:22:37.522: INFO: namespace kubectl-3927
Feb 23 18:22:37.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 create -f - --namespace=kubectl-3927'
Feb 23 18:22:38.422: INFO: stderr: ""
Feb 23 18:22:38.422: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 23 18:22:39.426: INFO: Selector matched 1 pods for map[app:redis]
Feb 23 18:22:39.426: INFO: Found 0 / 1
Feb 23 18:22:40.425: INFO: Selector matched 1 pods for map[app:redis]
Feb 23 18:22:40.425: INFO: Found 1 / 1
Feb 23 18:22:40.425: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 23 18:22:40.427: INFO: Selector matched 1 pods for map[app:redis]
Feb 23 18:22:40.427: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 23 18:22:40.427: INFO: wait on redis-master startup in kubectl-3927 
Feb 23 18:22:40.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 logs redis-master-ztc7j redis-master --namespace=kubectl-3927'
Feb 23 18:22:40.505: INFO: stderr: ""
Feb 23 18:22:40.505: INFO: stdout: "1:C 23 Feb 2020 18:22:39.278 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 23 Feb 2020 18:22:39.278 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 23 Feb 2020 18:22:39.278 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 23 Feb 2020 18:22:39.278 * Running mode=standalone, port=6379.\n1:M 23 Feb 2020 18:22:39.278 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 23 Feb 2020 18:22:39.278 # Server initialized\n1:M 23 Feb 2020 18:22:39.278 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 23 Feb 2020 18:22:39.278 * Ready to accept connections\n"
STEP: exposing RC
Feb 23 18:22:40.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-3927'
Feb 23 18:22:40.577: INFO: stderr: ""
Feb 23 18:22:40.577: INFO: stdout: "service/rm2 exposed\n"
Feb 23 18:22:40.586: INFO: Service rm2 in namespace kubectl-3927 found.
STEP: exposing service
Feb 23 18:22:42.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-3927'
Feb 23 18:22:42.668: INFO: stderr: ""
Feb 23 18:22:42.668: INFO: stdout: "service/rm3 exposed\n"
Feb 23 18:22:42.671: INFO: Service rm3 in namespace kubectl-3927 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:22:44.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3927" for this suite.
Feb 23 18:23:12.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:23:12.771: INFO: namespace kubectl-3927 deletion completed in 28.090195121s

• [SLOW TEST:35.277 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:23:12.771: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 18:23:12.799: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 23 18:23:19.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 --namespace=crd-publish-openapi-4103 create -f -'
Feb 23 18:23:20.516: INFO: stderr: ""
Feb 23 18:23:20.516: INFO: stdout: "e2e-test-crd-publish-openapi-7889-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 23 18:23:20.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 --namespace=crd-publish-openapi-4103 delete e2e-test-crd-publish-openapi-7889-crds test-cr'
Feb 23 18:23:20.581: INFO: stderr: ""
Feb 23 18:23:20.581: INFO: stdout: "e2e-test-crd-publish-openapi-7889-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Feb 23 18:23:20.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 --namespace=crd-publish-openapi-4103 apply -f -'
Feb 23 18:23:20.740: INFO: stderr: ""
Feb 23 18:23:20.740: INFO: stdout: "e2e-test-crd-publish-openapi-7889-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 23 18:23:20.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 --namespace=crd-publish-openapi-4103 delete e2e-test-crd-publish-openapi-7889-crds test-cr'
Feb 23 18:23:20.800: INFO: stderr: ""
Feb 23 18:23:20.800: INFO: stdout: "e2e-test-crd-publish-openapi-7889-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb 23 18:23:20.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 explain e2e-test-crd-publish-openapi-7889-crds'
Feb 23 18:23:20.925: INFO: stderr: ""
Feb 23 18:23:20.925: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7889-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:23:23.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4103" for this suite.
Feb 23 18:23:29.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:23:29.831: INFO: namespace crd-publish-openapi-4103 deletion completed in 6.072550479s

• [SLOW TEST:17.060 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:23:29.831: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Feb 23 18:23:29.857: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 23 18:23:29.864: INFO: Waiting for terminating namespaces to be deleted...
Feb 23 18:23:29.866: INFO: 
Logging pods the kubelet thinks is on node worker00 before test
Feb 23 18:23:29.873: INFO: calico-node-77p2v from networking started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.873: INFO: 	Container calico-node ready: true, restart count 0
Feb 23 18:23:29.873: INFO: metallb-speaker-nr599 from networking started at 2020-02-23 17:27:42 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.873: INFO: 	Container speaker ready: true, restart count 0
Feb 23 18:23:29.873: INFO: dashboard-metrics-scraper-58475bc987-mxs5c from kube-system started at 2020-02-23 17:29:50 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.873: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 23 18:23:29.873: INFO: sonobuoy-e2e-job-382cd34a40b246b9 from sonobuoy started at 2020-02-23 17:36:27 +0000 UTC (2 container statuses recorded)
Feb 23 18:23:29.873: INFO: 	Container e2e ready: true, restart count 0
Feb 23 18:23:29.873: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 18:23:29.874: INFO: sonobuoy-systemd-logs-daemon-set-e1933034003941b1-4v2r4 from sonobuoy started at 2020-02-23 17:36:27 +0000 UTC (2 container statuses recorded)
Feb 23 18:23:29.874: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 18:23:29.874: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 23 18:23:29.874: INFO: etcd-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.874: INFO: 	Container etcd ready: true, restart count 0
Feb 23 18:23:29.874: INFO: kube-apiserver-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.874: INFO: 	Container kube-apiserver ready: true, restart count 1
Feb 23 18:23:29.874: INFO: ceph-mon-worker00-5cf654d469-2mtnd from storage started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.874: INFO: 	Container ceph-mon ready: true, restart count 0
Feb 23 18:23:29.874: INFO: csi-rbdplugin-8cmnk from storage started at 2020-02-23 17:27:42 +0000 UTC (3 container statuses recorded)
Feb 23 18:23:29.874: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 18:23:29.874: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 18:23:29.874: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:23:29.874: INFO: kube-controller-manager-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.874: INFO: 	Container kube-controller-manager ready: true, restart count 1
Feb 23 18:23:29.874: INFO: ceph-osd-worker00-556546b495-4r2tp from storage started at 2020-02-23 17:25:27 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.874: INFO: 	Container ceph-osd ready: true, restart count 0
Feb 23 18:23:29.874: INFO: kubernetes-dashboard-f957cddcb-qplvv from kube-system started at 2020-02-23 17:29:50 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.874: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 23 18:23:29.874: INFO: sonobuoy from sonobuoy started at 2020-02-23 17:36:25 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.875: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 23 18:23:29.875: INFO: ceph-mds-worker00-6f479b4486-rczdx from storage started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.875: INFO: 	Container ceph-mds ready: true, restart count 0
Feb 23 18:23:29.875: INFO: csi-cephfsplugin-fg8mp from storage started at 2020-02-23 17:27:42 +0000 UTC (3 container statuses recorded)
Feb 23 18:23:29.875: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 18:23:29.875: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 18:23:29.875: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:23:29.875: INFO: gobetween-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.875: INFO: 	Container gobetween ready: true, restart count 0
Feb 23 18:23:29.875: INFO: kube-proxy-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.875: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 18:23:29.875: INFO: kube-scheduler-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.875: INFO: 	Container kube-scheduler ready: true, restart count 2
Feb 23 18:23:29.875: INFO: 
Logging pods the kubelet thinks is on node worker01 before test
Feb 23 18:23:29.894: INFO: metallb-controller-b96bfbbf8-p9qvp from networking started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.894: INFO: 	Container controller ready: true, restart count 0
Feb 23 18:23:29.894: INFO: coredns-676544c7b9-ntt4b from kube-system started at 2020-02-23 17:26:46 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.894: INFO: 	Container coredns ready: true, restart count 0
Feb 23 18:23:29.894: INFO: kube-proxy-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.894: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 18:23:29.894: INFO: calico-node-lhcql from networking started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.895: INFO: 	Container calico-node ready: true, restart count 0
Feb 23 18:23:29.895: INFO: csi-cephfsplugin-provisioner-6cd7596f75-b6s7q from storage started at 2020-02-23 17:26:43 +0000 UTC (4 container statuses recorded)
Feb 23 18:23:29.895: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 18:23:29.895: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 18:23:29.895: INFO: 	Container csi-provisioner ready: true, restart count 1
Feb 23 18:23:29.895: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:23:29.895: INFO: calico-kube-controllers-7cd585bcd-v4kk7 from networking started at 2020-02-23 17:26:46 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.895: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 23 18:23:29.895: INFO: csi-rbdplugin-provisioner-7494f65674-gt7nb from storage started at 2020-02-23 17:26:43 +0000 UTC (5 container statuses recorded)
Feb 23 18:23:29.895: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 23 18:23:29.895: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 18:23:29.895: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 18:23:29.895: INFO: 	Container csi-snapshotter ready: true, restart count 0
Feb 23 18:23:29.895: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:23:29.895: INFO: csi-rbdplugin-provisioner-7494f65674-vnnkm from storage started at 2020-02-23 17:26:43 +0000 UTC (5 container statuses recorded)
Feb 23 18:23:29.895: INFO: 	Container csi-attacher ready: true, restart count 1
Feb 23 18:23:29.895: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 18:23:29.895: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 18:23:29.895: INFO: 	Container csi-snapshotter ready: true, restart count 0
Feb 23 18:23:29.895: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:23:29.895: INFO: kube-apiserver-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.895: INFO: 	Container kube-apiserver ready: true, restart count 0
Feb 23 18:23:29.895: INFO: ceph-osd-worker01-67947c799-8n7pr from storage started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.895: INFO: 	Container ceph-osd ready: true, restart count 0
Feb 23 18:23:29.895: INFO: ceph-mds-worker01-7f5fdb58c6-m96hd from storage started at 2020-02-23 17:25:27 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.895: INFO: 	Container ceph-mds ready: true, restart count 2
Feb 23 18:23:29.895: INFO: csi-cephfsplugin-provisioner-6cd7596f75-n9kmr from storage started at 2020-02-23 17:26:43 +0000 UTC (4 container statuses recorded)
Feb 23 18:23:29.895: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 18:23:29.895: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 18:23:29.895: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 18:23:29.895: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:23:29.895: INFO: kube-scheduler-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.895: INFO: 	Container kube-scheduler ready: true, restart count 1
Feb 23 18:23:29.895: INFO: csi-rbdplugin-cfq94 from storage started at 2020-02-23 17:26:43 +0000 UTC (3 container statuses recorded)
Feb 23 18:23:29.895: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 18:23:29.895: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 18:23:29.895: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:23:29.895: INFO: sonobuoy-systemd-logs-daemon-set-e1933034003941b1-74kbd from sonobuoy started at 2020-02-23 17:36:27 +0000 UTC (2 container statuses recorded)
Feb 23 18:23:29.895: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 18:23:29.895: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 23 18:23:29.895: INFO: gobetween-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.895: INFO: 	Container gobetween ready: true, restart count 0
Feb 23 18:23:29.895: INFO: ceph-mon-worker01-bdb694876-9x2bs from storage started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.895: INFO: 	Container ceph-mon ready: true, restart count 0
Feb 23 18:23:29.895: INFO: csi-cephfsplugin-wj5f7 from storage started at 2020-02-23 17:26:43 +0000 UTC (3 container statuses recorded)
Feb 23 18:23:29.895: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 18:23:29.895: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 18:23:29.895: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:23:29.895: INFO: ceph-setup-xccxd from storage started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.895: INFO: 	Container ceph ready: false, restart count 2
Feb 23 18:23:29.895: INFO: coredns-676544c7b9-pz66t from kube-system started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.895: INFO: 	Container coredns ready: true, restart count 0
Feb 23 18:23:29.895: INFO: ceph-rgw-57cd48f74c-4nwcs from storage started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.895: INFO: 	Container ceph-rgw ready: true, restart count 1
Feb 23 18:23:29.895: INFO: ceph-mgr-94b9dd996-vfc9x from storage started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.895: INFO: 	Container ceph-mgr ready: true, restart count 0
Feb 23 18:23:29.895: INFO: csi-rbdplugin-provisioner-7494f65674-zk9l6 from storage started at 2020-02-23 17:26:43 +0000 UTC (5 container statuses recorded)
Feb 23 18:23:29.895: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 23 18:23:29.895: INFO: 	Container csi-provisioner ready: true, restart count 1
Feb 23 18:23:29.895: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 18:23:29.895: INFO: 	Container csi-snapshotter ready: true, restart count 0
Feb 23 18:23:29.895: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:23:29.895: INFO: etcd-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.895: INFO: 	Container etcd ready: true, restart count 0
Feb 23 18:23:29.895: INFO: kube-controller-manager-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.895: INFO: 	Container kube-controller-manager ready: true, restart count 1
Feb 23 18:23:29.895: INFO: csi-cephfsplugin-provisioner-6cd7596f75-tjxsr from storage started at 2020-02-23 17:26:43 +0000 UTC (4 container statuses recorded)
Feb 23 18:23:29.895: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 18:23:29.895: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 18:23:29.895: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 18:23:29.895: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:23:29.895: INFO: metallb-speaker-jpspl from networking started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:23:29.895: INFO: 	Container speaker ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15f61b2f80da5d77], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:23:30.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7448" for this suite.
Feb 23 18:23:36.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:23:37.007: INFO: namespace sched-pred-7448 deletion completed in 6.090439393s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:7.176 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:23:37.007: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-964
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-964 to expose endpoints map[]
Feb 23 18:23:37.056: INFO: Get endpoints failed (8.697276ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Feb 23 18:23:38.059: INFO: successfully validated that service multi-endpoint-test in namespace services-964 exposes endpoints map[] (1.011289132s elapsed)
STEP: Creating pod pod1 in namespace services-964
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-964 to expose endpoints map[pod1:[100]]
Feb 23 18:23:40.084: INFO: successfully validated that service multi-endpoint-test in namespace services-964 exposes endpoints map[pod1:[100]] (2.018909497s elapsed)
STEP: Creating pod pod2 in namespace services-964
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-964 to expose endpoints map[pod1:[100] pod2:[101]]
Feb 23 18:23:41.109: INFO: successfully validated that service multi-endpoint-test in namespace services-964 exposes endpoints map[pod1:[100] pod2:[101]] (1.020941026s elapsed)
STEP: Deleting pod pod1 in namespace services-964
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-964 to expose endpoints map[pod2:[101]]
Feb 23 18:23:42.147: INFO: successfully validated that service multi-endpoint-test in namespace services-964 exposes endpoints map[pod2:[101]] (1.025460463s elapsed)
STEP: Deleting pod pod2 in namespace services-964
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-964 to expose endpoints map[]
Feb 23 18:23:43.168: INFO: successfully validated that service multi-endpoint-test in namespace services-964 exposes endpoints map[] (1.013328274s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:23:43.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-964" for this suite.
Feb 23 18:24:11.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:24:11.267: INFO: namespace services-964 deletion completed in 28.078513211s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:34.260 seconds]
[sig-network] Services
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:24:11.267: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:24:14.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8379" for this suite.
Feb 23 18:24:26.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:24:26.416: INFO: namespace replication-controller-8379 deletion completed in 12.081017529s

• [SLOW TEST:15.149 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:24:26.418: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:24:42.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2649" for this suite.
Feb 23 18:24:48.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:24:48.639: INFO: namespace resourcequota-2649 deletion completed in 6.079617744s

• [SLOW TEST:22.221 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:24:48.639: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Feb 23 18:24:48.668: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-952870286 proxy --unix-socket=/tmp/kubectl-proxy-unix878069501/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:24:48.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2645" for this suite.
Feb 23 18:24:54.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:24:54.789: INFO: namespace kubectl-2645 deletion completed in 6.071841739s

• [SLOW TEST:6.150 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:24:54.789: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 23 18:24:54.824: INFO: Waiting up to 5m0s for pod "pod-21763a5e-0de9-4370-9cf7-c8a8bd41c2c1" in namespace "emptydir-2324" to be "success or failure"
Feb 23 18:24:54.826: INFO: Pod "pod-21763a5e-0de9-4370-9cf7-c8a8bd41c2c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021737ms
Feb 23 18:24:56.829: INFO: Pod "pod-21763a5e-0de9-4370-9cf7-c8a8bd41c2c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005593986s
STEP: Saw pod success
Feb 23 18:24:56.830: INFO: Pod "pod-21763a5e-0de9-4370-9cf7-c8a8bd41c2c1" satisfied condition "success or failure"
Feb 23 18:24:56.831: INFO: Trying to get logs from node worker00 pod pod-21763a5e-0de9-4370-9cf7-c8a8bd41c2c1 container test-container: <nil>
STEP: delete the pod
Feb 23 18:24:56.851: INFO: Waiting for pod pod-21763a5e-0de9-4370-9cf7-c8a8bd41c2c1 to disappear
Feb 23 18:24:56.855: INFO: Pod pod-21763a5e-0de9-4370-9cf7-c8a8bd41c2c1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:24:56.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2324" for this suite.
Feb 23 18:25:02.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:25:02.937: INFO: namespace emptydir-2324 deletion completed in 6.079058052s

• [SLOW TEST:8.148 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:25:02.940: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Feb 23 18:25:02.980: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
Feb 23 18:25:16.084: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:25:38.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-838" for this suite.
Feb 23 18:25:44.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:25:44.862: INFO: namespace crd-publish-openapi-838 deletion completed in 6.075225089s

• [SLOW TEST:41.922 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:25:44.863: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-b433724e-0546-4cb0-b38a-a66a5ce4d1df
STEP: Creating a pod to test consume secrets
Feb 23 18:25:44.906: INFO: Waiting up to 5m0s for pod "pod-secrets-bde5a573-167a-4194-8a19-ca0acf2e90a5" in namespace "secrets-362" to be "success or failure"
Feb 23 18:25:44.909: INFO: Pod "pod-secrets-bde5a573-167a-4194-8a19-ca0acf2e90a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.56255ms
Feb 23 18:25:47.166: INFO: Pod "pod-secrets-bde5a573-167a-4194-8a19-ca0acf2e90a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.259906833s
STEP: Saw pod success
Feb 23 18:25:47.166: INFO: Pod "pod-secrets-bde5a573-167a-4194-8a19-ca0acf2e90a5" satisfied condition "success or failure"
Feb 23 18:25:47.171: INFO: Trying to get logs from node worker00 pod pod-secrets-bde5a573-167a-4194-8a19-ca0acf2e90a5 container secret-volume-test: <nil>
STEP: delete the pod
Feb 23 18:25:47.188: INFO: Waiting for pod pod-secrets-bde5a573-167a-4194-8a19-ca0acf2e90a5 to disappear
Feb 23 18:25:47.194: INFO: Pod pod-secrets-bde5a573-167a-4194-8a19-ca0acf2e90a5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:25:47.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-362" for this suite.
Feb 23 18:25:53.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:25:53.289: INFO: namespace secrets-362 deletion completed in 6.089364167s

• [SLOW TEST:8.427 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:25:53.289: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-f078ff6f-6ef4-414b-a0b6-7f2f6f6d09c0
STEP: Creating a pod to test consume configMaps
Feb 23 18:25:53.330: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d64df7de-917f-4d96-8bdf-3e69146b2b9d" in namespace "projected-4295" to be "success or failure"
Feb 23 18:25:53.332: INFO: Pod "pod-projected-configmaps-d64df7de-917f-4d96-8bdf-3e69146b2b9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.77567ms
Feb 23 18:25:55.339: INFO: Pod "pod-projected-configmaps-d64df7de-917f-4d96-8bdf-3e69146b2b9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009391008s
Feb 23 18:25:57.342: INFO: Pod "pod-projected-configmaps-d64df7de-917f-4d96-8bdf-3e69146b2b9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01250704s
STEP: Saw pod success
Feb 23 18:25:57.342: INFO: Pod "pod-projected-configmaps-d64df7de-917f-4d96-8bdf-3e69146b2b9d" satisfied condition "success or failure"
Feb 23 18:25:57.344: INFO: Trying to get logs from node worker00 pod pod-projected-configmaps-d64df7de-917f-4d96-8bdf-3e69146b2b9d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 18:25:57.364: INFO: Waiting for pod pod-projected-configmaps-d64df7de-917f-4d96-8bdf-3e69146b2b9d to disappear
Feb 23 18:25:57.366: INFO: Pod pod-projected-configmaps-d64df7de-917f-4d96-8bdf-3e69146b2b9d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:25:57.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4295" for this suite.
Feb 23 18:26:03.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:26:03.445: INFO: namespace projected-4295 deletion completed in 6.074716136s

• [SLOW TEST:10.156 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:26:03.445: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-h6kd
STEP: Creating a pod to test atomic-volume-subpath
Feb 23 18:26:03.498: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-h6kd" in namespace "subpath-555" to be "success or failure"
Feb 23 18:26:03.506: INFO: Pod "pod-subpath-test-configmap-h6kd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.680429ms
Feb 23 18:26:05.509: INFO: Pod "pod-subpath-test-configmap-h6kd": Phase="Running", Reason="", readiness=true. Elapsed: 2.01099605s
Feb 23 18:26:07.513: INFO: Pod "pod-subpath-test-configmap-h6kd": Phase="Running", Reason="", readiness=true. Elapsed: 4.014640716s
Feb 23 18:26:09.518: INFO: Pod "pod-subpath-test-configmap-h6kd": Phase="Running", Reason="", readiness=true. Elapsed: 6.019642552s
Feb 23 18:26:11.521: INFO: Pod "pod-subpath-test-configmap-h6kd": Phase="Running", Reason="", readiness=true. Elapsed: 8.023264278s
Feb 23 18:26:13.565: INFO: Pod "pod-subpath-test-configmap-h6kd": Phase="Running", Reason="", readiness=true. Elapsed: 10.067565668s
Feb 23 18:26:15.571: INFO: Pod "pod-subpath-test-configmap-h6kd": Phase="Running", Reason="", readiness=true. Elapsed: 12.073519899s
Feb 23 18:26:18.069: INFO: Pod "pod-subpath-test-configmap-h6kd": Phase="Running", Reason="", readiness=true. Elapsed: 14.571342528s
Feb 23 18:26:20.073: INFO: Pod "pod-subpath-test-configmap-h6kd": Phase="Running", Reason="", readiness=true. Elapsed: 16.575043422s
Feb 23 18:26:22.076: INFO: Pod "pod-subpath-test-configmap-h6kd": Phase="Running", Reason="", readiness=true. Elapsed: 18.577803172s
Feb 23 18:26:24.079: INFO: Pod "pod-subpath-test-configmap-h6kd": Phase="Running", Reason="", readiness=true. Elapsed: 20.581223219s
Feb 23 18:26:26.083: INFO: Pod "pod-subpath-test-configmap-h6kd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.584857444s
STEP: Saw pod success
Feb 23 18:26:26.083: INFO: Pod "pod-subpath-test-configmap-h6kd" satisfied condition "success or failure"
Feb 23 18:26:26.085: INFO: Trying to get logs from node worker00 pod pod-subpath-test-configmap-h6kd container test-container-subpath-configmap-h6kd: <nil>
STEP: delete the pod
Feb 23 18:26:26.102: INFO: Waiting for pod pod-subpath-test-configmap-h6kd to disappear
Feb 23 18:26:26.106: INFO: Pod pod-subpath-test-configmap-h6kd no longer exists
STEP: Deleting pod pod-subpath-test-configmap-h6kd
Feb 23 18:26:26.106: INFO: Deleting pod "pod-subpath-test-configmap-h6kd" in namespace "subpath-555"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:26:26.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-555" for this suite.
Feb 23 18:26:32.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:26:32.186: INFO: namespace subpath-555 deletion completed in 6.072719838s

• [SLOW TEST:28.741 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:26:32.186: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 23 18:26:34.312: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:26:34.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2953" for this suite.
Feb 23 18:26:40.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:26:40.410: INFO: namespace container-runtime-2953 deletion completed in 6.080128717s

• [SLOW TEST:8.224 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:26:40.410: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0223 18:26:50.462981      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 23 18:26:50.463: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:26:50.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3565" for this suite.
Feb 23 18:26:56.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:26:56.543: INFO: namespace gc-3565 deletion completed in 6.076736795s

• [SLOW TEST:16.133 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:26:56.543: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 18:26:56.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 version'
Feb 23 18:26:56.627: INFO: stderr: ""
Feb 23 18:26:56.627: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.7\", GitCommit:\"be3d344ed06bff7a4fc60656200a93c74f31f9a4\", GitTreeState:\"clean\", BuildDate:\"2020-02-11T19:34:02Z\", GoVersion:\"go1.13.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.7\", GitCommit:\"be3d344ed06bff7a4fc60656200a93c74f31f9a4\", GitTreeState:\"clean\", BuildDate:\"2020-02-11T19:24:46Z\", GoVersion:\"go1.13.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:26:56.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2865" for this suite.
Feb 23 18:27:02.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:27:02.704: INFO: namespace kubectl-2865 deletion completed in 6.073953366s

• [SLOW TEST:6.160 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:27:02.704: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-90f6a8ae-4a32-499f-b61b-1a2b355bedfc
STEP: Creating secret with name s-test-opt-upd-41760b63-5311-48ef-b0cc-009454047931
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-90f6a8ae-4a32-499f-b61b-1a2b355bedfc
STEP: Updating secret s-test-opt-upd-41760b63-5311-48ef-b0cc-009454047931
STEP: Creating secret with name s-test-opt-create-c7b68183-6a0c-429f-ad32-098876f16f7b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:28:13.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6729" for this suite.
Feb 23 18:28:25.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:28:25.286: INFO: namespace projected-6729 deletion completed in 12.074882601s

• [SLOW TEST:82.583 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:28:25.287: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-7210/secret-test-dfb4cc5a-03bc-4829-be54-c4011008a9a2
STEP: Creating a pod to test consume secrets
Feb 23 18:28:25.324: INFO: Waiting up to 5m0s for pod "pod-configmaps-19b2d683-4913-4b95-87d2-f32c6e38f976" in namespace "secrets-7210" to be "success or failure"
Feb 23 18:28:25.328: INFO: Pod "pod-configmaps-19b2d683-4913-4b95-87d2-f32c6e38f976": Phase="Pending", Reason="", readiness=false. Elapsed: 3.210167ms
Feb 23 18:28:27.332: INFO: Pod "pod-configmaps-19b2d683-4913-4b95-87d2-f32c6e38f976": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006906903s
STEP: Saw pod success
Feb 23 18:28:27.332: INFO: Pod "pod-configmaps-19b2d683-4913-4b95-87d2-f32c6e38f976" satisfied condition "success or failure"
Feb 23 18:28:27.337: INFO: Trying to get logs from node worker00 pod pod-configmaps-19b2d683-4913-4b95-87d2-f32c6e38f976 container env-test: <nil>
STEP: delete the pod
Feb 23 18:28:27.356: INFO: Waiting for pod pod-configmaps-19b2d683-4913-4b95-87d2-f32c6e38f976 to disappear
Feb 23 18:28:27.362: INFO: Pod pod-configmaps-19b2d683-4913-4b95-87d2-f32c6e38f976 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:28:27.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7210" for this suite.
Feb 23 18:28:33.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:28:33.439: INFO: namespace secrets-7210 deletion completed in 6.074023684s

• [SLOW TEST:8.152 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:28:33.439: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 18:28:33.958: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8790718e-a0bb-4e9f-89ba-24256d625a39", Controller:(*bool)(0xc0042b6c96), BlockOwnerDeletion:(*bool)(0xc0042b6c97)}}
Feb 23 18:28:33.968: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"ca6cd933-22c2-4428-9226-04a5f1a8b6c8", Controller:(*bool)(0xc005760536), BlockOwnerDeletion:(*bool)(0xc005760537)}}
Feb 23 18:28:33.975: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"0827136b-2bbd-49db-9df6-02a6e660fe67", Controller:(*bool)(0xc005640356), BlockOwnerDeletion:(*bool)(0xc005640357)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:28:38.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2824" for this suite.
Feb 23 18:28:45.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:28:45.076: INFO: namespace gc-2824 deletion completed in 6.088037181s

• [SLOW TEST:11.637 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:28:45.078: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Feb 23 18:29:25.137: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0223 18:29:25.137637      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 23 18:29:25.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3517" for this suite.
Feb 23 18:29:33.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:29:33.210: INFO: namespace gc-3517 deletion completed in 8.070340759s

• [SLOW TEST:48.133 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:29:33.211: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 18:29:33.677: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 23 18:29:35.686: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718079373, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718079373, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718079373, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718079373, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 18:29:38.698: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:29:38.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1263" for this suite.
Feb 23 18:29:44.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:29:44.817: INFO: namespace webhook-1263 deletion completed in 6.069272005s
STEP: Destroying namespace "webhook-1263-markers" for this suite.
Feb 23 18:29:50.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:29:50.891: INFO: namespace webhook-1263-markers deletion completed in 6.073336622s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.691 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:29:50.905: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 18:29:50.932: INFO: Creating ReplicaSet my-hostname-basic-17789d6a-78d8-4cda-9540-75d2c0f0e758
Feb 23 18:29:50.941: INFO: Pod name my-hostname-basic-17789d6a-78d8-4cda-9540-75d2c0f0e758: Found 0 pods out of 1
Feb 23 18:29:55.944: INFO: Pod name my-hostname-basic-17789d6a-78d8-4cda-9540-75d2c0f0e758: Found 1 pods out of 1
Feb 23 18:29:55.944: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-17789d6a-78d8-4cda-9540-75d2c0f0e758" is running
Feb 23 18:29:55.948: INFO: Pod "my-hostname-basic-17789d6a-78d8-4cda-9540-75d2c0f0e758-d6tpc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-23 18:29:50 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-23 18:29:52 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-23 18:29:52 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-23 18:29:50 +0000 UTC Reason: Message:}])
Feb 23 18:29:55.948: INFO: Trying to dial the pod
Feb 23 18:30:01.224: INFO: Controller my-hostname-basic-17789d6a-78d8-4cda-9540-75d2c0f0e758: Got expected result from replica 1 [my-hostname-basic-17789d6a-78d8-4cda-9540-75d2c0f0e758-d6tpc]: "my-hostname-basic-17789d6a-78d8-4cda-9540-75d2c0f0e758-d6tpc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:30:01.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9938" for this suite.
Feb 23 18:30:07.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:30:07.311: INFO: namespace replicaset-9938 deletion completed in 6.084061728s

• [SLOW TEST:16.406 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:30:07.311: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-dhgx
STEP: Creating a pod to test atomic-volume-subpath
Feb 23 18:30:07.350: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-dhgx" in namespace "subpath-5458" to be "success or failure"
Feb 23 18:30:07.353: INFO: Pod "pod-subpath-test-projected-dhgx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.992462ms
Feb 23 18:30:09.355: INFO: Pod "pod-subpath-test-projected-dhgx": Phase="Running", Reason="", readiness=true. Elapsed: 2.005446382s
Feb 23 18:30:11.433: INFO: Pod "pod-subpath-test-projected-dhgx": Phase="Running", Reason="", readiness=true. Elapsed: 4.082713777s
Feb 23 18:30:13.436: INFO: Pod "pod-subpath-test-projected-dhgx": Phase="Running", Reason="", readiness=true. Elapsed: 6.086108238s
Feb 23 18:30:15.439: INFO: Pod "pod-subpath-test-projected-dhgx": Phase="Running", Reason="", readiness=true. Elapsed: 8.088724655s
Feb 23 18:30:17.442: INFO: Pod "pod-subpath-test-projected-dhgx": Phase="Running", Reason="", readiness=true. Elapsed: 10.091790306s
Feb 23 18:30:19.446: INFO: Pod "pod-subpath-test-projected-dhgx": Phase="Running", Reason="", readiness=true. Elapsed: 12.096063608s
Feb 23 18:30:21.449: INFO: Pod "pod-subpath-test-projected-dhgx": Phase="Running", Reason="", readiness=true. Elapsed: 14.099619065s
Feb 23 18:30:23.453: INFO: Pod "pod-subpath-test-projected-dhgx": Phase="Running", Reason="", readiness=true. Elapsed: 16.103252758s
Feb 23 18:30:25.458: INFO: Pod "pod-subpath-test-projected-dhgx": Phase="Running", Reason="", readiness=true. Elapsed: 18.108182468s
Feb 23 18:30:27.461: INFO: Pod "pod-subpath-test-projected-dhgx": Phase="Running", Reason="", readiness=true. Elapsed: 20.111061993s
Feb 23 18:30:29.465: INFO: Pod "pod-subpath-test-projected-dhgx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.114747301s
STEP: Saw pod success
Feb 23 18:30:29.465: INFO: Pod "pod-subpath-test-projected-dhgx" satisfied condition "success or failure"
Feb 23 18:30:29.467: INFO: Trying to get logs from node worker00 pod pod-subpath-test-projected-dhgx container test-container-subpath-projected-dhgx: <nil>
STEP: delete the pod
Feb 23 18:30:29.497: INFO: Waiting for pod pod-subpath-test-projected-dhgx to disappear
Feb 23 18:30:29.499: INFO: Pod pod-subpath-test-projected-dhgx no longer exists
STEP: Deleting pod pod-subpath-test-projected-dhgx
Feb 23 18:30:29.499: INFO: Deleting pod "pod-subpath-test-projected-dhgx" in namespace "subpath-5458"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:30:29.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5458" for this suite.
Feb 23 18:30:35.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:30:35.603: INFO: namespace subpath-5458 deletion completed in 6.099787603s

• [SLOW TEST:28.292 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:30:35.604: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 23 18:30:35.635: INFO: Waiting up to 5m0s for pod "pod-f1c1fc95-ee22-460d-b5f8-1e69c7c8e5e4" in namespace "emptydir-4214" to be "success or failure"
Feb 23 18:30:35.639: INFO: Pod "pod-f1c1fc95-ee22-460d-b5f8-1e69c7c8e5e4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.721834ms
Feb 23 18:30:37.642: INFO: Pod "pod-f1c1fc95-ee22-460d-b5f8-1e69c7c8e5e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007112128s
STEP: Saw pod success
Feb 23 18:30:37.642: INFO: Pod "pod-f1c1fc95-ee22-460d-b5f8-1e69c7c8e5e4" satisfied condition "success or failure"
Feb 23 18:30:37.644: INFO: Trying to get logs from node worker00 pod pod-f1c1fc95-ee22-460d-b5f8-1e69c7c8e5e4 container test-container: <nil>
STEP: delete the pod
Feb 23 18:30:37.660: INFO: Waiting for pod pod-f1c1fc95-ee22-460d-b5f8-1e69c7c8e5e4 to disappear
Feb 23 18:30:37.662: INFO: Pod pod-f1c1fc95-ee22-460d-b5f8-1e69c7c8e5e4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:30:37.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4214" for this suite.
Feb 23 18:30:43.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:30:43.743: INFO: namespace emptydir-4214 deletion completed in 6.075245076s

• [SLOW TEST:8.140 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:30:43.746: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:31:12.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5517" for this suite.
Feb 23 18:31:18.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:31:18.929: INFO: namespace namespaces-5517 deletion completed in 6.070118342s
STEP: Destroying namespace "nsdeletetest-7256" for this suite.
Feb 23 18:31:18.931: INFO: Namespace nsdeletetest-7256 was already deleted
STEP: Destroying namespace "nsdeletetest-5910" for this suite.
Feb 23 18:31:24.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:31:25.004: INFO: namespace nsdeletetest-5910 deletion completed in 6.073680271s

• [SLOW TEST:41.258 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:31:25.005: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Feb 23 18:31:25.032: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 23 18:31:25.040: INFO: Waiting for terminating namespaces to be deleted...
Feb 23 18:31:25.042: INFO: 
Logging pods the kubelet thinks is on node worker00 before test
Feb 23 18:31:25.049: INFO: etcd-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.049: INFO: 	Container etcd ready: true, restart count 0
Feb 23 18:31:25.049: INFO: kube-apiserver-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.049: INFO: 	Container kube-apiserver ready: true, restart count 1
Feb 23 18:31:25.049: INFO: ceph-mon-worker00-5cf654d469-2mtnd from storage started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.049: INFO: 	Container ceph-mon ready: true, restart count 0
Feb 23 18:31:25.049: INFO: sonobuoy-e2e-job-382cd34a40b246b9 from sonobuoy started at 2020-02-23 17:36:27 +0000 UTC (2 container statuses recorded)
Feb 23 18:31:25.049: INFO: 	Container e2e ready: true, restart count 0
Feb 23 18:31:25.049: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 18:31:25.049: INFO: sonobuoy-systemd-logs-daemon-set-e1933034003941b1-4v2r4 from sonobuoy started at 2020-02-23 17:36:27 +0000 UTC (2 container statuses recorded)
Feb 23 18:31:25.049: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 18:31:25.049: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 23 18:31:25.049: INFO: csi-rbdplugin-8cmnk from storage started at 2020-02-23 17:27:42 +0000 UTC (3 container statuses recorded)
Feb 23 18:31:25.049: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 18:31:25.049: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 18:31:25.049: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:31:25.049: INFO: kube-controller-manager-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.049: INFO: 	Container kube-controller-manager ready: true, restart count 1
Feb 23 18:31:25.049: INFO: ceph-osd-worker00-556546b495-4r2tp from storage started at 2020-02-23 17:25:27 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.049: INFO: 	Container ceph-osd ready: true, restart count 0
Feb 23 18:31:25.049: INFO: kubernetes-dashboard-f957cddcb-qplvv from kube-system started at 2020-02-23 17:29:50 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.049: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 23 18:31:25.049: INFO: sonobuoy from sonobuoy started at 2020-02-23 17:36:25 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.049: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 23 18:31:25.049: INFO: gobetween-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.049: INFO: 	Container gobetween ready: true, restart count 0
Feb 23 18:31:25.049: INFO: kube-proxy-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.049: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 18:31:25.049: INFO: kube-scheduler-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.049: INFO: 	Container kube-scheduler ready: true, restart count 2
Feb 23 18:31:25.049: INFO: ceph-mds-worker00-6f479b4486-rczdx from storage started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.049: INFO: 	Container ceph-mds ready: true, restart count 0
Feb 23 18:31:25.049: INFO: csi-cephfsplugin-fg8mp from storage started at 2020-02-23 17:27:42 +0000 UTC (3 container statuses recorded)
Feb 23 18:31:25.049: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 18:31:25.049: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 18:31:25.049: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:31:25.049: INFO: calico-node-77p2v from networking started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.049: INFO: 	Container calico-node ready: true, restart count 0
Feb 23 18:31:25.049: INFO: metallb-speaker-nr599 from networking started at 2020-02-23 17:27:42 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.050: INFO: 	Container speaker ready: true, restart count 0
Feb 23 18:31:25.050: INFO: dashboard-metrics-scraper-58475bc987-mxs5c from kube-system started at 2020-02-23 17:29:50 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.050: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 23 18:31:25.050: INFO: 
Logging pods the kubelet thinks is on node worker01 before test
Feb 23 18:31:25.070: INFO: ceph-mgr-94b9dd996-vfc9x from storage started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container ceph-mgr ready: true, restart count 0
Feb 23 18:31:25.070: INFO: csi-rbdplugin-provisioner-7494f65674-zk9l6 from storage started at 2020-02-23 17:26:43 +0000 UTC (5 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 23 18:31:25.070: INFO: 	Container csi-provisioner ready: true, restart count 1
Feb 23 18:31:25.070: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 18:31:25.070: INFO: 	Container csi-snapshotter ready: true, restart count 0
Feb 23 18:31:25.070: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:31:25.070: INFO: coredns-676544c7b9-pz66t from kube-system started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container coredns ready: true, restart count 0
Feb 23 18:31:25.070: INFO: ceph-rgw-57cd48f74c-4nwcs from storage started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container ceph-rgw ready: true, restart count 1
Feb 23 18:31:25.070: INFO: csi-cephfsplugin-provisioner-6cd7596f75-tjxsr from storage started at 2020-02-23 17:26:43 +0000 UTC (4 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 18:31:25.070: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 18:31:25.070: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 18:31:25.070: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:31:25.070: INFO: metallb-speaker-jpspl from networking started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container speaker ready: true, restart count 0
Feb 23 18:31:25.070: INFO: etcd-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container etcd ready: true, restart count 0
Feb 23 18:31:25.070: INFO: kube-controller-manager-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container kube-controller-manager ready: true, restart count 1
Feb 23 18:31:25.070: INFO: metallb-controller-b96bfbbf8-p9qvp from networking started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container controller ready: true, restart count 0
Feb 23 18:31:25.070: INFO: coredns-676544c7b9-ntt4b from kube-system started at 2020-02-23 17:26:46 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container coredns ready: true, restart count 0
Feb 23 18:31:25.070: INFO: csi-cephfsplugin-provisioner-6cd7596f75-b6s7q from storage started at 2020-02-23 17:26:43 +0000 UTC (4 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 18:31:25.070: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 18:31:25.070: INFO: 	Container csi-provisioner ready: true, restart count 1
Feb 23 18:31:25.070: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:31:25.070: INFO: calico-kube-controllers-7cd585bcd-v4kk7 from networking started at 2020-02-23 17:26:46 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 23 18:31:25.070: INFO: kube-proxy-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 18:31:25.070: INFO: calico-node-lhcql from networking started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container calico-node ready: true, restart count 0
Feb 23 18:31:25.070: INFO: ceph-mds-worker01-7f5fdb58c6-m96hd from storage started at 2020-02-23 17:25:27 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container ceph-mds ready: true, restart count 2
Feb 23 18:31:25.070: INFO: csi-cephfsplugin-provisioner-6cd7596f75-n9kmr from storage started at 2020-02-23 17:26:43 +0000 UTC (4 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 18:31:25.070: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 18:31:25.070: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 18:31:25.070: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:31:25.070: INFO: csi-rbdplugin-provisioner-7494f65674-gt7nb from storage started at 2020-02-23 17:26:43 +0000 UTC (5 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 23 18:31:25.070: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 18:31:25.070: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 18:31:25.070: INFO: 	Container csi-snapshotter ready: true, restart count 0
Feb 23 18:31:25.070: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:31:25.070: INFO: csi-rbdplugin-provisioner-7494f65674-vnnkm from storage started at 2020-02-23 17:26:43 +0000 UTC (5 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container csi-attacher ready: true, restart count 1
Feb 23 18:31:25.070: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 18:31:25.070: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 18:31:25.070: INFO: 	Container csi-snapshotter ready: true, restart count 0
Feb 23 18:31:25.070: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:31:25.070: INFO: kube-apiserver-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container kube-apiserver ready: true, restart count 0
Feb 23 18:31:25.070: INFO: ceph-osd-worker01-67947c799-8n7pr from storage started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container ceph-osd ready: true, restart count 0
Feb 23 18:31:25.070: INFO: sonobuoy-systemd-logs-daemon-set-e1933034003941b1-74kbd from sonobuoy started at 2020-02-23 17:36:27 +0000 UTC (2 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 18:31:25.070: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 23 18:31:25.070: INFO: kube-scheduler-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container kube-scheduler ready: true, restart count 1
Feb 23 18:31:25.070: INFO: csi-rbdplugin-cfq94 from storage started at 2020-02-23 17:26:43 +0000 UTC (3 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 18:31:25.070: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 18:31:25.070: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:31:25.070: INFO: csi-cephfsplugin-wj5f7 from storage started at 2020-02-23 17:26:43 +0000 UTC (3 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 18:31:25.070: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 18:31:25.070: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:31:25.070: INFO: gobetween-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container gobetween ready: true, restart count 0
Feb 23 18:31:25.070: INFO: ceph-mon-worker01-bdb694876-9x2bs from storage started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container ceph-mon ready: true, restart count 0
Feb 23 18:31:25.070: INFO: ceph-setup-xccxd from storage started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:31:25.070: INFO: 	Container ceph ready: false, restart count 2
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e2ece13f-e0b0-46f4-af34-094517b57194 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-e2ece13f-e0b0-46f4-af34-094517b57194 off the node worker00
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e2ece13f-e0b0-46f4-af34-094517b57194
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:31:29.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7046" for this suite.
Feb 23 18:31:37.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:31:37.252: INFO: namespace sched-pred-7046 deletion completed in 8.09415892s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:12.247 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:31:37.252: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7447
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-7447
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7447
Feb 23 18:31:37.298: INFO: Found 0 stateful pods, waiting for 1
Feb 23 18:31:47.302: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 23 18:31:47.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=statefulset-7447 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 23 18:31:47.443: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 23 18:31:47.443: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 23 18:31:47.443: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 23 18:31:47.446: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 23 18:31:57.486: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 23 18:31:57.486: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 18:31:57.504: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Feb 23 18:31:57.504: INFO: ss-0  worker00  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:37 +0000 UTC  }]
Feb 23 18:31:57.504: INFO: 
Feb 23 18:31:57.504: INFO: StatefulSet ss has not reached scale 3, at 1
Feb 23 18:31:58.510: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995396247s
Feb 23 18:31:59.514: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989799193s
Feb 23 18:32:00.517: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98582056s
Feb 23 18:32:01.520: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.98308408s
Feb 23 18:32:02.527: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.976850018s
Feb 23 18:32:03.534: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.970610157s
Feb 23 18:32:04.538: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.965587594s
Feb 23 18:32:05.542: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.961791129s
Feb 23 18:32:06.545: INFO: Verifying statefulset ss doesn't scale past 3 for another 958.27641ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7447
Feb 23 18:32:07.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=statefulset-7447 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 18:32:07.686: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 23 18:32:07.686: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 23 18:32:07.686: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 23 18:32:07.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=statefulset-7447 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 18:32:07.813: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 23 18:32:07.813: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 23 18:32:07.813: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 23 18:32:07.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=statefulset-7447 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 18:32:07.916: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 23 18:32:07.916: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 23 18:32:07.916: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 23 18:32:07.919: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Feb 23 18:32:17.923: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 18:32:17.923: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 18:32:17.923: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 23 18:32:17.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=statefulset-7447 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 23 18:32:18.041: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 23 18:32:18.041: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 23 18:32:18.041: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 23 18:32:18.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=statefulset-7447 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 23 18:32:18.166: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 23 18:32:18.166: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 23 18:32:18.166: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 23 18:32:18.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=statefulset-7447 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 23 18:32:18.299: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 23 18:32:18.299: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 23 18:32:18.299: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 23 18:32:18.299: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 18:32:18.302: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb 23 18:32:28.308: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 23 18:32:28.308: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 23 18:32:28.308: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 23 18:32:28.319: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Feb 23 18:32:28.319: INFO: ss-0  worker00  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:37 +0000 UTC  }]
Feb 23 18:32:28.319: INFO: ss-1  worker01  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  }]
Feb 23 18:32:28.319: INFO: ss-2  worker00  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  }]
Feb 23 18:32:28.319: INFO: 
Feb 23 18:32:28.319: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 23 18:32:29.323: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Feb 23 18:32:29.323: INFO: ss-0  worker00  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:37 +0000 UTC  }]
Feb 23 18:32:29.323: INFO: ss-1  worker01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  }]
Feb 23 18:32:29.323: INFO: ss-2  worker00  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  }]
Feb 23 18:32:29.323: INFO: 
Feb 23 18:32:29.323: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 23 18:32:30.327: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Feb 23 18:32:30.327: INFO: ss-1  worker01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  }]
Feb 23 18:32:30.327: INFO: ss-2  worker00  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  }]
Feb 23 18:32:30.327: INFO: 
Feb 23 18:32:30.327: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 23 18:32:31.332: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Feb 23 18:32:31.332: INFO: ss-1  worker01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  }]
Feb 23 18:32:31.332: INFO: ss-2  worker00  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  }]
Feb 23 18:32:31.332: INFO: 
Feb 23 18:32:31.332: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 23 18:32:32.335: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Feb 23 18:32:32.335: INFO: ss-1  worker01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  }]
Feb 23 18:32:32.335: INFO: ss-2  worker00  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  }]
Feb 23 18:32:32.335: INFO: 
Feb 23 18:32:32.335: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 23 18:32:33.340: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Feb 23 18:32:33.340: INFO: ss-1  worker01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  }]
Feb 23 18:32:33.340: INFO: ss-2  worker00  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  }]
Feb 23 18:32:33.340: INFO: 
Feb 23 18:32:33.340: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 23 18:32:34.343: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Feb 23 18:32:34.343: INFO: ss-1  worker01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  }]
Feb 23 18:32:34.343: INFO: 
Feb 23 18:32:34.343: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 23 18:32:35.471: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Feb 23 18:32:35.471: INFO: ss-1  worker01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  }]
Feb 23 18:32:35.471: INFO: 
Feb 23 18:32:35.471: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 23 18:32:36.474: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Feb 23 18:32:36.474: INFO: ss-1  worker01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:32:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 18:31:57 +0000 UTC  }]
Feb 23 18:32:36.474: INFO: 
Feb 23 18:32:36.474: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 23 18:32:37.477: INFO: Verifying statefulset ss doesn't scale past 0 for another 840.528045ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7447
Feb 23 18:32:38.480: INFO: Scaling statefulset ss to 0
Feb 23 18:32:38.486: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Feb 23 18:32:38.488: INFO: Deleting all statefulset in ns statefulset-7447
Feb 23 18:32:38.492: INFO: Scaling statefulset ss to 0
Feb 23 18:32:38.504: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 18:32:38.509: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:32:38.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7447" for this suite.
Feb 23 18:32:44.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:32:44.624: INFO: namespace statefulset-7447 deletion completed in 6.099822376s

• [SLOW TEST:67.372 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:32:44.624: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 18:32:45.468: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 18:32:48.482: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:32:48.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2389" for this suite.
Feb 23 18:32:54.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:32:54.591: INFO: namespace webhook-2389 deletion completed in 6.07429787s
STEP: Destroying namespace "webhook-2389-markers" for this suite.
Feb 23 18:33:00.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:33:00.671: INFO: namespace webhook-2389-markers deletion completed in 6.079985511s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:16.059 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:33:00.684: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:33:11.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2708" for this suite.
Feb 23 18:33:18.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:33:18.076: INFO: namespace resourcequota-2708 deletion completed in 6.078052249s

• [SLOW TEST:17.392 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:33:18.076: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 23 18:33:18.113: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9f89380b-897e-4ea6-a952-6421f7cfc842" in namespace "projected-1799" to be "success or failure"
Feb 23 18:33:18.115: INFO: Pod "downwardapi-volume-9f89380b-897e-4ea6-a952-6421f7cfc842": Phase="Pending", Reason="", readiness=false. Elapsed: 2.549467ms
Feb 23 18:33:20.570: INFO: Pod "downwardapi-volume-9f89380b-897e-4ea6-a952-6421f7cfc842": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.456772408s
STEP: Saw pod success
Feb 23 18:33:20.570: INFO: Pod "downwardapi-volume-9f89380b-897e-4ea6-a952-6421f7cfc842" satisfied condition "success or failure"
Feb 23 18:33:20.575: INFO: Trying to get logs from node worker00 pod downwardapi-volume-9f89380b-897e-4ea6-a952-6421f7cfc842 container client-container: <nil>
STEP: delete the pod
Feb 23 18:33:20.612: INFO: Waiting for pod downwardapi-volume-9f89380b-897e-4ea6-a952-6421f7cfc842 to disappear
Feb 23 18:33:20.618: INFO: Pod downwardapi-volume-9f89380b-897e-4ea6-a952-6421f7cfc842 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:33:20.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1799" for this suite.
Feb 23 18:33:26.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:33:26.699: INFO: namespace projected-1799 deletion completed in 6.077312882s

• [SLOW TEST:8.623 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:33:26.699: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Feb 23 18:33:26.725: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
Feb 23 18:33:33.634: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:33:49.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-690" for this suite.
Feb 23 18:33:55.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:33:55.108: INFO: namespace crd-publish-openapi-690 deletion completed in 6.07339575s

• [SLOW TEST:28.409 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:33:55.109: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-3632
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3632
STEP: Deleting pre-stop pod
Feb 23 18:34:06.174: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:34:06.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3632" for this suite.
Feb 23 18:34:50.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:34:50.264: INFO: namespace prestop-3632 deletion completed in 44.072570731s

• [SLOW TEST:55.155 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:34:50.266: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Feb 23 18:34:50.300: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:35:09.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2205" for this suite.
Feb 23 18:35:15.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:35:15.174: INFO: namespace crd-publish-openapi-2205 deletion completed in 6.074062942s

• [SLOW TEST:24.908 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:35:15.181: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 18:35:15.466: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 23 18:35:17.474: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718079715, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718079715, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718079715, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718079715, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 18:35:20.488: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 18:35:20.490: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5907-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:35:26.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4745" for this suite.
Feb 23 18:35:32.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:35:32.738: INFO: namespace webhook-4745 deletion completed in 6.106123369s
STEP: Destroying namespace "webhook-4745-markers" for this suite.
Feb 23 18:35:38.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:35:38.815: INFO: namespace webhook-4745-markers deletion completed in 6.076611793s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:23.647 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:35:38.828: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 23 18:35:38.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-6953'
Feb 23 18:35:39.580: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 23 18:35:39.580: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Feb 23 18:35:39.588: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Feb 23 18:35:39.602: INFO: scanned /root for discovery docs: <nil>
Feb 23 18:35:39.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-6953'
Feb 23 18:35:55.360: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 23 18:35:55.360: INFO: stdout: "Created e2e-test-httpd-rc-2c8f499e68f44b6b45348d218185925e\nScaling up e2e-test-httpd-rc-2c8f499e68f44b6b45348d218185925e from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-2c8f499e68f44b6b45348d218185925e up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-2c8f499e68f44b6b45348d218185925e to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Feb 23 18:35:55.360: INFO: stdout: "Created e2e-test-httpd-rc-2c8f499e68f44b6b45348d218185925e\nScaling up e2e-test-httpd-rc-2c8f499e68f44b6b45348d218185925e from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-2c8f499e68f44b6b45348d218185925e up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-2c8f499e68f44b6b45348d218185925e to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Feb 23 18:35:55.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-6953'
Feb 23 18:35:55.432: INFO: stderr: ""
Feb 23 18:35:55.432: INFO: stdout: "e2e-test-httpd-rc-2c8f499e68f44b6b45348d218185925e-m9mdg "
Feb 23 18:35:55.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods e2e-test-httpd-rc-2c8f499e68f44b6b45348d218185925e-m9mdg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6953'
Feb 23 18:35:55.497: INFO: stderr: ""
Feb 23 18:35:55.497: INFO: stdout: "true"
Feb 23 18:35:55.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods e2e-test-httpd-rc-2c8f499e68f44b6b45348d218185925e-m9mdg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6953'
Feb 23 18:35:55.547: INFO: stderr: ""
Feb 23 18:35:55.547: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Feb 23 18:35:55.547: INFO: e2e-test-httpd-rc-2c8f499e68f44b6b45348d218185925e-m9mdg is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Feb 23 18:35:55.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 delete rc e2e-test-httpd-rc --namespace=kubectl-6953'
Feb 23 18:35:55.606: INFO: stderr: ""
Feb 23 18:35:55.606: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:35:55.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6953" for this suite.
Feb 23 18:36:07.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:36:07.718: INFO: namespace kubectl-6953 deletion completed in 12.106847221s

• [SLOW TEST:28.890 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:36:07.718: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-86604e23-829b-43af-bc4a-bc8e80d0f9af
STEP: Creating a pod to test consume secrets
Feb 23 18:36:07.807: INFO: Waiting up to 5m0s for pod "pod-secrets-96338b96-4374-48f2-b9c4-fcfe0d375fa0" in namespace "secrets-8800" to be "success or failure"
Feb 23 18:36:07.813: INFO: Pod "pod-secrets-96338b96-4374-48f2-b9c4-fcfe0d375fa0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.348326ms
Feb 23 18:36:09.816: INFO: Pod "pod-secrets-96338b96-4374-48f2-b9c4-fcfe0d375fa0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008818753s
STEP: Saw pod success
Feb 23 18:36:09.816: INFO: Pod "pod-secrets-96338b96-4374-48f2-b9c4-fcfe0d375fa0" satisfied condition "success or failure"
Feb 23 18:36:09.818: INFO: Trying to get logs from node worker00 pod pod-secrets-96338b96-4374-48f2-b9c4-fcfe0d375fa0 container secret-volume-test: <nil>
STEP: delete the pod
Feb 23 18:36:09.847: INFO: Waiting for pod pod-secrets-96338b96-4374-48f2-b9c4-fcfe0d375fa0 to disappear
Feb 23 18:36:09.859: INFO: Pod pod-secrets-96338b96-4374-48f2-b9c4-fcfe0d375fa0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:36:09.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8800" for this suite.
Feb 23 18:36:15.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:36:15.940: INFO: namespace secrets-8800 deletion completed in 6.0774278s
STEP: Destroying namespace "secret-namespace-4952" for this suite.
Feb 23 18:36:21.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:36:22.029: INFO: namespace secret-namespace-4952 deletion completed in 6.089374312s

• [SLOW TEST:14.311 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:36:22.030: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 23 18:36:24.080: INFO: &Pod{ObjectMeta:{send-events-13c67907-c679-413e-b518-c78cf614bf7f  events-3824 /api/v1/namespaces/events-3824/pods/send-events-13c67907-c679-413e-b518-c78cf614bf7f 3462705b-4fbd-453c-8071-16f100ef3e6d 18489 0 2020-02-23 18:36:22 +0000 UTC <nil> <nil> map[name:foo time:57301729] map[cni.projectcalico.org/podIP:10.200.131.171/32] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2vdjn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2vdjn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2vdjn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:36:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:36:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:36:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:36:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:10.200.131.171,StartTime:2020-02-23 18:36:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 18:36:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:containerd://c6fb655572cca4a70b4588ce8defad3dd67f3341ea3c841c825997c14c356c20,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.131.171,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Feb 23 18:36:26.084: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 23 18:36:28.088: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:36:28.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3824" for this suite.
Feb 23 18:37:06.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:37:06.184: INFO: namespace events-3824 deletion completed in 38.079064692s

• [SLOW TEST:44.154 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:37:06.184: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 23 18:37:06.220: INFO: Waiting up to 5m0s for pod "pod-09de8b41-c289-4e54-98d9-19e9d1e828c7" in namespace "emptydir-9389" to be "success or failure"
Feb 23 18:37:06.223: INFO: Pod "pod-09de8b41-c289-4e54-98d9-19e9d1e828c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.418663ms
Feb 23 18:37:08.234: INFO: Pod "pod-09de8b41-c289-4e54-98d9-19e9d1e828c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013194036s
STEP: Saw pod success
Feb 23 18:37:08.234: INFO: Pod "pod-09de8b41-c289-4e54-98d9-19e9d1e828c7" satisfied condition "success or failure"
Feb 23 18:37:08.238: INFO: Trying to get logs from node worker00 pod pod-09de8b41-c289-4e54-98d9-19e9d1e828c7 container test-container: <nil>
STEP: delete the pod
Feb 23 18:37:08.256: INFO: Waiting for pod pod-09de8b41-c289-4e54-98d9-19e9d1e828c7 to disappear
Feb 23 18:37:08.259: INFO: Pod pod-09de8b41-c289-4e54-98d9-19e9d1e828c7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:37:08.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9389" for this suite.
Feb 23 18:37:14.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:37:14.334: INFO: namespace emptydir-9389 deletion completed in 6.072637393s

• [SLOW TEST:8.150 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:37:14.336: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 18:37:14.375: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 23 18:37:14.385: INFO: Number of nodes with available pods: 0
Feb 23 18:37:14.385: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 23 18:37:14.400: INFO: Number of nodes with available pods: 0
Feb 23 18:37:14.401: INFO: Node worker00 is running more than one daemon pod
Feb 23 18:37:15.406: INFO: Number of nodes with available pods: 0
Feb 23 18:37:15.406: INFO: Node worker00 is running more than one daemon pod
Feb 23 18:37:16.403: INFO: Number of nodes with available pods: 0
Feb 23 18:37:16.403: INFO: Node worker00 is running more than one daemon pod
Feb 23 18:37:17.404: INFO: Number of nodes with available pods: 1
Feb 23 18:37:17.404: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 23 18:37:17.417: INFO: Number of nodes with available pods: 1
Feb 23 18:37:17.417: INFO: Number of running nodes: 0, number of available pods: 1
Feb 23 18:37:18.422: INFO: Number of nodes with available pods: 0
Feb 23 18:37:18.422: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 23 18:37:18.430: INFO: Number of nodes with available pods: 0
Feb 23 18:37:18.430: INFO: Node worker00 is running more than one daemon pod
Feb 23 18:37:19.931: INFO: Number of nodes with available pods: 0
Feb 23 18:37:19.931: INFO: Node worker00 is running more than one daemon pod
Feb 23 18:37:20.434: INFO: Number of nodes with available pods: 0
Feb 23 18:37:20.434: INFO: Node worker00 is running more than one daemon pod
Feb 23 18:37:21.434: INFO: Number of nodes with available pods: 0
Feb 23 18:37:21.434: INFO: Node worker00 is running more than one daemon pod
Feb 23 18:37:22.435: INFO: Number of nodes with available pods: 0
Feb 23 18:37:22.435: INFO: Node worker00 is running more than one daemon pod
Feb 23 18:37:23.434: INFO: Number of nodes with available pods: 0
Feb 23 18:37:23.434: INFO: Node worker00 is running more than one daemon pod
Feb 23 18:37:24.433: INFO: Number of nodes with available pods: 0
Feb 23 18:37:24.433: INFO: Node worker00 is running more than one daemon pod
Feb 23 18:37:25.471: INFO: Number of nodes with available pods: 1
Feb 23 18:37:25.471: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2522, will wait for the garbage collector to delete the pods
Feb 23 18:37:25.546: INFO: Deleting DaemonSet.extensions daemon-set took: 11.167962ms
Feb 23 18:37:26.453: INFO: Terminating DaemonSet.extensions daemon-set pods took: 907.258474ms
Feb 23 18:37:29.655: INFO: Number of nodes with available pods: 0
Feb 23 18:37:29.655: INFO: Number of running nodes: 0, number of available pods: 0
Feb 23 18:37:29.658: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2522/daemonsets","resourceVersion":"18747"},"items":null}

Feb 23 18:37:29.663: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2522/pods","resourceVersion":"18747"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:37:29.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2522" for this suite.
Feb 23 18:37:35.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:37:35.755: INFO: namespace daemonsets-2522 deletion completed in 6.076297323s

• [SLOW TEST:21.420 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:37:35.755: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 23 18:37:38.906: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:37:38.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5567" for this suite.
Feb 23 18:37:44.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:37:45.008: INFO: namespace container-runtime-5567 deletion completed in 6.074281896s

• [SLOW TEST:9.252 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:37:45.008: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Feb 23 18:37:45.041: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:38:03.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2986" for this suite.
Feb 23 18:38:09.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:38:09.566: INFO: namespace pods-2986 deletion completed in 6.083548079s

• [SLOW TEST:24.558 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:38:09.566: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 23 18:38:09.625: INFO: Number of nodes with available pods: 0
Feb 23 18:38:09.625: INFO: Node worker00 is running more than one daemon pod
Feb 23 18:38:10.632: INFO: Number of nodes with available pods: 0
Feb 23 18:38:10.632: INFO: Node worker00 is running more than one daemon pod
Feb 23 18:38:11.631: INFO: Number of nodes with available pods: 2
Feb 23 18:38:11.632: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 23 18:38:11.653: INFO: Number of nodes with available pods: 1
Feb 23 18:38:11.653: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:38:12.660: INFO: Number of nodes with available pods: 1
Feb 23 18:38:12.660: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:38:13.660: INFO: Number of nodes with available pods: 2
Feb 23 18:38:13.660: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-530, will wait for the garbage collector to delete the pods
Feb 23 18:38:13.729: INFO: Deleting DaemonSet.extensions daemon-set took: 8.171398ms
Feb 23 18:38:14.629: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.215525ms
Feb 23 18:38:18.326: INFO: Number of nodes with available pods: 0
Feb 23 18:38:18.326: INFO: Number of running nodes: 0, number of available pods: 0
Feb 23 18:38:18.329: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-530/daemonsets","resourceVersion":"19003"},"items":null}

Feb 23 18:38:18.332: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-530/pods","resourceVersion":"19003"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:38:18.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-530" for this suite.
Feb 23 18:38:24.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:38:24.416: INFO: namespace daemonsets-530 deletion completed in 6.073515291s

• [SLOW TEST:14.849 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:38:24.416: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Feb 23 18:38:26.965: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4663 pod-service-account-793bb589-2451-41cd-be75-34f07ca47bd1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Feb 23 18:38:27.100: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4663 pod-service-account-793bb589-2451-41cd-be75-34f07ca47bd1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Feb 23 18:38:27.203: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4663 pod-service-account-793bb589-2451-41cd-be75-34f07ca47bd1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:38:27.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4663" for this suite.
Feb 23 18:38:33.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:38:33.401: INFO: namespace svcaccounts-4663 deletion completed in 6.084489429s

• [SLOW TEST:8.985 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:38:33.401: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 18:38:33.444: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 23 18:38:33.461: INFO: Number of nodes with available pods: 0
Feb 23 18:38:33.461: INFO: Node worker00 is running more than one daemon pod
Feb 23 18:38:34.468: INFO: Number of nodes with available pods: 0
Feb 23 18:38:34.468: INFO: Node worker00 is running more than one daemon pod
Feb 23 18:38:35.467: INFO: Number of nodes with available pods: 2
Feb 23 18:38:35.467: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 23 18:38:35.487: INFO: Wrong image for pod: daemon-set-dg56d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:35.487: INFO: Wrong image for pod: daemon-set-npftf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:36.492: INFO: Wrong image for pod: daemon-set-dg56d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:36.492: INFO: Wrong image for pod: daemon-set-npftf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:37.495: INFO: Wrong image for pod: daemon-set-dg56d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:37.495: INFO: Wrong image for pod: daemon-set-npftf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:38.494: INFO: Wrong image for pod: daemon-set-dg56d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:38.494: INFO: Wrong image for pod: daemon-set-npftf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:38.494: INFO: Pod daemon-set-npftf is not available
Feb 23 18:38:39.493: INFO: Wrong image for pod: daemon-set-dg56d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:39.493: INFO: Wrong image for pod: daemon-set-npftf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:39.493: INFO: Pod daemon-set-npftf is not available
Feb 23 18:38:40.495: INFO: Wrong image for pod: daemon-set-dg56d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:40.495: INFO: Wrong image for pod: daemon-set-npftf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:40.495: INFO: Pod daemon-set-npftf is not available
Feb 23 18:38:41.495: INFO: Wrong image for pod: daemon-set-dg56d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:41.495: INFO: Wrong image for pod: daemon-set-npftf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:41.495: INFO: Pod daemon-set-npftf is not available
Feb 23 18:38:42.493: INFO: Wrong image for pod: daemon-set-dg56d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:42.493: INFO: Wrong image for pod: daemon-set-npftf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:42.493: INFO: Pod daemon-set-npftf is not available
Feb 23 18:38:43.497: INFO: Pod daemon-set-6m5cr is not available
Feb 23 18:38:43.497: INFO: Wrong image for pod: daemon-set-dg56d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:44.496: INFO: Pod daemon-set-6m5cr is not available
Feb 23 18:38:44.496: INFO: Wrong image for pod: daemon-set-dg56d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:45.493: INFO: Wrong image for pod: daemon-set-dg56d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:46.493: INFO: Wrong image for pod: daemon-set-dg56d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:46.493: INFO: Pod daemon-set-dg56d is not available
Feb 23 18:38:47.494: INFO: Wrong image for pod: daemon-set-dg56d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:47.494: INFO: Pod daemon-set-dg56d is not available
Feb 23 18:38:48.493: INFO: Wrong image for pod: daemon-set-dg56d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:48.493: INFO: Pod daemon-set-dg56d is not available
Feb 23 18:38:49.493: INFO: Wrong image for pod: daemon-set-dg56d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:49.493: INFO: Pod daemon-set-dg56d is not available
Feb 23 18:38:50.494: INFO: Wrong image for pod: daemon-set-dg56d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:50.494: INFO: Pod daemon-set-dg56d is not available
Feb 23 18:38:51.494: INFO: Wrong image for pod: daemon-set-dg56d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:51.494: INFO: Pod daemon-set-dg56d is not available
Feb 23 18:38:52.495: INFO: Wrong image for pod: daemon-set-dg56d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:52.495: INFO: Pod daemon-set-dg56d is not available
Feb 23 18:38:53.494: INFO: Wrong image for pod: daemon-set-dg56d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:53.494: INFO: Pod daemon-set-dg56d is not available
Feb 23 18:38:54.495: INFO: Wrong image for pod: daemon-set-dg56d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:54.495: INFO: Pod daemon-set-dg56d is not available
Feb 23 18:38:55.496: INFO: Wrong image for pod: daemon-set-dg56d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:55.496: INFO: Pod daemon-set-dg56d is not available
Feb 23 18:38:56.493: INFO: Wrong image for pod: daemon-set-dg56d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 18:38:56.493: INFO: Pod daemon-set-dg56d is not available
Feb 23 18:38:57.494: INFO: Pod daemon-set-gcpj9 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 23 18:38:57.501: INFO: Number of nodes with available pods: 1
Feb 23 18:38:57.501: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:38:58.514: INFO: Number of nodes with available pods: 1
Feb 23 18:38:58.514: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:38:59.507: INFO: Number of nodes with available pods: 1
Feb 23 18:38:59.507: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:39:00.508: INFO: Number of nodes with available pods: 1
Feb 23 18:39:00.508: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:39:01.505: INFO: Number of nodes with available pods: 1
Feb 23 18:39:01.505: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:39:02.506: INFO: Number of nodes with available pods: 1
Feb 23 18:39:02.506: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:39:03.506: INFO: Number of nodes with available pods: 1
Feb 23 18:39:03.506: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:39:04.508: INFO: Number of nodes with available pods: 1
Feb 23 18:39:04.508: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:39:05.506: INFO: Number of nodes with available pods: 1
Feb 23 18:39:05.506: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:39:06.507: INFO: Number of nodes with available pods: 1
Feb 23 18:39:06.507: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:39:07.507: INFO: Number of nodes with available pods: 1
Feb 23 18:39:07.507: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:39:08.515: INFO: Number of nodes with available pods: 2
Feb 23 18:39:08.515: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8968, will wait for the garbage collector to delete the pods
Feb 23 18:39:08.593: INFO: Deleting DaemonSet.extensions daemon-set took: 8.845875ms
Feb 23 18:39:09.495: INFO: Terminating DaemonSet.extensions daemon-set pods took: 901.913187ms
Feb 23 18:39:16.997: INFO: Number of nodes with available pods: 0
Feb 23 18:39:16.997: INFO: Number of running nodes: 0, number of available pods: 0
Feb 23 18:39:17.000: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8968/daemonsets","resourceVersion":"19309"},"items":null}

Feb 23 18:39:17.002: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8968/pods","resourceVersion":"19309"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:39:17.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8968" for this suite.
Feb 23 18:39:23.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:39:23.100: INFO: namespace daemonsets-8968 deletion completed in 6.083118056s

• [SLOW TEST:49.699 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:39:23.101: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-5448
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 23 18:39:23.129: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 23 18:39:45.204: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.200.131.183:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5448 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 18:39:45.204: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
Feb 23 18:39:45.274: INFO: Found all expected endpoints: [netserver-0]
Feb 23 18:39:45.276: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.200.5.36:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5448 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 18:39:45.276: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
Feb 23 18:39:45.336: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:39:45.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5448" for this suite.
Feb 23 18:39:57.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:39:57.415: INFO: namespace pod-network-test-5448 deletion completed in 12.076698434s

• [SLOW TEST:34.315 seconds]
[sig-network] Networking
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:39:57.415: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 18:39:57.757: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 18:40:01.159: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:40:13.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6080" for this suite.
Feb 23 18:40:19.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:40:19.373: INFO: namespace webhook-6080 deletion completed in 6.11377216s
STEP: Destroying namespace "webhook-6080-markers" for this suite.
Feb 23 18:40:25.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:40:25.452: INFO: namespace webhook-6080-markers deletion completed in 6.079058591s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:28.048 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:40:25.463: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 23 18:40:25.515: INFO: Number of nodes with available pods: 0
Feb 23 18:40:25.515: INFO: Node worker00 is running more than one daemon pod
Feb 23 18:40:26.520: INFO: Number of nodes with available pods: 0
Feb 23 18:40:26.520: INFO: Node worker00 is running more than one daemon pod
Feb 23 18:40:27.528: INFO: Number of nodes with available pods: 2
Feb 23 18:40:27.528: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 23 18:40:27.543: INFO: Number of nodes with available pods: 1
Feb 23 18:40:27.543: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:40:28.786: INFO: Number of nodes with available pods: 1
Feb 23 18:40:28.786: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:40:29.550: INFO: Number of nodes with available pods: 1
Feb 23 18:40:29.550: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:40:30.553: INFO: Number of nodes with available pods: 1
Feb 23 18:40:30.553: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:40:31.554: INFO: Number of nodes with available pods: 1
Feb 23 18:40:31.554: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:40:32.552: INFO: Number of nodes with available pods: 1
Feb 23 18:40:32.552: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:40:33.551: INFO: Number of nodes with available pods: 1
Feb 23 18:40:33.551: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:40:34.549: INFO: Number of nodes with available pods: 1
Feb 23 18:40:34.549: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:40:35.548: INFO: Number of nodes with available pods: 1
Feb 23 18:40:35.548: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:40:36.550: INFO: Number of nodes with available pods: 1
Feb 23 18:40:36.550: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:40:37.548: INFO: Number of nodes with available pods: 1
Feb 23 18:40:37.548: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:40:38.548: INFO: Number of nodes with available pods: 1
Feb 23 18:40:38.548: INFO: Node worker01 is running more than one daemon pod
Feb 23 18:40:39.551: INFO: Number of nodes with available pods: 2
Feb 23 18:40:39.551: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8416, will wait for the garbage collector to delete the pods
Feb 23 18:40:39.619: INFO: Deleting DaemonSet.extensions daemon-set took: 9.132415ms
Feb 23 18:40:39.719: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.12582ms
Feb 23 18:40:47.022: INFO: Number of nodes with available pods: 0
Feb 23 18:40:47.022: INFO: Number of running nodes: 0, number of available pods: 0
Feb 23 18:40:47.025: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8416/daemonsets","resourceVersion":"19769"},"items":null}

Feb 23 18:40:47.028: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8416/pods","resourceVersion":"19769"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:40:47.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8416" for this suite.
Feb 23 18:40:53.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:40:53.126: INFO: namespace daemonsets-8416 deletion completed in 6.087390284s

• [SLOW TEST:27.662 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:40:53.126: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-ts24
STEP: Creating a pod to test atomic-volume-subpath
Feb 23 18:40:53.174: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-ts24" in namespace "subpath-4487" to be "success or failure"
Feb 23 18:40:53.178: INFO: Pod "pod-subpath-test-downwardapi-ts24": Phase="Pending", Reason="", readiness=false. Elapsed: 4.651503ms
Feb 23 18:40:55.181: INFO: Pod "pod-subpath-test-downwardapi-ts24": Phase="Running", Reason="", readiness=true. Elapsed: 2.007736113s
Feb 23 18:40:57.185: INFO: Pod "pod-subpath-test-downwardapi-ts24": Phase="Running", Reason="", readiness=true. Elapsed: 4.011615402s
Feb 23 18:40:59.189: INFO: Pod "pod-subpath-test-downwardapi-ts24": Phase="Running", Reason="", readiness=true. Elapsed: 6.015683735s
Feb 23 18:41:01.195: INFO: Pod "pod-subpath-test-downwardapi-ts24": Phase="Running", Reason="", readiness=true. Elapsed: 8.021687121s
Feb 23 18:41:03.198: INFO: Pod "pod-subpath-test-downwardapi-ts24": Phase="Running", Reason="", readiness=true. Elapsed: 10.024797528s
Feb 23 18:41:05.201: INFO: Pod "pod-subpath-test-downwardapi-ts24": Phase="Running", Reason="", readiness=true. Elapsed: 12.027837019s
Feb 23 18:41:07.206: INFO: Pod "pod-subpath-test-downwardapi-ts24": Phase="Running", Reason="", readiness=true. Elapsed: 14.032518786s
Feb 23 18:41:09.209: INFO: Pod "pod-subpath-test-downwardapi-ts24": Phase="Running", Reason="", readiness=true. Elapsed: 16.035535496s
Feb 23 18:41:11.212: INFO: Pod "pod-subpath-test-downwardapi-ts24": Phase="Running", Reason="", readiness=true. Elapsed: 18.038320924s
Feb 23 18:41:13.215: INFO: Pod "pod-subpath-test-downwardapi-ts24": Phase="Running", Reason="", readiness=true. Elapsed: 20.04132637s
Feb 23 18:41:15.218: INFO: Pod "pod-subpath-test-downwardapi-ts24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.043894997s
STEP: Saw pod success
Feb 23 18:41:15.218: INFO: Pod "pod-subpath-test-downwardapi-ts24" satisfied condition "success or failure"
Feb 23 18:41:15.220: INFO: Trying to get logs from node worker00 pod pod-subpath-test-downwardapi-ts24 container test-container-subpath-downwardapi-ts24: <nil>
STEP: delete the pod
Feb 23 18:41:15.241: INFO: Waiting for pod pod-subpath-test-downwardapi-ts24 to disappear
Feb 23 18:41:15.247: INFO: Pod pod-subpath-test-downwardapi-ts24 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-ts24
Feb 23 18:41:15.247: INFO: Deleting pod "pod-subpath-test-downwardapi-ts24" in namespace "subpath-4487"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:41:15.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4487" for this suite.
Feb 23 18:41:21.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:41:21.331: INFO: namespace subpath-4487 deletion completed in 6.07924231s

• [SLOW TEST:28.205 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:41:21.332: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 18:41:21.360: INFO: Creating deployment "webserver-deployment"
Feb 23 18:41:21.366: INFO: Waiting for observed generation 1
Feb 23 18:41:23.372: INFO: Waiting for all required pods to come up
Feb 23 18:41:23.376: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 23 18:41:25.385: INFO: Waiting for deployment "webserver-deployment" to complete
Feb 23 18:41:25.389: INFO: Updating deployment "webserver-deployment" with a non-existent image
Feb 23 18:41:25.395: INFO: Updating deployment webserver-deployment
Feb 23 18:41:25.395: INFO: Waiting for observed generation 2
Feb 23 18:41:27.403: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 23 18:41:27.407: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 23 18:41:27.409: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 23 18:41:27.416: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 23 18:41:27.416: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 23 18:41:27.418: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 23 18:41:27.422: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Feb 23 18:41:27.422: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Feb 23 18:41:27.430: INFO: Updating deployment webserver-deployment
Feb 23 18:41:27.430: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Feb 23 18:41:27.463: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 23 18:41:29.542: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Feb 23 18:41:29.635: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5560 /apis/apps/v1/namespaces/deployment-5560/deployments/webserver-deployment b6991225-edd4-41f0-9ecb-0a1d53468adf 20253 3 2020-02-23 18:41:21 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002d042c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-02-23 18:41:27 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-02-23 18:41:27 +0000 UTC,LastTransitionTime:2020-02-23 18:41:21 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Feb 23 18:41:29.650: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-5560 /apis/apps/v1/namespaces/deployment-5560/replicasets/webserver-deployment-c7997dcc8 9aac8ef9-584e-4197-a6c1-e678f871f7e3 20251 3 2020-02-23 18:41:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment b6991225-edd4-41f0-9ecb-0a1d53468adf 0xc002d047f7 0xc002d047f8}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002d04868 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 23 18:41:29.650: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Feb 23 18:41:29.650: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-5560 /apis/apps/v1/namespaces/deployment-5560/replicasets/webserver-deployment-595b5b9587 db391a11-6366-4b06-860e-874f349870ec 20393 3 2020-02-23 18:41:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment b6991225-edd4-41f0-9ecb-0a1d53468adf 0xc002d04737 0xc002d04738}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002d04798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:9,AvailableReplicas:9,Conditions:[]ReplicaSetCondition{},},}
Feb 23 18:41:29.690: INFO: Pod "webserver-deployment-595b5b9587-442lk" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-442lk webserver-deployment-595b5b9587- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-595b5b9587-442lk 21bcd5a0-74e1-4db0-ad84-eb661d2c1469 20377 0 2020-02-23 18:41:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.5.52/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 db391a11-6366-4b06-860e-874f349870ec 0xc002ac8dd7 0xc002ac8dd8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.691: INFO: Pod "webserver-deployment-595b5b9587-5kjgj" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5kjgj webserver-deployment-595b5b9587- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-595b5b9587-5kjgj 97d3a2ba-0818-4c30-a686-ec245e219878 20256 0 2020-02-23 18:41:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 db391a11-6366-4b06-860e-874f349870ec 0xc002ac8ef0 0xc002ac8ef1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:,StartTime:2020-02-23 18:41:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.691: INFO: Pod "webserver-deployment-595b5b9587-64lkf" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-64lkf webserver-deployment-595b5b9587- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-595b5b9587-64lkf 35bce938-4ae8-4e5c-9359-9ed251c960f7 20067 0 2020-02-23 18:41:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.5.42/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 db391a11-6366-4b06-860e-874f349870ec 0xc002ac9057 0xc002ac9058}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:10.200.5.42,StartTime:2020-02-23 18:41:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 18:41:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://078b7dc9ae625eb356ce21d1cf0785e982b8db34c388d44adee65a38d80820fd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.5.42,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.691: INFO: Pod "webserver-deployment-595b5b9587-68f27" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-68f27 webserver-deployment-595b5b9587- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-595b5b9587-68f27 d66a4edc-c36e-4ffb-a792-af571acb73c2 20052 0 2020-02-23 18:41:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.131.189/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 db391a11-6366-4b06-860e-874f349870ec 0xc002ac91d0 0xc002ac91d1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:10.200.131.189,StartTime:2020-02-23 18:41:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 18:41:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://6c7b6f72f51b434cc3c0792a61e2d4aa09ab2ce956a77a708b8577a4df976155,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.131.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.691: INFO: Pod "webserver-deployment-595b5b9587-7spsw" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-7spsw webserver-deployment-595b5b9587- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-595b5b9587-7spsw 524d61f9-7faf-4d89-bda5-7c1eeb052a68 20378 0 2020-02-23 18:41:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.131.143/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 db391a11-6366-4b06-860e-874f349870ec 0xc002ac9347 0xc002ac9348}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:,StartTime:2020-02-23 18:41:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.691: INFO: Pod "webserver-deployment-595b5b9587-8ksrj" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-8ksrj webserver-deployment-595b5b9587- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-595b5b9587-8ksrj 4dc5a651-7a71-445f-ace6-87b6e299cec2 20267 0 2020-02-23 18:41:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 db391a11-6366-4b06-860e-874f349870ec 0xc002ac94a7 0xc002ac94a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:,StartTime:2020-02-23 18:41:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.692: INFO: Pod "webserver-deployment-595b5b9587-97nmt" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-97nmt webserver-deployment-595b5b9587- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-595b5b9587-97nmt c3088d76-16fc-44d5-9478-f2c44395a8c5 20035 0 2020-02-23 18:41:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.5.39/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 db391a11-6366-4b06-860e-874f349870ec 0xc002ac9637 0xc002ac9638}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:10.200.5.39,StartTime:2020-02-23 18:41:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 18:41:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://ac93e1d487737d8979a1cb49f145cbbe192442ea08ee747351ae7cfedfd08c0b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.5.39,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.692: INFO: Pod "webserver-deployment-595b5b9587-bggz4" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bggz4 webserver-deployment-595b5b9587- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-595b5b9587-bggz4 d8037a43-af31-4e22-98db-7cf774f43ddc 20344 0 2020-02-23 18:41:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 db391a11-6366-4b06-860e-874f349870ec 0xc002ac97b0 0xc002ac97b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:,StartTime:2020-02-23 18:41:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.692: INFO: Pod "webserver-deployment-595b5b9587-bpqmg" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bpqmg webserver-deployment-595b5b9587- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-595b5b9587-bpqmg 91259aa1-ab13-4f61-8de7-e6193490d918 20284 0 2020-02-23 18:41:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.131.139/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 db391a11-6366-4b06-860e-874f349870ec 0xc002ac9907 0xc002ac9908}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:,StartTime:2020-02-23 18:41:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.692: INFO: Pod "webserver-deployment-595b5b9587-dmqbg" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-dmqbg webserver-deployment-595b5b9587- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-595b5b9587-dmqbg 8334a7d1-a2e4-4da2-b481-cc8e0a36611b 20073 0 2020-02-23 18:41:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.5.43/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 db391a11-6366-4b06-860e-874f349870ec 0xc002ac9a87 0xc002ac9a88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:10.200.5.43,StartTime:2020-02-23 18:41:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 18:41:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://f75dc4c1a54e3dc9f78fb8870f2186a32a5227bda8ebc38d342be6d57a51e508,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.5.43,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.692: INFO: Pod "webserver-deployment-595b5b9587-h6v6x" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-h6v6x webserver-deployment-595b5b9587- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-595b5b9587-h6v6x eec0a5d6-11df-4eb3-be41-5d6dde2e810f 20338 0 2020-02-23 18:41:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.131.142/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 db391a11-6366-4b06-860e-874f349870ec 0xc002ac9c20 0xc002ac9c21}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:,StartTime:2020-02-23 18:41:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.692: INFO: Pod "webserver-deployment-595b5b9587-hsm8f" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-hsm8f webserver-deployment-595b5b9587- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-595b5b9587-hsm8f b5bede99-cdc1-4bbf-865f-247f70eae8c9 20302 0 2020-02-23 18:41:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.131.140/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 db391a11-6366-4b06-860e-874f349870ec 0xc002ac9d77 0xc002ac9d78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:,StartTime:2020-02-23 18:41:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.693: INFO: Pod "webserver-deployment-595b5b9587-htwqm" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-htwqm webserver-deployment-595b5b9587- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-595b5b9587-htwqm 75e3d512-91e8-471f-8c5d-e0213e042b55 20071 0 2020-02-23 18:41:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.5.41/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 db391a11-6366-4b06-860e-874f349870ec 0xc002ac9ef7 0xc002ac9ef8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:10.200.5.41,StartTime:2020-02-23 18:41:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 18:41:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://81135e40183a4e2b80014e0eadc5d96b978d8d669496294d81b36aa46061b22f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.5.41,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.693: INFO: Pod "webserver-deployment-595b5b9587-pdcjg" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-pdcjg webserver-deployment-595b5b9587- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-595b5b9587-pdcjg 265beb1f-b62a-448f-b19e-e47da82463b0 20056 0 2020-02-23 18:41:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.131.132/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 db391a11-6366-4b06-860e-874f349870ec 0xc00386e1a0 0xc00386e1a1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:10.200.131.132,StartTime:2020-02-23 18:41:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 18:41:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://e1a8d9fc7589f1ad2630342ac721dc31d7f45c1124e0ce5f71ad15630fb534f6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.131.132,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.693: INFO: Pod "webserver-deployment-595b5b9587-pt5kq" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-pt5kq webserver-deployment-595b5b9587- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-595b5b9587-pt5kq ce5506bb-083e-4422-956e-26b44e215ad0 20049 0 2020-02-23 18:41:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.131.191/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 db391a11-6366-4b06-860e-874f349870ec 0xc00386e317 0xc00386e318}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:10.200.131.191,StartTime:2020-02-23 18:41:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 18:41:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://665c77fe601d7c8ddad393aa126360d1b1b36a8c5d84f349dc41de3b96578067,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.131.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.693: INFO: Pod "webserver-deployment-595b5b9587-q45fk" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-q45fk webserver-deployment-595b5b9587- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-595b5b9587-q45fk c35dad03-1841-493e-ba62-16dd16e71f6b 20295 0 2020-02-23 18:41:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.5.47/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 db391a11-6366-4b06-860e-874f349870ec 0xc00386e4b7 0xc00386e4b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:,StartTime:2020-02-23 18:41:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.693: INFO: Pod "webserver-deployment-595b5b9587-qf5hl" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qf5hl webserver-deployment-595b5b9587- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-595b5b9587-qf5hl 341aacd3-f31d-4e6f-980f-17881a413842 20062 0 2020-02-23 18:41:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.131.131/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 db391a11-6366-4b06-860e-874f349870ec 0xc00386e617 0xc00386e618}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:10.200.131.131,StartTime:2020-02-23 18:41:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 18:41:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://5969f5a50234b2b8b5c64e885fc3ddf3eeb159560be5a873bbbe84edd2bf92f4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.131.131,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.694: INFO: Pod "webserver-deployment-595b5b9587-rwxft" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rwxft webserver-deployment-595b5b9587- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-595b5b9587-rwxft efb030dd-c531-4c92-84cf-f7b2ddde7d3f 20389 0 2020-02-23 18:41:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.131.145/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 db391a11-6366-4b06-860e-874f349870ec 0xc00386e797 0xc00386e798}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:,StartTime:2020-02-23 18:41:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.694: INFO: Pod "webserver-deployment-595b5b9587-vq7f5" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-vq7f5 webserver-deployment-595b5b9587- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-595b5b9587-vq7f5 2e61ef6c-508e-4aca-8590-5c4a63ed8dca 20360 0 2020-02-23 18:41:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.5.51/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 db391a11-6366-4b06-860e-874f349870ec 0xc00386e907 0xc00386e908}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:,StartTime:2020-02-23 18:41:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.694: INFO: Pod "webserver-deployment-595b5b9587-vsndk" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-vsndk webserver-deployment-595b5b9587- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-595b5b9587-vsndk 430642c2-af77-49f8-b88d-ab49c9da22f4 20387 0 2020-02-23 18:41:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.131.137/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 db391a11-6366-4b06-860e-874f349870ec 0xc00386ea67 0xc00386ea68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:10.200.131.137,StartTime:2020-02-23 18:41:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 18:41:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://42b4a31e8def4e4cd93efff03f688dc74fc1f75ad3eacee5e67e2e13468728a9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.131.137,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.694: INFO: Pod "webserver-deployment-c7997dcc8-2fs82" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-2fs82 webserver-deployment-c7997dcc8- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-c7997dcc8-2fs82 40a085d1-0a46-40eb-a639-dc1830670f4a 20154 0 2020-02-23 18:41:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.200.131.136/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9aac8ef9-584e-4197-a6c1-e678f871f7e3 0xc00386ebe7 0xc00386ebe8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:,StartTime:2020-02-23 18:41:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.694: INFO: Pod "webserver-deployment-c7997dcc8-2rjxz" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-2rjxz webserver-deployment-c7997dcc8- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-c7997dcc8-2rjxz 6539888c-5314-4465-87c8-3c0f64a8ca22 20314 0 2020-02-23 18:41:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.200.5.48/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9aac8ef9-584e-4197-a6c1-e678f871f7e3 0xc00386ed77 0xc00386ed78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:,StartTime:2020-02-23 18:41:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.694: INFO: Pod "webserver-deployment-c7997dcc8-7zplj" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-7zplj webserver-deployment-c7997dcc8- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-c7997dcc8-7zplj ee327826-6a90-4675-9e91-660d45a38835 20145 0 2020-02-23 18:41:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.200.131.135/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9aac8ef9-584e-4197-a6c1-e678f871f7e3 0xc00386eef7 0xc00386eef8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:,StartTime:2020-02-23 18:41:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.694: INFO: Pod "webserver-deployment-c7997dcc8-h5jj2" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-h5jj2 webserver-deployment-c7997dcc8- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-c7997dcc8-h5jj2 f0f1e43a-cc2a-47af-a1c7-3b9086e9c921 20286 0 2020-02-23 18:41:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.200.5.46/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9aac8ef9-584e-4197-a6c1-e678f871f7e3 0xc00386f087 0xc00386f088}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:,StartTime:2020-02-23 18:41:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.695: INFO: Pod "webserver-deployment-c7997dcc8-lgzzd" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-lgzzd webserver-deployment-c7997dcc8- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-c7997dcc8-lgzzd 3bf9497a-9ca3-4b9d-a86b-d0fc6088db47 20390 0 2020-02-23 18:41:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.200.5.50/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9aac8ef9-584e-4197-a6c1-e678f871f7e3 0xc00386f217 0xc00386f218}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:,StartTime:2020-02-23 18:41:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.695: INFO: Pod "webserver-deployment-c7997dcc8-n7jqb" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-n7jqb webserver-deployment-c7997dcc8- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-c7997dcc8-n7jqb 16cf1b13-f931-4379-bbe4-9cb0541a4e82 20326 0 2020-02-23 18:41:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.200.131.141/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9aac8ef9-584e-4197-a6c1-e678f871f7e3 0xc00386f3a7 0xc00386f3a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:,StartTime:2020-02-23 18:41:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.695: INFO: Pod "webserver-deployment-c7997dcc8-qb66g" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qb66g webserver-deployment-c7997dcc8- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-c7997dcc8-qb66g f20086d7-c904-47e1-8ae5-6c3fe5920bb3 20153 0 2020-02-23 18:41:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.200.5.45/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9aac8ef9-584e-4197-a6c1-e678f871f7e3 0xc00386f537 0xc00386f538}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:,StartTime:2020-02-23 18:41:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.695: INFO: Pod "webserver-deployment-c7997dcc8-tn22n" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-tn22n webserver-deployment-c7997dcc8- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-c7997dcc8-tn22n 05f05336-ce61-45f1-8475-848cc4dba372 20146 0 2020-02-23 18:41:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.200.5.44/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9aac8ef9-584e-4197-a6c1-e678f871f7e3 0xc00386f7a7 0xc00386f7a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:,StartTime:2020-02-23 18:41:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.695: INFO: Pod "webserver-deployment-c7997dcc8-wd544" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-wd544 webserver-deployment-c7997dcc8- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-c7997dcc8-wd544 e72267bb-3a15-4cf1-b5db-ba66b90b1623 20342 0 2020-02-23 18:41:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.200.131.144/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9aac8ef9-584e-4197-a6c1-e678f871f7e3 0xc00386f927 0xc00386f928}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:,StartTime:2020-02-23 18:41:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.695: INFO: Pod "webserver-deployment-c7997dcc8-xrc26" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-xrc26 webserver-deployment-c7997dcc8- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-c7997dcc8-xrc26 b067379c-f3f6-4c51-a64b-ba6ab6d1f4f3 20331 0 2020-02-23 18:41:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.200.5.49/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9aac8ef9-584e-4197-a6c1-e678f871f7e3 0xc00386fae7 0xc00386fae8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:,StartTime:2020-02-23 18:41:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.695: INFO: Pod "webserver-deployment-c7997dcc8-xzl4k" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-xzl4k webserver-deployment-c7997dcc8- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-c7997dcc8-xzl4k 35a8dc1f-4d1e-493b-bf65-a361998ac7e5 20159 0 2020-02-23 18:41:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.200.131.138/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9aac8ef9-584e-4197-a6c1-e678f871f7e3 0xc00386fc87 0xc00386fc88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:,StartTime:2020-02-23 18:41:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.700: INFO: Pod "webserver-deployment-c7997dcc8-z4ssq" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-z4ssq webserver-deployment-c7997dcc8- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-c7997dcc8-z4ssq 9d34921d-d22f-47ca-a33e-519eecbf0992 20265 0 2020-02-23 18:41:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9aac8ef9-584e-4197-a6c1-e678f871f7e3 0xc00386fe17 0xc00386fe18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:,StartTime:2020-02-23 18:41:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 18:41:29.700: INFO: Pod "webserver-deployment-c7997dcc8-ztl4f" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-ztl4f webserver-deployment-c7997dcc8- deployment-5560 /api/v1/namespaces/deployment-5560/pods/webserver-deployment-c7997dcc8-ztl4f 5d13f5b8-f986-4e62-b12f-634e19b957ea 20396 0 2020-02-23 18:41:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.200.5.53/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 9aac8ef9-584e-4197-a6c1-e678f871f7e3 0xc00386ffa7 0xc00386ffa8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-trlqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-trlqw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-trlqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 18:41:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:,StartTime:2020-02-23 18:41:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:41:29.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5560" for this suite.
Feb 23 18:41:37.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:41:38.064: INFO: namespace deployment-5560 deletion completed in 8.33353927s

• [SLOW TEST:16.733 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:41:38.065: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 23 18:41:38.130: INFO: Waiting up to 5m0s for pod "pod-95e42709-c369-41a0-8301-6b20a7d8067b" in namespace "emptydir-3855" to be "success or failure"
Feb 23 18:41:38.142: INFO: Pod "pod-95e42709-c369-41a0-8301-6b20a7d8067b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.706862ms
Feb 23 18:41:40.146: INFO: Pod "pod-95e42709-c369-41a0-8301-6b20a7d8067b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016138753s
STEP: Saw pod success
Feb 23 18:41:40.146: INFO: Pod "pod-95e42709-c369-41a0-8301-6b20a7d8067b" satisfied condition "success or failure"
Feb 23 18:41:40.149: INFO: Trying to get logs from node worker00 pod pod-95e42709-c369-41a0-8301-6b20a7d8067b container test-container: <nil>
STEP: delete the pod
Feb 23 18:41:40.168: INFO: Waiting for pod pod-95e42709-c369-41a0-8301-6b20a7d8067b to disappear
Feb 23 18:41:40.171: INFO: Pod pod-95e42709-c369-41a0-8301-6b20a7d8067b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:41:40.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3855" for this suite.
Feb 23 18:41:46.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:41:46.255: INFO: namespace emptydir-3855 deletion completed in 6.080651926s

• [SLOW TEST:8.191 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:41:46.257: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-5acddc28-1c1c-49e3-af59-7797d2ae5300
STEP: Creating a pod to test consume secrets
Feb 23 18:41:46.292: INFO: Waiting up to 5m0s for pod "pod-secrets-7433e9b1-b841-47d5-a5d7-adb0eefcc676" in namespace "secrets-4399" to be "success or failure"
Feb 23 18:41:46.297: INFO: Pod "pod-secrets-7433e9b1-b841-47d5-a5d7-adb0eefcc676": Phase="Pending", Reason="", readiness=false. Elapsed: 4.545003ms
Feb 23 18:41:48.301: INFO: Pod "pod-secrets-7433e9b1-b841-47d5-a5d7-adb0eefcc676": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00836225s
STEP: Saw pod success
Feb 23 18:41:48.301: INFO: Pod "pod-secrets-7433e9b1-b841-47d5-a5d7-adb0eefcc676" satisfied condition "success or failure"
Feb 23 18:41:48.304: INFO: Trying to get logs from node worker00 pod pod-secrets-7433e9b1-b841-47d5-a5d7-adb0eefcc676 container secret-volume-test: <nil>
STEP: delete the pod
Feb 23 18:41:48.322: INFO: Waiting for pod pod-secrets-7433e9b1-b841-47d5-a5d7-adb0eefcc676 to disappear
Feb 23 18:41:48.325: INFO: Pod pod-secrets-7433e9b1-b841-47d5-a5d7-adb0eefcc676 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:41:48.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4399" for this suite.
Feb 23 18:41:54.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:41:54.416: INFO: namespace secrets-4399 deletion completed in 6.088339652s

• [SLOW TEST:8.159 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:41:54.417: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Feb 23 18:41:54.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 cluster-info'
Feb 23 18:41:54.500: INFO: stderr: ""
Feb 23 18:41:54.500: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.32.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.32.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:41:54.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6167" for this suite.
Feb 23 18:42:00.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:42:00.594: INFO: namespace kubectl-6167 deletion completed in 6.090989167s

• [SLOW TEST:6.177 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:42:00.595: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 23 18:42:00.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-1152'
Feb 23 18:42:00.694: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 23 18:42:00.694: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Feb 23 18:42:00.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 delete jobs e2e-test-httpd-job --namespace=kubectl-1152'
Feb 23 18:42:00.787: INFO: stderr: ""
Feb 23 18:42:00.787: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:42:00.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1152" for this suite.
Feb 23 18:42:28.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:42:28.875: INFO: namespace kubectl-1152 deletion completed in 28.077480935s

• [SLOW TEST:28.281 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:42:28.876: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:42:28.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3404" for this suite.
Feb 23 18:42:34.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:42:34.984: INFO: namespace custom-resource-definition-3404 deletion completed in 6.07504691s

• [SLOW TEST:6.109 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:42:34.985: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 18:42:53.030: INFO: Container started at 2020-02-23 18:42:35 +0000 UTC, pod became ready at 2020-02-23 18:42:51 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:42:53.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5160" for this suite.
Feb 23 18:43:05.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:43:05.109: INFO: namespace container-probe-5160 deletion completed in 12.075716682s

• [SLOW TEST:30.124 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:43:05.110: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 18:43:05.941: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 18:43:08.958: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:43:08.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1123" for this suite.
Feb 23 18:43:14.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:43:15.069: INFO: namespace webhook-1123 deletion completed in 6.102749513s
STEP: Destroying namespace "webhook-1123-markers" for this suite.
Feb 23 18:43:21.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:43:21.142: INFO: namespace webhook-1123-markers deletion completed in 6.072961182s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:16.043 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:43:21.154: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 18:43:21.206: INFO: Create a RollingUpdate DaemonSet
Feb 23 18:43:21.211: INFO: Check that daemon pods launch on every node of the cluster
Feb 23 18:43:21.220: INFO: Number of nodes with available pods: 0
Feb 23 18:43:21.220: INFO: Node worker00 is running more than one daemon pod
Feb 23 18:43:22.228: INFO: Number of nodes with available pods: 1
Feb 23 18:43:22.228: INFO: Node worker00 is running more than one daemon pod
Feb 23 18:43:23.229: INFO: Number of nodes with available pods: 2
Feb 23 18:43:23.229: INFO: Number of running nodes: 2, number of available pods: 2
Feb 23 18:43:23.229: INFO: Update the DaemonSet to trigger a rollout
Feb 23 18:43:23.234: INFO: Updating DaemonSet daemon-set
Feb 23 18:43:34.249: INFO: Roll back the DaemonSet before rollout is complete
Feb 23 18:43:34.256: INFO: Updating DaemonSet daemon-set
Feb 23 18:43:34.256: INFO: Make sure DaemonSet rollback is complete
Feb 23 18:43:34.262: INFO: Wrong image for pod: daemon-set-rwfnn. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 23 18:43:34.262: INFO: Pod daemon-set-rwfnn is not available
Feb 23 18:43:35.268: INFO: Wrong image for pod: daemon-set-rwfnn. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 23 18:43:35.268: INFO: Pod daemon-set-rwfnn is not available
Feb 23 18:43:36.268: INFO: Wrong image for pod: daemon-set-rwfnn. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 23 18:43:36.268: INFO: Pod daemon-set-rwfnn is not available
Feb 23 18:43:37.514: INFO: Wrong image for pod: daemon-set-rwfnn. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 23 18:43:37.514: INFO: Pod daemon-set-rwfnn is not available
Feb 23 18:43:38.268: INFO: Wrong image for pod: daemon-set-rwfnn. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 23 18:43:38.268: INFO: Pod daemon-set-rwfnn is not available
Feb 23 18:43:39.268: INFO: Wrong image for pod: daemon-set-rwfnn. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 23 18:43:39.268: INFO: Pod daemon-set-rwfnn is not available
Feb 23 18:43:40.274: INFO: Wrong image for pod: daemon-set-rwfnn. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 23 18:43:40.274: INFO: Pod daemon-set-rwfnn is not available
Feb 23 18:43:41.270: INFO: Wrong image for pod: daemon-set-rwfnn. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 23 18:43:41.270: INFO: Pod daemon-set-rwfnn is not available
Feb 23 18:43:42.268: INFO: Wrong image for pod: daemon-set-rwfnn. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 23 18:43:42.268: INFO: Pod daemon-set-rwfnn is not available
Feb 23 18:43:43.269: INFO: Wrong image for pod: daemon-set-rwfnn. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 23 18:43:43.269: INFO: Pod daemon-set-rwfnn is not available
Feb 23 18:43:44.269: INFO: Pod daemon-set-zl4kr is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8223, will wait for the garbage collector to delete the pods
Feb 23 18:43:44.334: INFO: Deleting DaemonSet.extensions daemon-set took: 6.655911ms
Feb 23 18:43:45.236: INFO: Terminating DaemonSet.extensions daemon-set pods took: 901.317884ms
Feb 23 18:44:59.939: INFO: Number of nodes with available pods: 0
Feb 23 18:44:59.939: INFO: Number of running nodes: 0, number of available pods: 0
Feb 23 18:44:59.940: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8223/daemonsets","resourceVersion":"21507"},"items":null}

Feb 23 18:44:59.942: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8223/pods","resourceVersion":"21507"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:44:59.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8223" for this suite.
Feb 23 18:45:05.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:45:06.024: INFO: namespace daemonsets-8223 deletion completed in 6.073149267s

• [SLOW TEST:104.870 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:45:06.025: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Feb 23 18:45:06.063: INFO: Waiting up to 5m0s for pod "downward-api-518eb905-2b20-4bec-9abe-ba917f5ca7ce" in namespace "downward-api-4762" to be "success or failure"
Feb 23 18:45:06.066: INFO: Pod "downward-api-518eb905-2b20-4bec-9abe-ba917f5ca7ce": Phase="Pending", Reason="", readiness=false. Elapsed: 3.340856ms
Feb 23 18:45:08.074: INFO: Pod "downward-api-518eb905-2b20-4bec-9abe-ba917f5ca7ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011164085s
Feb 23 18:45:10.077: INFO: Pod "downward-api-518eb905-2b20-4bec-9abe-ba917f5ca7ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014168525s
STEP: Saw pod success
Feb 23 18:45:10.077: INFO: Pod "downward-api-518eb905-2b20-4bec-9abe-ba917f5ca7ce" satisfied condition "success or failure"
Feb 23 18:45:10.079: INFO: Trying to get logs from node worker00 pod downward-api-518eb905-2b20-4bec-9abe-ba917f5ca7ce container dapi-container: <nil>
STEP: delete the pod
Feb 23 18:45:10.101: INFO: Waiting for pod downward-api-518eb905-2b20-4bec-9abe-ba917f5ca7ce to disappear
Feb 23 18:45:10.106: INFO: Pod downward-api-518eb905-2b20-4bec-9abe-ba917f5ca7ce no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:45:10.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4762" for this suite.
Feb 23 18:45:16.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:45:16.191: INFO: namespace downward-api-4762 deletion completed in 6.082952578s

• [SLOW TEST:10.167 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:45:16.191: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-e163bff5-882a-4ecb-b13e-aa3cfee1bac0
STEP: Creating a pod to test consume configMaps
Feb 23 18:45:16.238: INFO: Waiting up to 5m0s for pod "pod-configmaps-92047313-a118-4004-91c5-addfab93578e" in namespace "configmap-6523" to be "success or failure"
Feb 23 18:45:16.243: INFO: Pod "pod-configmaps-92047313-a118-4004-91c5-addfab93578e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.671679ms
Feb 23 18:45:18.248: INFO: Pod "pod-configmaps-92047313-a118-4004-91c5-addfab93578e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010002556s
Feb 23 18:45:20.253: INFO: Pod "pod-configmaps-92047313-a118-4004-91c5-addfab93578e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015173611s
STEP: Saw pod success
Feb 23 18:45:20.253: INFO: Pod "pod-configmaps-92047313-a118-4004-91c5-addfab93578e" satisfied condition "success or failure"
Feb 23 18:45:20.255: INFO: Trying to get logs from node worker00 pod pod-configmaps-92047313-a118-4004-91c5-addfab93578e container configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 18:45:20.270: INFO: Waiting for pod pod-configmaps-92047313-a118-4004-91c5-addfab93578e to disappear
Feb 23 18:45:20.274: INFO: Pod pod-configmaps-92047313-a118-4004-91c5-addfab93578e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:45:20.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6523" for this suite.
Feb 23 18:45:26.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:45:26.356: INFO: namespace configmap-6523 deletion completed in 6.077277929s

• [SLOW TEST:10.164 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:45:26.356: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-5903
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Feb 23 18:45:26.396: INFO: Found 0 stateful pods, waiting for 3
Feb 23 18:45:36.400: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 18:45:36.400: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 18:45:36.400: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Feb 23 18:45:36.424: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 23 18:45:46.456: INFO: Updating stateful set ss2
Feb 23 18:45:46.462: INFO: Waiting for Pod statefulset-5903/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Feb 23 18:45:56.542: INFO: Found 2 stateful pods, waiting for 3
Feb 23 18:46:06.546: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 18:46:06.546: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 18:46:06.546: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 23 18:46:06.568: INFO: Updating stateful set ss2
Feb 23 18:46:06.576: INFO: Waiting for Pod statefulset-5903/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 23 18:46:16.581: INFO: Waiting for Pod statefulset-5903/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 23 18:46:26.602: INFO: Updating stateful set ss2
Feb 23 18:46:26.622: INFO: Waiting for StatefulSet statefulset-5903/ss2 to complete update
Feb 23 18:46:26.625: INFO: Waiting for Pod statefulset-5903/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Feb 23 18:46:36.635: INFO: Deleting all statefulset in ns statefulset-5903
Feb 23 18:46:36.637: INFO: Scaling statefulset ss2 to 0
Feb 23 18:46:56.651: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 18:46:56.653: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:46:56.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5903" for this suite.
Feb 23 18:47:02.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:47:02.743: INFO: namespace statefulset-5903 deletion completed in 6.075088831s

• [SLOW TEST:96.387 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:47:02.743: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Feb 23 18:47:02.771: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 23 18:47:02.778: INFO: Waiting for terminating namespaces to be deleted...
Feb 23 18:47:02.779: INFO: 
Logging pods the kubelet thinks is on node worker00 before test
Feb 23 18:47:02.792: INFO: csi-rbdplugin-8cmnk from storage started at 2020-02-23 17:27:42 +0000 UTC (3 container statuses recorded)
Feb 23 18:47:02.792: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 18:47:02.792: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 18:47:02.792: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:47:02.793: INFO: kube-controller-manager-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.793: INFO: 	Container kube-controller-manager ready: true, restart count 1
Feb 23 18:47:02.793: INFO: kubernetes-dashboard-f957cddcb-qplvv from kube-system started at 2020-02-23 17:29:50 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.793: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 23 18:47:02.793: INFO: ceph-osd-worker00-556546b495-4r2tp from storage started at 2020-02-23 17:25:27 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.793: INFO: 	Container ceph-osd ready: true, restart count 0
Feb 23 18:47:02.793: INFO: sonobuoy from sonobuoy started at 2020-02-23 17:36:25 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.793: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 23 18:47:02.793: INFO: kube-proxy-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.793: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 18:47:02.793: INFO: kube-scheduler-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.793: INFO: 	Container kube-scheduler ready: true, restart count 2
Feb 23 18:47:02.793: INFO: ceph-mds-worker00-6f479b4486-rczdx from storage started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.793: INFO: 	Container ceph-mds ready: true, restart count 0
Feb 23 18:47:02.793: INFO: csi-cephfsplugin-fg8mp from storage started at 2020-02-23 17:27:42 +0000 UTC (3 container statuses recorded)
Feb 23 18:47:02.793: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 18:47:02.793: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 18:47:02.793: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:47:02.793: INFO: gobetween-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.793: INFO: 	Container gobetween ready: true, restart count 0
Feb 23 18:47:02.793: INFO: metallb-speaker-nr599 from networking started at 2020-02-23 17:27:42 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.793: INFO: 	Container speaker ready: true, restart count 0
Feb 23 18:47:02.793: INFO: dashboard-metrics-scraper-58475bc987-mxs5c from kube-system started at 2020-02-23 17:29:50 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.793: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 23 18:47:02.793: INFO: calico-node-77p2v from networking started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.793: INFO: 	Container calico-node ready: true, restart count 0
Feb 23 18:47:02.793: INFO: kube-apiserver-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.793: INFO: 	Container kube-apiserver ready: true, restart count 1
Feb 23 18:47:02.793: INFO: ceph-mon-worker00-5cf654d469-2mtnd from storage started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.793: INFO: 	Container ceph-mon ready: true, restart count 0
Feb 23 18:47:02.793: INFO: sonobuoy-e2e-job-382cd34a40b246b9 from sonobuoy started at 2020-02-23 17:36:27 +0000 UTC (2 container statuses recorded)
Feb 23 18:47:02.793: INFO: 	Container e2e ready: true, restart count 0
Feb 23 18:47:02.793: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 18:47:02.793: INFO: sonobuoy-systemd-logs-daemon-set-e1933034003941b1-4v2r4 from sonobuoy started at 2020-02-23 17:36:27 +0000 UTC (2 container statuses recorded)
Feb 23 18:47:02.794: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 23 18:47:02.794: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 23 18:47:02.794: INFO: etcd-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.794: INFO: 	Container etcd ready: true, restart count 0
Feb 23 18:47:02.794: INFO: 
Logging pods the kubelet thinks is on node worker01 before test
Feb 23 18:47:02.809: INFO: metallb-controller-b96bfbbf8-p9qvp from networking started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.809: INFO: 	Container controller ready: true, restart count 0
Feb 23 18:47:02.809: INFO: coredns-676544c7b9-ntt4b from kube-system started at 2020-02-23 17:26:46 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.810: INFO: 	Container coredns ready: true, restart count 0
Feb 23 18:47:02.810: INFO: kube-proxy-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.810: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 18:47:02.810: INFO: calico-node-lhcql from networking started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.810: INFO: 	Container calico-node ready: true, restart count 0
Feb 23 18:47:02.810: INFO: csi-cephfsplugin-provisioner-6cd7596f75-b6s7q from storage started at 2020-02-23 17:26:43 +0000 UTC (4 container statuses recorded)
Feb 23 18:47:02.810: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 18:47:02.810: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 18:47:02.810: INFO: 	Container csi-provisioner ready: true, restart count 1
Feb 23 18:47:02.810: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:47:02.810: INFO: calico-kube-controllers-7cd585bcd-v4kk7 from networking started at 2020-02-23 17:26:46 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.810: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 23 18:47:02.810: INFO: kube-apiserver-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.810: INFO: 	Container kube-apiserver ready: true, restart count 0
Feb 23 18:47:02.810: INFO: ceph-osd-worker01-67947c799-8n7pr from storage started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.810: INFO: 	Container ceph-osd ready: true, restart count 0
Feb 23 18:47:02.810: INFO: ceph-mds-worker01-7f5fdb58c6-m96hd from storage started at 2020-02-23 17:25:27 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.810: INFO: 	Container ceph-mds ready: true, restart count 2
Feb 23 18:47:02.810: INFO: csi-cephfsplugin-provisioner-6cd7596f75-n9kmr from storage started at 2020-02-23 17:26:43 +0000 UTC (4 container statuses recorded)
Feb 23 18:47:02.810: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 18:47:02.810: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 18:47:02.810: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 18:47:02.810: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:47:02.810: INFO: csi-rbdplugin-provisioner-7494f65674-gt7nb from storage started at 2020-02-23 17:26:43 +0000 UTC (5 container statuses recorded)
Feb 23 18:47:02.810: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 23 18:47:02.810: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 18:47:02.810: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 18:47:02.810: INFO: 	Container csi-snapshotter ready: true, restart count 0
Feb 23 18:47:02.810: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:47:02.810: INFO: csi-rbdplugin-provisioner-7494f65674-vnnkm from storage started at 2020-02-23 17:26:43 +0000 UTC (5 container statuses recorded)
Feb 23 18:47:02.810: INFO: 	Container csi-attacher ready: true, restart count 1
Feb 23 18:47:02.810: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 18:47:02.810: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 18:47:02.810: INFO: 	Container csi-snapshotter ready: true, restart count 0
Feb 23 18:47:02.810: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:47:02.810: INFO: kube-scheduler-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.810: INFO: 	Container kube-scheduler ready: true, restart count 1
Feb 23 18:47:02.810: INFO: csi-rbdplugin-cfq94 from storage started at 2020-02-23 17:26:43 +0000 UTC (3 container statuses recorded)
Feb 23 18:47:02.810: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 18:47:02.810: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 18:47:02.810: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:47:02.810: INFO: sonobuoy-systemd-logs-daemon-set-e1933034003941b1-74kbd from sonobuoy started at 2020-02-23 17:36:27 +0000 UTC (2 container statuses recorded)
Feb 23 18:47:02.810: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 23 18:47:02.810: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 23 18:47:02.810: INFO: gobetween-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.810: INFO: 	Container gobetween ready: true, restart count 0
Feb 23 18:47:02.810: INFO: ceph-mon-worker01-bdb694876-9x2bs from storage started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.810: INFO: 	Container ceph-mon ready: true, restart count 0
Feb 23 18:47:02.810: INFO: csi-cephfsplugin-wj5f7 from storage started at 2020-02-23 17:26:43 +0000 UTC (3 container statuses recorded)
Feb 23 18:47:02.810: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 18:47:02.810: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 18:47:02.810: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:47:02.810: INFO: ceph-setup-xccxd from storage started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.810: INFO: 	Container ceph ready: false, restart count 2
Feb 23 18:47:02.810: INFO: coredns-676544c7b9-pz66t from kube-system started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.810: INFO: 	Container coredns ready: true, restart count 0
Feb 23 18:47:02.810: INFO: ceph-rgw-57cd48f74c-4nwcs from storage started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.810: INFO: 	Container ceph-rgw ready: true, restart count 1
Feb 23 18:47:02.810: INFO: ceph-mgr-94b9dd996-vfc9x from storage started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.810: INFO: 	Container ceph-mgr ready: true, restart count 0
Feb 23 18:47:02.810: INFO: csi-rbdplugin-provisioner-7494f65674-zk9l6 from storage started at 2020-02-23 17:26:43 +0000 UTC (5 container statuses recorded)
Feb 23 18:47:02.810: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 23 18:47:02.810: INFO: 	Container csi-provisioner ready: true, restart count 1
Feb 23 18:47:02.810: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 18:47:02.811: INFO: 	Container csi-snapshotter ready: true, restart count 0
Feb 23 18:47:02.811: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:47:02.811: INFO: etcd-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.811: INFO: 	Container etcd ready: true, restart count 0
Feb 23 18:47:02.811: INFO: kube-controller-manager-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.811: INFO: 	Container kube-controller-manager ready: true, restart count 1
Feb 23 18:47:02.811: INFO: csi-cephfsplugin-provisioner-6cd7596f75-tjxsr from storage started at 2020-02-23 17:26:43 +0000 UTC (4 container statuses recorded)
Feb 23 18:47:02.811: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 18:47:02.811: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 18:47:02.811: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 18:47:02.811: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 18:47:02.811: INFO: metallb-speaker-jpspl from networking started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 18:47:02.811: INFO: 	Container speaker ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e7a7dd51-d4be-49f1-bb2e-5e3d2bdb7a9f 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-e7a7dd51-d4be-49f1-bb2e-5e3d2bdb7a9f off the node worker00
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e7a7dd51-d4be-49f1-bb2e-5e3d2bdb7a9f
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:52:08.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6949" for this suite.
Feb 23 18:52:28.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:52:29.001: INFO: namespace sched-pred-6949 deletion completed in 20.075930256s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:326.257 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:52:29.003: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Feb 23 18:52:31.063: INFO: Pod pod-hostip-f1a471fa-6490-4739-9571-aad66555c975 has hostIP: 192.168.180.100
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:52:31.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-494" for this suite.
Feb 23 18:52:43.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:52:43.146: INFO: namespace pods-494 deletion completed in 12.080668109s

• [SLOW TEST:14.143 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:52:43.152: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-56b6ead6-c2ad-4f78-810e-21740f03d484
STEP: Creating a pod to test consume configMaps
Feb 23 18:52:43.193: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-618f5b54-42de-43fc-933b-bca4fd7e6572" in namespace "projected-9520" to be "success or failure"
Feb 23 18:52:43.203: INFO: Pod "pod-projected-configmaps-618f5b54-42de-43fc-933b-bca4fd7e6572": Phase="Pending", Reason="", readiness=false. Elapsed: 8.999734ms
Feb 23 18:52:45.208: INFO: Pod "pod-projected-configmaps-618f5b54-42de-43fc-933b-bca4fd7e6572": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01440921s
STEP: Saw pod success
Feb 23 18:52:45.208: INFO: Pod "pod-projected-configmaps-618f5b54-42de-43fc-933b-bca4fd7e6572" satisfied condition "success or failure"
Feb 23 18:52:45.215: INFO: Trying to get logs from node worker00 pod pod-projected-configmaps-618f5b54-42de-43fc-933b-bca4fd7e6572 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 18:52:45.248: INFO: Waiting for pod pod-projected-configmaps-618f5b54-42de-43fc-933b-bca4fd7e6572 to disappear
Feb 23 18:52:45.251: INFO: Pod pod-projected-configmaps-618f5b54-42de-43fc-933b-bca4fd7e6572 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:52:45.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9520" for this suite.
Feb 23 18:52:51.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:52:51.354: INFO: namespace projected-9520 deletion completed in 6.099677163s

• [SLOW TEST:8.203 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:52:51.355: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-4b9f5076-74bf-458c-8cee-024344b92618
STEP: Creating a pod to test consume configMaps
Feb 23 18:52:51.397: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-71cac42f-a90c-4131-9ce2-070a014dc7d0" in namespace "projected-2212" to be "success or failure"
Feb 23 18:52:51.402: INFO: Pod "pod-projected-configmaps-71cac42f-a90c-4131-9ce2-070a014dc7d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.282731ms
Feb 23 18:52:53.404: INFO: Pod "pod-projected-configmaps-71cac42f-a90c-4131-9ce2-070a014dc7d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006623826s
STEP: Saw pod success
Feb 23 18:52:53.404: INFO: Pod "pod-projected-configmaps-71cac42f-a90c-4131-9ce2-070a014dc7d0" satisfied condition "success or failure"
Feb 23 18:52:53.407: INFO: Trying to get logs from node worker00 pod pod-projected-configmaps-71cac42f-a90c-4131-9ce2-070a014dc7d0 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 18:52:53.428: INFO: Waiting for pod pod-projected-configmaps-71cac42f-a90c-4131-9ce2-070a014dc7d0 to disappear
Feb 23 18:52:53.431: INFO: Pod pod-projected-configmaps-71cac42f-a90c-4131-9ce2-070a014dc7d0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:52:53.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2212" for this suite.
Feb 23 18:52:59.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:52:59.523: INFO: namespace projected-2212 deletion completed in 6.088945337s

• [SLOW TEST:8.168 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:52:59.523: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-dd3769de-e574-44da-b63e-d8ce2c0777ed
Feb 23 18:52:59.567: INFO: Pod name my-hostname-basic-dd3769de-e574-44da-b63e-d8ce2c0777ed: Found 0 pods out of 1
Feb 23 18:53:04.570: INFO: Pod name my-hostname-basic-dd3769de-e574-44da-b63e-d8ce2c0777ed: Found 1 pods out of 1
Feb 23 18:53:04.570: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-dd3769de-e574-44da-b63e-d8ce2c0777ed" are running
Feb 23 18:53:04.573: INFO: Pod "my-hostname-basic-dd3769de-e574-44da-b63e-d8ce2c0777ed-t566k" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-23 18:52:59 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-23 18:53:01 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-23 18:53:01 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-23 18:52:59 +0000 UTC Reason: Message:}])
Feb 23 18:53:04.573: INFO: Trying to dial the pod
Feb 23 18:53:09.584: INFO: Controller my-hostname-basic-dd3769de-e574-44da-b63e-d8ce2c0777ed: Got expected result from replica 1 [my-hostname-basic-dd3769de-e574-44da-b63e-d8ce2c0777ed-t566k]: "my-hostname-basic-dd3769de-e574-44da-b63e-d8ce2c0777ed-t566k", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:53:09.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9694" for this suite.
Feb 23 18:53:15.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:53:15.659: INFO: namespace replication-controller-9694 deletion completed in 6.071108983s

• [SLOW TEST:16.136 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:53:15.661: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Feb 23 18:53:18.227: INFO: Successfully updated pod "annotationupdatec1cbc392-94b0-434b-8a47-003aca0bf76a"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:53:22.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8322" for this suite.
Feb 23 18:53:34.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:53:34.330: INFO: namespace projected-8322 deletion completed in 12.077399248s

• [SLOW TEST:18.669 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:53:34.330: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Feb 23 18:53:34.358: INFO: PodSpec: initContainers in spec.initContainers
Feb 23 18:54:24.180: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-a03d8685-8a63-4998-89a3-257139582315", GenerateName:"", Namespace:"init-container-3471", SelfLink:"/api/v1/namespaces/init-container-3471/pods/pod-init-a03d8685-8a63-4998-89a3-257139582315", UID:"7d85041b-bd21-48cb-88d4-d1e60cf7b54f", ResourceVersion:"23323", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63718080814, loc:(*time.Location)(0x788c6e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"358656806"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.200.131.167/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-ww2cq", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0031ec300), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ww2cq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ww2cq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ww2cq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0037de4a8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"worker00", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00365e120), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0037de530)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0037de550)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0037de558), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0037de55c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718080814, loc:(*time.Location)(0x788c6e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718080814, loc:(*time.Location)(0x788c6e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718080814, loc:(*time.Location)(0x788c6e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718080814, loc:(*time.Location)(0x788c6e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.180.100", PodIP:"10.200.131.167", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.200.131.167"}}, StartTime:(*v1.Time)(0xc003b24240), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0035122a0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003512310)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://dadb5b0ef176f35034fd67ff610a7735782fabf683091757e8f96a210c4114b3", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003b24280), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003b24260), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc0037de5d4)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:54:24.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3471" for this suite.
Feb 23 18:54:36.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:54:36.270: INFO: namespace init-container-3471 deletion completed in 12.079703854s

• [SLOW TEST:61.941 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:54:36.270: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Feb 23 18:54:36.305: INFO: Waiting up to 5m0s for pod "downward-api-53ca3838-dcc7-44b6-8a43-0ba948efc844" in namespace "downward-api-3237" to be "success or failure"
Feb 23 18:54:36.309: INFO: Pod "downward-api-53ca3838-dcc7-44b6-8a43-0ba948efc844": Phase="Pending", Reason="", readiness=false. Elapsed: 4.18561ms
Feb 23 18:54:38.318: INFO: Pod "downward-api-53ca3838-dcc7-44b6-8a43-0ba948efc844": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01349194s
STEP: Saw pod success
Feb 23 18:54:38.318: INFO: Pod "downward-api-53ca3838-dcc7-44b6-8a43-0ba948efc844" satisfied condition "success or failure"
Feb 23 18:54:38.321: INFO: Trying to get logs from node worker00 pod downward-api-53ca3838-dcc7-44b6-8a43-0ba948efc844 container dapi-container: <nil>
STEP: delete the pod
Feb 23 18:54:38.342: INFO: Waiting for pod downward-api-53ca3838-dcc7-44b6-8a43-0ba948efc844 to disappear
Feb 23 18:54:38.345: INFO: Pod downward-api-53ca3838-dcc7-44b6-8a43-0ba948efc844 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:54:38.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3237" for this suite.
Feb 23 18:54:44.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:54:44.422: INFO: namespace downward-api-3237 deletion completed in 6.074292968s

• [SLOW TEST:8.151 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:54:44.424: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Feb 23 18:54:44.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 create -f - --namespace=kubectl-1676'
Feb 23 18:54:45.904: INFO: stderr: ""
Feb 23 18:54:45.904: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 23 18:54:45.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1676'
Feb 23 18:54:46.003: INFO: stderr: ""
Feb 23 18:54:46.003: INFO: stdout: "update-demo-nautilus-4fmx7 update-demo-nautilus-x5npw "
Feb 23 18:54:46.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-nautilus-4fmx7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1676'
Feb 23 18:54:46.066: INFO: stderr: ""
Feb 23 18:54:46.066: INFO: stdout: ""
Feb 23 18:54:46.066: INFO: update-demo-nautilus-4fmx7 is created but not running
Feb 23 18:54:51.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1676'
Feb 23 18:54:51.122: INFO: stderr: ""
Feb 23 18:54:51.122: INFO: stdout: "update-demo-nautilus-4fmx7 update-demo-nautilus-x5npw "
Feb 23 18:54:51.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-nautilus-4fmx7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1676'
Feb 23 18:54:51.182: INFO: stderr: ""
Feb 23 18:54:51.182: INFO: stdout: "true"
Feb 23 18:54:51.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-nautilus-4fmx7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1676'
Feb 23 18:54:51.247: INFO: stderr: ""
Feb 23 18:54:51.247: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 18:54:51.247: INFO: validating pod update-demo-nautilus-4fmx7
Feb 23 18:54:51.254: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 18:54:51.254: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 18:54:51.254: INFO: update-demo-nautilus-4fmx7 is verified up and running
Feb 23 18:54:51.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-nautilus-x5npw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1676'
Feb 23 18:54:51.311: INFO: stderr: ""
Feb 23 18:54:51.311: INFO: stdout: "true"
Feb 23 18:54:51.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-nautilus-x5npw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1676'
Feb 23 18:54:51.366: INFO: stderr: ""
Feb 23 18:54:51.366: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 18:54:51.366: INFO: validating pod update-demo-nautilus-x5npw
Feb 23 18:54:51.370: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 18:54:51.370: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 18:54:51.370: INFO: update-demo-nautilus-x5npw is verified up and running
STEP: scaling down the replication controller
Feb 23 18:54:51.372: INFO: scanned /root for discovery docs: <nil>
Feb 23 18:54:51.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-1676'
Feb 23 18:54:52.481: INFO: stderr: ""
Feb 23 18:54:52.481: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 23 18:54:52.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1676'
Feb 23 18:54:52.546: INFO: stderr: ""
Feb 23 18:54:52.546: INFO: stdout: "update-demo-nautilus-4fmx7 update-demo-nautilus-x5npw "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 23 18:54:57.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1676'
Feb 23 18:54:57.615: INFO: stderr: ""
Feb 23 18:54:57.615: INFO: stdout: "update-demo-nautilus-4fmx7 update-demo-nautilus-x5npw "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 23 18:55:02.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1676'
Feb 23 18:55:02.674: INFO: stderr: ""
Feb 23 18:55:02.674: INFO: stdout: "update-demo-nautilus-4fmx7 update-demo-nautilus-x5npw "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 23 18:55:07.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1676'
Feb 23 18:55:07.727: INFO: stderr: ""
Feb 23 18:55:07.727: INFO: stdout: "update-demo-nautilus-x5npw "
Feb 23 18:55:07.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-nautilus-x5npw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1676'
Feb 23 18:55:07.784: INFO: stderr: ""
Feb 23 18:55:07.784: INFO: stdout: "true"
Feb 23 18:55:07.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-nautilus-x5npw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1676'
Feb 23 18:55:07.842: INFO: stderr: ""
Feb 23 18:55:07.842: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 18:55:07.842: INFO: validating pod update-demo-nautilus-x5npw
Feb 23 18:55:07.845: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 18:55:07.845: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 18:55:07.845: INFO: update-demo-nautilus-x5npw is verified up and running
STEP: scaling up the replication controller
Feb 23 18:55:07.847: INFO: scanned /root for discovery docs: <nil>
Feb 23 18:55:07.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-1676'
Feb 23 18:55:08.924: INFO: stderr: ""
Feb 23 18:55:08.924: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 23 18:55:08.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1676'
Feb 23 18:55:08.978: INFO: stderr: ""
Feb 23 18:55:08.978: INFO: stdout: "update-demo-nautilus-fw544 update-demo-nautilus-x5npw "
Feb 23 18:55:08.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-nautilus-fw544 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1676'
Feb 23 18:55:09.038: INFO: stderr: ""
Feb 23 18:55:09.038: INFO: stdout: ""
Feb 23 18:55:09.038: INFO: update-demo-nautilus-fw544 is created but not running
Feb 23 18:55:14.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1676'
Feb 23 18:55:14.089: INFO: stderr: ""
Feb 23 18:55:14.089: INFO: stdout: "update-demo-nautilus-fw544 update-demo-nautilus-x5npw "
Feb 23 18:55:14.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-nautilus-fw544 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1676'
Feb 23 18:55:14.146: INFO: stderr: ""
Feb 23 18:55:14.146: INFO: stdout: "true"
Feb 23 18:55:14.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-nautilus-fw544 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1676'
Feb 23 18:55:14.197: INFO: stderr: ""
Feb 23 18:55:14.197: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 18:55:14.197: INFO: validating pod update-demo-nautilus-fw544
Feb 23 18:55:14.200: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 18:55:14.200: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 18:55:14.200: INFO: update-demo-nautilus-fw544 is verified up and running
Feb 23 18:55:14.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-nautilus-x5npw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1676'
Feb 23 18:55:14.253: INFO: stderr: ""
Feb 23 18:55:14.253: INFO: stdout: "true"
Feb 23 18:55:14.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-nautilus-x5npw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1676'
Feb 23 18:55:14.309: INFO: stderr: ""
Feb 23 18:55:14.309: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 18:55:14.309: INFO: validating pod update-demo-nautilus-x5npw
Feb 23 18:55:14.313: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 18:55:14.313: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 18:55:14.313: INFO: update-demo-nautilus-x5npw is verified up and running
STEP: using delete to clean up resources
Feb 23 18:55:14.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 delete --grace-period=0 --force -f - --namespace=kubectl-1676'
Feb 23 18:55:14.368: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 18:55:14.368: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 23 18:55:14.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1676'
Feb 23 18:55:14.450: INFO: stderr: "No resources found in kubectl-1676 namespace.\n"
Feb 23 18:55:14.450: INFO: stdout: ""
Feb 23 18:55:14.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods -l name=update-demo --namespace=kubectl-1676 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 23 18:55:14.502: INFO: stderr: ""
Feb 23 18:55:14.502: INFO: stdout: "update-demo-nautilus-fw544\nupdate-demo-nautilus-x5npw\n"
Feb 23 18:55:15.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1676'
Feb 23 18:55:15.059: INFO: stderr: "No resources found in kubectl-1676 namespace.\n"
Feb 23 18:55:15.059: INFO: stdout: ""
Feb 23 18:55:15.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods -l name=update-demo --namespace=kubectl-1676 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 23 18:55:15.128: INFO: stderr: ""
Feb 23 18:55:15.128: INFO: stdout: "update-demo-nautilus-fw544\nupdate-demo-nautilus-x5npw\n"
Feb 23 18:55:15.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1676'
Feb 23 18:55:15.566: INFO: stderr: "No resources found in kubectl-1676 namespace.\n"
Feb 23 18:55:15.566: INFO: stdout: ""
Feb 23 18:55:15.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods -l name=update-demo --namespace=kubectl-1676 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 23 18:55:15.622: INFO: stderr: ""
Feb 23 18:55:15.622: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:55:15.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1676" for this suite.
Feb 23 18:55:43.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:55:43.698: INFO: namespace kubectl-1676 deletion completed in 28.071239226s

• [SLOW TEST:59.274 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:55:43.699: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 18:55:43.724: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Creating first CR 
Feb 23 18:55:49.279: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-23T18:55:49Z generation:1 name:name1 resourceVersion:23660 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:7f7ca95d-980a-4af6-8d87-5d7424bcccfa] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Feb 23 18:55:59.285: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-23T18:55:59Z generation:1 name:name2 resourceVersion:23681 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:bc0f5a34-b1fd-4b8e-8fff-f9e615212c0c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Feb 23 18:56:09.293: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-23T18:55:49Z generation:2 name:name1 resourceVersion:23704 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:7f7ca95d-980a-4af6-8d87-5d7424bcccfa] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Feb 23 18:56:19.693: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-23T18:55:59Z generation:2 name:name2 resourceVersion:23727 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:bc0f5a34-b1fd-4b8e-8fff-f9e615212c0c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Feb 23 18:56:29.703: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-23T18:55:49Z generation:2 name:name1 resourceVersion:23749 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:7f7ca95d-980a-4af6-8d87-5d7424bcccfa] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Feb 23 18:56:39.711: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-23T18:55:59Z generation:2 name:name2 resourceVersion:23770 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:bc0f5a34-b1fd-4b8e-8fff-f9e615212c0c] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:56:50.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2749" for this suite.
Feb 23 18:56:56.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:56:56.303: INFO: namespace crd-watch-2749 deletion completed in 6.07694058s

• [SLOW TEST:72.604 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:56:56.304: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 18:56:56.333: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:56:58.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3937" for this suite.
Feb 23 18:57:46.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:57:46.495: INFO: namespace pods-3937 deletion completed in 48.09022288s

• [SLOW TEST:50.191 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:57:46.495: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 23 18:57:46.532: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e4fb75c-86c5-4016-b33c-4838551a52f9" in namespace "projected-9467" to be "success or failure"
Feb 23 18:57:46.534: INFO: Pod "downwardapi-volume-4e4fb75c-86c5-4016-b33c-4838551a52f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.733289ms
Feb 23 18:57:48.537: INFO: Pod "downwardapi-volume-4e4fb75c-86c5-4016-b33c-4838551a52f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005315741s
STEP: Saw pod success
Feb 23 18:57:48.537: INFO: Pod "downwardapi-volume-4e4fb75c-86c5-4016-b33c-4838551a52f9" satisfied condition "success or failure"
Feb 23 18:57:48.541: INFO: Trying to get logs from node worker00 pod downwardapi-volume-4e4fb75c-86c5-4016-b33c-4838551a52f9 container client-container: <nil>
STEP: delete the pod
Feb 23 18:57:48.563: INFO: Waiting for pod downwardapi-volume-4e4fb75c-86c5-4016-b33c-4838551a52f9 to disappear
Feb 23 18:57:48.567: INFO: Pod downwardapi-volume-4e4fb75c-86c5-4016-b33c-4838551a52f9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:57:48.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9467" for this suite.
Feb 23 18:57:54.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:57:54.642: INFO: namespace projected-9467 deletion completed in 6.071700207s

• [SLOW TEST:8.147 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:57:54.644: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb 23 18:57:57.703: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:57:58.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-979" for this suite.
Feb 23 18:58:26.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:58:26.798: INFO: namespace replicaset-979 deletion completed in 28.075054388s

• [SLOW TEST:32.155 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:58:26.798: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 23 18:58:26.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-8678'
Feb 23 18:58:26.900: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 23 18:58:26.900: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Feb 23 18:58:28.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 delete deployment e2e-test-httpd-deployment --namespace=kubectl-8678'
Feb 23 18:58:28.969: INFO: stderr: ""
Feb 23 18:58:28.969: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:58:28.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8678" for this suite.
Feb 23 18:58:34.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:58:35.057: INFO: namespace kubectl-8678 deletion completed in 6.083808851s

• [SLOW TEST:8.259 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:58:35.057: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 23 18:58:35.092: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3d6bb9d4-599d-47eb-afcd-9506f4f0d3d4" in namespace "downward-api-7694" to be "success or failure"
Feb 23 18:58:35.097: INFO: Pod "downwardapi-volume-3d6bb9d4-599d-47eb-afcd-9506f4f0d3d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.912145ms
Feb 23 18:58:37.100: INFO: Pod "downwardapi-volume-3d6bb9d4-599d-47eb-afcd-9506f4f0d3d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007924299s
Feb 23 18:58:39.105: INFO: Pod "downwardapi-volume-3d6bb9d4-599d-47eb-afcd-9506f4f0d3d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01228147s
STEP: Saw pod success
Feb 23 18:58:39.105: INFO: Pod "downwardapi-volume-3d6bb9d4-599d-47eb-afcd-9506f4f0d3d4" satisfied condition "success or failure"
Feb 23 18:58:39.107: INFO: Trying to get logs from node worker00 pod downwardapi-volume-3d6bb9d4-599d-47eb-afcd-9506f4f0d3d4 container client-container: <nil>
STEP: delete the pod
Feb 23 18:58:39.123: INFO: Waiting for pod downwardapi-volume-3d6bb9d4-599d-47eb-afcd-9506f4f0d3d4 to disappear
Feb 23 18:58:39.127: INFO: Pod downwardapi-volume-3d6bb9d4-599d-47eb-afcd-9506f4f0d3d4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:58:39.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7694" for this suite.
Feb 23 18:58:45.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:58:45.199: INFO: namespace downward-api-7694 deletion completed in 6.069885116s

• [SLOW TEST:10.142 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:58:45.199: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 23 18:58:47.756: INFO: Successfully updated pod "pod-update-activedeadlineseconds-8211951d-4714-41e5-a8e5-167951cffa5a"
Feb 23 18:58:47.756: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-8211951d-4714-41e5-a8e5-167951cffa5a" in namespace "pods-3706" to be "terminated due to deadline exceeded"
Feb 23 18:58:47.759: INFO: Pod "pod-update-activedeadlineseconds-8211951d-4714-41e5-a8e5-167951cffa5a": Phase="Running", Reason="", readiness=true. Elapsed: 2.153089ms
Feb 23 18:58:49.767: INFO: Pod "pod-update-activedeadlineseconds-8211951d-4714-41e5-a8e5-167951cffa5a": Phase="Running", Reason="", readiness=true. Elapsed: 2.010300231s
Feb 23 18:58:51.770: INFO: Pod "pod-update-activedeadlineseconds-8211951d-4714-41e5-a8e5-167951cffa5a": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.013415632s
Feb 23 18:58:51.770: INFO: Pod "pod-update-activedeadlineseconds-8211951d-4714-41e5-a8e5-167951cffa5a" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:58:51.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3706" for this suite.
Feb 23 18:58:57.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:58:57.853: INFO: namespace pods-3706 deletion completed in 6.080995686s

• [SLOW TEST:12.654 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:58:57.855: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-6955
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6955
STEP: Creating statefulset with conflicting port in namespace statefulset-6955
STEP: Waiting until pod test-pod will start running in namespace statefulset-6955
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6955
Feb 23 18:59:03.911: INFO: Observed stateful pod in namespace: statefulset-6955, name: ss-0, uid: 28ad7e1d-8c65-4d61-bc0c-2ad4b26a3a19, status phase: Pending. Waiting for statefulset controller to delete.
Feb 23 18:59:04.307: INFO: Observed stateful pod in namespace: statefulset-6955, name: ss-0, uid: 28ad7e1d-8c65-4d61-bc0c-2ad4b26a3a19, status phase: Failed. Waiting for statefulset controller to delete.
Feb 23 18:59:04.316: INFO: Observed stateful pod in namespace: statefulset-6955, name: ss-0, uid: 28ad7e1d-8c65-4d61-bc0c-2ad4b26a3a19, status phase: Failed. Waiting for statefulset controller to delete.
Feb 23 18:59:04.322: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6955
STEP: Removing pod with conflicting port in namespace statefulset-6955
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6955 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Feb 23 18:59:08.366: INFO: Deleting all statefulset in ns statefulset-6955
Feb 23 18:59:08.368: INFO: Scaling statefulset ss to 0
Feb 23 18:59:18.386: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 18:59:18.388: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:59:18.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6955" for this suite.
Feb 23 18:59:24.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:59:24.477: INFO: namespace statefulset-6955 deletion completed in 6.074810241s

• [SLOW TEST:26.623 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:59:24.478: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 23 18:59:24.521: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d8d75608-6425-402a-baea-c404cf12113f" in namespace "projected-1726" to be "success or failure"
Feb 23 18:59:24.523: INFO: Pod "downwardapi-volume-d8d75608-6425-402a-baea-c404cf12113f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.654574ms
Feb 23 18:59:26.527: INFO: Pod "downwardapi-volume-d8d75608-6425-402a-baea-c404cf12113f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005905853s
STEP: Saw pod success
Feb 23 18:59:26.527: INFO: Pod "downwardapi-volume-d8d75608-6425-402a-baea-c404cf12113f" satisfied condition "success or failure"
Feb 23 18:59:26.529: INFO: Trying to get logs from node worker00 pod downwardapi-volume-d8d75608-6425-402a-baea-c404cf12113f container client-container: <nil>
STEP: delete the pod
Feb 23 18:59:26.546: INFO: Waiting for pod downwardapi-volume-d8d75608-6425-402a-baea-c404cf12113f to disappear
Feb 23 18:59:26.552: INFO: Pod downwardapi-volume-d8d75608-6425-402a-baea-c404cf12113f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:59:26.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1726" for this suite.
Feb 23 18:59:33.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:59:33.141: INFO: namespace projected-1726 deletion completed in 6.582483473s

• [SLOW TEST:8.664 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:59:33.141: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:59:33.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9147" for this suite.
Feb 23 18:59:45.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:59:45.270: INFO: namespace pods-9147 deletion completed in 12.073070566s

• [SLOW TEST:12.129 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:59:45.270: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 23 18:59:45.308: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1686fa3-714e-4590-a10f-74c152c08a64" in namespace "downward-api-2161" to be "success or failure"
Feb 23 18:59:45.312: INFO: Pod "downwardapi-volume-e1686fa3-714e-4590-a10f-74c152c08a64": Phase="Pending", Reason="", readiness=false. Elapsed: 4.635952ms
Feb 23 18:59:47.316: INFO: Pod "downwardapi-volume-e1686fa3-714e-4590-a10f-74c152c08a64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007902506s
STEP: Saw pod success
Feb 23 18:59:47.316: INFO: Pod "downwardapi-volume-e1686fa3-714e-4590-a10f-74c152c08a64" satisfied condition "success or failure"
Feb 23 18:59:47.318: INFO: Trying to get logs from node worker00 pod downwardapi-volume-e1686fa3-714e-4590-a10f-74c152c08a64 container client-container: <nil>
STEP: delete the pod
Feb 23 18:59:47.334: INFO: Waiting for pod downwardapi-volume-e1686fa3-714e-4590-a10f-74c152c08a64 to disappear
Feb 23 18:59:47.336: INFO: Pod downwardapi-volume-e1686fa3-714e-4590-a10f-74c152c08a64 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:59:47.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2161" for this suite.
Feb 23 18:59:53.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 18:59:53.418: INFO: namespace downward-api-2161 deletion completed in 6.08043035s

• [SLOW TEST:8.148 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 18:59:53.419: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Feb 23 18:59:55.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec pod-sharedvolume-4088082f-77e7-430a-8355-ff20b2e33df1 -c busybox-main-container --namespace=emptydir-7937 -- cat /usr/share/volumeshare/shareddata.txt'
Feb 23 18:59:55.588: INFO: stderr: ""
Feb 23 18:59:55.588: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 18:59:55.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7937" for this suite.
Feb 23 19:00:01.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:00:01.665: INFO: namespace emptydir-7937 deletion completed in 6.073840459s

• [SLOW TEST:8.246 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:00:01.665: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1004.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1004.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 23 19:00:03.736: INFO: DNS probes using dns-1004/dns-test-1a4130fa-dd17-45c0-a417-a57287596ee0 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:00:03.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1004" for this suite.
Feb 23 19:00:09.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:00:09.842: INFO: namespace dns-1004 deletion completed in 6.075992534s

• [SLOW TEST:8.177 seconds]
[sig-network] DNS
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:00:09.842: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
Feb 23 19:00:09.869: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 23 19:01:09.884: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 19:01:09.890: INFO: Starting informer...
STEP: Starting pod...
Feb 23 19:01:10.103: INFO: Pod is running on worker00. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Feb 23 19:01:10.126: INFO: Pod wasn't evicted. Proceeding
Feb 23 19:01:10.126: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Feb 23 19:02:25.179: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:02:25.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-4682" for this suite.
Feb 23 19:02:37.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:02:37.253: INFO: namespace taint-single-pod-4682 deletion completed in 12.071199299s

• [SLOW TEST:147.411 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:02:37.254: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 23 19:02:37.289: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2db1680c-0e0e-48f7-a9a4-509819c77105" in namespace "downward-api-1075" to be "success or failure"
Feb 23 19:02:37.296: INFO: Pod "downwardapi-volume-2db1680c-0e0e-48f7-a9a4-509819c77105": Phase="Pending", Reason="", readiness=false. Elapsed: 6.541731ms
Feb 23 19:02:39.298: INFO: Pod "downwardapi-volume-2db1680c-0e0e-48f7-a9a4-509819c77105": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008749648s
STEP: Saw pod success
Feb 23 19:02:39.298: INFO: Pod "downwardapi-volume-2db1680c-0e0e-48f7-a9a4-509819c77105" satisfied condition "success or failure"
Feb 23 19:02:39.301: INFO: Trying to get logs from node worker00 pod downwardapi-volume-2db1680c-0e0e-48f7-a9a4-509819c77105 container client-container: <nil>
STEP: delete the pod
Feb 23 19:02:39.319: INFO: Waiting for pod downwardapi-volume-2db1680c-0e0e-48f7-a9a4-509819c77105 to disappear
Feb 23 19:02:39.323: INFO: Pod downwardapi-volume-2db1680c-0e0e-48f7-a9a4-509819c77105 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:02:39.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1075" for this suite.
Feb 23 19:02:45.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:02:45.401: INFO: namespace downward-api-1075 deletion completed in 6.074426877s

• [SLOW TEST:8.147 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:02:45.402: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-e581df1f-7ca2-4689-bfd3-3c4920f7f26e
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-e581df1f-7ca2-4689-bfd3-3c4920f7f26e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:03:51.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5427" for this suite.
Feb 23 19:04:19.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:04:19.743: INFO: namespace configmap-5427 deletion completed in 28.073000326s

• [SLOW TEST:94.341 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:04:19.744: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Feb 23 19:04:19.785: INFO: Waiting up to 5m0s for pod "client-containers-01a3a43d-5308-480b-a35b-dfab29d9da48" in namespace "containers-8660" to be "success or failure"
Feb 23 19:04:19.788: INFO: Pod "client-containers-01a3a43d-5308-480b-a35b-dfab29d9da48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.768659ms
Feb 23 19:04:21.793: INFO: Pod "client-containers-01a3a43d-5308-480b-a35b-dfab29d9da48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007413573s
STEP: Saw pod success
Feb 23 19:04:21.793: INFO: Pod "client-containers-01a3a43d-5308-480b-a35b-dfab29d9da48" satisfied condition "success or failure"
Feb 23 19:04:21.795: INFO: Trying to get logs from node worker00 pod client-containers-01a3a43d-5308-480b-a35b-dfab29d9da48 container test-container: <nil>
STEP: delete the pod
Feb 23 19:04:21.809: INFO: Waiting for pod client-containers-01a3a43d-5308-480b-a35b-dfab29d9da48 to disappear
Feb 23 19:04:21.815: INFO: Pod client-containers-01a3a43d-5308-480b-a35b-dfab29d9da48 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:04:21.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8660" for this suite.
Feb 23 19:04:27.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:04:27.886: INFO: namespace containers-8660 deletion completed in 6.068222629s

• [SLOW TEST:8.143 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:04:27.887: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 19:04:28.169: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 19:04:31.185: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 19:04:31.188: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2142-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:04:36.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1705" for this suite.
Feb 23 19:04:42.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:04:42.943: INFO: namespace webhook-1705 deletion completed in 6.079064156s
STEP: Destroying namespace "webhook-1705-markers" for this suite.
Feb 23 19:04:48.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:04:49.014: INFO: namespace webhook-1705-markers deletion completed in 6.071454549s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.140 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:04:49.027: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-385
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-385
STEP: creating replication controller externalsvc in namespace services-385
I0223 19:04:49.089778      26 runners.go:184] Created replication controller with name: externalsvc, namespace: services-385, replica count: 2
I0223 19:04:52.141280      26 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Feb 23 19:04:52.155: INFO: Creating new exec pod
Feb 23 19:04:54.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=services-385 execpodd5jtl -- /bin/sh -x -c nslookup clusterip-service'
Feb 23 19:04:54.978: INFO: stderr: "+ nslookup clusterip-service\n"
Feb 23 19:04:54.978: INFO: stdout: "Server:\t\t10.32.0.10\nAddress:\t10.32.0.10#53\n\nclusterip-service.services-385.svc.cluster.local\tcanonical name = externalsvc.services-385.svc.cluster.local.\nName:\texternalsvc.services-385.svc.cluster.local\nAddress: 10.32.0.126\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-385, will wait for the garbage collector to delete the pods
Feb 23 19:04:55.041: INFO: Deleting ReplicationController externalsvc took: 9.343707ms
Feb 23 19:04:55.942: INFO: Terminating ReplicationController externalsvc pods took: 901.566201ms
Feb 23 19:05:00.072: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:05:00.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-385" for this suite.
Feb 23 19:05:06.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:05:06.192: INFO: namespace services-385 deletion completed in 6.082976612s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:17.166 seconds]
[sig-network] Services
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:05:06.192: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-234
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-234
STEP: creating replication controller externalsvc in namespace services-234
I0223 19:05:06.254794      26 runners.go:184] Created replication controller with name: externalsvc, namespace: services-234, replica count: 2
I0223 19:05:09.306257      26 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Feb 23 19:05:09.324: INFO: Creating new exec pod
Feb 23 19:05:11.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=services-234 execpoddlc8q -- /bin/sh -x -c nslookup nodeport-service'
Feb 23 19:05:12.016: INFO: stderr: "+ nslookup nodeport-service\n"
Feb 23 19:05:12.016: INFO: stdout: "Server:\t\t10.32.0.10\nAddress:\t10.32.0.10#53\n\nnodeport-service.services-234.svc.cluster.local\tcanonical name = externalsvc.services-234.svc.cluster.local.\nName:\texternalsvc.services-234.svc.cluster.local\nAddress: 10.32.0.17\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-234, will wait for the garbage collector to delete the pods
Feb 23 19:05:12.078: INFO: Deleting ReplicationController externalsvc took: 8.502275ms
Feb 23 19:05:12.180: INFO: Terminating ReplicationController externalsvc pods took: 102.333572ms
Feb 23 19:05:16.224: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:05:16.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-234" for this suite.
Feb 23 19:05:22.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:05:22.342: INFO: namespace services-234 deletion completed in 6.078419625s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:16.150 seconds]
[sig-network] Services
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:05:22.342: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 23 19:05:22.388: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1534 /api/v1/namespaces/watch-1534/configmaps/e2e-watch-test-label-changed b66a3498-a16a-4646-8a7e-e91f47459491 26099 0 2020-02-23 19:05:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 23 19:05:22.388: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1534 /api/v1/namespaces/watch-1534/configmaps/e2e-watch-test-label-changed b66a3498-a16a-4646-8a7e-e91f47459491 26100 0 2020-02-23 19:05:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 23 19:05:22.388: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1534 /api/v1/namespaces/watch-1534/configmaps/e2e-watch-test-label-changed b66a3498-a16a-4646-8a7e-e91f47459491 26101 0 2020-02-23 19:05:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 23 19:05:32.415: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1534 /api/v1/namespaces/watch-1534/configmaps/e2e-watch-test-label-changed b66a3498-a16a-4646-8a7e-e91f47459491 26123 0 2020-02-23 19:05:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 23 19:05:32.415: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1534 /api/v1/namespaces/watch-1534/configmaps/e2e-watch-test-label-changed b66a3498-a16a-4646-8a7e-e91f47459491 26124 0 2020-02-23 19:05:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb 23 19:05:32.415: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1534 /api/v1/namespaces/watch-1534/configmaps/e2e-watch-test-label-changed b66a3498-a16a-4646-8a7e-e91f47459491 26125 0 2020-02-23 19:05:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:05:32.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1534" for this suite.
Feb 23 19:05:38.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:05:38.490: INFO: namespace watch-1534 deletion completed in 6.071360973s

• [SLOW TEST:16.148 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:05:38.490: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Feb 23 19:05:38.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 create -f - --namespace=kubectl-9297'
Feb 23 19:05:38.836: INFO: stderr: ""
Feb 23 19:05:38.836: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 23 19:05:38.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9297'
Feb 23 19:05:38.927: INFO: stderr: ""
Feb 23 19:05:38.927: INFO: stdout: "update-demo-nautilus-5phx5 update-demo-nautilus-r77vl "
Feb 23 19:05:38.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-nautilus-5phx5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9297'
Feb 23 19:05:38.979: INFO: stderr: ""
Feb 23 19:05:38.979: INFO: stdout: ""
Feb 23 19:05:38.979: INFO: update-demo-nautilus-5phx5 is created but not running
Feb 23 19:05:43.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9297'
Feb 23 19:05:44.036: INFO: stderr: ""
Feb 23 19:05:44.036: INFO: stdout: "update-demo-nautilus-5phx5 update-demo-nautilus-r77vl "
Feb 23 19:05:44.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-nautilus-5phx5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9297'
Feb 23 19:05:44.091: INFO: stderr: ""
Feb 23 19:05:44.091: INFO: stdout: "true"
Feb 23 19:05:44.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-nautilus-5phx5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9297'
Feb 23 19:05:44.157: INFO: stderr: ""
Feb 23 19:05:44.157: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 19:05:44.157: INFO: validating pod update-demo-nautilus-5phx5
Feb 23 19:05:44.161: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 19:05:44.161: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 19:05:44.161: INFO: update-demo-nautilus-5phx5 is verified up and running
Feb 23 19:05:44.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-nautilus-r77vl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9297'
Feb 23 19:05:44.223: INFO: stderr: ""
Feb 23 19:05:44.223: INFO: stdout: "true"
Feb 23 19:05:44.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-nautilus-r77vl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9297'
Feb 23 19:05:44.275: INFO: stderr: ""
Feb 23 19:05:44.275: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 19:05:44.275: INFO: validating pod update-demo-nautilus-r77vl
Feb 23 19:05:44.284: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 19:05:44.284: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 19:05:44.284: INFO: update-demo-nautilus-r77vl is verified up and running
STEP: rolling-update to new replication controller
Feb 23 19:05:44.285: INFO: scanned /root for discovery docs: <nil>
Feb 23 19:05:44.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-9297'
Feb 23 19:06:07.257: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 23 19:06:07.257: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 23 19:06:07.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9297'
Feb 23 19:06:07.333: INFO: stderr: ""
Feb 23 19:06:07.333: INFO: stdout: "update-demo-kitten-lzm56 update-demo-kitten-n4zqp "
Feb 23 19:06:07.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-kitten-lzm56 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9297'
Feb 23 19:06:07.388: INFO: stderr: ""
Feb 23 19:06:07.388: INFO: stdout: "true"
Feb 23 19:06:07.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-kitten-lzm56 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9297'
Feb 23 19:06:07.446: INFO: stderr: ""
Feb 23 19:06:07.446: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 23 19:06:07.446: INFO: validating pod update-demo-kitten-lzm56
Feb 23 19:06:07.453: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 23 19:06:07.453: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 23 19:06:07.453: INFO: update-demo-kitten-lzm56 is verified up and running
Feb 23 19:06:07.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-kitten-n4zqp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9297'
Feb 23 19:06:07.504: INFO: stderr: ""
Feb 23 19:06:07.504: INFO: stdout: "true"
Feb 23 19:06:07.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-kitten-n4zqp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9297'
Feb 23 19:06:07.565: INFO: stderr: ""
Feb 23 19:06:07.565: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 23 19:06:07.565: INFO: validating pod update-demo-kitten-n4zqp
Feb 23 19:06:07.568: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 23 19:06:07.568: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 23 19:06:07.568: INFO: update-demo-kitten-n4zqp is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:06:07.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9297" for this suite.
Feb 23 19:06:19.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:06:19.648: INFO: namespace kubectl-9297 deletion completed in 12.076592992s

• [SLOW TEST:41.158 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:06:19.649: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Feb 23 19:06:19.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-4633 -- logs-generator --log-lines-total 100 --run-duration 20s'
Feb 23 19:06:19.746: INFO: stderr: ""
Feb 23 19:06:19.746: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Feb 23 19:06:19.746: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Feb 23 19:06:19.746: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4633" to be "running and ready, or succeeded"
Feb 23 19:06:19.751: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.972165ms
Feb 23 19:06:21.754: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.007978167s
Feb 23 19:06:21.754: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Feb 23 19:06:21.754: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Feb 23 19:06:21.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 logs logs-generator logs-generator --namespace=kubectl-4633'
Feb 23 19:06:21.816: INFO: stderr: ""
Feb 23 19:06:21.816: INFO: stdout: "I0223 19:06:20.871596       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/mvgj 521\nI0223 19:06:21.071671       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/r82r 485\nI0223 19:06:21.271685       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/npz 581\nI0223 19:06:21.471684       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/zkc 268\nI0223 19:06:21.673936       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/wdn 229\n"
STEP: limiting log lines
Feb 23 19:06:21.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 logs logs-generator logs-generator --namespace=kubectl-4633 --tail=1'
Feb 23 19:06:21.884: INFO: stderr: ""
Feb 23 19:06:21.884: INFO: stdout: "I0223 19:06:21.871736       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/sjcp 302\n"
STEP: limiting log bytes
Feb 23 19:06:21.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 logs logs-generator logs-generator --namespace=kubectl-4633 --limit-bytes=1'
Feb 23 19:06:21.941: INFO: stderr: ""
Feb 23 19:06:21.941: INFO: stdout: "I"
STEP: exposing timestamps
Feb 23 19:06:21.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 logs logs-generator logs-generator --namespace=kubectl-4633 --tail=1 --timestamps'
Feb 23 19:06:22.005: INFO: stderr: ""
Feb 23 19:06:22.005: INFO: stdout: "2020-02-23T19:06:21.871826029Z I0223 19:06:21.871736       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/sjcp 302\n"
STEP: restricting to a time range
Feb 23 19:06:24.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 logs logs-generator logs-generator --namespace=kubectl-4633 --since=1s'
Feb 23 19:06:24.567: INFO: stderr: ""
Feb 23 19:06:24.567: INFO: stdout: "I0223 19:06:23.671833       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/5s7m 394\nI0223 19:06:23.871666       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/p2zt 285\nI0223 19:06:24.071788       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/2zth 499\nI0223 19:06:24.271716       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/862j 420\nI0223 19:06:24.471801       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/ck6q 449\n"
Feb 23 19:06:24.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 logs logs-generator logs-generator --namespace=kubectl-4633 --since=24h'
Feb 23 19:06:24.628: INFO: stderr: ""
Feb 23 19:06:24.628: INFO: stdout: "I0223 19:06:20.871596       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/mvgj 521\nI0223 19:06:21.071671       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/r82r 485\nI0223 19:06:21.271685       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/npz 581\nI0223 19:06:21.471684       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/zkc 268\nI0223 19:06:21.673936       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/wdn 229\nI0223 19:06:21.871736       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/sjcp 302\nI0223 19:06:22.072519       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/jsln 359\nI0223 19:06:22.271816       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/n6m 577\nI0223 19:06:22.471710       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/pfn 306\nI0223 19:06:22.671774       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/lztn 499\nI0223 19:06:22.871663       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/xpc 210\nI0223 19:06:23.071878       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/46h 470\nI0223 19:06:23.271728       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/gfst 215\nI0223 19:06:23.471719       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/fqx5 373\nI0223 19:06:23.671833       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/5s7m 394\nI0223 19:06:23.871666       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/p2zt 285\nI0223 19:06:24.071788       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/2zth 499\nI0223 19:06:24.271716       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/862j 420\nI0223 19:06:24.471801       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/ck6q 449\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Feb 23 19:06:24.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 delete pod logs-generator --namespace=kubectl-4633'
Feb 23 19:06:33.482: INFO: stderr: ""
Feb 23 19:06:33.482: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:06:33.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4633" for this suite.
Feb 23 19:06:39.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:06:39.582: INFO: namespace kubectl-4633 deletion completed in 6.096469205s

• [SLOW TEST:19.933 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:06:39.582: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-bc5dd1c0-ef80-4b0b-b9a8-f25722a9d42f
STEP: Creating a pod to test consume configMaps
Feb 23 19:06:39.618: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-80d5aee6-c537-4b2e-b7e2-69b436af6e25" in namespace "projected-6281" to be "success or failure"
Feb 23 19:06:39.622: INFO: Pod "pod-projected-configmaps-80d5aee6-c537-4b2e-b7e2-69b436af6e25": Phase="Pending", Reason="", readiness=false. Elapsed: 3.019054ms
Feb 23 19:06:41.625: INFO: Pod "pod-projected-configmaps-80d5aee6-c537-4b2e-b7e2-69b436af6e25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006848666s
STEP: Saw pod success
Feb 23 19:06:41.626: INFO: Pod "pod-projected-configmaps-80d5aee6-c537-4b2e-b7e2-69b436af6e25" satisfied condition "success or failure"
Feb 23 19:06:41.627: INFO: Trying to get logs from node worker00 pod pod-projected-configmaps-80d5aee6-c537-4b2e-b7e2-69b436af6e25 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 19:06:41.901: INFO: Waiting for pod pod-projected-configmaps-80d5aee6-c537-4b2e-b7e2-69b436af6e25 to disappear
Feb 23 19:06:41.908: INFO: Pod pod-projected-configmaps-80d5aee6-c537-4b2e-b7e2-69b436af6e25 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:06:41.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6281" for this suite.
Feb 23 19:06:47.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:06:47.990: INFO: namespace projected-6281 deletion completed in 6.079670168s

• [SLOW TEST:8.408 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:06:47.990: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 23 19:06:48.029: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a3c91fd4-1b29-4bc3-84b8-0ecd50e33b4d" in namespace "downward-api-7575" to be "success or failure"
Feb 23 19:06:48.035: INFO: Pod "downwardapi-volume-a3c91fd4-1b29-4bc3-84b8-0ecd50e33b4d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.913212ms
Feb 23 19:06:50.037: INFO: Pod "downwardapi-volume-a3c91fd4-1b29-4bc3-84b8-0ecd50e33b4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008637141s
STEP: Saw pod success
Feb 23 19:06:50.037: INFO: Pod "downwardapi-volume-a3c91fd4-1b29-4bc3-84b8-0ecd50e33b4d" satisfied condition "success or failure"
Feb 23 19:06:50.040: INFO: Trying to get logs from node worker00 pod downwardapi-volume-a3c91fd4-1b29-4bc3-84b8-0ecd50e33b4d container client-container: <nil>
STEP: delete the pod
Feb 23 19:06:50.056: INFO: Waiting for pod downwardapi-volume-a3c91fd4-1b29-4bc3-84b8-0ecd50e33b4d to disappear
Feb 23 19:06:50.058: INFO: Pod downwardapi-volume-a3c91fd4-1b29-4bc3-84b8-0ecd50e33b4d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:06:50.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7575" for this suite.
Feb 23 19:06:56.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:06:56.144: INFO: namespace downward-api-7575 deletion completed in 6.082710184s

• [SLOW TEST:8.154 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:06:56.145: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-618
STEP: creating replication controller nodeport-test in namespace services-618
I0223 19:06:56.200418      26 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-618, replica count: 2
Feb 23 19:06:59.251: INFO: Creating new exec pod
I0223 19:06:59.251648      26 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 23 19:07:02.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=services-618 execpodpk82z -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Feb 23 19:07:02.371: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 23 19:07:02.371: INFO: stdout: ""
Feb 23 19:07:02.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=services-618 execpodpk82z -- /bin/sh -x -c nc -zv -t -w 2 10.32.0.184 80'
Feb 23 19:07:02.921: INFO: stderr: "+ nc -zv -t -w 2 10.32.0.184 80\nConnection to 10.32.0.184 80 port [tcp/http] succeeded!\n"
Feb 23 19:07:02.921: INFO: stdout: ""
Feb 23 19:07:02.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=services-618 execpodpk82z -- /bin/sh -x -c nc -zv -t -w 2 192.168.180.100 32309'
Feb 23 19:07:03.026: INFO: stderr: "+ nc -zv -t -w 2 192.168.180.100 32309\nConnection to 192.168.180.100 32309 port [tcp/32309] succeeded!\n"
Feb 23 19:07:03.026: INFO: stdout: ""
Feb 23 19:07:03.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=services-618 execpodpk82z -- /bin/sh -x -c nc -zv -t -w 2 192.168.180.101 32309'
Feb 23 19:07:03.144: INFO: stderr: "+ nc -zv -t -w 2 192.168.180.101 32309\nConnection to 192.168.180.101 32309 port [tcp/32309] succeeded!\n"
Feb 23 19:07:03.144: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:07:03.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-618" for this suite.
Feb 23 19:07:09.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:07:09.245: INFO: namespace services-618 deletion completed in 6.097622611s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:13.100 seconds]
[sig-network] Services
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:07:09.245: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 19:07:09.273: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:08:09.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2731" for this suite.
Feb 23 19:08:14.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:08:15.063: INFO: namespace custom-resource-definition-2731 deletion completed in 5.803989096s

• [SLOW TEST:65.818 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:08:15.064: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 23 19:08:15.101: INFO: Waiting up to 5m0s for pod "downwardapi-volume-94613750-d4e2-444d-86cb-dc6d015a32b8" in namespace "projected-2584" to be "success or failure"
Feb 23 19:08:15.105: INFO: Pod "downwardapi-volume-94613750-d4e2-444d-86cb-dc6d015a32b8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.864962ms
Feb 23 19:08:17.108: INFO: Pod "downwardapi-volume-94613750-d4e2-444d-86cb-dc6d015a32b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006707416s
Feb 23 19:08:19.111: INFO: Pod "downwardapi-volume-94613750-d4e2-444d-86cb-dc6d015a32b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009235136s
STEP: Saw pod success
Feb 23 19:08:19.111: INFO: Pod "downwardapi-volume-94613750-d4e2-444d-86cb-dc6d015a32b8" satisfied condition "success or failure"
Feb 23 19:08:19.114: INFO: Trying to get logs from node worker00 pod downwardapi-volume-94613750-d4e2-444d-86cb-dc6d015a32b8 container client-container: <nil>
STEP: delete the pod
Feb 23 19:08:19.138: INFO: Waiting for pod downwardapi-volume-94613750-d4e2-444d-86cb-dc6d015a32b8 to disappear
Feb 23 19:08:19.142: INFO: Pod downwardapi-volume-94613750-d4e2-444d-86cb-dc6d015a32b8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:08:19.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2584" for this suite.
Feb 23 19:08:25.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:08:25.231: INFO: namespace projected-2584 deletion completed in 6.083788877s

• [SLOW TEST:10.168 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:08:25.234: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-b0694c65-953e-483f-9309-fde8ecfa42c3
STEP: Creating a pod to test consume secrets
Feb 23 19:08:25.273: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-36dcd1e9-8072-4b57-82fd-a5d523c72e54" in namespace "projected-492" to be "success or failure"
Feb 23 19:08:25.277: INFO: Pod "pod-projected-secrets-36dcd1e9-8072-4b57-82fd-a5d523c72e54": Phase="Pending", Reason="", readiness=false. Elapsed: 3.376248ms
Feb 23 19:08:27.521: INFO: Pod "pod-projected-secrets-36dcd1e9-8072-4b57-82fd-a5d523c72e54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.247107096s
STEP: Saw pod success
Feb 23 19:08:27.521: INFO: Pod "pod-projected-secrets-36dcd1e9-8072-4b57-82fd-a5d523c72e54" satisfied condition "success or failure"
Feb 23 19:08:27.527: INFO: Trying to get logs from node worker00 pod pod-projected-secrets-36dcd1e9-8072-4b57-82fd-a5d523c72e54 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 23 19:08:27.550: INFO: Waiting for pod pod-projected-secrets-36dcd1e9-8072-4b57-82fd-a5d523c72e54 to disappear
Feb 23 19:08:27.554: INFO: Pod pod-projected-secrets-36dcd1e9-8072-4b57-82fd-a5d523c72e54 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:08:27.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-492" for this suite.
Feb 23 19:08:33.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:08:33.642: INFO: namespace projected-492 deletion completed in 6.085981957s

• [SLOW TEST:8.408 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:08:33.644: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 23 19:08:33.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-6520'
Feb 23 19:08:33.910: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 23 19:08:33.910: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Feb 23 19:08:33.923: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-58p98]
Feb 23 19:08:33.925: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-58p98" in namespace "kubectl-6520" to be "running and ready"
Feb 23 19:08:33.928: INFO: Pod "e2e-test-httpd-rc-58p98": Phase="Pending", Reason="", readiness=false. Elapsed: 3.429521ms
Feb 23 19:08:35.931: INFO: Pod "e2e-test-httpd-rc-58p98": Phase="Running", Reason="", readiness=true. Elapsed: 2.006253126s
Feb 23 19:08:35.931: INFO: Pod "e2e-test-httpd-rc-58p98" satisfied condition "running and ready"
Feb 23 19:08:35.931: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-58p98]
Feb 23 19:08:35.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 logs rc/e2e-test-httpd-rc --namespace=kubectl-6520'
Feb 23 19:08:36.009: INFO: stderr: ""
Feb 23 19:08:36.009: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.200.131.148. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.200.131.148. Set the 'ServerName' directive globally to suppress this message\n[Sun Feb 23 19:08:34.733069 2020] [mpm_event:notice] [pid 1:tid 140615624207208] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Sun Feb 23 19:08:34.733180 2020] [core:notice] [pid 1:tid 140615624207208] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Feb 23 19:08:36.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 delete rc e2e-test-httpd-rc --namespace=kubectl-6520'
Feb 23 19:08:36.072: INFO: stderr: ""
Feb 23 19:08:36.072: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:08:36.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6520" for this suite.
Feb 23 19:08:48.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:08:48.157: INFO: namespace kubectl-6520 deletion completed in 12.078688197s

• [SLOW TEST:14.512 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:08:48.158: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-92bt
STEP: Creating a pod to test atomic-volume-subpath
Feb 23 19:08:48.201: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-92bt" in namespace "subpath-8721" to be "success or failure"
Feb 23 19:08:48.204: INFO: Pod "pod-subpath-test-secret-92bt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.916547ms
Feb 23 19:08:50.207: INFO: Pod "pod-subpath-test-secret-92bt": Phase="Running", Reason="", readiness=true. Elapsed: 2.005719596s
Feb 23 19:08:52.210: INFO: Pod "pod-subpath-test-secret-92bt": Phase="Running", Reason="", readiness=true. Elapsed: 4.008473817s
Feb 23 19:08:54.212: INFO: Pod "pod-subpath-test-secret-92bt": Phase="Running", Reason="", readiness=true. Elapsed: 6.011059737s
Feb 23 19:08:56.614: INFO: Pod "pod-subpath-test-secret-92bt": Phase="Running", Reason="", readiness=true. Elapsed: 8.412490734s
Feb 23 19:08:58.617: INFO: Pod "pod-subpath-test-secret-92bt": Phase="Running", Reason="", readiness=true. Elapsed: 10.415707395s
Feb 23 19:09:00.620: INFO: Pod "pod-subpath-test-secret-92bt": Phase="Running", Reason="", readiness=true. Elapsed: 12.419086527s
Feb 23 19:09:02.624: INFO: Pod "pod-subpath-test-secret-92bt": Phase="Running", Reason="", readiness=true. Elapsed: 14.422479741s
Feb 23 19:09:04.626: INFO: Pod "pod-subpath-test-secret-92bt": Phase="Running", Reason="", readiness=true. Elapsed: 16.425380169s
Feb 23 19:09:06.630: INFO: Pod "pod-subpath-test-secret-92bt": Phase="Running", Reason="", readiness=true. Elapsed: 18.428754852s
Feb 23 19:09:08.635: INFO: Pod "pod-subpath-test-secret-92bt": Phase="Running", Reason="", readiness=true. Elapsed: 20.433679285s
Feb 23 19:09:10.638: INFO: Pod "pod-subpath-test-secret-92bt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.436924957s
STEP: Saw pod success
Feb 23 19:09:10.638: INFO: Pod "pod-subpath-test-secret-92bt" satisfied condition "success or failure"
Feb 23 19:09:10.640: INFO: Trying to get logs from node worker00 pod pod-subpath-test-secret-92bt container test-container-subpath-secret-92bt: <nil>
STEP: delete the pod
Feb 23 19:09:10.656: INFO: Waiting for pod pod-subpath-test-secret-92bt to disappear
Feb 23 19:09:10.659: INFO: Pod pod-subpath-test-secret-92bt no longer exists
STEP: Deleting pod pod-subpath-test-secret-92bt
Feb 23 19:09:10.659: INFO: Deleting pod "pod-subpath-test-secret-92bt" in namespace "subpath-8721"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:09:10.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8721" for this suite.
Feb 23 19:09:16.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:09:16.744: INFO: namespace subpath-8721 deletion completed in 6.080363114s

• [SLOW TEST:28.586 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:09:16.744: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-1837658a-2365-4911-b805-839806d2e331
STEP: Creating a pod to test consume configMaps
Feb 23 19:09:16.786: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2685117a-ffa1-4a2d-9821-986c593ad042" in namespace "projected-7863" to be "success or failure"
Feb 23 19:09:16.792: INFO: Pod "pod-projected-configmaps-2685117a-ffa1-4a2d-9821-986c593ad042": Phase="Pending", Reason="", readiness=false. Elapsed: 6.709555ms
Feb 23 19:09:18.796: INFO: Pod "pod-projected-configmaps-2685117a-ffa1-4a2d-9821-986c593ad042": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010273177s
Feb 23 19:09:20.799: INFO: Pod "pod-projected-configmaps-2685117a-ffa1-4a2d-9821-986c593ad042": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013123647s
STEP: Saw pod success
Feb 23 19:09:20.799: INFO: Pod "pod-projected-configmaps-2685117a-ffa1-4a2d-9821-986c593ad042" satisfied condition "success or failure"
Feb 23 19:09:20.802: INFO: Trying to get logs from node worker00 pod pod-projected-configmaps-2685117a-ffa1-4a2d-9821-986c593ad042 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 19:09:20.817: INFO: Waiting for pod pod-projected-configmaps-2685117a-ffa1-4a2d-9821-986c593ad042 to disappear
Feb 23 19:09:20.820: INFO: Pod pod-projected-configmaps-2685117a-ffa1-4a2d-9821-986c593ad042 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:09:20.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7863" for this suite.
Feb 23 19:09:26.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:09:26.904: INFO: namespace projected-7863 deletion completed in 6.080586063s

• [SLOW TEST:10.160 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:09:26.904: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:09:34.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4526" for this suite.
Feb 23 19:09:40.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:09:41.027: INFO: namespace job-4526 deletion completed in 6.081713645s

• [SLOW TEST:14.123 seconds]
[sig-apps] Job
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:09:41.027: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 23 19:09:41.064: INFO: Waiting up to 5m0s for pod "pod-e9f555cb-7421-4d3a-b5d5-ab30c5d8497e" in namespace "emptydir-8565" to be "success or failure"
Feb 23 19:09:41.068: INFO: Pod "pod-e9f555cb-7421-4d3a-b5d5-ab30c5d8497e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.404195ms
Feb 23 19:09:43.071: INFO: Pod "pod-e9f555cb-7421-4d3a-b5d5-ab30c5d8497e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007037616s
STEP: Saw pod success
Feb 23 19:09:43.071: INFO: Pod "pod-e9f555cb-7421-4d3a-b5d5-ab30c5d8497e" satisfied condition "success or failure"
Feb 23 19:09:43.074: INFO: Trying to get logs from node worker00 pod pod-e9f555cb-7421-4d3a-b5d5-ab30c5d8497e container test-container: <nil>
STEP: delete the pod
Feb 23 19:09:43.098: INFO: Waiting for pod pod-e9f555cb-7421-4d3a-b5d5-ab30c5d8497e to disappear
Feb 23 19:09:43.103: INFO: Pod pod-e9f555cb-7421-4d3a-b5d5-ab30c5d8497e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:09:43.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8565" for this suite.
Feb 23 19:09:49.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:09:49.192: INFO: namespace emptydir-8565 deletion completed in 6.084172978s

• [SLOW TEST:8.165 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:09:49.194: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 19:09:49.725: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 23 19:09:57.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 --namespace=crd-publish-openapi-6990 create -f -'
Feb 23 19:09:58.592: INFO: stderr: ""
Feb 23 19:09:58.592: INFO: stdout: "e2e-test-crd-publish-openapi-5986-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 23 19:09:58.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 --namespace=crd-publish-openapi-6990 delete e2e-test-crd-publish-openapi-5986-crds test-cr'
Feb 23 19:09:58.676: INFO: stderr: ""
Feb 23 19:09:58.676: INFO: stdout: "e2e-test-crd-publish-openapi-5986-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Feb 23 19:09:58.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 --namespace=crd-publish-openapi-6990 apply -f -'
Feb 23 19:09:58.814: INFO: stderr: ""
Feb 23 19:09:58.814: INFO: stdout: "e2e-test-crd-publish-openapi-5986-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 23 19:09:58.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 --namespace=crd-publish-openapi-6990 delete e2e-test-crd-publish-openapi-5986-crds test-cr'
Feb 23 19:09:58.871: INFO: stderr: ""
Feb 23 19:09:58.871: INFO: stdout: "e2e-test-crd-publish-openapi-5986-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb 23 19:09:58.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 explain e2e-test-crd-publish-openapi-5986-crds'
Feb 23 19:09:58.991: INFO: stderr: ""
Feb 23 19:09:58.991: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5986-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:10:00.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6990" for this suite.
Feb 23 19:10:06.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:10:06.938: INFO: namespace crd-publish-openapi-6990 deletion completed in 6.081654086s

• [SLOW TEST:17.745 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:10:06.940: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb 23 19:10:06.985: INFO: Pod name pod-release: Found 0 pods out of 1
Feb 23 19:10:11.989: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:10:13.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2805" for this suite.
Feb 23 19:10:21.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:10:21.109: INFO: namespace replication-controller-2805 deletion completed in 8.097195722s

• [SLOW TEST:14.169 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:10:21.111: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 19:10:21.149: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-7aaf333a-6db1-43d8-b0cd-f1c0f032a5f9" in namespace "security-context-test-93" to be "success or failure"
Feb 23 19:10:21.156: INFO: Pod "busybox-readonly-false-7aaf333a-6db1-43d8-b0cd-f1c0f032a5f9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.10163ms
Feb 23 19:10:23.159: INFO: Pod "busybox-readonly-false-7aaf333a-6db1-43d8-b0cd-f1c0f032a5f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00999844s
Feb 23 19:10:25.163: INFO: Pod "busybox-readonly-false-7aaf333a-6db1-43d8-b0cd-f1c0f032a5f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013890359s
Feb 23 19:10:25.163: INFO: Pod "busybox-readonly-false-7aaf333a-6db1-43d8-b0cd-f1c0f032a5f9" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:10:25.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-93" for this suite.
Feb 23 19:10:31.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:10:31.244: INFO: namespace security-context-test-93 deletion completed in 6.078209771s

• [SLOW TEST:10.133 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:10:31.246: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 23 19:10:37.334: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 23 19:10:37.337: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 23 19:10:39.337: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 23 19:10:39.340: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 23 19:10:41.337: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 23 19:10:41.340: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 23 19:10:43.337: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 23 19:10:43.341: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 23 19:10:45.337: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 23 19:10:45.340: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:10:45.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6445" for this suite.
Feb 23 19:11:13.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:11:13.428: INFO: namespace container-lifecycle-hook-6445 deletion completed in 28.084893086s

• [SLOW TEST:42.182 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:11:13.429: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:11:30.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4394" for this suite.
Feb 23 19:11:36.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:11:36.576: INFO: namespace resourcequota-4394 deletion completed in 6.076243368s

• [SLOW TEST:23.147 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:11:36.577: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 23 19:11:36.610: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dbc4f00e-7924-4d5b-9fb0-4b125117956e" in namespace "projected-2809" to be "success or failure"
Feb 23 19:11:36.631: INFO: Pod "downwardapi-volume-dbc4f00e-7924-4d5b-9fb0-4b125117956e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.504024ms
Feb 23 19:11:38.635: INFO: Pod "downwardapi-volume-dbc4f00e-7924-4d5b-9fb0-4b125117956e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024226876s
STEP: Saw pod success
Feb 23 19:11:38.635: INFO: Pod "downwardapi-volume-dbc4f00e-7924-4d5b-9fb0-4b125117956e" satisfied condition "success or failure"
Feb 23 19:11:38.638: INFO: Trying to get logs from node worker00 pod downwardapi-volume-dbc4f00e-7924-4d5b-9fb0-4b125117956e container client-container: <nil>
STEP: delete the pod
Feb 23 19:11:38.652: INFO: Waiting for pod downwardapi-volume-dbc4f00e-7924-4d5b-9fb0-4b125117956e to disappear
Feb 23 19:11:38.655: INFO: Pod downwardapi-volume-dbc4f00e-7924-4d5b-9fb0-4b125117956e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:11:38.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2809" for this suite.
Feb 23 19:11:44.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:11:44.739: INFO: namespace projected-2809 deletion completed in 6.080800394s

• [SLOW TEST:8.163 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:11:44.741: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Feb 23 19:11:54.865: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0223 19:11:54.865604      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:11:54.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3236" for this suite.
Feb 23 19:12:00.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:12:00.959: INFO: namespace gc-3236 deletion completed in 6.09090421s

• [SLOW TEST:16.218 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:12:00.959: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 23 19:12:01.004: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3dec72d-f585-4801-a98a-d263ad7b0c51" in namespace "downward-api-7795" to be "success or failure"
Feb 23 19:12:01.008: INFO: Pod "downwardapi-volume-d3dec72d-f585-4801-a98a-d263ad7b0c51": Phase="Pending", Reason="", readiness=false. Elapsed: 4.090227ms
Feb 23 19:12:03.013: INFO: Pod "downwardapi-volume-d3dec72d-f585-4801-a98a-d263ad7b0c51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008755559s
STEP: Saw pod success
Feb 23 19:12:03.013: INFO: Pod "downwardapi-volume-d3dec72d-f585-4801-a98a-d263ad7b0c51" satisfied condition "success or failure"
Feb 23 19:12:03.015: INFO: Trying to get logs from node worker00 pod downwardapi-volume-d3dec72d-f585-4801-a98a-d263ad7b0c51 container client-container: <nil>
STEP: delete the pod
Feb 23 19:12:03.080: INFO: Waiting for pod downwardapi-volume-d3dec72d-f585-4801-a98a-d263ad7b0c51 to disappear
Feb 23 19:12:03.083: INFO: Pod downwardapi-volume-d3dec72d-f585-4801-a98a-d263ad7b0c51 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:12:03.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7795" for this suite.
Feb 23 19:12:09.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:12:09.396: INFO: namespace downward-api-7795 deletion completed in 6.31019398s

• [SLOW TEST:8.437 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:12:09.396: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 19:12:09.968: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 23 19:12:11.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718081929, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718081929, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718081930, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718081929, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 19:12:14.992: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:12:15.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9388" for this suite.
Feb 23 19:12:21.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:12:21.119: INFO: namespace webhook-9388 deletion completed in 6.081389877s
STEP: Destroying namespace "webhook-9388-markers" for this suite.
Feb 23 19:12:27.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:12:27.189: INFO: namespace webhook-9388-markers deletion completed in 6.070578086s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.804 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:12:27.200: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 19:12:27.236: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 23 19:12:32.245: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 23 19:12:32.246: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Feb 23 19:12:32.270: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7845 /apis/apps/v1/namespaces/deployment-7845/deployments/test-cleanup-deployment d7dd7c2d-5263-4878-9d58-bccdcc8695cc 28383 1 2020-02-23 19:12:32 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002b1d988 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Feb 23 19:12:32.279: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-7845 /apis/apps/v1/namespaces/deployment-7845/replicasets/test-cleanup-deployment-65db99849b 9a116606-4ec0-499f-b769-0a89ecdbdb15 28385 1 2020-02-23 19:12:32 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment d7dd7c2d-5263-4878-9d58-bccdcc8695cc 0xc004e63d27 0xc004e63d28}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004e63d88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 23 19:12:32.279: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Feb 23 19:12:32.279: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-7845 /apis/apps/v1/namespaces/deployment-7845/replicasets/test-cleanup-controller 359363d4-f4fb-4965-a810-86723d707944 28384 1 2020-02-23 19:12:27 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment d7dd7c2d-5263-4878-9d58-bccdcc8695cc 0xc004e63c57 0xc004e63c58}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004e63cb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 23 19:12:32.286: INFO: Pod "test-cleanup-controller-rgd7m" is available:
&Pod{ObjectMeta:{test-cleanup-controller-rgd7m test-cleanup-controller- deployment-7845 /api/v1/namespaces/deployment-7845/pods/test-cleanup-controller-rgd7m 37cf813a-241e-405c-b960-0c340d0cf4f4 28370 0 2020-02-23 19:12:27 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:10.200.131.174/32] [{apps/v1 ReplicaSet test-cleanup-controller 359363d4-f4fb-4965-a810-86723d707944 0xc00390a1f7 0xc00390a1f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wrx9m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wrx9m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wrx9m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 19:12:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 19:12:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 19:12:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 19:12:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:10.200.131.174,StartTime:2020-02-23 19:12:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 19:12:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://b11bc8132760f3a666c56915d026bd750f245e9e6a1e2f71cadd0668221688c8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.131.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 19:12:32.287: INFO: Pod "test-cleanup-deployment-65db99849b-tcg78" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-65db99849b-tcg78 test-cleanup-deployment-65db99849b- deployment-7845 /api/v1/namespaces/deployment-7845/pods/test-cleanup-deployment-65db99849b-tcg78 558c9cdc-7460-48a3-8948-e030f67caba7 28386 0 2020-02-23 19:12:32 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-65db99849b 9a116606-4ec0-499f-b769-0a89ecdbdb15 0xc00390a3a7 0xc00390a3a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wrx9m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wrx9m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wrx9m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:12:32.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7845" for this suite.
Feb 23 19:12:38.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:12:38.397: INFO: namespace deployment-7845 deletion completed in 6.094022622s

• [SLOW TEST:11.197 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:12:38.397: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-rbwsj in namespace proxy-9122
I0223 19:12:38.443022      26 runners.go:184] Created replication controller with name: proxy-service-rbwsj, namespace: proxy-9122, replica count: 1
I0223 19:12:39.493770      26 runners.go:184] proxy-service-rbwsj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0223 19:12:40.497027      26 runners.go:184] proxy-service-rbwsj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0223 19:12:41.498153      26 runners.go:184] proxy-service-rbwsj Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 23 19:12:41.500: INFO: setup took 3.075765567s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 23 19:12:41.509: INFO: (0) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 8.865799ms)
Feb 23 19:12:41.509: INFO: (0) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 8.212508ms)
Feb 23 19:12:41.509: INFO: (0) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">... (200; 8.363174ms)
Feb 23 19:12:41.509: INFO: (0) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 9.046049ms)
Feb 23 19:12:41.512: INFO: (0) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 11.458805ms)
Feb 23 19:12:41.512: INFO: (0) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/rewriteme">test</a> (200; 11.119686ms)
Feb 23 19:12:41.512: INFO: (0) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">test<... (200; 11.670535ms)
Feb 23 19:12:41.513: INFO: (0) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname1/proxy/: foo (200; 12.679773ms)
Feb 23 19:12:41.514: INFO: (0) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname2/proxy/: tls qux (200; 14.016402ms)
Feb 23 19:12:41.514: INFO: (0) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:462/proxy/: tls qux (200; 14.478317ms)
Feb 23 19:12:41.514: INFO: (0) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/tlsrewritem... (200; 13.96763ms)
Feb 23 19:12:41.515: INFO: (0) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:460/proxy/: tls baz (200; 15.08237ms)
Feb 23 19:12:41.515: INFO: (0) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname1/proxy/: tls baz (200; 14.564814ms)
Feb 23 19:12:41.518: INFO: (0) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname1/proxy/: foo (200; 18.01084ms)
Feb 23 19:12:41.518: INFO: (0) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname2/proxy/: bar (200; 17.963607ms)
Feb 23 19:12:41.518: INFO: (0) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname2/proxy/: bar (200; 17.262444ms)
Feb 23 19:12:41.524: INFO: (1) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:462/proxy/: tls qux (200; 5.480193ms)
Feb 23 19:12:41.524: INFO: (1) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/rewriteme">test</a> (200; 5.224934ms)
Feb 23 19:12:41.525: INFO: (1) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 6.72506ms)
Feb 23 19:12:41.525: INFO: (1) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/tlsrewritem... (200; 6.561665ms)
Feb 23 19:12:41.525: INFO: (1) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname2/proxy/: bar (200; 6.536744ms)
Feb 23 19:12:41.525: INFO: (1) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 6.680297ms)
Feb 23 19:12:41.525: INFO: (1) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:460/proxy/: tls baz (200; 6.870817ms)
Feb 23 19:12:41.525: INFO: (1) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">test<... (200; 6.829192ms)
Feb 23 19:12:41.525: INFO: (1) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 6.725901ms)
Feb 23 19:12:41.525: INFO: (1) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">... (200; 6.88951ms)
Feb 23 19:12:41.526: INFO: (1) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname1/proxy/: foo (200; 7.449769ms)
Feb 23 19:12:41.526: INFO: (1) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname2/proxy/: tls qux (200; 7.470508ms)
Feb 23 19:12:41.526: INFO: (1) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 8.290073ms)
Feb 23 19:12:41.527: INFO: (1) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname1/proxy/: foo (200; 8.858237ms)
Feb 23 19:12:41.527: INFO: (1) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname2/proxy/: bar (200; 9.11402ms)
Feb 23 19:12:41.528: INFO: (1) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname1/proxy/: tls baz (200; 9.230454ms)
Feb 23 19:12:41.530: INFO: (2) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 2.747357ms)
Feb 23 19:12:41.534: INFO: (2) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname1/proxy/: tls baz (200; 6.365732ms)
Feb 23 19:12:41.534: INFO: (2) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 6.182513ms)
Feb 23 19:12:41.534: INFO: (2) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 6.155867ms)
Feb 23 19:12:41.534: INFO: (2) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">... (200; 5.643009ms)
Feb 23 19:12:41.534: INFO: (2) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:462/proxy/: tls qux (200; 6.284953ms)
Feb 23 19:12:41.535: INFO: (2) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 6.753613ms)
Feb 23 19:12:41.535: INFO: (2) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">test<... (200; 6.901423ms)
Feb 23 19:12:41.535: INFO: (2) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:460/proxy/: tls baz (200; 6.995454ms)
Feb 23 19:12:41.536: INFO: (2) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/tlsrewritem... (200; 7.641237ms)
Feb 23 19:12:41.536: INFO: (2) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/rewriteme">test</a> (200; 8.152059ms)
Feb 23 19:12:41.536: INFO: (2) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname2/proxy/: bar (200; 8.609087ms)
Feb 23 19:12:41.537: INFO: (2) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname1/proxy/: foo (200; 9.117895ms)
Feb 23 19:12:41.537: INFO: (2) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname2/proxy/: tls qux (200; 8.56661ms)
Feb 23 19:12:41.538: INFO: (2) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname2/proxy/: bar (200; 9.455652ms)
Feb 23 19:12:41.538: INFO: (2) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname1/proxy/: foo (200; 9.522801ms)
Feb 23 19:12:41.542: INFO: (3) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 4.177426ms)
Feb 23 19:12:41.542: INFO: (3) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/rewriteme">test</a> (200; 4.388768ms)
Feb 23 19:12:41.542: INFO: (3) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 4.416561ms)
Feb 23 19:12:41.542: INFO: (3) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:462/proxy/: tls qux (200; 4.670093ms)
Feb 23 19:12:41.542: INFO: (3) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/tlsrewritem... (200; 4.674945ms)
Feb 23 19:12:41.544: INFO: (3) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 6.561649ms)
Feb 23 19:12:41.546: INFO: (3) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:460/proxy/: tls baz (200; 7.79009ms)
Feb 23 19:12:41.546: INFO: (3) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">... (200; 8.270911ms)
Feb 23 19:12:41.546: INFO: (3) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 8.265752ms)
Feb 23 19:12:41.546: INFO: (3) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">test<... (200; 8.56437ms)
Feb 23 19:12:41.548: INFO: (3) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname2/proxy/: tls qux (200; 9.877337ms)
Feb 23 19:12:41.548: INFO: (3) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname1/proxy/: tls baz (200; 9.875216ms)
Feb 23 19:12:41.548: INFO: (3) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname1/proxy/: foo (200; 10.075647ms)
Feb 23 19:12:41.548: INFO: (3) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname1/proxy/: foo (200; 10.240534ms)
Feb 23 19:12:41.548: INFO: (3) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname2/proxy/: bar (200; 10.391318ms)
Feb 23 19:12:41.548: INFO: (3) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname2/proxy/: bar (200; 10.534835ms)
Feb 23 19:12:41.551: INFO: (4) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:462/proxy/: tls qux (200; 2.933897ms)
Feb 23 19:12:41.555: INFO: (4) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 6.616789ms)
Feb 23 19:12:41.555: INFO: (4) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 6.634823ms)
Feb 23 19:12:41.555: INFO: (4) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/rewriteme">test</a> (200; 6.403174ms)
Feb 23 19:12:41.555: INFO: (4) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/tlsrewritem... (200; 6.680868ms)
Feb 23 19:12:41.555: INFO: (4) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 6.811162ms)
Feb 23 19:12:41.555: INFO: (4) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 6.797081ms)
Feb 23 19:12:41.556: INFO: (4) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">test<... (200; 7.440401ms)
Feb 23 19:12:41.556: INFO: (4) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname2/proxy/: tls qux (200; 7.175487ms)
Feb 23 19:12:41.556: INFO: (4) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:460/proxy/: tls baz (200; 7.178125ms)
Feb 23 19:12:41.557: INFO: (4) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname2/proxy/: bar (200; 8.030678ms)
Feb 23 19:12:41.557: INFO: (4) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname2/proxy/: bar (200; 8.174901ms)
Feb 23 19:12:41.557: INFO: (4) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname1/proxy/: tls baz (200; 8.421999ms)
Feb 23 19:12:41.558: INFO: (4) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname1/proxy/: foo (200; 9.138321ms)
Feb 23 19:12:41.558: INFO: (4) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname1/proxy/: foo (200; 8.99996ms)
Feb 23 19:12:41.558: INFO: (4) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">... (200; 9.255554ms)
Feb 23 19:12:41.565: INFO: (5) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:462/proxy/: tls qux (200; 7.165489ms)
Feb 23 19:12:41.565: INFO: (5) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">test<... (200; 7.252925ms)
Feb 23 19:12:41.565: INFO: (5) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 7.341174ms)
Feb 23 19:12:41.565: INFO: (5) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 7.233534ms)
Feb 23 19:12:41.565: INFO: (5) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 7.351662ms)
Feb 23 19:12:41.566: INFO: (5) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/tlsrewritem... (200; 7.707642ms)
Feb 23 19:12:41.566: INFO: (5) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">... (200; 7.810889ms)
Feb 23 19:12:41.566: INFO: (5) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/rewriteme">test</a> (200; 7.841261ms)
Feb 23 19:12:41.566: INFO: (5) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 7.948183ms)
Feb 23 19:12:41.566: INFO: (5) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:460/proxy/: tls baz (200; 8.211005ms)
Feb 23 19:12:41.567: INFO: (5) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname2/proxy/: bar (200; 9.600397ms)
Feb 23 19:12:41.568: INFO: (5) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname2/proxy/: bar (200; 9.615982ms)
Feb 23 19:12:41.568: INFO: (5) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname2/proxy/: tls qux (200; 9.657975ms)
Feb 23 19:12:41.568: INFO: (5) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname1/proxy/: tls baz (200; 9.850511ms)
Feb 23 19:12:41.568: INFO: (5) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname1/proxy/: foo (200; 9.830564ms)
Feb 23 19:12:41.570: INFO: (5) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname1/proxy/: foo (200; 11.562786ms)
Feb 23 19:12:41.573: INFO: (6) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 3.182439ms)
Feb 23 19:12:41.573: INFO: (6) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">test<... (200; 3.177662ms)
Feb 23 19:12:41.574: INFO: (6) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">... (200; 4.06505ms)
Feb 23 19:12:41.576: INFO: (6) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:462/proxy/: tls qux (200; 5.933789ms)
Feb 23 19:12:41.576: INFO: (6) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:460/proxy/: tls baz (200; 5.887409ms)
Feb 23 19:12:41.577: INFO: (6) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/tlsrewritem... (200; 7.096226ms)
Feb 23 19:12:41.577: INFO: (6) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 7.095482ms)
Feb 23 19:12:41.578: INFO: (6) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 8.045564ms)
Feb 23 19:12:41.578: INFO: (6) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname2/proxy/: bar (200; 7.672261ms)
Feb 23 19:12:41.578: INFO: (6) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 7.956523ms)
Feb 23 19:12:41.578: INFO: (6) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname1/proxy/: foo (200; 7.945629ms)
Feb 23 19:12:41.579: INFO: (6) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname1/proxy/: tls baz (200; 9.591704ms)
Feb 23 19:12:41.579: INFO: (6) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/rewriteme">test</a> (200; 9.329998ms)
Feb 23 19:12:41.580: INFO: (6) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname2/proxy/: tls qux (200; 9.478044ms)
Feb 23 19:12:41.580: INFO: (6) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname1/proxy/: foo (200; 9.599439ms)
Feb 23 19:12:41.580: INFO: (6) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname2/proxy/: bar (200; 9.719704ms)
Feb 23 19:12:41.585: INFO: (7) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/rewriteme">test</a> (200; 4.695315ms)
Feb 23 19:12:41.585: INFO: (7) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 4.781323ms)
Feb 23 19:12:41.585: INFO: (7) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">... (200; 4.754122ms)
Feb 23 19:12:41.586: INFO: (7) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:460/proxy/: tls baz (200; 6.179369ms)
Feb 23 19:12:41.586: INFO: (7) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 6.104725ms)
Feb 23 19:12:41.586: INFO: (7) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname2/proxy/: bar (200; 6.440183ms)
Feb 23 19:12:41.586: INFO: (7) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 6.452398ms)
Feb 23 19:12:41.586: INFO: (7) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">test<... (200; 6.382031ms)
Feb 23 19:12:41.589: INFO: (7) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:462/proxy/: tls qux (200; 8.402835ms)
Feb 23 19:12:41.589: INFO: (7) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 8.488578ms)
Feb 23 19:12:41.589: INFO: (7) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname2/proxy/: tls qux (200; 8.689932ms)
Feb 23 19:12:41.589: INFO: (7) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/tlsrewritem... (200; 8.721999ms)
Feb 23 19:12:41.590: INFO: (7) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname1/proxy/: foo (200; 10.431542ms)
Feb 23 19:12:41.591: INFO: (7) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname2/proxy/: bar (200; 11.237881ms)
Feb 23 19:12:41.591: INFO: (7) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname1/proxy/: foo (200; 11.335956ms)
Feb 23 19:12:41.592: INFO: (7) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname1/proxy/: tls baz (200; 11.5911ms)
Feb 23 19:12:41.595: INFO: (8) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:462/proxy/: tls qux (200; 2.967658ms)
Feb 23 19:12:41.596: INFO: (8) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 4.608638ms)
Feb 23 19:12:41.596: INFO: (8) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 4.544401ms)
Feb 23 19:12:41.597: INFO: (8) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:460/proxy/: tls baz (200; 4.464324ms)
Feb 23 19:12:41.598: INFO: (8) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">test<... (200; 5.567067ms)
Feb 23 19:12:41.599: INFO: (8) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 6.474757ms)
Feb 23 19:12:41.599: INFO: (8) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">... (200; 6.890426ms)
Feb 23 19:12:41.600: INFO: (8) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/tlsrewritem... (200; 7.542908ms)
Feb 23 19:12:41.600: INFO: (8) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/rewriteme">test</a> (200; 6.912864ms)
Feb 23 19:12:41.601: INFO: (8) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname2/proxy/: bar (200; 8.78575ms)
Feb 23 19:12:41.601: INFO: (8) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname2/proxy/: bar (200; 8.411827ms)
Feb 23 19:12:41.602: INFO: (8) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname1/proxy/: tls baz (200; 9.839297ms)
Feb 23 19:12:41.603: INFO: (8) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 10.368682ms)
Feb 23 19:12:41.603: INFO: (8) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname1/proxy/: foo (200; 10.811407ms)
Feb 23 19:12:41.603: INFO: (8) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname1/proxy/: foo (200; 11.011842ms)
Feb 23 19:12:41.604: INFO: (8) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname2/proxy/: tls qux (200; 12.413596ms)
Feb 23 19:12:41.609: INFO: (9) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">... (200; 4.058658ms)
Feb 23 19:12:41.609: INFO: (9) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/rewriteme">test</a> (200; 4.372291ms)
Feb 23 19:12:41.609: INFO: (9) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">test<... (200; 4.699128ms)
Feb 23 19:12:41.609: INFO: (9) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:462/proxy/: tls qux (200; 4.574386ms)
Feb 23 19:12:41.610: INFO: (9) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:460/proxy/: tls baz (200; 5.203862ms)
Feb 23 19:12:41.612: INFO: (9) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 7.12012ms)
Feb 23 19:12:41.612: INFO: (9) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 7.024242ms)
Feb 23 19:12:41.612: INFO: (9) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/tlsrewritem... (200; 6.990606ms)
Feb 23 19:12:41.612: INFO: (9) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 7.692126ms)
Feb 23 19:12:41.785: INFO: (9) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname1/proxy/: tls baz (200; 179.68416ms)
Feb 23 19:12:41.785: INFO: (9) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname2/proxy/: bar (200; 179.881079ms)
Feb 23 19:12:41.785: INFO: (9) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 179.913117ms)
Feb 23 19:12:41.785: INFO: (9) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname1/proxy/: foo (200; 180.242455ms)
Feb 23 19:12:41.785: INFO: (9) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname2/proxy/: bar (200; 180.055046ms)
Feb 23 19:12:41.785: INFO: (9) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname2/proxy/: tls qux (200; 179.950211ms)
Feb 23 19:12:41.785: INFO: (9) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname1/proxy/: foo (200; 179.833132ms)
Feb 23 19:12:41.794: INFO: (10) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/tlsrewritem... (200; 8.161788ms)
Feb 23 19:12:41.795: INFO: (10) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">test<... (200; 9.03682ms)
Feb 23 19:12:41.795: INFO: (10) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 8.879448ms)
Feb 23 19:12:41.795: INFO: (10) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/rewriteme">test</a> (200; 8.665005ms)
Feb 23 19:12:41.795: INFO: (10) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 10.587755ms)
Feb 23 19:12:41.797: INFO: (10) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname2/proxy/: bar (200; 11.265307ms)
Feb 23 19:12:41.797: INFO: (10) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 11.57187ms)
Feb 23 19:12:41.797: INFO: (10) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:460/proxy/: tls baz (200; 12.422747ms)
Feb 23 19:12:41.797: INFO: (10) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:462/proxy/: tls qux (200; 11.640111ms)
Feb 23 19:12:41.799: INFO: (10) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname2/proxy/: bar (200; 14.163907ms)
Feb 23 19:12:41.799: INFO: (10) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">... (200; 13.786513ms)
Feb 23 19:12:41.799: INFO: (10) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 13.287057ms)
Feb 23 19:12:41.801: INFO: (10) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname1/proxy/: tls baz (200; 14.683083ms)
Feb 23 19:12:41.803: INFO: (10) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname1/proxy/: foo (200; 17.007547ms)
Feb 23 19:12:41.803: INFO: (10) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname1/proxy/: foo (200; 17.772474ms)
Feb 23 19:12:41.803: INFO: (10) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname2/proxy/: tls qux (200; 17.585075ms)
Feb 23 19:12:41.809: INFO: (11) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 5.398443ms)
Feb 23 19:12:41.809: INFO: (11) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">test<... (200; 5.401767ms)
Feb 23 19:12:41.809: INFO: (11) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:460/proxy/: tls baz (200; 5.474775ms)
Feb 23 19:12:41.811: INFO: (11) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">... (200; 7.686405ms)
Feb 23 19:12:41.811: INFO: (11) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/tlsrewritem... (200; 7.951285ms)
Feb 23 19:12:41.811: INFO: (11) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 8.168886ms)
Feb 23 19:12:41.811: INFO: (11) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 8.1678ms)
Feb 23 19:12:41.812: INFO: (11) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 8.35735ms)
Feb 23 19:12:41.812: INFO: (11) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname2/proxy/: bar (200; 8.715195ms)
Feb 23 19:12:41.812: INFO: (11) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/rewriteme">test</a> (200; 9.195827ms)
Feb 23 19:12:41.812: INFO: (11) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname2/proxy/: bar (200; 8.958863ms)
Feb 23 19:12:41.812: INFO: (11) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:462/proxy/: tls qux (200; 9.133739ms)
Feb 23 19:12:41.813: INFO: (11) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname1/proxy/: foo (200; 9.466485ms)
Feb 23 19:12:41.813: INFO: (11) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname2/proxy/: tls qux (200; 9.600236ms)
Feb 23 19:12:41.815: INFO: (11) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname1/proxy/: tls baz (200; 11.886676ms)
Feb 23 19:12:41.815: INFO: (11) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname1/proxy/: foo (200; 11.839086ms)
Feb 23 19:12:41.824: INFO: (12) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">... (200; 8.538558ms)
Feb 23 19:12:41.830: INFO: (12) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 14.965836ms)
Feb 23 19:12:41.831: INFO: (12) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">test<... (200; 15.454521ms)
Feb 23 19:12:41.831: INFO: (12) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:462/proxy/: tls qux (200; 14.647777ms)
Feb 23 19:12:41.831: INFO: (12) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 14.541841ms)
Feb 23 19:12:41.834: INFO: (12) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:460/proxy/: tls baz (200; 19.159527ms)
Feb 23 19:12:41.834: INFO: (12) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/tlsrewritem... (200; 17.74392ms)
Feb 23 19:12:41.834: INFO: (12) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 18.758553ms)
Feb 23 19:12:41.834: INFO: (12) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname2/proxy/: bar (200; 17.888741ms)
Feb 23 19:12:41.834: INFO: (12) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/rewriteme">test</a> (200; 18.360165ms)
Feb 23 19:12:41.835: INFO: (12) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 18.661297ms)
Feb 23 19:12:41.838: INFO: (12) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname1/proxy/: tls baz (200; 22.022184ms)
Feb 23 19:12:41.838: INFO: (12) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname1/proxy/: foo (200; 21.544663ms)
Feb 23 19:12:41.838: INFO: (12) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname2/proxy/: bar (200; 22.178038ms)
Feb 23 19:12:41.838: INFO: (12) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname2/proxy/: tls qux (200; 21.443495ms)
Feb 23 19:12:41.838: INFO: (12) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname1/proxy/: foo (200; 22.627765ms)
Feb 23 19:12:41.846: INFO: (13) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 7.403553ms)
Feb 23 19:12:41.846: INFO: (13) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 7.613678ms)
Feb 23 19:12:41.846: INFO: (13) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 7.573706ms)
Feb 23 19:12:41.846: INFO: (13) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname1/proxy/: foo (200; 7.612492ms)
Feb 23 19:12:41.846: INFO: (13) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:460/proxy/: tls baz (200; 7.338327ms)
Feb 23 19:12:41.849: INFO: (13) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname1/proxy/: foo (200; 10.131395ms)
Feb 23 19:12:41.849: INFO: (13) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/tlsrewritem... (200; 9.957948ms)
Feb 23 19:12:41.849: INFO: (13) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:462/proxy/: tls qux (200; 9.635374ms)
Feb 23 19:12:41.849: INFO: (13) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/rewriteme">test</a> (200; 9.8871ms)
Feb 23 19:12:41.849: INFO: (13) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">test<... (200; 10.013088ms)
Feb 23 19:12:41.849: INFO: (13) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">... (200; 10.014087ms)
Feb 23 19:12:41.849: INFO: (13) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname2/proxy/: tls qux (200; 10.111974ms)
Feb 23 19:12:41.850: INFO: (13) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname2/proxy/: bar (200; 11.151998ms)
Feb 23 19:12:41.850: INFO: (13) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 10.774696ms)
Feb 23 19:12:41.851: INFO: (13) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname1/proxy/: tls baz (200; 11.540464ms)
Feb 23 19:12:41.852: INFO: (13) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname2/proxy/: bar (200; 13.073106ms)
Feb 23 19:12:41.860: INFO: (14) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 7.030341ms)
Feb 23 19:12:41.860: INFO: (14) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 7.170832ms)
Feb 23 19:12:41.860: INFO: (14) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:462/proxy/: tls qux (200; 7.147876ms)
Feb 23 19:12:41.860: INFO: (14) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/tlsrewritem... (200; 7.323782ms)
Feb 23 19:12:41.860: INFO: (14) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:460/proxy/: tls baz (200; 7.334656ms)
Feb 23 19:12:41.860: INFO: (14) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname1/proxy/: foo (200; 7.472841ms)
Feb 23 19:12:41.861: INFO: (14) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname2/proxy/: bar (200; 8.648882ms)
Feb 23 19:12:41.862: INFO: (14) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">... (200; 9.288236ms)
Feb 23 19:12:41.862: INFO: (14) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 9.28294ms)
Feb 23 19:12:41.862: INFO: (14) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 9.975088ms)
Feb 23 19:12:41.862: INFO: (14) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">test<... (200; 9.415008ms)
Feb 23 19:12:41.862: INFO: (14) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname1/proxy/: foo (200; 9.992175ms)
Feb 23 19:12:41.862: INFO: (14) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname2/proxy/: tls qux (200; 9.841471ms)
Feb 23 19:12:41.865: INFO: (14) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/rewriteme">test</a> (200; 12.561251ms)
Feb 23 19:12:41.866: INFO: (14) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname1/proxy/: tls baz (200; 13.66257ms)
Feb 23 19:12:41.868: INFO: (14) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname2/proxy/: bar (200; 15.700816ms)
Feb 23 19:12:41.873: INFO: (15) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 4.939668ms)
Feb 23 19:12:41.875: INFO: (15) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:462/proxy/: tls qux (200; 6.59622ms)
Feb 23 19:12:41.879: INFO: (15) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/rewriteme">test</a> (200; 10.163239ms)
Feb 23 19:12:41.879: INFO: (15) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname2/proxy/: tls qux (200; 9.85383ms)
Feb 23 19:12:41.879: INFO: (15) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 9.527744ms)
Feb 23 19:12:41.879: INFO: (15) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname1/proxy/: foo (200; 10.750437ms)
Feb 23 19:12:41.879: INFO: (15) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/tlsrewritem... (200; 10.130233ms)
Feb 23 19:12:41.879: INFO: (15) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 10.469694ms)
Feb 23 19:12:41.881: INFO: (15) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname1/proxy/: foo (200; 12.181421ms)
Feb 23 19:12:41.881: INFO: (15) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 12.319969ms)
Feb 23 19:12:41.881: INFO: (15) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">test<... (200; 11.822736ms)
Feb 23 19:12:41.881: INFO: (15) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname2/proxy/: bar (200; 12.644891ms)
Feb 23 19:12:41.881: INFO: (15) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:460/proxy/: tls baz (200; 11.896659ms)
Feb 23 19:12:41.881: INFO: (15) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">... (200; 11.776621ms)
Feb 23 19:12:41.884: INFO: (15) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname1/proxy/: tls baz (200; 14.962498ms)
Feb 23 19:12:41.884: INFO: (15) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname2/proxy/: bar (200; 14.533458ms)
Feb 23 19:12:41.887: INFO: (16) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">test<... (200; 3.632168ms)
Feb 23 19:12:41.888: INFO: (16) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:462/proxy/: tls qux (200; 4.193272ms)
Feb 23 19:12:41.889: INFO: (16) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">... (200; 5.314522ms)
Feb 23 19:12:41.891: INFO: (16) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/tlsrewritem... (200; 7.036472ms)
Feb 23 19:12:41.891: INFO: (16) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 7.080972ms)
Feb 23 19:12:41.891: INFO: (16) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 7.604334ms)
Feb 23 19:12:41.893: INFO: (16) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:460/proxy/: tls baz (200; 9.003565ms)
Feb 23 19:12:41.893: INFO: (16) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 9.668754ms)
Feb 23 19:12:41.894: INFO: (16) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/rewriteme">test</a> (200; 8.854345ms)
Feb 23 19:12:41.896: INFO: (16) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname1/proxy/: foo (200; 12.30016ms)
Feb 23 19:12:41.897: INFO: (16) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname2/proxy/: tls qux (200; 12.619514ms)
Feb 23 19:12:41.897: INFO: (16) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname2/proxy/: bar (200; 12.238201ms)
Feb 23 19:12:41.897: INFO: (16) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname2/proxy/: bar (200; 12.798378ms)
Feb 23 19:12:41.897: INFO: (16) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname1/proxy/: tls baz (200; 12.24212ms)
Feb 23 19:12:41.897: INFO: (16) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname1/proxy/: foo (200; 12.350118ms)
Feb 23 19:12:41.897: INFO: (16) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 12.696778ms)
Feb 23 19:12:41.902: INFO: (17) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 4.461939ms)
Feb 23 19:12:41.904: INFO: (17) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/rewriteme">test</a> (200; 5.666778ms)
Feb 23 19:12:41.905: INFO: (17) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname2/proxy/: bar (200; 6.742797ms)
Feb 23 19:12:41.906: INFO: (17) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 7.938235ms)
Feb 23 19:12:41.906: INFO: (17) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 8.836671ms)
Feb 23 19:12:41.906: INFO: (17) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/tlsrewritem... (200; 8.252786ms)
Feb 23 19:12:41.906: INFO: (17) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:462/proxy/: tls qux (200; 8.231151ms)
Feb 23 19:12:41.907: INFO: (17) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">test<... (200; 9.027448ms)
Feb 23 19:12:41.908: INFO: (17) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 10.2502ms)
Feb 23 19:12:41.908: INFO: (17) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">... (200; 10.286412ms)
Feb 23 19:12:41.908: INFO: (17) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname1/proxy/: foo (200; 10.827046ms)
Feb 23 19:12:41.909: INFO: (17) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:460/proxy/: tls baz (200; 10.768071ms)
Feb 23 19:12:41.909: INFO: (17) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname1/proxy/: tls baz (200; 11.198681ms)
Feb 23 19:12:41.911: INFO: (17) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname1/proxy/: foo (200; 13.534538ms)
Feb 23 19:12:41.911: INFO: (17) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname2/proxy/: bar (200; 13.474244ms)
Feb 23 19:12:41.911: INFO: (17) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname2/proxy/: tls qux (200; 13.309184ms)
Feb 23 19:12:41.915: INFO: (18) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:462/proxy/: tls qux (200; 4.53992ms)
Feb 23 19:12:41.916: INFO: (18) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 4.595555ms)
Feb 23 19:12:41.916: INFO: (18) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:460/proxy/: tls baz (200; 4.785244ms)
Feb 23 19:12:41.917: INFO: (18) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">test<... (200; 6.113902ms)
Feb 23 19:12:41.917: INFO: (18) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 6.158232ms)
Feb 23 19:12:41.921: INFO: (18) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">... (200; 10.382398ms)
Feb 23 19:12:41.922: INFO: (18) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname2/proxy/: bar (200; 10.664731ms)
Feb 23 19:12:41.922: INFO: (18) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname2/proxy/: tls qux (200; 10.727451ms)
Feb 23 19:12:41.922: INFO: (18) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/rewriteme">test</a> (200; 10.541078ms)
Feb 23 19:12:41.922: INFO: (18) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname2/proxy/: bar (200; 10.577696ms)
Feb 23 19:12:41.922: INFO: (18) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 10.611242ms)
Feb 23 19:12:41.922: INFO: (18) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/tlsrewritem... (200; 10.759341ms)
Feb 23 19:12:41.922: INFO: (18) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 10.949023ms)
Feb 23 19:12:41.924: INFO: (18) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname1/proxy/: tls baz (200; 12.913926ms)
Feb 23 19:12:41.925: INFO: (18) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname1/proxy/: foo (200; 13.616538ms)
Feb 23 19:12:41.927: INFO: (18) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname1/proxy/: foo (200; 16.16995ms)
Feb 23 19:12:41.934: INFO: (19) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">... (200; 6.484438ms)
Feb 23 19:12:41.934: INFO: (19) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:443/proxy/tlsrewritem... (200; 6.002272ms)
Feb 23 19:12:41.934: INFO: (19) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 6.408654ms)
Feb 23 19:12:41.934: INFO: (19) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:162/proxy/: bar (200; 6.595194ms)
Feb 23 19:12:41.936: INFO: (19) /api/v1/namespaces/proxy-9122/pods/http:proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 7.831138ms)
Feb 23 19:12:41.936: INFO: (19) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:1080/proxy/rewriteme">test<... (200; 7.705684ms)
Feb 23 19:12:41.936: INFO: (19) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:460/proxy/: tls baz (200; 8.141171ms)
Feb 23 19:12:41.937: INFO: (19) /api/v1/namespaces/proxy-9122/pods/https:proxy-service-rbwsj-nqn8v:462/proxy/: tls qux (200; 8.951328ms)
Feb 23 19:12:41.937: INFO: (19) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname1/proxy/: foo (200; 9.250309ms)
Feb 23 19:12:41.937: INFO: (19) /api/v1/namespaces/proxy-9122/services/http:proxy-service-rbwsj:portname2/proxy/: bar (200; 8.675569ms)
Feb 23 19:12:41.937: INFO: (19) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/: <a href="/api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v/proxy/rewriteme">test</a> (200; 8.619697ms)
Feb 23 19:12:41.937: INFO: (19) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname2/proxy/: bar (200; 9.275384ms)
Feb 23 19:12:41.937: INFO: (19) /api/v1/namespaces/proxy-9122/services/proxy-service-rbwsj:portname1/proxy/: foo (200; 9.256647ms)
Feb 23 19:12:41.937: INFO: (19) /api/v1/namespaces/proxy-9122/pods/proxy-service-rbwsj-nqn8v:160/proxy/: foo (200; 9.923286ms)
Feb 23 19:12:41.940: INFO: (19) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname1/proxy/: tls baz (200; 12.053307ms)
Feb 23 19:12:41.940: INFO: (19) /api/v1/namespaces/proxy-9122/services/https:proxy-service-rbwsj:tlsportname2/proxy/: tls qux (200; 12.564949ms)
STEP: deleting ReplicationController proxy-service-rbwsj in namespace proxy-9122, will wait for the garbage collector to delete the pods
Feb 23 19:12:42.003: INFO: Deleting ReplicationController proxy-service-rbwsj took: 9.812751ms
Feb 23 19:12:42.105: INFO: Terminating ReplicationController proxy-service-rbwsj pods took: 101.993903ms
[AfterEach] version v1
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:12:53.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9122" for this suite.
Feb 23 19:12:59.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:12:59.601: INFO: namespace proxy-9122 deletion completed in 6.091932416s

• [SLOW TEST:21.204 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:12:59.603: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 19:12:59.633: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:13:03.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-686" for this suite.
Feb 23 19:13:47.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:13:47.740: INFO: namespace pods-686 deletion completed in 44.073946132s

• [SLOW TEST:48.137 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:13:47.741: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Feb 23 19:13:47.775: INFO: Waiting up to 5m0s for pod "var-expansion-3778ebac-9a40-4f4e-82c2-82570a581711" in namespace "var-expansion-8899" to be "success or failure"
Feb 23 19:13:47.781: INFO: Pod "var-expansion-3778ebac-9a40-4f4e-82c2-82570a581711": Phase="Pending", Reason="", readiness=false. Elapsed: 5.811ms
Feb 23 19:13:49.791: INFO: Pod "var-expansion-3778ebac-9a40-4f4e-82c2-82570a581711": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015876471s
Feb 23 19:13:51.913: INFO: Pod "var-expansion-3778ebac-9a40-4f4e-82c2-82570a581711": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.138108164s
STEP: Saw pod success
Feb 23 19:13:51.913: INFO: Pod "var-expansion-3778ebac-9a40-4f4e-82c2-82570a581711" satisfied condition "success or failure"
Feb 23 19:13:51.917: INFO: Trying to get logs from node worker00 pod var-expansion-3778ebac-9a40-4f4e-82c2-82570a581711 container dapi-container: <nil>
STEP: delete the pod
Feb 23 19:13:51.945: INFO: Waiting for pod var-expansion-3778ebac-9a40-4f4e-82c2-82570a581711 to disappear
Feb 23 19:13:51.947: INFO: Pod var-expansion-3778ebac-9a40-4f4e-82c2-82570a581711 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:13:51.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8899" for this suite.
Feb 23 19:13:57.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:13:58.027: INFO: namespace var-expansion-8899 deletion completed in 6.076459938s

• [SLOW TEST:10.286 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:13:58.027: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 23 19:13:58.059: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e3ded4b-189d-4d7d-a150-e507daf3d7d6" in namespace "projected-1148" to be "success or failure"
Feb 23 19:13:58.063: INFO: Pod "downwardapi-volume-4e3ded4b-189d-4d7d-a150-e507daf3d7d6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.481733ms
Feb 23 19:14:00.065: INFO: Pod "downwardapi-volume-4e3ded4b-189d-4d7d-a150-e507daf3d7d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006069023s
Feb 23 19:14:02.069: INFO: Pod "downwardapi-volume-4e3ded4b-189d-4d7d-a150-e507daf3d7d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009908566s
STEP: Saw pod success
Feb 23 19:14:02.069: INFO: Pod "downwardapi-volume-4e3ded4b-189d-4d7d-a150-e507daf3d7d6" satisfied condition "success or failure"
Feb 23 19:14:02.071: INFO: Trying to get logs from node worker00 pod downwardapi-volume-4e3ded4b-189d-4d7d-a150-e507daf3d7d6 container client-container: <nil>
STEP: delete the pod
Feb 23 19:14:02.090: INFO: Waiting for pod downwardapi-volume-4e3ded4b-189d-4d7d-a150-e507daf3d7d6 to disappear
Feb 23 19:14:02.094: INFO: Pod downwardapi-volume-4e3ded4b-189d-4d7d-a150-e507daf3d7d6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:14:02.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1148" for this suite.
Feb 23 19:14:08.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:14:08.174: INFO: namespace projected-1148 deletion completed in 6.076386958s

• [SLOW TEST:10.148 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:14:08.175: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-9332, will wait for the garbage collector to delete the pods
Feb 23 19:14:10.328: INFO: Deleting Job.batch foo took: 5.932599ms
Feb 23 19:14:11.228: INFO: Terminating Job.batch foo pods took: 900.260358ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:14:53.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9332" for this suite.
Feb 23 19:14:59.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:14:59.621: INFO: namespace job-9332 deletion completed in 6.086247601s

• [SLOW TEST:51.446 seconds]
[sig-apps] Job
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:14:59.622: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 19:14:59.658: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-50fac75c-265e-4d5a-966c-75288ac83c5b" in namespace "security-context-test-4076" to be "success or failure"
Feb 23 19:14:59.664: INFO: Pod "busybox-privileged-false-50fac75c-265e-4d5a-966c-75288ac83c5b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.336757ms
Feb 23 19:15:01.667: INFO: Pod "busybox-privileged-false-50fac75c-265e-4d5a-966c-75288ac83c5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008742231s
Feb 23 19:15:01.667: INFO: Pod "busybox-privileged-false-50fac75c-265e-4d5a-966c-75288ac83c5b" satisfied condition "success or failure"
Feb 23 19:15:01.672: INFO: Got logs for pod "busybox-privileged-false-50fac75c-265e-4d5a-966c-75288ac83c5b": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:15:01.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4076" for this suite.
Feb 23 19:15:07.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:15:07.746: INFO: namespace security-context-test-4076 deletion completed in 6.071484156s

• [SLOW TEST:8.124 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:15:07.746: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-959
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-959
I0223 19:15:07.803760      26 runners.go:184] Created replication controller with name: externalname-service, namespace: services-959, replica count: 2
I0223 19:15:10.858470      26 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 23 19:15:10.858: INFO: Creating new exec pod
Feb 23 19:15:13.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=services-959 execpodz5tmx -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Feb 23 19:15:14.004: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 23 19:15:14.004: INFO: stdout: ""
Feb 23 19:15:14.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 exec --namespace=services-959 execpodz5tmx -- /bin/sh -x -c nc -zv -t -w 2 10.32.0.8 80'
Feb 23 19:15:14.610: INFO: stderr: "+ nc -zv -t -w 2 10.32.0.8 80\nConnection to 10.32.0.8 80 port [tcp/http] succeeded!\n"
Feb 23 19:15:14.610: INFO: stdout: ""
Feb 23 19:15:14.610: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:15:14.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-959" for this suite.
Feb 23 19:15:20.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:15:21.482: INFO: namespace services-959 deletion completed in 6.845265874s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:13.736 seconds]
[sig-network] Services
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:15:21.483: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 19:15:21.736: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 23 19:15:23.744: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082121, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082121, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082121, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082121, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 19:15:26.756: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 19:15:26.758: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5505-crds.webhook.example.com via the AdmissionRegistration API
Feb 23 19:15:32.294: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:15:32.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1479" for this suite.
Feb 23 19:15:38.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:15:39.071: INFO: namespace webhook-1479 deletion completed in 6.083281329s
STEP: Destroying namespace "webhook-1479-markers" for this suite.
Feb 23 19:15:45.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:15:45.151: INFO: namespace webhook-1479-markers deletion completed in 6.080170614s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:23.680 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:15:45.163: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 23 19:15:45.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-7484'
Feb 23 19:15:45.254: INFO: stderr: ""
Feb 23 19:15:45.254: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Feb 23 19:15:50.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pod e2e-test-httpd-pod --namespace=kubectl-7484 -o json'
Feb 23 19:15:50.371: INFO: stderr: ""
Feb 23 19:15:50.371: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.200.131.184/32\"\n        },\n        \"creationTimestamp\": \"2020-02-23T19:15:45Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7484\",\n        \"resourceVersion\": \"29252\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-7484/pods/e2e-test-httpd-pod\",\n        \"uid\": \"aabb73cd-13f3-4afa-8bec-83a303042318\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-bn6j9\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker00\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-bn6j9\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-bn6j9\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-23T19:15:45Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-23T19:15:46Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-23T19:15:46Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-23T19:15:45Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://8dbaee9117ff64c1018047638f01761dc3bceb6bc3241beb66b77db2e8f6eb35\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-02-23T19:15:46Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.180.100\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.200.131.184\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.200.131.184\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-02-23T19:15:45Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 23 19:15:50.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 replace -f - --namespace=kubectl-7484'
Feb 23 19:15:50.540: INFO: stderr: ""
Feb 23 19:15:50.540: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Feb 23 19:15:50.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 delete pods e2e-test-httpd-pod --namespace=kubectl-7484'
Feb 23 19:16:03.470: INFO: stderr: ""
Feb 23 19:16:03.470: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:16:03.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7484" for this suite.
Feb 23 19:16:09.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:16:09.586: INFO: namespace kubectl-7484 deletion completed in 6.110728348s

• [SLOW TEST:24.423 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:16:09.586: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Feb 23 19:16:10.075: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-352" to be "success or failure"
Feb 23 19:16:10.078: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.051005ms
Feb 23 19:16:12.080: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005194623s
STEP: Saw pod success
Feb 23 19:16:12.080: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb 23 19:16:12.082: INFO: Trying to get logs from node worker00 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb 23 19:16:12.098: INFO: Waiting for pod pod-host-path-test to disappear
Feb 23 19:16:12.101: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:16:12.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-352" for this suite.
Feb 23 19:16:18.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:16:18.180: INFO: namespace hostpath-352 deletion completed in 6.075911163s

• [SLOW TEST:8.593 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:16:18.180: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:16:18.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-701" for this suite.
Feb 23 19:16:24.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:16:24.317: INFO: namespace kubelet-test-701 deletion completed in 6.07204823s

• [SLOW TEST:6.137 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:16:24.317: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 19:16:24.346: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Feb 23 19:16:32.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 --namespace=crd-publish-openapi-9113 create -f -'
Feb 23 19:16:33.108: INFO: stderr: ""
Feb 23 19:16:33.108: INFO: stdout: "e2e-test-crd-publish-openapi-2694-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 23 19:16:33.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 --namespace=crd-publish-openapi-9113 delete e2e-test-crd-publish-openapi-2694-crds test-foo'
Feb 23 19:16:33.195: INFO: stderr: ""
Feb 23 19:16:33.195: INFO: stdout: "e2e-test-crd-publish-openapi-2694-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Feb 23 19:16:33.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 --namespace=crd-publish-openapi-9113 apply -f -'
Feb 23 19:16:33.376: INFO: stderr: ""
Feb 23 19:16:33.376: INFO: stdout: "e2e-test-crd-publish-openapi-2694-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 23 19:16:33.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 --namespace=crd-publish-openapi-9113 delete e2e-test-crd-publish-openapi-2694-crds test-foo'
Feb 23 19:16:33.434: INFO: stderr: ""
Feb 23 19:16:33.434: INFO: stdout: "e2e-test-crd-publish-openapi-2694-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Feb 23 19:16:33.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 --namespace=crd-publish-openapi-9113 create -f -'
Feb 23 19:16:33.553: INFO: rc: 1
Feb 23 19:16:33.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 --namespace=crd-publish-openapi-9113 apply -f -'
Feb 23 19:16:34.227: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Feb 23 19:16:34.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 --namespace=crd-publish-openapi-9113 create -f -'
Feb 23 19:16:34.398: INFO: rc: 1
Feb 23 19:16:34.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 --namespace=crd-publish-openapi-9113 apply -f -'
Feb 23 19:16:34.559: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Feb 23 19:16:34.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 explain e2e-test-crd-publish-openapi-2694-crds'
Feb 23 19:16:34.712: INFO: stderr: ""
Feb 23 19:16:34.712: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2694-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Feb 23 19:16:34.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 explain e2e-test-crd-publish-openapi-2694-crds.metadata'
Feb 23 19:16:34.833: INFO: stderr: ""
Feb 23 19:16:34.833: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2694-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Feb 23 19:16:34.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 explain e2e-test-crd-publish-openapi-2694-crds.spec'
Feb 23 19:16:35.019: INFO: stderr: ""
Feb 23 19:16:35.019: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2694-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Feb 23 19:16:35.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 explain e2e-test-crd-publish-openapi-2694-crds.spec.bars'
Feb 23 19:16:35.148: INFO: stderr: ""
Feb 23 19:16:35.148: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2694-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Feb 23 19:16:35.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 explain e2e-test-crd-publish-openapi-2694-crds.spec.bars2'
Feb 23 19:16:35.326: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:16:38.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9113" for this suite.
Feb 23 19:16:44.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:16:44.249: INFO: namespace crd-publish-openapi-9113 deletion completed in 6.080463921s

• [SLOW TEST:19.932 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:16:44.249: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Feb 23 19:16:44.284: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Feb 23 19:16:44.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 create -f - --namespace=kubectl-7927'
Feb 23 19:16:44.477: INFO: stderr: ""
Feb 23 19:16:44.477: INFO: stdout: "service/redis-slave created\n"
Feb 23 19:16:44.477: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Feb 23 19:16:44.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 create -f - --namespace=kubectl-7927'
Feb 23 19:16:44.632: INFO: stderr: ""
Feb 23 19:16:44.632: INFO: stdout: "service/redis-master created\n"
Feb 23 19:16:44.632: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 23 19:16:44.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 create -f - --namespace=kubectl-7927'
Feb 23 19:16:44.822: INFO: stderr: ""
Feb 23 19:16:44.822: INFO: stdout: "service/frontend created\n"
Feb 23 19:16:44.822: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Feb 23 19:16:44.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 create -f - --namespace=kubectl-7927'
Feb 23 19:16:45.015: INFO: stderr: ""
Feb 23 19:16:45.015: INFO: stdout: "deployment.apps/frontend created\n"
Feb 23 19:16:45.015: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 23 19:16:45.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 create -f - --namespace=kubectl-7927'
Feb 23 19:16:45.244: INFO: stderr: ""
Feb 23 19:16:45.244: INFO: stdout: "deployment.apps/redis-master created\n"
Feb 23 19:16:45.244: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Feb 23 19:16:45.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 create -f - --namespace=kubectl-7927'
Feb 23 19:16:45.513: INFO: stderr: ""
Feb 23 19:16:45.513: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Feb 23 19:16:45.513: INFO: Waiting for all frontend pods to be Running.
Feb 23 19:17:26.173: INFO: Waiting for frontend to serve content.
Feb 23 19:17:26.183: INFO: Trying to add a new entry to the guestbook.
Feb 23 19:17:26.193: INFO: Verifying that added entry can be retrieved.
Feb 23 19:17:26.202: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Feb 23 19:17:31.667: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Feb 23 19:17:36.677: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Feb 23 19:17:41.687: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Feb 23 19:17:46.697: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Feb 23 19:17:51.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 delete --grace-period=0 --force -f - --namespace=kubectl-7927'
Feb 23 19:17:52.490: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 19:17:52.490: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb 23 19:17:52.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 delete --grace-period=0 --force -f - --namespace=kubectl-7927'
Feb 23 19:17:52.588: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 19:17:52.588: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 23 19:17:52.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 delete --grace-period=0 --force -f - --namespace=kubectl-7927'
Feb 23 19:17:52.700: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 19:17:52.700: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 23 19:17:52.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 delete --grace-period=0 --force -f - --namespace=kubectl-7927'
Feb 23 19:17:52.767: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 19:17:52.767: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 23 19:17:52.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 delete --grace-period=0 --force -f - --namespace=kubectl-7927'
Feb 23 19:17:52.828: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 19:17:52.828: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 23 19:17:52.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 delete --grace-period=0 --force -f - --namespace=kubectl-7927'
Feb 23 19:17:52.887: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 19:17:52.887: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:17:52.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7927" for this suite.
Feb 23 19:18:20.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:18:20.979: INFO: namespace kubectl-7927 deletion completed in 28.089558839s

• [SLOW TEST:96.730 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:18:20.987: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-dfb80425-5554-43b8-8528-1e000adc1c37
STEP: Creating a pod to test consume secrets
Feb 23 19:18:21.023: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ab219510-3557-44a7-be3f-8874ac37652f" in namespace "projected-5102" to be "success or failure"
Feb 23 19:18:21.028: INFO: Pod "pod-projected-secrets-ab219510-3557-44a7-be3f-8874ac37652f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.115678ms
Feb 23 19:18:23.031: INFO: Pod "pod-projected-secrets-ab219510-3557-44a7-be3f-8874ac37652f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007875189s
STEP: Saw pod success
Feb 23 19:18:23.031: INFO: Pod "pod-projected-secrets-ab219510-3557-44a7-be3f-8874ac37652f" satisfied condition "success or failure"
Feb 23 19:18:23.034: INFO: Trying to get logs from node worker00 pod pod-projected-secrets-ab219510-3557-44a7-be3f-8874ac37652f container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 23 19:18:23.064: INFO: Waiting for pod pod-projected-secrets-ab219510-3557-44a7-be3f-8874ac37652f to disappear
Feb 23 19:18:23.067: INFO: Pod pod-projected-secrets-ab219510-3557-44a7-be3f-8874ac37652f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:18:23.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5102" for this suite.
Feb 23 19:18:29.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:18:29.144: INFO: namespace projected-5102 deletion completed in 6.073776308s

• [SLOW TEST:8.157 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:18:29.144: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 23 19:18:29.178: INFO: Waiting up to 5m0s for pod "pod-d4dc5382-236d-4fdf-9b8a-20876b16535e" in namespace "emptydir-1967" to be "success or failure"
Feb 23 19:18:29.182: INFO: Pod "pod-d4dc5382-236d-4fdf-9b8a-20876b16535e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.479417ms
Feb 23 19:18:31.185: INFO: Pod "pod-d4dc5382-236d-4fdf-9b8a-20876b16535e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006404431s
STEP: Saw pod success
Feb 23 19:18:31.185: INFO: Pod "pod-d4dc5382-236d-4fdf-9b8a-20876b16535e" satisfied condition "success or failure"
Feb 23 19:18:31.187: INFO: Trying to get logs from node worker00 pod pod-d4dc5382-236d-4fdf-9b8a-20876b16535e container test-container: <nil>
STEP: delete the pod
Feb 23 19:18:31.203: INFO: Waiting for pod pod-d4dc5382-236d-4fdf-9b8a-20876b16535e to disappear
Feb 23 19:18:31.206: INFO: Pod pod-d4dc5382-236d-4fdf-9b8a-20876b16535e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:18:31.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1967" for this suite.
Feb 23 19:18:37.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:18:37.280: INFO: namespace emptydir-1967 deletion completed in 6.071757418s

• [SLOW TEST:8.136 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:18:37.280: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-404
I0223 19:18:37.315948      26 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-404, replica count: 1
I0223 19:18:38.367413      26 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0223 19:18:39.372735      26 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 23 19:18:39.483: INFO: Created: latency-svc-knhmf
Feb 23 19:18:39.491: INFO: Got endpoints: latency-svc-knhmf [18.164751ms]
Feb 23 19:18:39.506: INFO: Created: latency-svc-67kwz
Feb 23 19:18:39.514: INFO: Got endpoints: latency-svc-67kwz [22.27544ms]
Feb 23 19:18:39.519: INFO: Created: latency-svc-6nrjf
Feb 23 19:18:39.529: INFO: Created: latency-svc-h7mdt
Feb 23 19:18:39.544: INFO: Got endpoints: latency-svc-h7mdt [51.361877ms]
Feb 23 19:18:39.549: INFO: Created: latency-svc-v92bm
Feb 23 19:18:39.549: INFO: Got endpoints: latency-svc-6nrjf [56.809772ms]
Feb 23 19:18:39.563: INFO: Got endpoints: latency-svc-v92bm [69.846257ms]
Feb 23 19:18:39.570: INFO: Created: latency-svc-vjvcp
Feb 23 19:18:39.583: INFO: Got endpoints: latency-svc-vjvcp [90.216001ms]
Feb 23 19:18:39.585: INFO: Created: latency-svc-vbhhz
Feb 23 19:18:39.604: INFO: Got endpoints: latency-svc-vbhhz [111.428445ms]
Feb 23 19:18:39.609: INFO: Created: latency-svc-c9sjr
Feb 23 19:18:39.616: INFO: Got endpoints: latency-svc-c9sjr [123.487936ms]
Feb 23 19:18:39.631: INFO: Created: latency-svc-9qsv6
Feb 23 19:18:39.644: INFO: Got endpoints: latency-svc-9qsv6 [151.252807ms]
Feb 23 19:18:39.650: INFO: Created: latency-svc-bpf9h
Feb 23 19:18:39.663: INFO: Got endpoints: latency-svc-bpf9h [169.807957ms]
Feb 23 19:18:39.672: INFO: Created: latency-svc-gjqkq
Feb 23 19:18:39.679: INFO: Got endpoints: latency-svc-gjqkq [186.381973ms]
Feb 23 19:18:39.682: INFO: Created: latency-svc-62b74
Feb 23 19:18:39.695: INFO: Created: latency-svc-nb7wn
Feb 23 19:18:39.697: INFO: Got endpoints: latency-svc-62b74 [203.909262ms]
Feb 23 19:18:39.704: INFO: Got endpoints: latency-svc-nb7wn [210.797432ms]
Feb 23 19:18:39.706: INFO: Created: latency-svc-tvvcm
Feb 23 19:18:39.714: INFO: Got endpoints: latency-svc-tvvcm [220.583653ms]
Feb 23 19:18:39.721: INFO: Created: latency-svc-bcjxt
Feb 23 19:18:39.726: INFO: Got endpoints: latency-svc-bcjxt [233.031829ms]
Feb 23 19:18:39.730: INFO: Created: latency-svc-4ppcp
Feb 23 19:18:39.736: INFO: Got endpoints: latency-svc-4ppcp [243.235685ms]
Feb 23 19:18:39.740: INFO: Created: latency-svc-szg85
Feb 23 19:18:39.749: INFO: Got endpoints: latency-svc-szg85 [234.80361ms]
Feb 23 19:18:39.752: INFO: Created: latency-svc-2knmc
Feb 23 19:18:39.764: INFO: Got endpoints: latency-svc-2knmc [219.319568ms]
Feb 23 19:18:39.776: INFO: Created: latency-svc-dssmn
Feb 23 19:18:39.785: INFO: Got endpoints: latency-svc-dssmn [235.937208ms]
Feb 23 19:18:39.789: INFO: Created: latency-svc-4pzd6
Feb 23 19:18:39.800: INFO: Got endpoints: latency-svc-4pzd6 [237.419494ms]
Feb 23 19:18:39.806: INFO: Created: latency-svc-vpb7n
Feb 23 19:18:39.812: INFO: Got endpoints: latency-svc-vpb7n [228.640687ms]
Feb 23 19:18:39.819: INFO: Created: latency-svc-t6sfn
Feb 23 19:18:39.828: INFO: Got endpoints: latency-svc-t6sfn [223.937812ms]
Feb 23 19:18:39.835: INFO: Created: latency-svc-5grqd
Feb 23 19:18:39.841: INFO: Got endpoints: latency-svc-5grqd [224.40356ms]
Feb 23 19:18:39.848: INFO: Created: latency-svc-79vzb
Feb 23 19:18:39.856: INFO: Got endpoints: latency-svc-79vzb [211.403024ms]
Feb 23 19:18:39.861: INFO: Created: latency-svc-5twx7
Feb 23 19:18:39.873: INFO: Created: latency-svc-r6h8d
Feb 23 19:18:39.873: INFO: Got endpoints: latency-svc-5twx7 [210.047263ms]
Feb 23 19:18:39.881: INFO: Got endpoints: latency-svc-r6h8d [202.006ms]
Feb 23 19:18:39.888: INFO: Created: latency-svc-rhz84
Feb 23 19:18:39.892: INFO: Got endpoints: latency-svc-rhz84 [193.791013ms]
Feb 23 19:18:39.898: INFO: Created: latency-svc-q6sd9
Feb 23 19:18:39.903: INFO: Got endpoints: latency-svc-q6sd9 [199.552033ms]
Feb 23 19:18:39.908: INFO: Created: latency-svc-n9wzj
Feb 23 19:18:39.913: INFO: Got endpoints: latency-svc-n9wzj [199.616707ms]
Feb 23 19:18:39.919: INFO: Created: latency-svc-lksfd
Feb 23 19:18:39.928: INFO: Created: latency-svc-kn6l6
Feb 23 19:18:39.928: INFO: Got endpoints: latency-svc-lksfd [200.144129ms]
Feb 23 19:18:39.938: INFO: Created: latency-svc-x5lxg
Feb 23 19:18:39.938: INFO: Got endpoints: latency-svc-kn6l6 [202.012745ms]
Feb 23 19:18:39.945: INFO: Got endpoints: latency-svc-x5lxg [195.830783ms]
Feb 23 19:18:39.949: INFO: Created: latency-svc-4kbmh
Feb 23 19:18:39.955: INFO: Got endpoints: latency-svc-4kbmh [189.876806ms]
Feb 23 19:18:39.958: INFO: Created: latency-svc-jbblq
Feb 23 19:18:39.968: INFO: Got endpoints: latency-svc-jbblq [183.002424ms]
Feb 23 19:18:39.973: INFO: Created: latency-svc-4k4ps
Feb 23 19:18:39.978: INFO: Got endpoints: latency-svc-4k4ps [177.340492ms]
Feb 23 19:18:39.983: INFO: Created: latency-svc-t8rnz
Feb 23 19:18:39.992: INFO: Got endpoints: latency-svc-t8rnz [180.174085ms]
Feb 23 19:18:40.009: INFO: Created: latency-svc-pq5d5
Feb 23 19:18:40.014: INFO: Created: latency-svc-jmzlh
Feb 23 19:18:40.019: INFO: Got endpoints: latency-svc-pq5d5 [190.344653ms]
Feb 23 19:18:40.021: INFO: Got endpoints: latency-svc-jmzlh [178.62831ms]
Feb 23 19:18:40.026: INFO: Created: latency-svc-mxcs9
Feb 23 19:18:40.038: INFO: Got endpoints: latency-svc-mxcs9 [181.501058ms]
Feb 23 19:18:40.038: INFO: Created: latency-svc-wfn5m
Feb 23 19:18:40.044: INFO: Created: latency-svc-2dnhh
Feb 23 19:18:40.044: INFO: Got endpoints: latency-svc-wfn5m [171.075468ms]
Feb 23 19:18:40.053: INFO: Got endpoints: latency-svc-2dnhh [171.325848ms]
Feb 23 19:18:40.054: INFO: Created: latency-svc-x2znf
Feb 23 19:18:40.074: INFO: Created: latency-svc-z9f5s
Feb 23 19:18:40.089: INFO: Created: latency-svc-vg46t
Feb 23 19:18:40.093: INFO: Got endpoints: latency-svc-x2znf [200.937238ms]
Feb 23 19:18:40.094: INFO: Created: latency-svc-4n5p8
Feb 23 19:18:40.101: INFO: Created: latency-svc-sc5rn
Feb 23 19:18:40.111: INFO: Created: latency-svc-hfpt7
Feb 23 19:18:40.122: INFO: Created: latency-svc-z5jdv
Feb 23 19:18:40.129: INFO: Created: latency-svc-5dd4v
Feb 23 19:18:40.137: INFO: Created: latency-svc-hfhfr
Feb 23 19:18:40.142: INFO: Got endpoints: latency-svc-z9f5s [238.617257ms]
Feb 23 19:18:40.150: INFO: Created: latency-svc-7gqlk
Feb 23 19:18:40.154: INFO: Created: latency-svc-r25vj
Feb 23 19:18:40.163: INFO: Created: latency-svc-9w9n5
Feb 23 19:18:40.171: INFO: Created: latency-svc-ps9mf
Feb 23 19:18:40.178: INFO: Created: latency-svc-ds24g
Feb 23 19:18:40.183: INFO: Created: latency-svc-f94ln
Feb 23 19:18:40.192: INFO: Got endpoints: latency-svc-vg46t [278.535532ms]
Feb 23 19:18:40.196: INFO: Created: latency-svc-z4npb
Feb 23 19:18:40.203: INFO: Created: latency-svc-tz8ld
Feb 23 19:18:40.208: INFO: Created: latency-svc-5w8cn
Feb 23 19:18:40.241: INFO: Got endpoints: latency-svc-4n5p8 [313.028978ms]
Feb 23 19:18:40.254: INFO: Created: latency-svc-8jnq7
Feb 23 19:18:40.290: INFO: Got endpoints: latency-svc-sc5rn [351.848328ms]
Feb 23 19:18:40.300: INFO: Created: latency-svc-55dlg
Feb 23 19:18:40.351: INFO: Got endpoints: latency-svc-hfpt7 [406.35253ms]
Feb 23 19:18:40.362: INFO: Created: latency-svc-vcc59
Feb 23 19:18:40.391: INFO: Got endpoints: latency-svc-z5jdv [435.727399ms]
Feb 23 19:18:40.403: INFO: Created: latency-svc-kph2j
Feb 23 19:18:40.440: INFO: Got endpoints: latency-svc-5dd4v [470.968661ms]
Feb 23 19:18:40.446: INFO: Created: latency-svc-wpp8x
Feb 23 19:18:40.489: INFO: Got endpoints: latency-svc-hfhfr [511.03742ms]
Feb 23 19:18:40.500: INFO: Created: latency-svc-cwxs8
Feb 23 19:18:40.540: INFO: Got endpoints: latency-svc-7gqlk [547.112078ms]
Feb 23 19:18:40.566: INFO: Created: latency-svc-96jtr
Feb 23 19:18:40.592: INFO: Got endpoints: latency-svc-r25vj [572.505299ms]
Feb 23 19:18:40.609: INFO: Created: latency-svc-xcnrm
Feb 23 19:18:40.654: INFO: Got endpoints: latency-svc-9w9n5 [633.1494ms]
Feb 23 19:18:40.667: INFO: Created: latency-svc-9l4jh
Feb 23 19:18:40.690: INFO: Got endpoints: latency-svc-ps9mf [652.131833ms]
Feb 23 19:18:40.702: INFO: Created: latency-svc-cqnt8
Feb 23 19:18:40.739: INFO: Got endpoints: latency-svc-ds24g [695.229587ms]
Feb 23 19:18:40.751: INFO: Created: latency-svc-2kz26
Feb 23 19:18:40.799: INFO: Got endpoints: latency-svc-f94ln [745.636314ms]
Feb 23 19:18:40.809: INFO: Created: latency-svc-djwqw
Feb 23 19:18:40.840: INFO: Got endpoints: latency-svc-z4npb [747.164623ms]
Feb 23 19:18:40.854: INFO: Created: latency-svc-r7cwl
Feb 23 19:18:40.891: INFO: Got endpoints: latency-svc-tz8ld [748.746522ms]
Feb 23 19:18:40.904: INFO: Created: latency-svc-tr9s8
Feb 23 19:18:40.944: INFO: Got endpoints: latency-svc-5w8cn [751.719797ms]
Feb 23 19:18:40.960: INFO: Created: latency-svc-gv7xm
Feb 23 19:18:40.993: INFO: Got endpoints: latency-svc-8jnq7 [752.541181ms]
Feb 23 19:18:41.003: INFO: Created: latency-svc-8r2rp
Feb 23 19:18:41.040: INFO: Got endpoints: latency-svc-55dlg [750.31508ms]
Feb 23 19:18:41.051: INFO: Created: latency-svc-csj9s
Feb 23 19:18:41.091: INFO: Got endpoints: latency-svc-vcc59 [740.35769ms]
Feb 23 19:18:41.106: INFO: Created: latency-svc-pqv5s
Feb 23 19:18:41.142: INFO: Got endpoints: latency-svc-kph2j [750.445404ms]
Feb 23 19:18:41.153: INFO: Created: latency-svc-nkg4q
Feb 23 19:18:41.190: INFO: Got endpoints: latency-svc-wpp8x [750.350621ms]
Feb 23 19:18:41.204: INFO: Created: latency-svc-z9dnm
Feb 23 19:18:41.241: INFO: Got endpoints: latency-svc-cwxs8 [751.873206ms]
Feb 23 19:18:41.251: INFO: Created: latency-svc-wltv7
Feb 23 19:18:41.291: INFO: Got endpoints: latency-svc-96jtr [750.852996ms]
Feb 23 19:18:41.302: INFO: Created: latency-svc-dzhdx
Feb 23 19:18:41.340: INFO: Got endpoints: latency-svc-xcnrm [748.026468ms]
Feb 23 19:18:41.348: INFO: Created: latency-svc-bbfqt
Feb 23 19:18:41.393: INFO: Got endpoints: latency-svc-9l4jh [738.972092ms]
Feb 23 19:18:41.402: INFO: Created: latency-svc-fv66l
Feb 23 19:18:41.440: INFO: Got endpoints: latency-svc-cqnt8 [749.777088ms]
Feb 23 19:18:41.450: INFO: Created: latency-svc-4rbkf
Feb 23 19:18:41.925: INFO: Got endpoints: latency-svc-2kz26 [1.18561936s]
Feb 23 19:18:41.927: INFO: Got endpoints: latency-svc-tr9s8 [1.036098513s]
Feb 23 19:18:41.931: INFO: Got endpoints: latency-svc-djwqw [1.132772789s]
Feb 23 19:18:41.938: INFO: Got endpoints: latency-svc-gv7xm [993.945863ms]
Feb 23 19:18:41.938: INFO: Got endpoints: latency-svc-r7cwl [1.097708999s]
Feb 23 19:18:41.943: INFO: Got endpoints: latency-svc-csj9s [901.879965ms]
Feb 23 19:18:41.944: INFO: Got endpoints: latency-svc-8r2rp [950.275842ms]
Feb 23 19:18:41.944: INFO: Got endpoints: latency-svc-pqv5s [852.579983ms]
Feb 23 19:18:41.947: INFO: Got endpoints: latency-svc-z9dnm [756.910111ms]
Feb 23 19:18:41.950: INFO: Got endpoints: latency-svc-nkg4q [808.191903ms]
Feb 23 19:18:41.976: INFO: Created: latency-svc-h25mh
Feb 23 19:18:41.983: INFO: Created: latency-svc-6b2sl
Feb 23 19:18:41.992: INFO: Got endpoints: latency-svc-wltv7 [750.830753ms]
Feb 23 19:18:41.997: INFO: Created: latency-svc-6v98w
Feb 23 19:18:42.019: INFO: Created: latency-svc-bxklm
Feb 23 19:18:42.031: INFO: Created: latency-svc-pwkcq
Feb 23 19:18:42.036: INFO: Created: latency-svc-424dj
Feb 23 19:18:42.042: INFO: Got endpoints: latency-svc-dzhdx [751.657433ms]
Feb 23 19:18:42.047: INFO: Created: latency-svc-pznsz
Feb 23 19:18:42.058: INFO: Created: latency-svc-2gmpq
Feb 23 19:18:42.074: INFO: Created: latency-svc-t2wlb
Feb 23 19:18:42.079: INFO: Created: latency-svc-64bkl
Feb 23 19:18:42.093: INFO: Got endpoints: latency-svc-bbfqt [752.573314ms]
Feb 23 19:18:42.093: INFO: Created: latency-svc-x5dsd
Feb 23 19:18:42.098: INFO: Created: latency-svc-bq92f
Feb 23 19:18:42.109: INFO: Created: latency-svc-2z5fs
Feb 23 19:18:42.140: INFO: Got endpoints: latency-svc-fv66l [746.71516ms]
Feb 23 19:18:42.149: INFO: Created: latency-svc-4q92b
Feb 23 19:18:42.192: INFO: Got endpoints: latency-svc-4rbkf [751.457269ms]
Feb 23 19:18:42.209: INFO: Created: latency-svc-drrpc
Feb 23 19:18:42.243: INFO: Got endpoints: latency-svc-h25mh [317.421228ms]
Feb 23 19:18:42.255: INFO: Created: latency-svc-m8ks9
Feb 23 19:18:42.289: INFO: Got endpoints: latency-svc-6b2sl [361.931895ms]
Feb 23 19:18:42.301: INFO: Created: latency-svc-4gczg
Feb 23 19:18:42.340: INFO: Got endpoints: latency-svc-6v98w [408.795084ms]
Feb 23 19:18:42.351: INFO: Created: latency-svc-2wn6t
Feb 23 19:18:42.389: INFO: Got endpoints: latency-svc-bxklm [451.494439ms]
Feb 23 19:18:42.411: INFO: Created: latency-svc-9xvcz
Feb 23 19:18:42.441: INFO: Got endpoints: latency-svc-pwkcq [503.516303ms]
Feb 23 19:18:42.449: INFO: Created: latency-svc-b69ph
Feb 23 19:18:42.491: INFO: Got endpoints: latency-svc-424dj [547.716418ms]
Feb 23 19:18:42.503: INFO: Created: latency-svc-scd9l
Feb 23 19:18:42.538: INFO: Got endpoints: latency-svc-pznsz [593.948973ms]
Feb 23 19:18:42.545: INFO: Created: latency-svc-t684l
Feb 23 19:18:42.590: INFO: Got endpoints: latency-svc-2gmpq [646.41103ms]
Feb 23 19:18:42.605: INFO: Created: latency-svc-rx4pg
Feb 23 19:18:42.640: INFO: Got endpoints: latency-svc-t2wlb [690.457001ms]
Feb 23 19:18:42.648: INFO: Created: latency-svc-hm4qb
Feb 23 19:18:42.693: INFO: Got endpoints: latency-svc-64bkl [742.977059ms]
Feb 23 19:18:42.704: INFO: Created: latency-svc-nbcr8
Feb 23 19:18:42.742: INFO: Got endpoints: latency-svc-x5dsd [750.514955ms]
Feb 23 19:18:42.752: INFO: Created: latency-svc-g47l6
Feb 23 19:18:42.791: INFO: Got endpoints: latency-svc-bq92f [748.341073ms]
Feb 23 19:18:42.804: INFO: Created: latency-svc-rm6m6
Feb 23 19:18:42.840: INFO: Got endpoints: latency-svc-2z5fs [747.061072ms]
Feb 23 19:18:42.848: INFO: Created: latency-svc-tzcbj
Feb 23 19:18:42.891: INFO: Got endpoints: latency-svc-4q92b [751.225835ms]
Feb 23 19:18:42.909: INFO: Created: latency-svc-vx4rl
Feb 23 19:18:42.939: INFO: Got endpoints: latency-svc-drrpc [747.749593ms]
Feb 23 19:18:42.959: INFO: Created: latency-svc-b8ct6
Feb 23 19:18:42.992: INFO: Got endpoints: latency-svc-m8ks9 [748.315571ms]
Feb 23 19:18:43.004: INFO: Created: latency-svc-lbpzg
Feb 23 19:18:43.041: INFO: Got endpoints: latency-svc-4gczg [751.138894ms]
Feb 23 19:18:43.050: INFO: Created: latency-svc-kc6px
Feb 23 19:18:43.091: INFO: Got endpoints: latency-svc-2wn6t [750.461982ms]
Feb 23 19:18:43.100: INFO: Created: latency-svc-8r6rl
Feb 23 19:18:43.140: INFO: Got endpoints: latency-svc-9xvcz [750.844875ms]
Feb 23 19:18:43.149: INFO: Created: latency-svc-cfq4b
Feb 23 19:18:43.194: INFO: Got endpoints: latency-svc-b69ph [752.094803ms]
Feb 23 19:18:43.210: INFO: Created: latency-svc-m4cnd
Feb 23 19:18:43.240: INFO: Got endpoints: latency-svc-scd9l [749.207818ms]
Feb 23 19:18:43.248: INFO: Created: latency-svc-7nvbx
Feb 23 19:18:43.289: INFO: Got endpoints: latency-svc-t684l [751.235329ms]
Feb 23 19:18:43.300: INFO: Created: latency-svc-jk526
Feb 23 19:18:43.339: INFO: Got endpoints: latency-svc-rx4pg [748.940123ms]
Feb 23 19:18:43.348: INFO: Created: latency-svc-lpgfj
Feb 23 19:18:43.389: INFO: Got endpoints: latency-svc-hm4qb [749.0001ms]
Feb 23 19:18:43.400: INFO: Created: latency-svc-qrfwp
Feb 23 19:18:43.440: INFO: Got endpoints: latency-svc-nbcr8 [747.055192ms]
Feb 23 19:18:43.464: INFO: Created: latency-svc-m7k2j
Feb 23 19:18:43.492: INFO: Got endpoints: latency-svc-g47l6 [749.892349ms]
Feb 23 19:18:43.502: INFO: Created: latency-svc-cqvgg
Feb 23 19:18:43.538: INFO: Got endpoints: latency-svc-rm6m6 [747.440061ms]
Feb 23 19:18:43.548: INFO: Created: latency-svc-vx9g7
Feb 23 19:18:43.589: INFO: Got endpoints: latency-svc-tzcbj [749.453735ms]
Feb 23 19:18:43.601: INFO: Created: latency-svc-crmmv
Feb 23 19:18:43.640: INFO: Got endpoints: latency-svc-vx4rl [749.087297ms]
Feb 23 19:18:43.655: INFO: Created: latency-svc-8c6tj
Feb 23 19:18:43.690: INFO: Got endpoints: latency-svc-b8ct6 [750.455201ms]
Feb 23 19:18:43.699: INFO: Created: latency-svc-c64qk
Feb 23 19:18:43.741: INFO: Got endpoints: latency-svc-lbpzg [749.280388ms]
Feb 23 19:18:43.751: INFO: Created: latency-svc-vf272
Feb 23 19:18:43.791: INFO: Got endpoints: latency-svc-kc6px [750.581551ms]
Feb 23 19:18:43.803: INFO: Created: latency-svc-gt2c4
Feb 23 19:18:43.841: INFO: Got endpoints: latency-svc-8r6rl [750.268778ms]
Feb 23 19:18:43.851: INFO: Created: latency-svc-7z4zh
Feb 23 19:18:43.900: INFO: Got endpoints: latency-svc-cfq4b [759.001881ms]
Feb 23 19:18:43.913: INFO: Created: latency-svc-ccf7k
Feb 23 19:18:43.941: INFO: Got endpoints: latency-svc-m4cnd [747.216651ms]
Feb 23 19:18:43.950: INFO: Created: latency-svc-qhp8l
Feb 23 19:18:43.993: INFO: Got endpoints: latency-svc-7nvbx [752.632293ms]
Feb 23 19:18:44.505: INFO: Created: latency-svc-z2cpv
Feb 23 19:18:44.505: INFO: Got endpoints: latency-svc-cqvgg [1.012917205s]
Feb 23 19:18:44.507: INFO: Got endpoints: latency-svc-qrfwp [1.118572045s]
Feb 23 19:18:44.508: INFO: Got endpoints: latency-svc-m7k2j [1.067428611s]
Feb 23 19:18:44.510: INFO: Got endpoints: latency-svc-jk526 [1.220177972s]
Feb 23 19:18:44.510: INFO: Got endpoints: latency-svc-lpgfj [1.170633476s]
Feb 23 19:18:44.511: INFO: Got endpoints: latency-svc-vx9g7 [973.131371ms]
Feb 23 19:18:44.521: INFO: Got endpoints: latency-svc-c64qk [831.192ms]
Feb 23 19:18:44.522: INFO: Got endpoints: latency-svc-crmmv [932.265896ms]
Feb 23 19:18:44.523: INFO: Got endpoints: latency-svc-8c6tj [882.462929ms]
Feb 23 19:18:44.523: INFO: Got endpoints: latency-svc-vf272 [781.738788ms]
Feb 23 19:18:44.527: INFO: Created: latency-svc-dm8dz
Feb 23 19:18:44.541: INFO: Created: latency-svc-6vnmq
Feb 23 19:18:44.543: INFO: Got endpoints: latency-svc-gt2c4 [751.123006ms]
Feb 23 19:18:44.547: INFO: Created: latency-svc-hwrtr
Feb 23 19:18:44.561: INFO: Created: latency-svc-t2g8m
Feb 23 19:18:44.569: INFO: Created: latency-svc-ck5tp
Feb 23 19:18:44.576: INFO: Created: latency-svc-jbccx
Feb 23 19:18:44.594: INFO: Got endpoints: latency-svc-7z4zh [752.629889ms]
Feb 23 19:18:44.597: INFO: Created: latency-svc-rtslr
Feb 23 19:18:44.603: INFO: Created: latency-svc-dx7tt
Feb 23 19:18:44.612: INFO: Created: latency-svc-d9h54
Feb 23 19:18:44.620: INFO: Created: latency-svc-kfgfz
Feb 23 19:18:44.642: INFO: Created: latency-svc-rwbz7
Feb 23 19:18:44.647: INFO: Got endpoints: latency-svc-ccf7k [747.624067ms]
Feb 23 19:18:44.662: INFO: Created: latency-svc-8mhvz
Feb 23 19:18:44.666: INFO: Created: latency-svc-tg9xx
Feb 23 19:18:44.690: INFO: Got endpoints: latency-svc-qhp8l [749.111658ms]
Feb 23 19:18:44.705: INFO: Created: latency-svc-h6f6k
Feb 23 19:18:44.739: INFO: Got endpoints: latency-svc-z2cpv [745.828804ms]
Feb 23 19:18:44.749: INFO: Created: latency-svc-flb9l
Feb 23 19:18:44.793: INFO: Got endpoints: latency-svc-dm8dz [287.38783ms]
Feb 23 19:18:44.802: INFO: Created: latency-svc-ts9gb
Feb 23 19:18:44.839: INFO: Got endpoints: latency-svc-6vnmq [331.031455ms]
Feb 23 19:18:44.847: INFO: Created: latency-svc-nhnck
Feb 23 19:18:44.890: INFO: Got endpoints: latency-svc-hwrtr [379.766623ms]
Feb 23 19:18:44.904: INFO: Created: latency-svc-f8zwn
Feb 23 19:18:44.941: INFO: Got endpoints: latency-svc-t2g8m [431.148937ms]
Feb 23 19:18:44.953: INFO: Created: latency-svc-dtvnz
Feb 23 19:18:44.991: INFO: Got endpoints: latency-svc-ck5tp [482.945115ms]
Feb 23 19:18:45.005: INFO: Created: latency-svc-d98pj
Feb 23 19:18:45.040: INFO: Got endpoints: latency-svc-jbccx [528.015467ms]
Feb 23 19:18:45.049: INFO: Created: latency-svc-kdc8v
Feb 23 19:18:45.094: INFO: Got endpoints: latency-svc-rtslr [570.826507ms]
Feb 23 19:18:45.103: INFO: Created: latency-svc-4lrp9
Feb 23 19:18:45.139: INFO: Got endpoints: latency-svc-dx7tt [618.3299ms]
Feb 23 19:18:45.148: INFO: Created: latency-svc-qtgg8
Feb 23 19:18:45.190: INFO: Got endpoints: latency-svc-d9h54 [595.74189ms]
Feb 23 19:18:45.204: INFO: Created: latency-svc-kv5lv
Feb 23 19:18:45.241: INFO: Got endpoints: latency-svc-kfgfz [717.855087ms]
Feb 23 19:18:45.252: INFO: Created: latency-svc-d5k55
Feb 23 19:18:45.293: INFO: Got endpoints: latency-svc-rwbz7 [771.110703ms]
Feb 23 19:18:45.305: INFO: Created: latency-svc-2jwvq
Feb 23 19:18:45.341: INFO: Got endpoints: latency-svc-8mhvz [797.952063ms]
Feb 23 19:18:45.359: INFO: Created: latency-svc-wjsnm
Feb 23 19:18:45.390: INFO: Got endpoints: latency-svc-tg9xx [742.050506ms]
Feb 23 19:18:45.407: INFO: Created: latency-svc-8h6sh
Feb 23 19:18:45.439: INFO: Got endpoints: latency-svc-h6f6k [748.589985ms]
Feb 23 19:18:45.461: INFO: Created: latency-svc-mlgpt
Feb 23 19:18:45.489: INFO: Got endpoints: latency-svc-flb9l [749.566664ms]
Feb 23 19:18:45.509: INFO: Created: latency-svc-rd9vt
Feb 23 19:18:45.538: INFO: Got endpoints: latency-svc-ts9gb [745.202732ms]
Feb 23 19:18:45.546: INFO: Created: latency-svc-hkkvq
Feb 23 19:18:45.593: INFO: Got endpoints: latency-svc-nhnck [754.575575ms]
Feb 23 19:18:45.604: INFO: Created: latency-svc-8h5fq
Feb 23 19:18:45.641: INFO: Got endpoints: latency-svc-f8zwn [749.814193ms]
Feb 23 19:18:45.652: INFO: Created: latency-svc-mhr6z
Feb 23 19:18:45.691: INFO: Got endpoints: latency-svc-dtvnz [749.593657ms]
Feb 23 19:18:45.698: INFO: Created: latency-svc-xz9p9
Feb 23 19:18:45.750: INFO: Got endpoints: latency-svc-d98pj [758.920015ms]
Feb 23 19:18:45.760: INFO: Created: latency-svc-4bg9b
Feb 23 19:18:45.790: INFO: Got endpoints: latency-svc-kdc8v [749.584623ms]
Feb 23 19:18:45.799: INFO: Created: latency-svc-95dmh
Feb 23 19:18:45.838: INFO: Got endpoints: latency-svc-4lrp9 [744.577124ms]
Feb 23 19:18:45.848: INFO: Created: latency-svc-wcrhw
Feb 23 19:18:45.896: INFO: Got endpoints: latency-svc-qtgg8 [756.316326ms]
Feb 23 19:18:45.917: INFO: Created: latency-svc-zp4ng
Feb 23 19:18:45.941: INFO: Got endpoints: latency-svc-kv5lv [751.269455ms]
Feb 23 19:18:45.950: INFO: Created: latency-svc-98qm5
Feb 23 19:18:45.997: INFO: Got endpoints: latency-svc-d5k55 [755.724257ms]
Feb 23 19:18:46.010: INFO: Created: latency-svc-mtwrx
Feb 23 19:18:46.040: INFO: Got endpoints: latency-svc-2jwvq [747.127177ms]
Feb 23 19:18:46.049: INFO: Created: latency-svc-sm6br
Feb 23 19:18:46.089: INFO: Got endpoints: latency-svc-wjsnm [748.662371ms]
Feb 23 19:18:46.100: INFO: Created: latency-svc-255nc
Feb 23 19:18:46.138: INFO: Got endpoints: latency-svc-8h6sh [747.890072ms]
Feb 23 19:18:46.146: INFO: Created: latency-svc-zmc55
Feb 23 19:18:46.189: INFO: Got endpoints: latency-svc-mlgpt [750.324799ms]
Feb 23 19:18:46.206: INFO: Created: latency-svc-2tr4f
Feb 23 19:18:46.239: INFO: Got endpoints: latency-svc-rd9vt [750.237463ms]
Feb 23 19:18:46.248: INFO: Created: latency-svc-58slb
Feb 23 19:18:46.290: INFO: Got endpoints: latency-svc-hkkvq [752.199612ms]
Feb 23 19:18:46.303: INFO: Created: latency-svc-jqz8b
Feb 23 19:18:46.339: INFO: Got endpoints: latency-svc-8h5fq [746.274495ms]
Feb 23 19:18:46.347: INFO: Created: latency-svc-2mkp2
Feb 23 19:18:46.390: INFO: Got endpoints: latency-svc-mhr6z [748.494548ms]
Feb 23 19:18:46.400: INFO: Created: latency-svc-qj9v4
Feb 23 19:18:46.440: INFO: Got endpoints: latency-svc-xz9p9 [749.102873ms]
Feb 23 19:18:46.446: INFO: Created: latency-svc-pp9p4
Feb 23 19:18:46.489: INFO: Got endpoints: latency-svc-4bg9b [739.300516ms]
Feb 23 19:18:46.498: INFO: Created: latency-svc-lh9nd
Feb 23 19:18:46.541: INFO: Got endpoints: latency-svc-95dmh [750.956859ms]
Feb 23 19:18:46.548: INFO: Created: latency-svc-nnsgv
Feb 23 19:18:46.591: INFO: Got endpoints: latency-svc-wcrhw [752.9606ms]
Feb 23 19:18:46.605: INFO: Created: latency-svc-br6cx
Feb 23 19:18:46.646: INFO: Got endpoints: latency-svc-zp4ng [750.146887ms]
Feb 23 19:18:46.658: INFO: Created: latency-svc-kxd7s
Feb 23 19:18:46.694: INFO: Got endpoints: latency-svc-98qm5 [752.588306ms]
Feb 23 19:18:46.701: INFO: Created: latency-svc-7bk8z
Feb 23 19:18:46.744: INFO: Got endpoints: latency-svc-mtwrx [747.002944ms]
Feb 23 19:18:46.760: INFO: Created: latency-svc-m6gpl
Feb 23 19:18:46.791: INFO: Got endpoints: latency-svc-sm6br [750.90781ms]
Feb 23 19:18:46.800: INFO: Created: latency-svc-scc68
Feb 23 19:18:46.842: INFO: Got endpoints: latency-svc-255nc [753.018855ms]
Feb 23 19:18:46.850: INFO: Created: latency-svc-xqmkx
Feb 23 19:18:46.891: INFO: Got endpoints: latency-svc-zmc55 [753.104202ms]
Feb 23 19:18:46.904: INFO: Created: latency-svc-vzc64
Feb 23 19:18:46.939: INFO: Got endpoints: latency-svc-2tr4f [750.263611ms]
Feb 23 19:18:46.955: INFO: Created: latency-svc-rs5f4
Feb 23 19:18:46.992: INFO: Got endpoints: latency-svc-58slb [753.439286ms]
Feb 23 19:18:47.005: INFO: Created: latency-svc-mdtcn
Feb 23 19:18:47.045: INFO: Got endpoints: latency-svc-jqz8b [755.076164ms]
Feb 23 19:18:47.057: INFO: Created: latency-svc-wxq4z
Feb 23 19:18:47.092: INFO: Got endpoints: latency-svc-2mkp2 [752.30562ms]
Feb 23 19:18:47.101: INFO: Created: latency-svc-n7gjr
Feb 23 19:18:47.141: INFO: Got endpoints: latency-svc-qj9v4 [751.000697ms]
Feb 23 19:18:47.150: INFO: Created: latency-svc-f2xtt
Feb 23 19:18:47.193: INFO: Got endpoints: latency-svc-pp9p4 [753.604673ms]
Feb 23 19:18:47.209: INFO: Created: latency-svc-55m4z
Feb 23 19:18:47.241: INFO: Got endpoints: latency-svc-lh9nd [752.324309ms]
Feb 23 19:18:47.259: INFO: Created: latency-svc-r6kqs
Feb 23 19:18:47.291: INFO: Got endpoints: latency-svc-nnsgv [749.798451ms]
Feb 23 19:18:47.305: INFO: Created: latency-svc-gtr5j
Feb 23 19:18:47.340: INFO: Got endpoints: latency-svc-br6cx [748.484145ms]
Feb 23 19:18:47.394: INFO: Got endpoints: latency-svc-kxd7s [748.078192ms]
Feb 23 19:18:47.440: INFO: Got endpoints: latency-svc-7bk8z [745.974607ms]
Feb 23 19:18:47.495: INFO: Got endpoints: latency-svc-m6gpl [751.043175ms]
Feb 23 19:18:47.538: INFO: Got endpoints: latency-svc-scc68 [747.390174ms]
Feb 23 19:18:47.592: INFO: Got endpoints: latency-svc-xqmkx [749.643287ms]
Feb 23 19:18:47.640: INFO: Got endpoints: latency-svc-vzc64 [748.172262ms]
Feb 23 19:18:47.691: INFO: Got endpoints: latency-svc-rs5f4 [750.154139ms]
Feb 23 19:18:47.740: INFO: Got endpoints: latency-svc-mdtcn [747.582072ms]
Feb 23 19:18:47.797: INFO: Got endpoints: latency-svc-wxq4z [751.167019ms]
Feb 23 19:18:47.841: INFO: Got endpoints: latency-svc-n7gjr [748.800497ms]
Feb 23 19:18:47.889: INFO: Got endpoints: latency-svc-f2xtt [748.535004ms]
Feb 23 19:18:47.939: INFO: Got endpoints: latency-svc-55m4z [745.905716ms]
Feb 23 19:18:47.991: INFO: Got endpoints: latency-svc-r6kqs [749.955456ms]
Feb 23 19:18:48.041: INFO: Got endpoints: latency-svc-gtr5j [749.648216ms]
Feb 23 19:18:48.042: INFO: Latencies: [22.27544ms 51.361877ms 56.809772ms 69.846257ms 90.216001ms 111.428445ms 123.487936ms 151.252807ms 169.807957ms 171.075468ms 171.325848ms 177.340492ms 178.62831ms 180.174085ms 181.501058ms 183.002424ms 186.381973ms 189.876806ms 190.344653ms 193.791013ms 195.830783ms 199.552033ms 199.616707ms 200.144129ms 200.937238ms 202.006ms 202.012745ms 203.909262ms 210.047263ms 210.797432ms 211.403024ms 219.319568ms 220.583653ms 223.937812ms 224.40356ms 228.640687ms 233.031829ms 234.80361ms 235.937208ms 237.419494ms 238.617257ms 243.235685ms 278.535532ms 287.38783ms 313.028978ms 317.421228ms 331.031455ms 351.848328ms 361.931895ms 379.766623ms 406.35253ms 408.795084ms 431.148937ms 435.727399ms 451.494439ms 470.968661ms 482.945115ms 503.516303ms 511.03742ms 528.015467ms 547.112078ms 547.716418ms 570.826507ms 572.505299ms 593.948973ms 595.74189ms 618.3299ms 633.1494ms 646.41103ms 652.131833ms 690.457001ms 695.229587ms 717.855087ms 738.972092ms 739.300516ms 740.35769ms 742.050506ms 742.977059ms 744.577124ms 745.202732ms 745.636314ms 745.828804ms 745.905716ms 745.974607ms 746.274495ms 746.71516ms 747.002944ms 747.055192ms 747.061072ms 747.127177ms 747.164623ms 747.216651ms 747.390174ms 747.440061ms 747.582072ms 747.624067ms 747.749593ms 747.890072ms 748.026468ms 748.078192ms 748.172262ms 748.315571ms 748.341073ms 748.484145ms 748.494548ms 748.535004ms 748.589985ms 748.662371ms 748.746522ms 748.800497ms 748.940123ms 749.0001ms 749.087297ms 749.102873ms 749.111658ms 749.207818ms 749.280388ms 749.453735ms 749.566664ms 749.584623ms 749.593657ms 749.643287ms 749.648216ms 749.777088ms 749.798451ms 749.814193ms 749.892349ms 749.955456ms 750.146887ms 750.154139ms 750.237463ms 750.263611ms 750.268778ms 750.31508ms 750.324799ms 750.350621ms 750.445404ms 750.455201ms 750.461982ms 750.514955ms 750.581551ms 750.830753ms 750.844875ms 750.852996ms 750.90781ms 750.956859ms 751.000697ms 751.043175ms 751.123006ms 751.138894ms 751.167019ms 751.225835ms 751.235329ms 751.269455ms 751.457269ms 751.657433ms 751.719797ms 751.873206ms 752.094803ms 752.199612ms 752.30562ms 752.324309ms 752.541181ms 752.573314ms 752.588306ms 752.629889ms 752.632293ms 752.9606ms 753.018855ms 753.104202ms 753.439286ms 753.604673ms 754.575575ms 755.076164ms 755.724257ms 756.316326ms 756.910111ms 758.920015ms 759.001881ms 771.110703ms 781.738788ms 797.952063ms 808.191903ms 831.192ms 852.579983ms 882.462929ms 901.879965ms 932.265896ms 950.275842ms 973.131371ms 993.945863ms 1.012917205s 1.036098513s 1.067428611s 1.097708999s 1.118572045s 1.132772789s 1.170633476s 1.18561936s 1.220177972s]
Feb 23 19:18:48.042: INFO: 50 %ile: 748.172262ms
Feb 23 19:18:48.042: INFO: 90 %ile: 781.738788ms
Feb 23 19:18:48.043: INFO: 99 %ile: 1.18561936s
Feb 23 19:18:48.043: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:18:48.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-404" for this suite.
Feb 23 19:19:06.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:19:06.132: INFO: namespace svc-latency-404 deletion completed in 18.083916701s

• [SLOW TEST:28.853 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:19:06.133: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Feb 23 19:19:06.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 create -f - --namespace=kubectl-4823'
Feb 23 19:19:06.348: INFO: stderr: ""
Feb 23 19:19:06.348: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 23 19:19:06.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4823'
Feb 23 19:19:06.436: INFO: stderr: ""
Feb 23 19:19:06.436: INFO: stdout: "update-demo-nautilus-g5vw4 update-demo-nautilus-jqzm2 "
Feb 23 19:19:06.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-nautilus-g5vw4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4823'
Feb 23 19:19:06.493: INFO: stderr: ""
Feb 23 19:19:06.493: INFO: stdout: ""
Feb 23 19:19:06.493: INFO: update-demo-nautilus-g5vw4 is created but not running
Feb 23 19:19:11.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4823'
Feb 23 19:19:11.551: INFO: stderr: ""
Feb 23 19:19:11.551: INFO: stdout: "update-demo-nautilus-g5vw4 update-demo-nautilus-jqzm2 "
Feb 23 19:19:11.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-nautilus-g5vw4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4823'
Feb 23 19:19:11.608: INFO: stderr: ""
Feb 23 19:19:11.608: INFO: stdout: "true"
Feb 23 19:19:11.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-nautilus-g5vw4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4823'
Feb 23 19:19:11.665: INFO: stderr: ""
Feb 23 19:19:11.665: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 19:19:11.665: INFO: validating pod update-demo-nautilus-g5vw4
Feb 23 19:19:11.671: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 19:19:11.671: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 19:19:11.671: INFO: update-demo-nautilus-g5vw4 is verified up and running
Feb 23 19:19:11.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-nautilus-jqzm2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4823'
Feb 23 19:19:11.724: INFO: stderr: ""
Feb 23 19:19:11.724: INFO: stdout: "true"
Feb 23 19:19:11.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods update-demo-nautilus-jqzm2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4823'
Feb 23 19:19:11.775: INFO: stderr: ""
Feb 23 19:19:11.775: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 19:19:11.775: INFO: validating pod update-demo-nautilus-jqzm2
Feb 23 19:19:11.782: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 19:19:11.782: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 19:19:11.782: INFO: update-demo-nautilus-jqzm2 is verified up and running
STEP: using delete to clean up resources
Feb 23 19:19:11.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 delete --grace-period=0 --force -f - --namespace=kubectl-4823'
Feb 23 19:19:11.841: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 19:19:11.842: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 23 19:19:11.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4823'
Feb 23 19:19:11.928: INFO: stderr: "No resources found in kubectl-4823 namespace.\n"
Feb 23 19:19:11.928: INFO: stdout: ""
Feb 23 19:19:11.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods -l name=update-demo --namespace=kubectl-4823 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 23 19:19:11.989: INFO: stderr: ""
Feb 23 19:19:11.989: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:19:11.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4823" for this suite.
Feb 23 19:19:40.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:19:40.072: INFO: namespace kubectl-4823 deletion completed in 28.077460264s

• [SLOW TEST:33.939 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:19:40.072: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4262.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4262.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4262.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4262.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4262.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4262.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 23 19:19:42.153: INFO: DNS probes using dns-4262/dns-test-61af74f6-9e1b-4f0c-9d85-7307a770c58a succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:19:42.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4262" for this suite.
Feb 23 19:19:48.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:19:48.278: INFO: namespace dns-4262 deletion completed in 6.075901539s

• [SLOW TEST:8.206 seconds]
[sig-network] DNS
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:19:48.280: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 23 19:19:48.732: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79ed7e59-4014-43dc-a75b-c4518514f58d" in namespace "projected-7883" to be "success or failure"
Feb 23 19:19:48.739: INFO: Pod "downwardapi-volume-79ed7e59-4014-43dc-a75b-c4518514f58d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.032824ms
Feb 23 19:19:50.742: INFO: Pod "downwardapi-volume-79ed7e59-4014-43dc-a75b-c4518514f58d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009919113s
STEP: Saw pod success
Feb 23 19:19:50.743: INFO: Pod "downwardapi-volume-79ed7e59-4014-43dc-a75b-c4518514f58d" satisfied condition "success or failure"
Feb 23 19:19:50.746: INFO: Trying to get logs from node worker00 pod downwardapi-volume-79ed7e59-4014-43dc-a75b-c4518514f58d container client-container: <nil>
STEP: delete the pod
Feb 23 19:19:50.762: INFO: Waiting for pod downwardapi-volume-79ed7e59-4014-43dc-a75b-c4518514f58d to disappear
Feb 23 19:19:50.767: INFO: Pod downwardapi-volume-79ed7e59-4014-43dc-a75b-c4518514f58d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:19:50.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7883" for this suite.
Feb 23 19:19:56.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:19:56.839: INFO: namespace projected-7883 deletion completed in 6.068517014s

• [SLOW TEST:8.559 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:19:56.839: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Feb 23 19:20:02.926: INFO: 0 pods remaining
Feb 23 19:20:02.926: INFO: 0 pods has nil DeletionTimestamp
Feb 23 19:20:02.926: INFO: 
STEP: Gathering metrics
Feb 23 19:20:03.919: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:20:03.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0223 19:20:03.919222      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-3198" for this suite.
Feb 23 19:20:09.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:20:10.003: INFO: namespace gc-3198 deletion completed in 6.079760703s

• [SLOW TEST:13.164 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:20:10.003: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 23 19:20:10.042: INFO: Waiting up to 5m0s for pod "pod-b6ce63f2-f49c-4ef5-8045-84eac5614c58" in namespace "emptydir-265" to be "success or failure"
Feb 23 19:20:10.046: INFO: Pod "pod-b6ce63f2-f49c-4ef5-8045-84eac5614c58": Phase="Pending", Reason="", readiness=false. Elapsed: 3.557255ms
Feb 23 19:20:12.051: INFO: Pod "pod-b6ce63f2-f49c-4ef5-8045-84eac5614c58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008521546s
STEP: Saw pod success
Feb 23 19:20:12.051: INFO: Pod "pod-b6ce63f2-f49c-4ef5-8045-84eac5614c58" satisfied condition "success or failure"
Feb 23 19:20:12.053: INFO: Trying to get logs from node worker00 pod pod-b6ce63f2-f49c-4ef5-8045-84eac5614c58 container test-container: <nil>
STEP: delete the pod
Feb 23 19:20:12.069: INFO: Waiting for pod pod-b6ce63f2-f49c-4ef5-8045-84eac5614c58 to disappear
Feb 23 19:20:12.072: INFO: Pod pod-b6ce63f2-f49c-4ef5-8045-84eac5614c58 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:20:12.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-265" for this suite.
Feb 23 19:20:18.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:20:18.149: INFO: namespace emptydir-265 deletion completed in 6.073708157s

• [SLOW TEST:8.146 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:20:18.151: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 23 19:20:18.207: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9137 /api/v1/namespaces/watch-9137/configmaps/e2e-watch-test-resource-version a20c87c5-aa06-4773-ba44-8ebb47c81020 31967 0 2020-02-23 19:20:18 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 23 19:20:18.208: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9137 /api/v1/namespaces/watch-9137/configmaps/e2e-watch-test-resource-version a20c87c5-aa06-4773-ba44-8ebb47c81020 31968 0 2020-02-23 19:20:18 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:20:18.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9137" for this suite.
Feb 23 19:20:24.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:20:24.282: INFO: namespace watch-9137 deletion completed in 6.070978236s

• [SLOW TEST:6.131 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:20:24.282: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Feb 23 19:20:24.309: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:20:28.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5877" for this suite.
Feb 23 19:20:56.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:20:56.235: INFO: namespace init-container-5877 deletion completed in 28.075041839s

• [SLOW TEST:31.953 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:20:56.235: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:21:00.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1288" for this suite.
Feb 23 19:21:07.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:21:07.224: INFO: namespace watch-1288 deletion completed in 6.183565167s

• [SLOW TEST:10.990 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:21:07.224: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-69f50258-0d06-482d-9fe8-cdfc56dfe6e6
STEP: Creating a pod to test consume secrets
Feb 23 19:21:07.262: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e4729e19-ce01-4761-9d44-4482e555e842" in namespace "projected-7753" to be "success or failure"
Feb 23 19:21:07.271: INFO: Pod "pod-projected-secrets-e4729e19-ce01-4761-9d44-4482e555e842": Phase="Pending", Reason="", readiness=false. Elapsed: 9.041033ms
Feb 23 19:21:09.275: INFO: Pod "pod-projected-secrets-e4729e19-ce01-4761-9d44-4482e555e842": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012663867s
STEP: Saw pod success
Feb 23 19:21:09.275: INFO: Pod "pod-projected-secrets-e4729e19-ce01-4761-9d44-4482e555e842" satisfied condition "success or failure"
Feb 23 19:21:09.280: INFO: Trying to get logs from node worker00 pod pod-projected-secrets-e4729e19-ce01-4761-9d44-4482e555e842 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 23 19:21:09.296: INFO: Waiting for pod pod-projected-secrets-e4729e19-ce01-4761-9d44-4482e555e842 to disappear
Feb 23 19:21:09.299: INFO: Pod pod-projected-secrets-e4729e19-ce01-4761-9d44-4482e555e842 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:21:09.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7753" for this suite.
Feb 23 19:21:15.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:21:15.387: INFO: namespace projected-7753 deletion completed in 6.085505421s

• [SLOW TEST:8.163 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:21:15.392: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Feb 23 19:21:19.950: INFO: Successfully updated pod "labelsupdate346042f1-8d2d-476c-8ba5-e01a34e7b3f5"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:21:21.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4731" for this suite.
Feb 23 19:21:49.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:21:50.047: INFO: namespace downward-api-4731 deletion completed in 28.076116477s

• [SLOW TEST:34.655 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:21:50.047: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:22:06.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2245" for this suite.
Feb 23 19:22:12.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:22:12.650: INFO: namespace resourcequota-2245 deletion completed in 6.076959203s

• [SLOW TEST:22.603 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:22:12.653: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Feb 23 19:22:12.679: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 23 19:22:12.687: INFO: Waiting for terminating namespaces to be deleted...
Feb 23 19:22:12.689: INFO: 
Logging pods the kubelet thinks is on node worker00 before test
Feb 23 19:22:12.695: INFO: gobetween-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.696: INFO: 	Container gobetween ready: true, restart count 0
Feb 23 19:22:12.696: INFO: kube-proxy-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.696: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 19:22:12.696: INFO: kube-scheduler-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.696: INFO: 	Container kube-scheduler ready: true, restart count 2
Feb 23 19:22:12.696: INFO: calico-node-77p2v from networking started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.696: INFO: 	Container calico-node ready: true, restart count 0
Feb 23 19:22:12.696: INFO: ceph-osd-worker00-556546b495-nz47w from storage started at 2020-02-23 19:01:10 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.697: INFO: 	Container ceph-osd ready: true, restart count 2
Feb 23 19:22:12.697: INFO: etcd-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.697: INFO: 	Container etcd ready: true, restart count 0
Feb 23 19:22:12.697: INFO: kube-apiserver-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.697: INFO: 	Container kube-apiserver ready: true, restart count 1
Feb 23 19:22:12.697: INFO: sonobuoy-e2e-job-382cd34a40b246b9 from sonobuoy started at 2020-02-23 17:36:27 +0000 UTC (2 container statuses recorded)
Feb 23 19:22:12.697: INFO: 	Container e2e ready: true, restart count 0
Feb 23 19:22:12.697: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 19:22:12.697: INFO: sonobuoy-systemd-logs-daemon-set-e1933034003941b1-4v2r4 from sonobuoy started at 2020-02-23 17:36:27 +0000 UTC (2 container statuses recorded)
Feb 23 19:22:12.697: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 23 19:22:12.697: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 23 19:22:12.698: INFO: dashboard-metrics-scraper-58475bc987-29h5g from kube-system started at 2020-02-23 19:01:10 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.698: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 23 19:22:12.698: INFO: csi-cephfsplugin-tbdkm from storage started at 2020-02-23 19:01:18 +0000 UTC (3 container statuses recorded)
Feb 23 19:22:12.698: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 19:22:12.698: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 19:22:12.698: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 19:22:12.698: INFO: kube-controller-manager-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.698: INFO: 	Container kube-controller-manager ready: true, restart count 1
Feb 23 19:22:12.698: INFO: csi-rbdplugin-9ghvp from storage started at 2020-02-23 19:01:16 +0000 UTC (3 container statuses recorded)
Feb 23 19:22:12.698: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 19:22:12.698: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 19:22:12.698: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 19:22:12.698: INFO: sonobuoy from sonobuoy started at 2020-02-23 17:36:25 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.698: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 23 19:22:12.698: INFO: ceph-mon-worker00-5cf654d469-jcn96 from storage started at 2020-02-23 19:01:10 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.698: INFO: 	Container ceph-mon ready: true, restart count 0
Feb 23 19:22:12.698: INFO: metallb-speaker-7bnlk from networking started at 2020-02-23 19:01:10 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.698: INFO: 	Container speaker ready: true, restart count 0
Feb 23 19:22:12.698: INFO: ceph-mds-worker00-6f479b4486-nrbn8 from storage started at 2020-02-23 19:01:41 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.698: INFO: 	Container ceph-mds ready: true, restart count 0
Feb 23 19:22:12.699: INFO: 
Logging pods the kubelet thinks is on node worker01 before test
Feb 23 19:22:12.714: INFO: ceph-setup-xccxd from storage started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.714: INFO: 	Container ceph ready: false, restart count 2
Feb 23 19:22:12.714: INFO: kubernetes-dashboard-f957cddcb-qq7vr from kube-system started at 2020-02-23 19:01:10 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.714: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 23 19:22:12.714: INFO: ceph-mgr-94b9dd996-vfc9x from storage started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.714: INFO: 	Container ceph-mgr ready: true, restart count 0
Feb 23 19:22:12.714: INFO: csi-rbdplugin-provisioner-7494f65674-zk9l6 from storage started at 2020-02-23 17:26:43 +0000 UTC (5 container statuses recorded)
Feb 23 19:22:12.714: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 23 19:22:12.714: INFO: 	Container csi-provisioner ready: true, restart count 1
Feb 23 19:22:12.714: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 19:22:12.714: INFO: 	Container csi-snapshotter ready: true, restart count 0
Feb 23 19:22:12.714: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 19:22:12.714: INFO: coredns-676544c7b9-pz66t from kube-system started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.714: INFO: 	Container coredns ready: true, restart count 0
Feb 23 19:22:12.714: INFO: ceph-rgw-57cd48f74c-4nwcs from storage started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.714: INFO: 	Container ceph-rgw ready: true, restart count 1
Feb 23 19:22:12.714: INFO: csi-cephfsplugin-provisioner-6cd7596f75-tjxsr from storage started at 2020-02-23 17:26:43 +0000 UTC (4 container statuses recorded)
Feb 23 19:22:12.714: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 19:22:12.714: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 19:22:12.714: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 19:22:12.714: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 19:22:12.714: INFO: metallb-speaker-jpspl from networking started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.714: INFO: 	Container speaker ready: true, restart count 0
Feb 23 19:22:12.714: INFO: etcd-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.714: INFO: 	Container etcd ready: true, restart count 0
Feb 23 19:22:12.714: INFO: kube-controller-manager-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.714: INFO: 	Container kube-controller-manager ready: true, restart count 1
Feb 23 19:22:12.715: INFO: metallb-controller-b96bfbbf8-p9qvp from networking started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.715: INFO: 	Container controller ready: true, restart count 0
Feb 23 19:22:12.715: INFO: coredns-676544c7b9-ntt4b from kube-system started at 2020-02-23 17:26:46 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.715: INFO: 	Container coredns ready: true, restart count 0
Feb 23 19:22:12.715: INFO: csi-cephfsplugin-provisioner-6cd7596f75-b6s7q from storage started at 2020-02-23 17:26:43 +0000 UTC (4 container statuses recorded)
Feb 23 19:22:12.715: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 19:22:12.715: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 19:22:12.715: INFO: 	Container csi-provisioner ready: true, restart count 1
Feb 23 19:22:12.715: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 19:22:12.715: INFO: calico-kube-controllers-7cd585bcd-v4kk7 from networking started at 2020-02-23 17:26:46 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.715: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 23 19:22:12.715: INFO: kube-proxy-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.715: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 19:22:12.715: INFO: calico-node-lhcql from networking started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.715: INFO: 	Container calico-node ready: true, restart count 0
Feb 23 19:22:12.715: INFO: ceph-mds-worker01-7f5fdb58c6-m96hd from storage started at 2020-02-23 17:25:27 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.715: INFO: 	Container ceph-mds ready: true, restart count 2
Feb 23 19:22:12.715: INFO: csi-cephfsplugin-provisioner-6cd7596f75-n9kmr from storage started at 2020-02-23 17:26:43 +0000 UTC (4 container statuses recorded)
Feb 23 19:22:12.715: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 19:22:12.715: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 19:22:12.715: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 19:22:12.715: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 19:22:12.715: INFO: csi-rbdplugin-provisioner-7494f65674-gt7nb from storage started at 2020-02-23 17:26:43 +0000 UTC (5 container statuses recorded)
Feb 23 19:22:12.715: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 23 19:22:12.715: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 19:22:12.715: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 19:22:12.715: INFO: 	Container csi-snapshotter ready: true, restart count 0
Feb 23 19:22:12.715: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 19:22:12.716: INFO: csi-rbdplugin-provisioner-7494f65674-vnnkm from storage started at 2020-02-23 17:26:43 +0000 UTC (5 container statuses recorded)
Feb 23 19:22:12.716: INFO: 	Container csi-attacher ready: true, restart count 1
Feb 23 19:22:12.716: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 19:22:12.716: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 19:22:12.716: INFO: 	Container csi-snapshotter ready: true, restart count 0
Feb 23 19:22:12.716: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 19:22:12.716: INFO: kube-apiserver-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.716: INFO: 	Container kube-apiserver ready: true, restart count 0
Feb 23 19:22:12.716: INFO: ceph-osd-worker01-67947c799-8n7pr from storage started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.716: INFO: 	Container ceph-osd ready: true, restart count 0
Feb 23 19:22:12.716: INFO: sonobuoy-systemd-logs-daemon-set-e1933034003941b1-74kbd from sonobuoy started at 2020-02-23 17:36:27 +0000 UTC (2 container statuses recorded)
Feb 23 19:22:12.716: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 23 19:22:12.716: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 23 19:22:12.716: INFO: kube-scheduler-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.716: INFO: 	Container kube-scheduler ready: true, restart count 1
Feb 23 19:22:12.716: INFO: csi-rbdplugin-cfq94 from storage started at 2020-02-23 17:26:43 +0000 UTC (3 container statuses recorded)
Feb 23 19:22:12.716: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 19:22:12.716: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 19:22:12.716: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 19:22:12.716: INFO: csi-cephfsplugin-wj5f7 from storage started at 2020-02-23 17:26:43 +0000 UTC (3 container statuses recorded)
Feb 23 19:22:12.716: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 19:22:12.716: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 19:22:12.716: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 19:22:12.716: INFO: gobetween-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.716: INFO: 	Container gobetween ready: true, restart count 0
Feb 23 19:22:12.716: INFO: ceph-mon-worker01-bdb694876-9x2bs from storage started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 19:22:12.716: INFO: 	Container ceph-mon ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-40ddc5b3-a62d-4e91-a51d-d849a28680f8 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-40ddc5b3-a62d-4e91-a51d-d849a28680f8 off the node worker00
STEP: verifying the node doesn't have the label kubernetes.io/e2e-40ddc5b3-a62d-4e91-a51d-d849a28680f8
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:22:22.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5915" for this suite.
Feb 23 19:22:36.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:22:36.909: INFO: namespace sched-pred-5915 deletion completed in 14.076447267s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:24.256 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:22:36.910: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with configMap that has name projected-configmap-test-upd-fb4c357c-2b51-4a82-9895-3fdb1dcb164a
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-fb4c357c-2b51-4a82-9895-3fdb1dcb164a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:23:49.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6387" for this suite.
Feb 23 19:24:05.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:24:05.304: INFO: namespace projected-6387 deletion completed in 16.077985804s

• [SLOW TEST:88.394 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:24:05.305: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-3530
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 23 19:24:05.331: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 23 19:24:27.577: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.200.5.28 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3530 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 19:24:27.577: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
Feb 23 19:24:28.652: INFO: Found all expected endpoints: [netserver-0]
Feb 23 19:24:28.655: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.200.131.149 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3530 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 19:24:28.655: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
Feb 23 19:24:30.209: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:24:30.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3530" for this suite.
Feb 23 19:24:42.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:24:42.310: INFO: namespace pod-network-test-3530 deletion completed in 12.097343554s

• [SLOW TEST:37.006 seconds]
[sig-network] Networking
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:24:42.312: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-f4f6d0e1-656e-4917-844f-2a44b0de06f5
STEP: Creating a pod to test consume secrets
Feb 23 19:24:42.350: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ccff6724-1efa-45e6-b4ef-173569c76611" in namespace "projected-6642" to be "success or failure"
Feb 23 19:24:42.363: INFO: Pod "pod-projected-secrets-ccff6724-1efa-45e6-b4ef-173569c76611": Phase="Pending", Reason="", readiness=false. Elapsed: 13.285317ms
Feb 23 19:24:44.365: INFO: Pod "pod-projected-secrets-ccff6724-1efa-45e6-b4ef-173569c76611": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015683199s
STEP: Saw pod success
Feb 23 19:24:44.365: INFO: Pod "pod-projected-secrets-ccff6724-1efa-45e6-b4ef-173569c76611" satisfied condition "success or failure"
Feb 23 19:24:44.373: INFO: Trying to get logs from node worker00 pod pod-projected-secrets-ccff6724-1efa-45e6-b4ef-173569c76611 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 23 19:24:44.393: INFO: Waiting for pod pod-projected-secrets-ccff6724-1efa-45e6-b4ef-173569c76611 to disappear
Feb 23 19:24:44.897: INFO: Pod pod-projected-secrets-ccff6724-1efa-45e6-b4ef-173569c76611 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:24:44.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6642" for this suite.
Feb 23 19:24:50.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:24:50.981: INFO: namespace projected-6642 deletion completed in 6.07430618s

• [SLOW TEST:8.669 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:24:50.981: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:24:53.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4351" for this suite.
Feb 23 19:25:37.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:25:37.145: INFO: namespace kubelet-test-4351 deletion completed in 44.092274401s

• [SLOW TEST:46.164 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:25:37.149: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 23 19:25:41.219: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 23 19:25:41.224: INFO: Pod pod-with-poststart-http-hook still exists
Feb 23 19:25:43.708: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 23 19:25:43.728: INFO: Pod pod-with-poststart-http-hook still exists
Feb 23 19:25:45.225: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 23 19:25:45.229: INFO: Pod pod-with-poststart-http-hook still exists
Feb 23 19:25:47.226: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 23 19:25:47.229: INFO: Pod pod-with-poststart-http-hook still exists
Feb 23 19:25:49.226: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 23 19:25:49.229: INFO: Pod pod-with-poststart-http-hook still exists
Feb 23 19:25:51.224: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 23 19:25:51.226: INFO: Pod pod-with-poststart-http-hook still exists
Feb 23 19:25:53.226: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 23 19:25:53.229: INFO: Pod pod-with-poststart-http-hook still exists
Feb 23 19:25:55.228: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 23 19:25:55.232: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:25:55.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7409" for this suite.
Feb 23 19:26:23.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:26:23.317: INFO: namespace container-lifecycle-hook-7409 deletion completed in 28.081253456s

• [SLOW TEST:46.169 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:26:23.318: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 19:26:23.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 create -f - --namespace=kubectl-9851'
Feb 23 19:26:23.482: INFO: stderr: ""
Feb 23 19:26:23.482: INFO: stdout: "replicationcontroller/redis-master created\n"
Feb 23 19:26:23.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 create -f - --namespace=kubectl-9851'
Feb 23 19:26:23.622: INFO: stderr: ""
Feb 23 19:26:23.623: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 23 19:26:24.626: INFO: Selector matched 1 pods for map[app:redis]
Feb 23 19:26:24.626: INFO: Found 0 / 1
Feb 23 19:26:25.625: INFO: Selector matched 1 pods for map[app:redis]
Feb 23 19:26:25.625: INFO: Found 1 / 1
Feb 23 19:26:25.625: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 23 19:26:25.628: INFO: Selector matched 1 pods for map[app:redis]
Feb 23 19:26:25.628: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 23 19:26:25.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 describe pod redis-master-qvqt7 --namespace=kubectl-9851'
Feb 23 19:26:25.701: INFO: stderr: ""
Feb 23 19:26:25.701: INFO: stdout: "Name:         redis-master-qvqt7\nNamespace:    kubectl-9851\nPriority:     0\nNode:         worker00/192.168.180.100\nStart Time:   Sun, 23 Feb 2020 19:26:23 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 10.200.131.162/32\nStatus:       Running\nIP:           10.200.131.162\nIPs:\n  IP:           10.200.131.162\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   containerd://0fdc83362d06b63ffa29f53c02bf43865367bac43b50e5f090443cc7f0ec045c\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sun, 23 Feb 2020 19:26:24 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-r5nc4 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-r5nc4:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-r5nc4\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From               Message\n  ----    ------     ----       ----               -------\n  Normal  Scheduled  <unknown>  default-scheduler  Successfully assigned kubectl-9851/redis-master-qvqt7 to worker00\n  Normal  Pulled     1s         kubelet, worker00  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    1s         kubelet, worker00  Created container redis-master\n  Normal  Started    1s         kubelet, worker00  Started container redis-master\n"
Feb 23 19:26:25.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 describe rc redis-master --namespace=kubectl-9851'
Feb 23 19:26:25.772: INFO: stderr: ""
Feb 23 19:26:25.772: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-9851\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-qvqt7\n"
Feb 23 19:26:25.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 describe service redis-master --namespace=kubectl-9851'
Feb 23 19:26:25.828: INFO: stderr: ""
Feb 23 19:26:25.828: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-9851\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.32.0.61\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.200.131.162:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 23 19:26:25.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 describe node worker00'
Feb 23 19:26:25.909: INFO: stderr: ""
Feb 23 19:26:25.909: INFO: stdout: "Name:               worker00\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=worker00\n                    kubernetes.io/os=linux\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"cephfs.csi.ceph.com\":\"worker00\",\"rbd.csi.ceph.com\":\"worker00\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.180.100/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.200.131.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sun, 23 Feb 2020 17:25:11 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sun, 23 Feb 2020 17:27:51 +0000   Sun, 23 Feb 2020 17:27:51 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Sun, 23 Feb 2020 19:25:53 +0000   Sun, 23 Feb 2020 17:25:11 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sun, 23 Feb 2020 19:25:53 +0000   Sun, 23 Feb 2020 17:25:11 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sun, 23 Feb 2020 19:25:53 +0000   Sun, 23 Feb 2020 17:25:11 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sun, 23 Feb 2020 19:25:53 +0000   Sun, 23 Feb 2020 17:27:41 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.180.100\n  Hostname:    worker00\nCapacity:\n cpu:                4\n ephemeral-storage:  64800356Ki\n hugepages-2Mi:      0\n memory:             4038884Ki\n pods:               110\nAllocatable:\n cpu:                4\n ephemeral-storage:  59720007991\n hugepages-2Mi:      0\n memory:             3936484Ki\n pods:               110\nSystem Info:\n Machine ID:                 f3ae96b90155485ab1c7eefacafa096b\n System UUID:                AD0FC6EE-BB1C-CA43-8086-08DECAE2FEFA\n Boot ID:                    c0781078-84ae-43f7-8245-7e8caf7201d4\n Kernel Version:             4.15.0-76-generic\n OS Image:                   Ubuntu 18.04.4 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  containerd://1.3.3\n Kubelet Version:            v1.16.7\n Kube-Proxy Version:         v1.16.7\nPodCIDR:                     10.200.1.0/24\nPodCIDRs:                    10.200.1.0/24\nNon-terminated Pods:         (18 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                dashboard-metrics-scraper-58475bc987-29h5g                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         25m\n  kube-system                etcd-worker00                                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         120m\n  kube-system                gobetween-worker00                                         100m (2%)     0 (0%)      0 (0%)           0 (0%)         120m\n  kube-system                kube-apiserver-worker00                                    250m (6%)     0 (0%)      0 (0%)           0 (0%)         120m\n  kube-system                kube-controller-manager-worker00                           200m (5%)     0 (0%)      0 (0%)           0 (0%)         120m\n  kube-system                kube-proxy-worker00                                        200m (5%)     0 (0%)      0 (0%)           0 (0%)         120m\n  kube-system                kube-scheduler-worker00                                    100m (2%)     0 (0%)      0 (0%)           0 (0%)         120m\n  kubectl-9851               redis-master-qvqt7                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  networking                 calico-node-77p2v                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         120m\n  networking                 metallb-speaker-7bnlk                                      100m (2%)     100m (2%)   100Mi (2%)       100Mi (2%)     25m\n  sonobuoy                   sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         110m\n  sonobuoy                   sonobuoy-e2e-job-382cd34a40b246b9                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         109m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-e1933034003941b1-4v2r4    0 (0%)        0 (0%)      0 (0%)           0 (0%)         109m\n  storage                    ceph-mds-worker00-6f479b4486-nrbn8                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         25m\n  storage                    ceph-mon-worker00-5cf654d469-jcn96                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         25m\n  storage                    ceph-osd-worker00-556546b495-nz47w                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         25m\n  storage                    csi-cephfsplugin-tbdkm                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         25m\n  storage                    csi-rbdplugin-9ghvp                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         25m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1200m (30%)  100m (2%)\n  memory             100Mi (2%)   100Mi (2%)\n  ephemeral-storage  0 (0%)       0 (0%)\nEvents:              <none>\n"
Feb 23 19:26:25.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 describe namespace kubectl-9851'
Feb 23 19:26:25.972: INFO: stderr: ""
Feb 23 19:26:25.972: INFO: stdout: "Name:         kubectl-9851\nLabels:       e2e-framework=kubectl\n              e2e-run=35e2b7b8-edf0-48b8-a136-1c3777f7c450\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:26:25.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9851" for this suite.
Feb 23 19:26:53.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:26:54.049: INFO: namespace kubectl-9851 deletion completed in 28.074466072s

• [SLOW TEST:30.732 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:26:54.049: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Feb 23 19:26:54.082: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Feb 23 19:26:54.761: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Feb 23 19:26:56.806: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 19:26:58.808: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 19:27:00.981: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 19:27:02.808: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 19:27:05.227: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 19:27:06.808: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 19:27:08.924: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 19:27:10.811: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 19:27:12.811: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718082814, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 19:27:15.535: INFO: Waited 716.642277ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:27:16.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-255" for this suite.
Feb 23 19:27:22.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:27:22.777: INFO: namespace aggregator-255 deletion completed in 6.157037348s

• [SLOW TEST:28.728 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:27:22.777: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:27:24.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6239" for this suite.
Feb 23 19:27:52.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:27:52.908: INFO: namespace containers-6239 deletion completed in 28.074501464s

• [SLOW TEST:30.131 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:27:52.909: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Feb 23 19:27:52.935: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 23 19:27:52.944: INFO: Waiting for terminating namespaces to be deleted...
Feb 23 19:27:52.946: INFO: 
Logging pods the kubelet thinks is on node worker00 before test
Feb 23 19:27:52.952: INFO: sonobuoy from sonobuoy started at 2020-02-23 17:36:25 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.952: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 23 19:27:52.952: INFO: ceph-mon-worker00-5cf654d469-jcn96 from storage started at 2020-02-23 19:01:10 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.952: INFO: 	Container ceph-mon ready: true, restart count 0
Feb 23 19:27:52.952: INFO: metallb-speaker-7bnlk from networking started at 2020-02-23 19:01:10 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.952: INFO: 	Container speaker ready: true, restart count 0
Feb 23 19:27:52.952: INFO: ceph-mds-worker00-6f479b4486-nrbn8 from storage started at 2020-02-23 19:01:41 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.952: INFO: 	Container ceph-mds ready: true, restart count 0
Feb 23 19:27:52.952: INFO: gobetween-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.952: INFO: 	Container gobetween ready: true, restart count 0
Feb 23 19:27:52.952: INFO: kube-proxy-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.952: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 19:27:52.952: INFO: kube-scheduler-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.952: INFO: 	Container kube-scheduler ready: true, restart count 2
Feb 23 19:27:52.952: INFO: calico-node-77p2v from networking started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.952: INFO: 	Container calico-node ready: true, restart count 0
Feb 23 19:27:52.952: INFO: ceph-osd-worker00-556546b495-nz47w from storage started at 2020-02-23 19:01:10 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.952: INFO: 	Container ceph-osd ready: true, restart count 2
Feb 23 19:27:52.952: INFO: dashboard-metrics-scraper-58475bc987-29h5g from kube-system started at 2020-02-23 19:01:10 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.952: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 23 19:27:52.952: INFO: csi-cephfsplugin-tbdkm from storage started at 2020-02-23 19:01:18 +0000 UTC (3 container statuses recorded)
Feb 23 19:27:52.952: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 19:27:52.952: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 19:27:52.952: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 19:27:52.952: INFO: etcd-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.952: INFO: 	Container etcd ready: true, restart count 0
Feb 23 19:27:52.952: INFO: kube-apiserver-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.952: INFO: 	Container kube-apiserver ready: true, restart count 1
Feb 23 19:27:52.952: INFO: sonobuoy-e2e-job-382cd34a40b246b9 from sonobuoy started at 2020-02-23 17:36:27 +0000 UTC (2 container statuses recorded)
Feb 23 19:27:52.952: INFO: 	Container e2e ready: true, restart count 0
Feb 23 19:27:52.952: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 19:27:52.952: INFO: sonobuoy-systemd-logs-daemon-set-e1933034003941b1-4v2r4 from sonobuoy started at 2020-02-23 17:36:27 +0000 UTC (2 container statuses recorded)
Feb 23 19:27:52.952: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 23 19:27:52.952: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 23 19:27:52.952: INFO: kube-controller-manager-worker00 from kube-system started at 2020-02-23 17:23:43 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.952: INFO: 	Container kube-controller-manager ready: true, restart count 1
Feb 23 19:27:52.952: INFO: csi-rbdplugin-9ghvp from storage started at 2020-02-23 19:01:16 +0000 UTC (3 container statuses recorded)
Feb 23 19:27:52.952: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 19:27:52.952: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 19:27:52.952: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 19:27:52.952: INFO: 
Logging pods the kubelet thinks is on node worker01 before test
Feb 23 19:27:52.967: INFO: csi-cephfsplugin-provisioner-6cd7596f75-tjxsr from storage started at 2020-02-23 17:26:43 +0000 UTC (4 container statuses recorded)
Feb 23 19:27:52.967: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 19:27:52.967: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 19:27:52.967: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 19:27:52.967: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 19:27:52.967: INFO: metallb-speaker-jpspl from networking started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.967: INFO: 	Container speaker ready: true, restart count 0
Feb 23 19:27:52.967: INFO: etcd-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.967: INFO: 	Container etcd ready: true, restart count 0
Feb 23 19:27:52.967: INFO: kube-controller-manager-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.967: INFO: 	Container kube-controller-manager ready: true, restart count 1
Feb 23 19:27:52.967: INFO: metallb-controller-b96bfbbf8-p9qvp from networking started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.967: INFO: 	Container controller ready: true, restart count 0
Feb 23 19:27:52.967: INFO: coredns-676544c7b9-ntt4b from kube-system started at 2020-02-23 17:26:46 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.967: INFO: 	Container coredns ready: true, restart count 0
Feb 23 19:27:52.967: INFO: csi-cephfsplugin-provisioner-6cd7596f75-b6s7q from storage started at 2020-02-23 17:26:43 +0000 UTC (4 container statuses recorded)
Feb 23 19:27:52.967: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 19:27:52.967: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 19:27:52.967: INFO: 	Container csi-provisioner ready: true, restart count 1
Feb 23 19:27:52.967: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 19:27:52.967: INFO: calico-kube-controllers-7cd585bcd-v4kk7 from networking started at 2020-02-23 17:26:46 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.967: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 23 19:27:52.967: INFO: kube-proxy-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.967: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 19:27:52.967: INFO: calico-node-lhcql from networking started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.967: INFO: 	Container calico-node ready: true, restart count 0
Feb 23 19:27:52.967: INFO: ceph-mds-worker01-7f5fdb58c6-m96hd from storage started at 2020-02-23 17:25:27 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.967: INFO: 	Container ceph-mds ready: true, restart count 2
Feb 23 19:27:52.967: INFO: csi-cephfsplugin-provisioner-6cd7596f75-n9kmr from storage started at 2020-02-23 17:26:43 +0000 UTC (4 container statuses recorded)
Feb 23 19:27:52.967: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 19:27:52.967: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 19:27:52.967: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 19:27:52.967: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 19:27:52.967: INFO: csi-rbdplugin-provisioner-7494f65674-gt7nb from storage started at 2020-02-23 17:26:43 +0000 UTC (5 container statuses recorded)
Feb 23 19:27:52.967: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 23 19:27:52.967: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 19:27:52.967: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 19:27:52.967: INFO: 	Container csi-snapshotter ready: true, restart count 0
Feb 23 19:27:52.967: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 19:27:52.967: INFO: csi-rbdplugin-provisioner-7494f65674-vnnkm from storage started at 2020-02-23 17:26:43 +0000 UTC (5 container statuses recorded)
Feb 23 19:27:52.967: INFO: 	Container csi-attacher ready: true, restart count 1
Feb 23 19:27:52.967: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 19:27:52.967: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 19:27:52.967: INFO: 	Container csi-snapshotter ready: true, restart count 0
Feb 23 19:27:52.967: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 19:27:52.967: INFO: kube-apiserver-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.967: INFO: 	Container kube-apiserver ready: true, restart count 0
Feb 23 19:27:52.967: INFO: ceph-osd-worker01-67947c799-8n7pr from storage started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.967: INFO: 	Container ceph-osd ready: true, restart count 0
Feb 23 19:27:52.967: INFO: sonobuoy-systemd-logs-daemon-set-e1933034003941b1-74kbd from sonobuoy started at 2020-02-23 17:36:27 +0000 UTC (2 container statuses recorded)
Feb 23 19:27:52.967: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 23 19:27:52.967: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 23 19:27:52.967: INFO: kube-scheduler-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.968: INFO: 	Container kube-scheduler ready: true, restart count 1
Feb 23 19:27:52.968: INFO: csi-rbdplugin-cfq94 from storage started at 2020-02-23 17:26:43 +0000 UTC (3 container statuses recorded)
Feb 23 19:27:52.968: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 19:27:52.968: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 19:27:52.968: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 19:27:52.968: INFO: csi-cephfsplugin-wj5f7 from storage started at 2020-02-23 17:26:43 +0000 UTC (3 container statuses recorded)
Feb 23 19:27:52.968: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 19:27:52.968: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 19:27:52.968: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 19:27:52.968: INFO: gobetween-worker01 from kube-system started at 2020-02-23 17:24:07 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.968: INFO: 	Container gobetween ready: true, restart count 0
Feb 23 19:27:52.968: INFO: ceph-mon-worker01-bdb694876-9x2bs from storage started at 2020-02-23 17:25:26 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.968: INFO: 	Container ceph-mon ready: true, restart count 0
Feb 23 19:27:52.968: INFO: ceph-setup-xccxd from storage started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.968: INFO: 	Container ceph ready: false, restart count 2
Feb 23 19:27:52.968: INFO: kubernetes-dashboard-f957cddcb-qq7vr from kube-system started at 2020-02-23 19:01:10 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.968: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 23 19:27:52.968: INFO: ceph-mgr-94b9dd996-vfc9x from storage started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.968: INFO: 	Container ceph-mgr ready: true, restart count 0
Feb 23 19:27:52.968: INFO: csi-rbdplugin-provisioner-7494f65674-zk9l6 from storage started at 2020-02-23 17:26:43 +0000 UTC (5 container statuses recorded)
Feb 23 19:27:52.968: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 23 19:27:52.968: INFO: 	Container csi-provisioner ready: true, restart count 1
Feb 23 19:27:52.968: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 19:27:52.968: INFO: 	Container csi-snapshotter ready: true, restart count 0
Feb 23 19:27:52.968: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 19:27:52.968: INFO: coredns-676544c7b9-pz66t from kube-system started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.968: INFO: 	Container coredns ready: true, restart count 0
Feb 23 19:27:52.968: INFO: ceph-rgw-57cd48f74c-4nwcs from storage started at 2020-02-23 17:26:43 +0000 UTC (1 container statuses recorded)
Feb 23 19:27:52.968: INFO: 	Container ceph-rgw ready: true, restart count 1
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node worker00
STEP: verifying the node has the label node worker01
Feb 23 19:27:52.998: INFO: Pod coredns-676544c7b9-ntt4b requesting resource cpu=0m on Node worker01
Feb 23 19:27:52.998: INFO: Pod coredns-676544c7b9-pz66t requesting resource cpu=0m on Node worker01
Feb 23 19:27:52.998: INFO: Pod dashboard-metrics-scraper-58475bc987-29h5g requesting resource cpu=0m on Node worker00
Feb 23 19:27:52.998: INFO: Pod etcd-worker00 requesting resource cpu=0m on Node worker00
Feb 23 19:27:52.998: INFO: Pod etcd-worker01 requesting resource cpu=0m on Node worker01
Feb 23 19:27:52.998: INFO: Pod gobetween-worker00 requesting resource cpu=100m on Node worker00
Feb 23 19:27:52.998: INFO: Pod gobetween-worker01 requesting resource cpu=100m on Node worker01
Feb 23 19:27:52.998: INFO: Pod kube-apiserver-worker00 requesting resource cpu=250m on Node worker00
Feb 23 19:27:52.998: INFO: Pod kube-apiserver-worker01 requesting resource cpu=250m on Node worker01
Feb 23 19:27:52.998: INFO: Pod kube-controller-manager-worker00 requesting resource cpu=200m on Node worker00
Feb 23 19:27:52.998: INFO: Pod kube-controller-manager-worker01 requesting resource cpu=200m on Node worker01
Feb 23 19:27:52.998: INFO: Pod kube-proxy-worker00 requesting resource cpu=200m on Node worker00
Feb 23 19:27:52.998: INFO: Pod kube-proxy-worker01 requesting resource cpu=200m on Node worker01
Feb 23 19:27:52.998: INFO: Pod kube-scheduler-worker00 requesting resource cpu=100m on Node worker00
Feb 23 19:27:52.998: INFO: Pod kube-scheduler-worker01 requesting resource cpu=100m on Node worker01
Feb 23 19:27:52.998: INFO: Pod kubernetes-dashboard-f957cddcb-qq7vr requesting resource cpu=100m on Node worker01
Feb 23 19:27:52.998: INFO: Pod calico-kube-controllers-7cd585bcd-v4kk7 requesting resource cpu=0m on Node worker01
Feb 23 19:27:52.998: INFO: Pod calico-node-77p2v requesting resource cpu=250m on Node worker00
Feb 23 19:27:52.998: INFO: Pod calico-node-lhcql requesting resource cpu=250m on Node worker01
Feb 23 19:27:52.998: INFO: Pod metallb-controller-b96bfbbf8-p9qvp requesting resource cpu=100m on Node worker01
Feb 23 19:27:52.998: INFO: Pod metallb-speaker-7bnlk requesting resource cpu=100m on Node worker00
Feb 23 19:27:52.998: INFO: Pod metallb-speaker-jpspl requesting resource cpu=100m on Node worker01
Feb 23 19:27:52.998: INFO: Pod sonobuoy requesting resource cpu=0m on Node worker00
Feb 23 19:27:52.998: INFO: Pod sonobuoy-e2e-job-382cd34a40b246b9 requesting resource cpu=0m on Node worker00
Feb 23 19:27:52.998: INFO: Pod sonobuoy-systemd-logs-daemon-set-e1933034003941b1-4v2r4 requesting resource cpu=0m on Node worker00
Feb 23 19:27:52.998: INFO: Pod sonobuoy-systemd-logs-daemon-set-e1933034003941b1-74kbd requesting resource cpu=0m on Node worker01
Feb 23 19:27:52.998: INFO: Pod ceph-mds-worker00-6f479b4486-nrbn8 requesting resource cpu=0m on Node worker00
Feb 23 19:27:52.998: INFO: Pod ceph-mds-worker01-7f5fdb58c6-m96hd requesting resource cpu=0m on Node worker01
Feb 23 19:27:52.998: INFO: Pod ceph-mgr-94b9dd996-vfc9x requesting resource cpu=0m on Node worker01
Feb 23 19:27:52.998: INFO: Pod ceph-mon-worker00-5cf654d469-jcn96 requesting resource cpu=0m on Node worker00
Feb 23 19:27:52.998: INFO: Pod ceph-mon-worker01-bdb694876-9x2bs requesting resource cpu=0m on Node worker01
Feb 23 19:27:52.998: INFO: Pod ceph-osd-worker00-556546b495-nz47w requesting resource cpu=0m on Node worker00
Feb 23 19:27:52.998: INFO: Pod ceph-osd-worker01-67947c799-8n7pr requesting resource cpu=0m on Node worker01
Feb 23 19:27:52.998: INFO: Pod ceph-rgw-57cd48f74c-4nwcs requesting resource cpu=0m on Node worker01
Feb 23 19:27:52.998: INFO: Pod csi-cephfsplugin-provisioner-6cd7596f75-b6s7q requesting resource cpu=0m on Node worker01
Feb 23 19:27:52.998: INFO: Pod csi-cephfsplugin-provisioner-6cd7596f75-n9kmr requesting resource cpu=0m on Node worker01
Feb 23 19:27:52.998: INFO: Pod csi-cephfsplugin-provisioner-6cd7596f75-tjxsr requesting resource cpu=0m on Node worker01
Feb 23 19:27:52.998: INFO: Pod csi-cephfsplugin-tbdkm requesting resource cpu=0m on Node worker00
Feb 23 19:27:52.998: INFO: Pod csi-cephfsplugin-wj5f7 requesting resource cpu=0m on Node worker01
Feb 23 19:27:52.998: INFO: Pod csi-rbdplugin-9ghvp requesting resource cpu=0m on Node worker00
Feb 23 19:27:52.999: INFO: Pod csi-rbdplugin-cfq94 requesting resource cpu=0m on Node worker01
Feb 23 19:27:52.999: INFO: Pod csi-rbdplugin-provisioner-7494f65674-gt7nb requesting resource cpu=0m on Node worker01
Feb 23 19:27:52.999: INFO: Pod csi-rbdplugin-provisioner-7494f65674-vnnkm requesting resource cpu=0m on Node worker01
Feb 23 19:27:52.999: INFO: Pod csi-rbdplugin-provisioner-7494f65674-zk9l6 requesting resource cpu=0m on Node worker01
STEP: Starting Pods to consume most of the cluster CPU.
Feb 23 19:27:52.999: INFO: Creating a pod which consumes cpu=1960m on Node worker00
Feb 23 19:27:53.006: INFO: Creating a pod which consumes cpu=1820m on Node worker01
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c49b833-2fe8-47fd-966d-a767b98117b2.15f61eb2f39f8914], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7094/filler-pod-8c49b833-2fe8-47fd-966d-a767b98117b2 to worker00]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c49b833-2fe8-47fd-966d-a767b98117b2.15f61eb329ed9bf0], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c49b833-2fe8-47fd-966d-a767b98117b2.15f61eb32c88d92d], Reason = [Created], Message = [Created container filler-pod-8c49b833-2fe8-47fd-966d-a767b98117b2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c49b833-2fe8-47fd-966d-a767b98117b2.15f61eb352a5859c], Reason = [Started], Message = [Started container filler-pod-8c49b833-2fe8-47fd-966d-a767b98117b2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e5040b4b-d923-4a6b-8088-0585ed0fa066.15f61eb2f48c050e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7094/filler-pod-e5040b4b-d923-4a6b-8088-0585ed0fa066 to worker01]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e5040b4b-d923-4a6b-8088-0585ed0fa066.15f61eb31988533c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e5040b4b-d923-4a6b-8088-0585ed0fa066.15f61eb31ce1f688], Reason = [Created], Message = [Created container filler-pod-e5040b4b-d923-4a6b-8088-0585ed0fa066]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e5040b4b-d923-4a6b-8088-0585ed0fa066.15f61eb3275e16ba], Reason = [Started], Message = [Started container filler-pod-e5040b4b-d923-4a6b-8088-0585ed0fa066]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15f61eb3e402dc30], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15f61eb3e4a43d66], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node worker00
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker01
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:27:58.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7094" for this suite.
Feb 23 19:28:04.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:28:04.182: INFO: namespace sched-pred-7094 deletion completed in 6.074791162s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:11.273 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:28:04.183: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-8464/configmap-test-29411a57-395f-4c1e-aaa3-da5ce8b74364
STEP: Creating a pod to test consume configMaps
Feb 23 19:28:04.226: INFO: Waiting up to 5m0s for pod "pod-configmaps-1d49ce9d-77e2-49f8-97ac-8b13c3012efe" in namespace "configmap-8464" to be "success or failure"
Feb 23 19:28:04.229: INFO: Pod "pod-configmaps-1d49ce9d-77e2-49f8-97ac-8b13c3012efe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.030496ms
Feb 23 19:28:06.233: INFO: Pod "pod-configmaps-1d49ce9d-77e2-49f8-97ac-8b13c3012efe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006604499s
STEP: Saw pod success
Feb 23 19:28:06.233: INFO: Pod "pod-configmaps-1d49ce9d-77e2-49f8-97ac-8b13c3012efe" satisfied condition "success or failure"
Feb 23 19:28:06.235: INFO: Trying to get logs from node worker00 pod pod-configmaps-1d49ce9d-77e2-49f8-97ac-8b13c3012efe container env-test: <nil>
STEP: delete the pod
Feb 23 19:28:06.249: INFO: Waiting for pod pod-configmaps-1d49ce9d-77e2-49f8-97ac-8b13c3012efe to disappear
Feb 23 19:28:06.253: INFO: Pod pod-configmaps-1d49ce9d-77e2-49f8-97ac-8b13c3012efe no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:28:06.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8464" for this suite.
Feb 23 19:28:12.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:28:12.329: INFO: namespace configmap-8464 deletion completed in 6.07297587s

• [SLOW TEST:8.147 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:28:12.331: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-26efd2ec-cad7-4654-9b99-c647b911819e
STEP: Creating configMap with name cm-test-opt-upd-45520ca1-eb75-43f9-8fb6-2b45a09ef779
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-26efd2ec-cad7-4654-9b99-c647b911819e
STEP: Updating configmap cm-test-opt-upd-45520ca1-eb75-43f9-8fb6-2b45a09ef779
STEP: Creating configMap with name cm-test-opt-create-12a1d336-fc65-4093-83cc-3d72f32753d1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:29:33.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-835" for this suite.
Feb 23 19:29:44.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:29:44.267: INFO: namespace configmap-835 deletion completed in 10.58748228s

• [SLOW TEST:91.936 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:29:44.267: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
Feb 23 19:29:44.295: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 23 19:30:44.312: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 19:30:44.315: INFO: Starting informer...
STEP: Starting pods...
Feb 23 19:30:44.530: INFO: Pod1 is running on worker00. Tainting Node
Feb 23 19:30:46.752: INFO: Pod2 is running on worker00. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Feb 23 19:30:53.794: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Feb 23 19:31:13.666: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:31:13.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-6065" for this suite.
Feb 23 19:31:19.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:31:19.881: INFO: namespace taint-multiple-pods-6065 deletion completed in 6.178169474s

• [SLOW TEST:95.614 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:31:19.882: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 23 19:31:19.919: INFO: Waiting up to 5m0s for pod "pod-2d8884dd-de81-41c9-983b-e9eceaac52bd" in namespace "emptydir-764" to be "success or failure"
Feb 23 19:31:19.926: INFO: Pod "pod-2d8884dd-de81-41c9-983b-e9eceaac52bd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.167223ms
Feb 23 19:31:21.929: INFO: Pod "pod-2d8884dd-de81-41c9-983b-e9eceaac52bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009603476s
Feb 23 19:31:23.932: INFO: Pod "pod-2d8884dd-de81-41c9-983b-e9eceaac52bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012050577s
STEP: Saw pod success
Feb 23 19:31:23.932: INFO: Pod "pod-2d8884dd-de81-41c9-983b-e9eceaac52bd" satisfied condition "success or failure"
Feb 23 19:31:23.935: INFO: Trying to get logs from node worker00 pod pod-2d8884dd-de81-41c9-983b-e9eceaac52bd container test-container: <nil>
STEP: delete the pod
Feb 23 19:31:23.954: INFO: Waiting for pod pod-2d8884dd-de81-41c9-983b-e9eceaac52bd to disappear
Feb 23 19:31:23.957: INFO: Pod pod-2d8884dd-de81-41c9-983b-e9eceaac52bd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:31:23.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-764" for this suite.
Feb 23 19:31:29.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:31:30.031: INFO: namespace emptydir-764 deletion completed in 6.070422288s

• [SLOW TEST:10.149 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:31:30.031: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 23 19:31:30.065: INFO: Waiting up to 5m0s for pod "downwardapi-volume-81e39a57-5f65-4f26-9499-6a85ef0ef0dc" in namespace "downward-api-4414" to be "success or failure"
Feb 23 19:31:30.067: INFO: Pod "downwardapi-volume-81e39a57-5f65-4f26-9499-6a85ef0ef0dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.318316ms
Feb 23 19:31:32.070: INFO: Pod "downwardapi-volume-81e39a57-5f65-4f26-9499-6a85ef0ef0dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005357462s
STEP: Saw pod success
Feb 23 19:31:32.070: INFO: Pod "downwardapi-volume-81e39a57-5f65-4f26-9499-6a85ef0ef0dc" satisfied condition "success or failure"
Feb 23 19:31:32.073: INFO: Trying to get logs from node worker00 pod downwardapi-volume-81e39a57-5f65-4f26-9499-6a85ef0ef0dc container client-container: <nil>
STEP: delete the pod
Feb 23 19:31:32.088: INFO: Waiting for pod downwardapi-volume-81e39a57-5f65-4f26-9499-6a85ef0ef0dc to disappear
Feb 23 19:31:32.092: INFO: Pod downwardapi-volume-81e39a57-5f65-4f26-9499-6a85ef0ef0dc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:31:32.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4414" for this suite.
Feb 23 19:31:38.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:31:38.171: INFO: namespace downward-api-4414 deletion completed in 6.076302187s

• [SLOW TEST:8.140 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:31:38.171: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 23 19:31:38.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-9536'
Feb 23 19:31:40.083: INFO: stderr: ""
Feb 23 19:31:40.083: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Feb 23 19:31:40.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 delete pods e2e-test-httpd-pod --namespace=kubectl-9536'
Feb 23 19:31:53.466: INFO: stderr: ""
Feb 23 19:31:53.466: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:31:53.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9536" for this suite.
Feb 23 19:31:59.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:31:59.574: INFO: namespace kubectl-9536 deletion completed in 6.104679397s

• [SLOW TEST:21.403 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:31:59.576: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 23 19:32:02.738: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:32:02.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4795" for this suite.
Feb 23 19:32:08.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:32:08.828: INFO: namespace container-runtime-4795 deletion completed in 6.072818415s

• [SLOW TEST:9.252 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:32:08.830: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb 23 19:32:09.271: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Feb 23 19:32:11.278: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718083129, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718083129, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718083129, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718083129, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 19:32:14.720: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 19:32:14.723: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:32:20.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-334" for this suite.
Feb 23 19:32:27.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:32:27.181: INFO: namespace crd-webhook-334 deletion completed in 6.090250163s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:18.367 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:32:27.197: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 23 19:32:27.229: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 23 19:32:27.239: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 23 19:32:32.244: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 23 19:32:32.244: INFO: Creating deployment "test-rolling-update-deployment"
Feb 23 19:32:32.248: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 23 19:32:32.254: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 23 19:32:34.650: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 23 19:32:34.657: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718083152, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718083152, loc:(*time.Location)(0x788c6e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718083152, loc:(*time.Location)(0x788c6e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718083152, loc:(*time.Location)(0x788c6e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 19:32:36.661: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Feb 23 19:32:36.672: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9779 /apis/apps/v1/namespaces/deployment-9779/deployments/test-rolling-update-deployment 8ce39ca8-fed2-4b82-8202-44d1d0ed1aad 34882 1 2020-02-23 19:32:32 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0049960b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-02-23 19:32:32 +0000 UTC,LastTransitionTime:2020-02-23 19:32:32 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2020-02-23 19:32:34 +0000 UTC,LastTransitionTime:2020-02-23 19:32:32 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 23 19:32:36.675: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-9779 /apis/apps/v1/namespaces/deployment-9779/replicasets/test-rolling-update-deployment-55d946486 f6d70c4d-8297-4ec2-bcf8-bb7e45a357ab 34871 1 2020-02-23 19:32:32 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 8ce39ca8-fed2-4b82-8202-44d1d0ed1aad 0xc0049965a0 0xc0049965a1}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004996608 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 23 19:32:36.675: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 23 19:32:36.675: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9779 /apis/apps/v1/namespaces/deployment-9779/replicasets/test-rolling-update-controller 70577cd0-f844-4e0a-a6af-fbbeb67d84ac 34881 2 2020-02-23 19:32:27 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 8ce39ca8-fed2-4b82-8202-44d1d0ed1aad 0xc0049964d7 0xc0049964d8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004996538 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 23 19:32:36.678: INFO: Pod "test-rolling-update-deployment-55d946486-vzw5r" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-vzw5r test-rolling-update-deployment-55d946486- deployment-9779 /api/v1/namespaces/deployment-9779/pods/test-rolling-update-deployment-55d946486-vzw5r 198d11aa-6eb9-4414-bc6a-bac72c1763d9 34870 0 2020-02-23 19:32:32 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[cni.projectcalico.org/podIP:10.200.131.168/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 f6d70c4d-8297-4ec2-bcf8-bb7e45a357ab 0xc004996a70 0xc004996a71}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rbrm2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rbrm2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rbrm2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 19:32:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 19:32:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 19:32:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 19:32:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:10.200.131.168,StartTime:2020-02-23 19:32:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 19:32:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:containerd://72b3637ca419041442a2b1884355aa2f5533d6792aa5899decfbfc843dd810a2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.131.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:32:36.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9779" for this suite.
Feb 23 19:32:42.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:32:42.759: INFO: namespace deployment-9779 deletion completed in 6.07874589s

• [SLOW TEST:15.563 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:32:42.759: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Feb 23 19:32:42.797: INFO: Waiting up to 5m0s for pod "downward-api-41a8e0b9-fc71-45ef-85f2-d2b45db87390" in namespace "downward-api-6093" to be "success or failure"
Feb 23 19:32:42.804: INFO: Pod "downward-api-41a8e0b9-fc71-45ef-85f2-d2b45db87390": Phase="Pending", Reason="", readiness=false. Elapsed: 6.299794ms
Feb 23 19:32:44.807: INFO: Pod "downward-api-41a8e0b9-fc71-45ef-85f2-d2b45db87390": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009929678s
STEP: Saw pod success
Feb 23 19:32:44.807: INFO: Pod "downward-api-41a8e0b9-fc71-45ef-85f2-d2b45db87390" satisfied condition "success or failure"
Feb 23 19:32:44.809: INFO: Trying to get logs from node worker00 pod downward-api-41a8e0b9-fc71-45ef-85f2-d2b45db87390 container dapi-container: <nil>
STEP: delete the pod
Feb 23 19:32:44.826: INFO: Waiting for pod downward-api-41a8e0b9-fc71-45ef-85f2-d2b45db87390 to disappear
Feb 23 19:32:44.828: INFO: Pod downward-api-41a8e0b9-fc71-45ef-85f2-d2b45db87390 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:32:44.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6093" for this suite.
Feb 23 19:32:50.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:32:50.904: INFO: namespace downward-api-6093 deletion completed in 6.073493228s

• [SLOW TEST:8.145 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:32:50.905: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Feb 23 19:32:53.449: INFO: Successfully updated pod "adopt-release-bjjn8"
STEP: Checking that the Job readopts the Pod
Feb 23 19:32:53.449: INFO: Waiting up to 15m0s for pod "adopt-release-bjjn8" in namespace "job-4239" to be "adopted"
Feb 23 19:32:53.454: INFO: Pod "adopt-release-bjjn8": Phase="Running", Reason="", readiness=true. Elapsed: 4.79531ms
Feb 23 19:32:55.458: INFO: Pod "adopt-release-bjjn8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008818537s
Feb 23 19:32:55.458: INFO: Pod "adopt-release-bjjn8" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Feb 23 19:32:55.965: INFO: Successfully updated pod "adopt-release-bjjn8"
STEP: Checking that the Job releases the Pod
Feb 23 19:32:55.965: INFO: Waiting up to 15m0s for pod "adopt-release-bjjn8" in namespace "job-4239" to be "released"
Feb 23 19:32:55.970: INFO: Pod "adopt-release-bjjn8": Phase="Running", Reason="", readiness=true. Elapsed: 4.406719ms
Feb 23 19:32:58.455: INFO: Pod "adopt-release-bjjn8": Phase="Running", Reason="", readiness=true. Elapsed: 2.489514909s
Feb 23 19:32:58.455: INFO: Pod "adopt-release-bjjn8" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:32:58.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4239" for this suite.
Feb 23 19:33:44.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:33:44.537: INFO: namespace job-4239 deletion completed in 46.078682949s

• [SLOW TEST:53.633 seconds]
[sig-apps] Job
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:33:44.537: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-722992f5-5d60-417c-b3ed-01751a5bc9f4
STEP: Creating secret with name s-test-opt-upd-d0bfb0a4-a48e-4444-8d25-fc076d972a66
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-722992f5-5d60-417c-b3ed-01751a5bc9f4
STEP: Updating secret s-test-opt-upd-d0bfb0a4-a48e-4444-8d25-fc076d972a66
STEP: Creating secret with name s-test-opt-create-9f5fef80-d4af-4715-8299-b377ec0539a2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:33:50.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2847" for this suite.
Feb 23 19:34:02.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:34:02.823: INFO: namespace secrets-2847 deletion completed in 12.070824057s

• [SLOW TEST:18.285 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:34:02.824: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-8b3d2c87-2242-4dab-9eae-971eb04fb856
STEP: Creating a pod to test consume secrets
Feb 23 19:34:02.863: INFO: Waiting up to 5m0s for pod "pod-secrets-9ae53f63-967d-4308-8358-279235cd062d" in namespace "secrets-4914" to be "success or failure"
Feb 23 19:34:02.868: INFO: Pod "pod-secrets-9ae53f63-967d-4308-8358-279235cd062d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.291225ms
Feb 23 19:34:04.870: INFO: Pod "pod-secrets-9ae53f63-967d-4308-8358-279235cd062d": Phase="Running", Reason="", readiness=true. Elapsed: 2.006913611s
Feb 23 19:34:06.873: INFO: Pod "pod-secrets-9ae53f63-967d-4308-8358-279235cd062d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009885092s
STEP: Saw pod success
Feb 23 19:34:06.873: INFO: Pod "pod-secrets-9ae53f63-967d-4308-8358-279235cd062d" satisfied condition "success or failure"
Feb 23 19:34:06.876: INFO: Trying to get logs from node worker00 pod pod-secrets-9ae53f63-967d-4308-8358-279235cd062d container secret-env-test: <nil>
STEP: delete the pod
Feb 23 19:34:06.897: INFO: Waiting for pod pod-secrets-9ae53f63-967d-4308-8358-279235cd062d to disappear
Feb 23 19:34:06.900: INFO: Pod pod-secrets-9ae53f63-967d-4308-8358-279235cd062d no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:34:06.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4914" for this suite.
Feb 23 19:34:12.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:34:13.503: INFO: namespace secrets-4914 deletion completed in 6.600608385s

• [SLOW TEST:10.680 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:34:13.504: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 23 19:34:13.540: INFO: Waiting up to 5m0s for pod "downwardapi-volume-254d7663-0fd2-44f0-ae4f-051e7c265108" in namespace "downward-api-4266" to be "success or failure"
Feb 23 19:34:13.543: INFO: Pod "downwardapi-volume-254d7663-0fd2-44f0-ae4f-051e7c265108": Phase="Pending", Reason="", readiness=false. Elapsed: 2.884038ms
Feb 23 19:34:15.546: INFO: Pod "downwardapi-volume-254d7663-0fd2-44f0-ae4f-051e7c265108": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00627907s
STEP: Saw pod success
Feb 23 19:34:15.546: INFO: Pod "downwardapi-volume-254d7663-0fd2-44f0-ae4f-051e7c265108" satisfied condition "success or failure"
Feb 23 19:34:15.549: INFO: Trying to get logs from node worker00 pod downwardapi-volume-254d7663-0fd2-44f0-ae4f-051e7c265108 container client-container: <nil>
STEP: delete the pod
Feb 23 19:34:15.565: INFO: Waiting for pod downwardapi-volume-254d7663-0fd2-44f0-ae4f-051e7c265108 to disappear
Feb 23 19:34:15.568: INFO: Pod downwardapi-volume-254d7663-0fd2-44f0-ae4f-051e7c265108 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:34:15.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4266" for this suite.
Feb 23 19:34:21.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:34:21.642: INFO: namespace downward-api-4266 deletion completed in 6.070717222s

• [SLOW TEST:8.138 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:34:21.642: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Feb 23 19:34:21.679: INFO: Waiting up to 5m0s for pod "var-expansion-172b0d97-8d03-4df5-9a1e-b31e435a9a40" in namespace "var-expansion-4602" to be "success or failure"
Feb 23 19:34:21.683: INFO: Pod "var-expansion-172b0d97-8d03-4df5-9a1e-b31e435a9a40": Phase="Pending", Reason="", readiness=false. Elapsed: 3.551522ms
Feb 23 19:34:23.688: INFO: Pod "var-expansion-172b0d97-8d03-4df5-9a1e-b31e435a9a40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008293569s
STEP: Saw pod success
Feb 23 19:34:23.688: INFO: Pod "var-expansion-172b0d97-8d03-4df5-9a1e-b31e435a9a40" satisfied condition "success or failure"
Feb 23 19:34:23.690: INFO: Trying to get logs from node worker00 pod var-expansion-172b0d97-8d03-4df5-9a1e-b31e435a9a40 container dapi-container: <nil>
STEP: delete the pod
Feb 23 19:34:23.706: INFO: Waiting for pod var-expansion-172b0d97-8d03-4df5-9a1e-b31e435a9a40 to disappear
Feb 23 19:34:23.709: INFO: Pod var-expansion-172b0d97-8d03-4df5-9a1e-b31e435a9a40 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:34:23.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4602" for this suite.
Feb 23 19:34:29.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:34:29.788: INFO: namespace var-expansion-4602 deletion completed in 6.075426026s

• [SLOW TEST:8.146 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:34:29.789: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 23 19:34:29.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-294'
Feb 23 19:34:29.875: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 23 19:34:29.875: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Feb 23 19:34:31.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 delete deployment e2e-test-httpd-deployment --namespace=kubectl-294'
Feb 23 19:34:31.957: INFO: stderr: ""
Feb 23 19:34:31.957: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:34:31.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-294" for this suite.
Feb 23 19:34:59.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:35:00.042: INFO: namespace kubectl-294 deletion completed in 28.082386069s

• [SLOW TEST:30.253 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:35:00.042: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Feb 23 19:35:00.069: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:35:20.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3518" for this suite.
Feb 23 19:35:26.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:35:26.330: INFO: namespace crd-publish-openapi-3518 deletion completed in 6.076053312s

• [SLOW TEST:26.288 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:35:26.331: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 23 19:35:28.523: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:35:28.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5043" for this suite.
Feb 23 19:35:34.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:35:34.632: INFO: namespace container-runtime-5043 deletion completed in 6.087669322s

• [SLOW TEST:8.302 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:35:34.635: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8977.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8977.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8977.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8977.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 23 19:35:36.699: INFO: DNS probes using dns-test-e5d5f69e-c85e-42ce-9bdd-5b25b5187f1f succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8977.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8977.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8977.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8977.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 23 19:35:40.770: INFO: File wheezy_udp@dns-test-service-3.dns-8977.svc.cluster.local from pod  dns-8977/dns-test-c6b333f8-7d15-4b0f-a031-89440147767f contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 23 19:35:40.777: INFO: File jessie_udp@dns-test-service-3.dns-8977.svc.cluster.local from pod  dns-8977/dns-test-c6b333f8-7d15-4b0f-a031-89440147767f contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 23 19:35:40.777: INFO: Lookups using dns-8977/dns-test-c6b333f8-7d15-4b0f-a031-89440147767f failed for: [wheezy_udp@dns-test-service-3.dns-8977.svc.cluster.local jessie_udp@dns-test-service-3.dns-8977.svc.cluster.local]

Feb 23 19:35:45.783: INFO: File wheezy_udp@dns-test-service-3.dns-8977.svc.cluster.local from pod  dns-8977/dns-test-c6b333f8-7d15-4b0f-a031-89440147767f contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 23 19:35:45.786: INFO: File jessie_udp@dns-test-service-3.dns-8977.svc.cluster.local from pod  dns-8977/dns-test-c6b333f8-7d15-4b0f-a031-89440147767f contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 23 19:35:45.786: INFO: Lookups using dns-8977/dns-test-c6b333f8-7d15-4b0f-a031-89440147767f failed for: [wheezy_udp@dns-test-service-3.dns-8977.svc.cluster.local jessie_udp@dns-test-service-3.dns-8977.svc.cluster.local]

Feb 23 19:35:50.781: INFO: File wheezy_udp@dns-test-service-3.dns-8977.svc.cluster.local from pod  dns-8977/dns-test-c6b333f8-7d15-4b0f-a031-89440147767f contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 23 19:35:50.791: INFO: File jessie_udp@dns-test-service-3.dns-8977.svc.cluster.local from pod  dns-8977/dns-test-c6b333f8-7d15-4b0f-a031-89440147767f contains '' instead of 'bar.example.com.'
Feb 23 19:35:50.791: INFO: Lookups using dns-8977/dns-test-c6b333f8-7d15-4b0f-a031-89440147767f failed for: [wheezy_udp@dns-test-service-3.dns-8977.svc.cluster.local jessie_udp@dns-test-service-3.dns-8977.svc.cluster.local]

Feb 23 19:35:55.781: INFO: File wheezy_udp@dns-test-service-3.dns-8977.svc.cluster.local from pod  dns-8977/dns-test-c6b333f8-7d15-4b0f-a031-89440147767f contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 23 19:35:55.784: INFO: File jessie_udp@dns-test-service-3.dns-8977.svc.cluster.local from pod  dns-8977/dns-test-c6b333f8-7d15-4b0f-a031-89440147767f contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 23 19:35:55.784: INFO: Lookups using dns-8977/dns-test-c6b333f8-7d15-4b0f-a031-89440147767f failed for: [wheezy_udp@dns-test-service-3.dns-8977.svc.cluster.local jessie_udp@dns-test-service-3.dns-8977.svc.cluster.local]

Feb 23 19:36:00.789: INFO: File wheezy_udp@dns-test-service-3.dns-8977.svc.cluster.local from pod  dns-8977/dns-test-c6b333f8-7d15-4b0f-a031-89440147767f contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 23 19:36:00.793: INFO: File jessie_udp@dns-test-service-3.dns-8977.svc.cluster.local from pod  dns-8977/dns-test-c6b333f8-7d15-4b0f-a031-89440147767f contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 23 19:36:00.793: INFO: Lookups using dns-8977/dns-test-c6b333f8-7d15-4b0f-a031-89440147767f failed for: [wheezy_udp@dns-test-service-3.dns-8977.svc.cluster.local jessie_udp@dns-test-service-3.dns-8977.svc.cluster.local]

Feb 23 19:36:05.780: INFO: File wheezy_udp@dns-test-service-3.dns-8977.svc.cluster.local from pod  dns-8977/dns-test-c6b333f8-7d15-4b0f-a031-89440147767f contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 23 19:36:05.783: INFO: File jessie_udp@dns-test-service-3.dns-8977.svc.cluster.local from pod  dns-8977/dns-test-c6b333f8-7d15-4b0f-a031-89440147767f contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 23 19:36:05.784: INFO: Lookups using dns-8977/dns-test-c6b333f8-7d15-4b0f-a031-89440147767f failed for: [wheezy_udp@dns-test-service-3.dns-8977.svc.cluster.local jessie_udp@dns-test-service-3.dns-8977.svc.cluster.local]

Feb 23 19:36:10.783: INFO: DNS probes using dns-test-c6b333f8-7d15-4b0f-a031-89440147767f succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8977.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8977.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8977.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8977.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 23 19:36:12.853: INFO: DNS probes using dns-test-cf547c88-afbb-4e1a-aa15-b40c1c851a26 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:36:12.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8977" for this suite.
Feb 23 19:36:18.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:36:18.987: INFO: namespace dns-8977 deletion completed in 6.089469098s

• [SLOW TEST:44.352 seconds]
[sig-network] DNS
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:36:18.989: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-k2sg
STEP: Creating a pod to test atomic-volume-subpath
Feb 23 19:36:19.033: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-k2sg" in namespace "subpath-9468" to be "success or failure"
Feb 23 19:36:19.037: INFO: Pod "pod-subpath-test-configmap-k2sg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.18411ms
Feb 23 19:36:21.042: INFO: Pod "pod-subpath-test-configmap-k2sg": Phase="Running", Reason="", readiness=true. Elapsed: 2.00913252s
Feb 23 19:36:23.045: INFO: Pod "pod-subpath-test-configmap-k2sg": Phase="Running", Reason="", readiness=true. Elapsed: 4.012082365s
Feb 23 19:36:25.050: INFO: Pod "pod-subpath-test-configmap-k2sg": Phase="Running", Reason="", readiness=true. Elapsed: 6.017007622s
Feb 23 19:36:27.053: INFO: Pod "pod-subpath-test-configmap-k2sg": Phase="Running", Reason="", readiness=true. Elapsed: 8.019787834s
Feb 23 19:36:29.057: INFO: Pod "pod-subpath-test-configmap-k2sg": Phase="Running", Reason="", readiness=true. Elapsed: 10.023252348s
Feb 23 19:36:31.061: INFO: Pod "pod-subpath-test-configmap-k2sg": Phase="Running", Reason="", readiness=true. Elapsed: 12.027727113s
Feb 23 19:36:33.064: INFO: Pod "pod-subpath-test-configmap-k2sg": Phase="Running", Reason="", readiness=true. Elapsed: 14.030397544s
Feb 23 19:36:35.066: INFO: Pod "pod-subpath-test-configmap-k2sg": Phase="Running", Reason="", readiness=true. Elapsed: 16.033059104s
Feb 23 19:36:37.069: INFO: Pod "pod-subpath-test-configmap-k2sg": Phase="Running", Reason="", readiness=true. Elapsed: 18.0356768s
Feb 23 19:36:39.072: INFO: Pod "pod-subpath-test-configmap-k2sg": Phase="Running", Reason="", readiness=true. Elapsed: 20.038661909s
Feb 23 19:36:41.075: INFO: Pod "pod-subpath-test-configmap-k2sg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.041661706s
STEP: Saw pod success
Feb 23 19:36:41.075: INFO: Pod "pod-subpath-test-configmap-k2sg" satisfied condition "success or failure"
Feb 23 19:36:41.078: INFO: Trying to get logs from node worker00 pod pod-subpath-test-configmap-k2sg container test-container-subpath-configmap-k2sg: <nil>
STEP: delete the pod
Feb 23 19:36:41.101: INFO: Waiting for pod pod-subpath-test-configmap-k2sg to disappear
Feb 23 19:36:41.104: INFO: Pod pod-subpath-test-configmap-k2sg no longer exists
STEP: Deleting pod pod-subpath-test-configmap-k2sg
Feb 23 19:36:41.104: INFO: Deleting pod "pod-subpath-test-configmap-k2sg" in namespace "subpath-9468"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:36:41.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9468" for this suite.
Feb 23 19:36:47.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:36:47.189: INFO: namespace subpath-9468 deletion completed in 6.080224091s

• [SLOW TEST:28.200 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:36:47.189: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:36:58.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5674" for this suite.
Feb 23 19:37:04.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:37:04.375: INFO: namespace resourcequota-5674 deletion completed in 6.112857789s

• [SLOW TEST:17.186 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:37:04.375: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-7804/configmap-test-866173c5-417a-4063-9a48-a3811994abe5
STEP: Creating a pod to test consume configMaps
Feb 23 19:37:04.414: INFO: Waiting up to 5m0s for pod "pod-configmaps-ec255bc1-2bf3-4412-9292-1bb373525ac3" in namespace "configmap-7804" to be "success or failure"
Feb 23 19:37:04.419: INFO: Pod "pod-configmaps-ec255bc1-2bf3-4412-9292-1bb373525ac3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.347155ms
Feb 23 19:37:06.421: INFO: Pod "pod-configmaps-ec255bc1-2bf3-4412-9292-1bb373525ac3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007060856s
STEP: Saw pod success
Feb 23 19:37:06.421: INFO: Pod "pod-configmaps-ec255bc1-2bf3-4412-9292-1bb373525ac3" satisfied condition "success or failure"
Feb 23 19:37:06.424: INFO: Trying to get logs from node worker00 pod pod-configmaps-ec255bc1-2bf3-4412-9292-1bb373525ac3 container env-test: <nil>
STEP: delete the pod
Feb 23 19:37:06.441: INFO: Waiting for pod pod-configmaps-ec255bc1-2bf3-4412-9292-1bb373525ac3 to disappear
Feb 23 19:37:06.446: INFO: Pod pod-configmaps-ec255bc1-2bf3-4412-9292-1bb373525ac3 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:37:06.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7804" for this suite.
Feb 23 19:37:12.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:37:12.526: INFO: namespace configmap-7804 deletion completed in 6.076775713s

• [SLOW TEST:8.151 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:37:12.528: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 23 19:37:12.567: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7606 /api/v1/namespaces/watch-7606/configmaps/e2e-watch-test-watch-closed 29421386-e0ad-4073-881c-8f15716a465c 36073 0 2020-02-23 19:37:12 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 23 19:37:12.567: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7606 /api/v1/namespaces/watch-7606/configmaps/e2e-watch-test-watch-closed 29421386-e0ad-4073-881c-8f15716a465c 36074 0 2020-02-23 19:37:12 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 23 19:37:12.579: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7606 /api/v1/namespaces/watch-7606/configmaps/e2e-watch-test-watch-closed 29421386-e0ad-4073-881c-8f15716a465c 36075 0 2020-02-23 19:37:12 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 23 19:37:12.580: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7606 /api/v1/namespaces/watch-7606/configmaps/e2e-watch-test-watch-closed 29421386-e0ad-4073-881c-8f15716a465c 36076 0 2020-02-23 19:37:12 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:37:12.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7606" for this suite.
Feb 23 19:37:18.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:37:18.653: INFO: namespace watch-7606 deletion completed in 6.07025711s

• [SLOW TEST:6.125 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:37:18.653: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-05f33b8e-1dec-4e84-ab0e-3a5b82f7525f
STEP: Creating secret with name secret-projected-all-test-volume-e5e541a2-ff13-4d03-8b2d-9fe128d2e823
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 23 19:37:18.696: INFO: Waiting up to 5m0s for pod "projected-volume-761d9c03-3ba8-4249-95ab-fe35b92ad99f" in namespace "projected-289" to be "success or failure"
Feb 23 19:37:18.703: INFO: Pod "projected-volume-761d9c03-3ba8-4249-95ab-fe35b92ad99f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.180802ms
Feb 23 19:37:20.706: INFO: Pod "projected-volume-761d9c03-3ba8-4249-95ab-fe35b92ad99f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009697609s
STEP: Saw pod success
Feb 23 19:37:20.706: INFO: Pod "projected-volume-761d9c03-3ba8-4249-95ab-fe35b92ad99f" satisfied condition "success or failure"
Feb 23 19:37:20.709: INFO: Trying to get logs from node worker00 pod projected-volume-761d9c03-3ba8-4249-95ab-fe35b92ad99f container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 23 19:37:20.734: INFO: Waiting for pod projected-volume-761d9c03-3ba8-4249-95ab-fe35b92ad99f to disappear
Feb 23 19:37:20.739: INFO: Pod projected-volume-761d9c03-3ba8-4249-95ab-fe35b92ad99f no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:37:20.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-289" for this suite.
Feb 23 19:37:26.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:37:26.824: INFO: namespace projected-289 deletion completed in 6.082213731s

• [SLOW TEST:8.171 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:37:26.825: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:37:48.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4092" for this suite.
Feb 23 19:37:54.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:37:54.708: INFO: namespace container-runtime-4092 deletion completed in 6.078908845s

• [SLOW TEST:27.883 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:37:54.708: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Feb 23 19:37:54.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 create -f - --namespace=kubectl-1621'
Feb 23 19:37:54.968: INFO: stderr: ""
Feb 23 19:37:54.968: INFO: stdout: "pod/pause created\n"
Feb 23 19:37:54.968: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 23 19:37:54.968: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1621" to be "running and ready"
Feb 23 19:37:54.981: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 12.8036ms
Feb 23 19:37:56.984: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.015841589s
Feb 23 19:37:56.984: INFO: Pod "pause" satisfied condition "running and ready"
Feb 23 19:37:56.984: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 23 19:37:56.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 label pods pause testing-label=testing-label-value --namespace=kubectl-1621'
Feb 23 19:37:57.057: INFO: stderr: ""
Feb 23 19:37:57.057: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 23 19:37:57.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pod pause -L testing-label --namespace=kubectl-1621'
Feb 23 19:37:57.118: INFO: stderr: ""
Feb 23 19:37:57.118: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 23 19:37:57.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 label pods pause testing-label- --namespace=kubectl-1621'
Feb 23 19:37:57.185: INFO: stderr: ""
Feb 23 19:37:57.185: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 23 19:37:57.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pod pause -L testing-label --namespace=kubectl-1621'
Feb 23 19:37:57.239: INFO: stderr: ""
Feb 23 19:37:57.239: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Feb 23 19:37:57.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 delete --grace-period=0 --force -f - --namespace=kubectl-1621'
Feb 23 19:37:57.316: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 19:37:57.316: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 23 19:37:57.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get rc,svc -l name=pause --no-headers --namespace=kubectl-1621'
Feb 23 19:37:57.386: INFO: stderr: "No resources found in kubectl-1621 namespace.\n"
Feb 23 19:37:57.386: INFO: stdout: ""
Feb 23 19:37:57.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-952870286 get pods -l name=pause --namespace=kubectl-1621 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 23 19:37:57.449: INFO: stderr: ""
Feb 23 19:37:57.449: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:37:57.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1621" for this suite.
Feb 23 19:38:03.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:38:03.527: INFO: namespace kubectl-1621 deletion completed in 6.075954185s

• [SLOW TEST:8.819 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:38:03.528: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:38:07.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4561" for this suite.
Feb 23 19:38:13.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:38:13.653: INFO: namespace kubelet-test-4561 deletion completed in 6.079107274s

• [SLOW TEST:10.126 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:38:13.655: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0223 19:38:43.721921      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 23 19:38:43.722: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:38:43.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3383" for this suite.
Feb 23 19:38:49.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:38:49.795: INFO: namespace gc-3383 deletion completed in 6.070059609s

• [SLOW TEST:36.139 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 19:38:49.795: INFO: >>> kubeConfig: /tmp/kubeconfig-952870286
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Feb 23 19:38:49.824: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-952870286 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 19:38:49.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5111" for this suite.
Feb 23 19:38:55.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 23 19:38:55.956: INFO: namespace kubectl-5111 deletion completed in 6.082145902s

• [SLOW TEST:6.162 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.7-beta.0.33+fd5f0b8795e3d6/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSFeb 23 19:38:55.957: INFO: Running AfterSuite actions on all nodes
Feb 23 19:38:55.957: INFO: Running AfterSuite actions on node 1
Feb 23 19:38:55.957: INFO: Skipping dumping logs from cluster

Ran 276 of 4731 Specs in 7338.798 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4455 Skipped
PASS

Ginkgo ran 1 suite in 2h2m20.294657896s
Test Suite Passed
