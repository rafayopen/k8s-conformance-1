I0920 21:46:47.730460      16 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-786974626
I0920 21:46:47.730656      16 e2e.go:92] Starting e2e run "4bfd9ca6-3642-4c8a-99a9-b535da36cab7" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1569016006 - Will randomize all specs
Will run 274 of 4897 specs

Sep 20 21:46:47.771: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 21:46:47.772: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep 20 21:46:47.782: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep 20 21:46:47.804: INFO: 12 / 12 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep 20 21:46:47.805: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Sep 20 21:46:47.805: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep 20 21:46:47.810: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Sep 20 21:46:47.810: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'weave-net' (0 seconds elapsed)
Sep 20 21:46:47.810: INFO: e2e test version: v1.16.0
Sep 20 21:46:47.811: INFO: kube-apiserver version: v1.16.0
Sep 20 21:46:47.812: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 21:46:47.816: INFO: Cluster IP family: ipv4
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:46:47.816: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename pod-network-test
Sep 20 21:46:47.852: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-6537
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 20 21:46:47.853: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 20 21:47:45.978: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.40.0.4:8080/dial?request=hostName&protocol=udp&host=10.32.0.4&port=8081&tries=1'] Namespace:pod-network-test-6537 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 21:47:45.978: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 21:47:46.115: INFO: Waiting for endpoints: map[]
Sep 20 21:47:46.117: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.40.0.4:8080/dial?request=hostName&protocol=udp&host=10.40.0.3&port=8081&tries=1'] Namespace:pod-network-test-6537 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 21:47:46.117: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 21:47:46.205: INFO: Waiting for endpoints: map[]
Sep 20 21:47:46.207: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.40.0.4:8080/dial?request=hostName&protocol=udp&host=10.38.0.1&port=8081&tries=1'] Namespace:pod-network-test-6537 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 21:47:46.208: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 21:47:46.313: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:47:46.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6537" for this suite.
Sep 20 21:47:58.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:47:58.415: INFO: namespace pod-network-test-6537 deletion completed in 12.098573125s

• [SLOW TEST:70.599 seconds]
[sig-network] Networking
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:47:58.416: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 21:47:58.747: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 21:48:00.770: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704612878, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704612878, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704612878, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704612878, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 21:48:03.801: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:48:03.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4750" for this suite.
Sep 20 21:48:09.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:48:09.911: INFO: namespace webhook-4750 deletion completed in 6.096526575s
STEP: Destroying namespace "webhook-4750-markers" for this suite.
Sep 20 21:48:15.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:48:15.967: INFO: namespace webhook-4750-markers deletion completed in 6.056917531s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.560 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:48:15.976: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-fc5587d6-cf28-4275-bd9b-d7736c54ef6d
STEP: Creating a pod to test consume secrets
Sep 20 21:48:16.011: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a02cbecd-bc66-4f65-9b63-26fc2ca0d79d" in namespace "projected-5617" to be "success or failure"
Sep 20 21:48:16.023: INFO: Pod "pod-projected-secrets-a02cbecd-bc66-4f65-9b63-26fc2ca0d79d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.691127ms
Sep 20 21:48:18.028: INFO: Pod "pod-projected-secrets-a02cbecd-bc66-4f65-9b63-26fc2ca0d79d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016626202s
Sep 20 21:48:20.033: INFO: Pod "pod-projected-secrets-a02cbecd-bc66-4f65-9b63-26fc2ca0d79d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021714316s
STEP: Saw pod success
Sep 20 21:48:20.034: INFO: Pod "pod-projected-secrets-a02cbecd-bc66-4f65-9b63-26fc2ca0d79d" satisfied condition "success or failure"
Sep 20 21:48:20.039: INFO: Trying to get logs from node worker-2 pod pod-projected-secrets-a02cbecd-bc66-4f65-9b63-26fc2ca0d79d container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 20 21:48:20.102: INFO: Waiting for pod pod-projected-secrets-a02cbecd-bc66-4f65-9b63-26fc2ca0d79d to disappear
Sep 20 21:48:20.104: INFO: Pod pod-projected-secrets-a02cbecd-bc66-4f65-9b63-26fc2ca0d79d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:48:20.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5617" for this suite.
Sep 20 21:48:26.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:48:26.183: INFO: namespace projected-5617 deletion completed in 6.077551976s

• [SLOW TEST:10.207 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:48:26.183: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 21:48:26.215: INFO: Waiting up to 5m0s for pod "busybox-user-65534-3c50af25-cbec-488c-acee-28c2917c420b" in namespace "security-context-test-7649" to be "success or failure"
Sep 20 21:48:26.219: INFO: Pod "busybox-user-65534-3c50af25-cbec-488c-acee-28c2917c420b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.984187ms
Sep 20 21:48:28.232: INFO: Pod "busybox-user-65534-3c50af25-cbec-488c-acee-28c2917c420b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016583307s
Sep 20 21:48:30.242: INFO: Pod "busybox-user-65534-3c50af25-cbec-488c-acee-28c2917c420b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027486163s
Sep 20 21:48:32.248: INFO: Pod "busybox-user-65534-3c50af25-cbec-488c-acee-28c2917c420b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032685961s
Sep 20 21:48:32.248: INFO: Pod "busybox-user-65534-3c50af25-cbec-488c-acee-28c2917c420b" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:48:32.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7649" for this suite.
Sep 20 21:48:38.287: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:48:38.358: INFO: namespace security-context-test-7649 deletion completed in 6.103130327s

• [SLOW TEST:12.175 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:48:38.358: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Sep 20 21:48:38.450: INFO: Waiting up to 5m0s for pod "downward-api-29f38084-7310-4dca-ba9f-c8b9619903c1" in namespace "downward-api-9063" to be "success or failure"
Sep 20 21:48:38.462: INFO: Pod "downward-api-29f38084-7310-4dca-ba9f-c8b9619903c1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.577499ms
Sep 20 21:48:40.468: INFO: Pod "downward-api-29f38084-7310-4dca-ba9f-c8b9619903c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018197058s
Sep 20 21:48:42.470: INFO: Pod "downward-api-29f38084-7310-4dca-ba9f-c8b9619903c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020407582s
STEP: Saw pod success
Sep 20 21:48:42.470: INFO: Pod "downward-api-29f38084-7310-4dca-ba9f-c8b9619903c1" satisfied condition "success or failure"
Sep 20 21:48:42.472: INFO: Trying to get logs from node worker-2 pod downward-api-29f38084-7310-4dca-ba9f-c8b9619903c1 container dapi-container: <nil>
STEP: delete the pod
Sep 20 21:48:42.493: INFO: Waiting for pod downward-api-29f38084-7310-4dca-ba9f-c8b9619903c1 to disappear
Sep 20 21:48:42.495: INFO: Pod downward-api-29f38084-7310-4dca-ba9f-c8b9619903c1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:48:42.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9063" for this suite.
Sep 20 21:48:48.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:48:48.582: INFO: namespace downward-api-9063 deletion completed in 6.08443227s

• [SLOW TEST:10.224 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:48:48.582: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Sep 20 21:49:18.718: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0920 21:49:18.718836      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:49:18.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-658" for this suite.
Sep 20 21:49:24.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:49:24.800: INFO: namespace gc-658 deletion completed in 6.078315604s

• [SLOW TEST:36.218 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:49:24.801: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-8f4d5c49-9f77-41f7-b2e3-37c0082744c3
STEP: Creating a pod to test consume configMaps
Sep 20 21:49:24.828: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5308482d-a048-468b-938d-d9cf4697e68a" in namespace "projected-1902" to be "success or failure"
Sep 20 21:49:24.832: INFO: Pod "pod-projected-configmaps-5308482d-a048-468b-938d-d9cf4697e68a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.986994ms
Sep 20 21:49:26.834: INFO: Pod "pod-projected-configmaps-5308482d-a048-468b-938d-d9cf4697e68a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005883989s
Sep 20 21:49:28.843: INFO: Pod "pod-projected-configmaps-5308482d-a048-468b-938d-d9cf4697e68a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015335403s
STEP: Saw pod success
Sep 20 21:49:28.843: INFO: Pod "pod-projected-configmaps-5308482d-a048-468b-938d-d9cf4697e68a" satisfied condition "success or failure"
Sep 20 21:49:28.847: INFO: Trying to get logs from node worker-2 pod pod-projected-configmaps-5308482d-a048-468b-938d-d9cf4697e68a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 21:49:28.871: INFO: Waiting for pod pod-projected-configmaps-5308482d-a048-468b-938d-d9cf4697e68a to disappear
Sep 20 21:49:28.889: INFO: Pod pod-projected-configmaps-5308482d-a048-468b-938d-d9cf4697e68a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:49:28.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1902" for this suite.
Sep 20 21:49:34.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:49:34.994: INFO: namespace projected-1902 deletion completed in 6.098072483s

• [SLOW TEST:10.193 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:49:34.994: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep 20 21:49:35.031: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9871 /api/v1/namespaces/watch-9871/configmaps/e2e-watch-test-label-changed 3ef2ba0e-cbc0-4604-9e95-d05ed110efbc 2777 0 2019-09-20 21:49:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 20 21:49:35.031: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9871 /api/v1/namespaces/watch-9871/configmaps/e2e-watch-test-label-changed 3ef2ba0e-cbc0-4604-9e95-d05ed110efbc 2778 0 2019-09-20 21:49:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep 20 21:49:35.031: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9871 /api/v1/namespaces/watch-9871/configmaps/e2e-watch-test-label-changed 3ef2ba0e-cbc0-4604-9e95-d05ed110efbc 2779 0 2019-09-20 21:49:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep 20 21:49:45.077: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9871 /api/v1/namespaces/watch-9871/configmaps/e2e-watch-test-label-changed 3ef2ba0e-cbc0-4604-9e95-d05ed110efbc 2795 0 2019-09-20 21:49:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 20 21:49:45.078: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9871 /api/v1/namespaces/watch-9871/configmaps/e2e-watch-test-label-changed 3ef2ba0e-cbc0-4604-9e95-d05ed110efbc 2796 0 2019-09-20 21:49:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Sep 20 21:49:45.078: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9871 /api/v1/namespaces/watch-9871/configmaps/e2e-watch-test-label-changed 3ef2ba0e-cbc0-4604-9e95-d05ed110efbc 2797 0 2019-09-20 21:49:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:49:45.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9871" for this suite.
Sep 20 21:49:51.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:49:51.143: INFO: namespace watch-9871 deletion completed in 6.061110004s

• [SLOW TEST:16.148 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:49:51.143: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Sep 20 21:49:51.172: INFO: Waiting up to 5m0s for pod "client-containers-f58e4437-ec28-467f-bca1-481207808a86" in namespace "containers-3179" to be "success or failure"
Sep 20 21:49:51.177: INFO: Pod "client-containers-f58e4437-ec28-467f-bca1-481207808a86": Phase="Pending", Reason="", readiness=false. Elapsed: 5.733611ms
Sep 20 21:49:53.186: INFO: Pod "client-containers-f58e4437-ec28-467f-bca1-481207808a86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014653678s
STEP: Saw pod success
Sep 20 21:49:53.186: INFO: Pod "client-containers-f58e4437-ec28-467f-bca1-481207808a86" satisfied condition "success or failure"
Sep 20 21:49:53.188: INFO: Trying to get logs from node worker-2 pod client-containers-f58e4437-ec28-467f-bca1-481207808a86 container test-container: <nil>
STEP: delete the pod
Sep 20 21:49:53.205: INFO: Waiting for pod client-containers-f58e4437-ec28-467f-bca1-481207808a86 to disappear
Sep 20 21:49:53.207: INFO: Pod client-containers-f58e4437-ec28-467f-bca1-481207808a86 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:49:53.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3179" for this suite.
Sep 20 21:49:59.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:49:59.289: INFO: namespace containers-3179 deletion completed in 6.079582317s

• [SLOW TEST:8.146 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:49:59.289: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-0ddeb565-0003-40c8-b560-d1d54c2db25a
STEP: Creating a pod to test consume configMaps
Sep 20 21:49:59.319: INFO: Waiting up to 5m0s for pod "pod-configmaps-6f0a9974-4009-4da7-a781-94973c74c45d" in namespace "configmap-9315" to be "success or failure"
Sep 20 21:49:59.320: INFO: Pod "pod-configmaps-6f0a9974-4009-4da7-a781-94973c74c45d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.366136ms
Sep 20 21:50:01.322: INFO: Pod "pod-configmaps-6f0a9974-4009-4da7-a781-94973c74c45d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003291127s
STEP: Saw pod success
Sep 20 21:50:01.322: INFO: Pod "pod-configmaps-6f0a9974-4009-4da7-a781-94973c74c45d" satisfied condition "success or failure"
Sep 20 21:50:01.325: INFO: Trying to get logs from node worker-2 pod pod-configmaps-6f0a9974-4009-4da7-a781-94973c74c45d container configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 21:50:01.338: INFO: Waiting for pod pod-configmaps-6f0a9974-4009-4da7-a781-94973c74c45d to disappear
Sep 20 21:50:01.340: INFO: Pod pod-configmaps-6f0a9974-4009-4da7-a781-94973c74c45d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:50:01.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9315" for this suite.
Sep 20 21:50:07.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:50:07.450: INFO: namespace configmap-9315 deletion completed in 6.107335906s

• [SLOW TEST:8.160 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:50:07.450: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-159
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-159
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-159
Sep 20 21:50:07.481: INFO: Found 0 stateful pods, waiting for 1
Sep 20 21:50:17.487: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Sep 20 21:50:27.488: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep 20 21:50:27.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-159 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 21:50:28.179: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 21:50:28.179: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 21:50:28.179: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 21:50:28.182: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 20 21:50:38.188: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 20 21:50:38.188: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 21:50:38.222: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999052s
Sep 20 21:50:39.226: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.986912158s
Sep 20 21:50:40.234: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.983233704s
Sep 20 21:50:41.254: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.967108148s
Sep 20 21:50:42.256: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.955613826s
Sep 20 21:50:43.269: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.95328736s
Sep 20 21:50:44.272: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.939925243s
Sep 20 21:50:45.279: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.93674536s
Sep 20 21:50:46.293: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.923175311s
Sep 20 21:50:47.306: INFO: Verifying statefulset ss doesn't scale past 1 for another 915.440988ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-159
Sep 20 21:50:48.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-159 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 21:50:48.456: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 20 21:50:48.456: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 21:50:48.456: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 20 21:50:48.459: INFO: Found 1 stateful pods, waiting for 3
Sep 20 21:50:58.466: INFO: Found 2 stateful pods, waiting for 3
Sep 20 21:51:08.474: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 21:51:08.474: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 21:51:08.474: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 20 21:51:18.468: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 21:51:18.468: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 21:51:18.468: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 20 21:51:28.471: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 21:51:28.471: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 21:51:28.471: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep 20 21:51:28.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-159 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 21:51:28.708: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 21:51:28.708: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 21:51:28.708: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 21:51:28.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-159 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 21:51:28.866: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 21:51:28.866: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 21:51:28.866: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 21:51:28.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-159 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 21:51:29.115: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 21:51:29.115: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 21:51:29.115: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 21:51:29.115: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 21:51:29.120: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep 20 21:51:39.134: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 20 21:51:39.134: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 20 21:51:39.134: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 20 21:51:39.160: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999682s
Sep 20 21:51:40.185: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.975434519s
Sep 20 21:51:41.196: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.963977272s
Sep 20 21:51:42.206: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.953892722s
Sep 20 21:51:43.212: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.945956447s
Sep 20 21:51:44.217: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.940113895s
Sep 20 21:51:45.229: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.931092117s
Sep 20 21:51:46.233: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.923525022s
Sep 20 21:51:47.242: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.919242004s
Sep 20 21:51:48.249: INFO: Verifying statefulset ss doesn't scale past 3 for another 910.145959ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-159
Sep 20 21:51:49.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-159 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 21:51:49.431: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 20 21:51:49.431: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 21:51:49.431: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 20 21:51:49.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-159 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 21:51:49.622: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 20 21:51:49.622: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 21:51:49.622: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 20 21:51:49.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-159 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 21:51:49.762: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 20 21:51:49.762: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 21:51:49.762: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 20 21:51:49.762: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Sep 20 21:52:09.772: INFO: Deleting all statefulset in ns statefulset-159
Sep 20 21:52:09.773: INFO: Scaling statefulset ss to 0
Sep 20 21:52:09.777: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 21:52:09.778: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:52:09.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-159" for this suite.
Sep 20 21:52:15.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:52:15.870: INFO: namespace statefulset-159 deletion completed in 6.081861751s

• [SLOW TEST:128.421 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:52:15.871: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep 20 21:52:15.904: INFO: Pod name pod-release: Found 0 pods out of 1
Sep 20 21:52:20.906: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:52:21.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6922" for this suite.
Sep 20 21:52:27.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:52:28.044: INFO: namespace replication-controller-6922 deletion completed in 6.120618524s

• [SLOW TEST:12.173 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:52:28.045: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-50ec84d1-e8ed-43da-87fc-d8ccf259d51b
STEP: Creating secret with name s-test-opt-upd-2d008271-4c35-4db3-b125-9a788bd36c6b
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-50ec84d1-e8ed-43da-87fc-d8ccf259d51b
STEP: Updating secret s-test-opt-upd-2d008271-4c35-4db3-b125-9a788bd36c6b
STEP: Creating secret with name s-test-opt-create-a3e92498-bb19-4939-b671-f46ee28afbe6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:52:34.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4724" for this suite.
Sep 20 21:52:46.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:52:46.313: INFO: namespace secrets-4724 deletion completed in 12.056721194s

• [SLOW TEST:18.268 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:52:46.313: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 20 21:52:46.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-8834'
Sep 20 21:52:46.388: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 20 21:52:46.388: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Sep 20 21:52:46.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 delete deployment e2e-test-httpd-deployment --namespace=kubectl-8834'
Sep 20 21:52:46.458: INFO: stderr: ""
Sep 20 21:52:46.458: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:52:46.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8834" for this suite.
Sep 20 21:52:58.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:52:58.571: INFO: namespace kubectl-8834 deletion completed in 12.105092121s

• [SLOW TEST:12.257 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:52:58.571: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Sep 20 21:52:59.138: INFO: created pod pod-service-account-defaultsa
Sep 20 21:52:59.138: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep 20 21:52:59.160: INFO: created pod pod-service-account-mountsa
Sep 20 21:52:59.160: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep 20 21:52:59.177: INFO: created pod pod-service-account-nomountsa
Sep 20 21:52:59.177: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep 20 21:52:59.184: INFO: created pod pod-service-account-defaultsa-mountspec
Sep 20 21:52:59.184: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep 20 21:52:59.192: INFO: created pod pod-service-account-mountsa-mountspec
Sep 20 21:52:59.192: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep 20 21:52:59.196: INFO: created pod pod-service-account-nomountsa-mountspec
Sep 20 21:52:59.196: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep 20 21:52:59.204: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep 20 21:52:59.204: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep 20 21:52:59.215: INFO: created pod pod-service-account-mountsa-nomountspec
Sep 20 21:52:59.215: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep 20 21:52:59.235: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep 20 21:52:59.235: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:52:59.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4731" for this suite.
Sep 20 21:53:05.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:53:05.369: INFO: namespace svcaccounts-4731 deletion completed in 6.13170241s

• [SLOW TEST:6.798 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:53:05.370: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Sep 20 21:53:05.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 cluster-info'
Sep 20 21:53:05.453: INFO: stderr: ""
Sep 20 21:53:05.453: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:53:05.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9938" for this suite.
Sep 20 21:53:11.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:53:11.573: INFO: namespace kubectl-9938 deletion completed in 6.117334637s

• [SLOW TEST:6.203 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:53:11.573: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Sep 20 21:53:15.672: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6520 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 21:53:15.672: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 21:53:15.808: INFO: Exec stderr: ""
Sep 20 21:53:15.808: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6520 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 21:53:15.808: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 21:53:15.907: INFO: Exec stderr: ""
Sep 20 21:53:15.907: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6520 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 21:53:15.907: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 21:53:15.993: INFO: Exec stderr: ""
Sep 20 21:53:15.993: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6520 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 21:53:15.993: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 21:53:16.074: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Sep 20 21:53:16.074: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6520 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 21:53:16.074: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 21:53:16.156: INFO: Exec stderr: ""
Sep 20 21:53:16.156: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6520 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 21:53:16.156: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 21:53:16.236: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Sep 20 21:53:16.236: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6520 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 21:53:16.236: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 21:53:16.325: INFO: Exec stderr: ""
Sep 20 21:53:16.325: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6520 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 21:53:16.325: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 21:53:16.406: INFO: Exec stderr: ""
Sep 20 21:53:16.406: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6520 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 21:53:16.406: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 21:53:16.476: INFO: Exec stderr: ""
Sep 20 21:53:16.476: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6520 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 21:53:16.476: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 21:53:16.556: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:53:16.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6520" for this suite.
Sep 20 21:54:06.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:54:06.673: INFO: namespace e2e-kubelet-etc-hosts-6520 deletion completed in 50.114843402s

• [SLOW TEST:55.100 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:54:06.673: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 20 21:54:06.702: INFO: Waiting up to 5m0s for pod "pod-9d2488cf-aa6a-44f6-99b1-e70aabb5cca9" in namespace "emptydir-1560" to be "success or failure"
Sep 20 21:54:06.711: INFO: Pod "pod-9d2488cf-aa6a-44f6-99b1-e70aabb5cca9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.464286ms
Sep 20 21:54:08.713: INFO: Pod "pod-9d2488cf-aa6a-44f6-99b1-e70aabb5cca9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010892291s
STEP: Saw pod success
Sep 20 21:54:08.713: INFO: Pod "pod-9d2488cf-aa6a-44f6-99b1-e70aabb5cca9" satisfied condition "success or failure"
Sep 20 21:54:08.714: INFO: Trying to get logs from node worker-2 pod pod-9d2488cf-aa6a-44f6-99b1-e70aabb5cca9 container test-container: <nil>
STEP: delete the pod
Sep 20 21:54:08.735: INFO: Waiting for pod pod-9d2488cf-aa6a-44f6-99b1-e70aabb5cca9 to disappear
Sep 20 21:54:08.737: INFO: Pod pod-9d2488cf-aa6a-44f6-99b1-e70aabb5cca9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:54:08.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1560" for this suite.
Sep 20 21:54:14.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:54:14.845: INFO: namespace emptydir-1560 deletion completed in 6.106053131s

• [SLOW TEST:8.172 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:54:14.846: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 21:54:15.434: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 21:54:17.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704613254, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704613254, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704613254, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704613254, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 21:54:20.457: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:54:20.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5728" for this suite.
Sep 20 21:54:26.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:54:26.740: INFO: namespace webhook-5728 deletion completed in 6.074785638s
STEP: Destroying namespace "webhook-5728-markers" for this suite.
Sep 20 21:54:32.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:54:32.791: INFO: namespace webhook-5728-markers deletion completed in 6.050452692s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.951 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:54:32.797: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 21:54:32.823: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-687d0455-e30e-4683-abc5-91dad2e5f18d" in namespace "security-context-test-1040" to be "success or failure"
Sep 20 21:54:32.828: INFO: Pod "alpine-nnp-false-687d0455-e30e-4683-abc5-91dad2e5f18d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.965905ms
Sep 20 21:54:34.830: INFO: Pod "alpine-nnp-false-687d0455-e30e-4683-abc5-91dad2e5f18d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00712974s
Sep 20 21:54:36.836: INFO: Pod "alpine-nnp-false-687d0455-e30e-4683-abc5-91dad2e5f18d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012832208s
Sep 20 21:54:38.841: INFO: Pod "alpine-nnp-false-687d0455-e30e-4683-abc5-91dad2e5f18d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017860769s
Sep 20 21:54:38.841: INFO: Pod "alpine-nnp-false-687d0455-e30e-4683-abc5-91dad2e5f18d" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:54:38.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1040" for this suite.
Sep 20 21:54:44.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:54:44.933: INFO: namespace security-context-test-1040 deletion completed in 6.072426621s

• [SLOW TEST:12.136 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:54:44.936: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-ww5h
STEP: Creating a pod to test atomic-volume-subpath
Sep 20 21:54:45.013: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-ww5h" in namespace "subpath-9118" to be "success or failure"
Sep 20 21:54:45.018: INFO: Pod "pod-subpath-test-projected-ww5h": Phase="Pending", Reason="", readiness=false. Elapsed: 4.734657ms
Sep 20 21:54:47.020: INFO: Pod "pod-subpath-test-projected-ww5h": Phase="Running", Reason="", readiness=true. Elapsed: 2.007204169s
Sep 20 21:54:49.027: INFO: Pod "pod-subpath-test-projected-ww5h": Phase="Running", Reason="", readiness=true. Elapsed: 4.01370698s
Sep 20 21:54:51.030: INFO: Pod "pod-subpath-test-projected-ww5h": Phase="Running", Reason="", readiness=true. Elapsed: 6.017624744s
Sep 20 21:54:53.039: INFO: Pod "pod-subpath-test-projected-ww5h": Phase="Running", Reason="", readiness=true. Elapsed: 8.025834701s
Sep 20 21:54:55.041: INFO: Pod "pod-subpath-test-projected-ww5h": Phase="Running", Reason="", readiness=true. Elapsed: 10.02820517s
Sep 20 21:54:57.052: INFO: Pod "pod-subpath-test-projected-ww5h": Phase="Running", Reason="", readiness=true. Elapsed: 12.039132539s
Sep 20 21:54:59.055: INFO: Pod "pod-subpath-test-projected-ww5h": Phase="Running", Reason="", readiness=true. Elapsed: 14.042005731s
Sep 20 21:55:01.067: INFO: Pod "pod-subpath-test-projected-ww5h": Phase="Running", Reason="", readiness=true. Elapsed: 16.05379108s
Sep 20 21:55:03.073: INFO: Pod "pod-subpath-test-projected-ww5h": Phase="Running", Reason="", readiness=true. Elapsed: 18.059857799s
Sep 20 21:55:05.080: INFO: Pod "pod-subpath-test-projected-ww5h": Phase="Running", Reason="", readiness=true. Elapsed: 20.067169706s
Sep 20 21:55:07.087: INFO: Pod "pod-subpath-test-projected-ww5h": Phase="Running", Reason="", readiness=true. Elapsed: 22.07424197s
Sep 20 21:55:09.099: INFO: Pod "pod-subpath-test-projected-ww5h": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.086298462s
STEP: Saw pod success
Sep 20 21:55:09.099: INFO: Pod "pod-subpath-test-projected-ww5h" satisfied condition "success or failure"
Sep 20 21:55:09.104: INFO: Trying to get logs from node worker-2 pod pod-subpath-test-projected-ww5h container test-container-subpath-projected-ww5h: <nil>
STEP: delete the pod
Sep 20 21:55:09.142: INFO: Waiting for pod pod-subpath-test-projected-ww5h to disappear
Sep 20 21:55:09.143: INFO: Pod pod-subpath-test-projected-ww5h no longer exists
STEP: Deleting pod pod-subpath-test-projected-ww5h
Sep 20 21:55:09.143: INFO: Deleting pod "pod-subpath-test-projected-ww5h" in namespace "subpath-9118"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:55:09.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9118" for this suite.
Sep 20 21:55:15.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:55:15.208: INFO: namespace subpath-9118 deletion completed in 6.061257718s

• [SLOW TEST:30.272 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:55:15.209: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Sep 20 21:55:15.232: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:55:18.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-707" for this suite.
Sep 20 21:55:24.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:55:24.479: INFO: namespace init-container-707 deletion completed in 6.102429696s

• [SLOW TEST:9.269 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:55:24.479: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 21:55:24.695: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 21:55:26.708: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704613324, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704613324, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704613324, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704613324, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 21:55:29.744: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:55:29.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5863" for this suite.
Sep 20 21:55:35.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:55:35.885: INFO: namespace webhook-5863 deletion completed in 6.086911622s
STEP: Destroying namespace "webhook-5863-markers" for this suite.
Sep 20 21:55:41.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:55:41.964: INFO: namespace webhook-5863-markers deletion completed in 6.079401686s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.492 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:55:41.972: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 21:55:41.995: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 20 21:55:43.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 --namespace=crd-publish-openapi-1166 create -f -'
Sep 20 21:55:44.431: INFO: stderr: ""
Sep 20 21:55:44.431: INFO: stdout: "e2e-test-crd-publish-openapi-623-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 20 21:55:44.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 --namespace=crd-publish-openapi-1166 delete e2e-test-crd-publish-openapi-623-crds test-cr'
Sep 20 21:55:44.507: INFO: stderr: ""
Sep 20 21:55:44.507: INFO: stdout: "e2e-test-crd-publish-openapi-623-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Sep 20 21:55:44.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 --namespace=crd-publish-openapi-1166 apply -f -'
Sep 20 21:55:44.613: INFO: stderr: ""
Sep 20 21:55:44.613: INFO: stdout: "e2e-test-crd-publish-openapi-623-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 20 21:55:44.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 --namespace=crd-publish-openapi-1166 delete e2e-test-crd-publish-openapi-623-crds test-cr'
Sep 20 21:55:44.668: INFO: stderr: ""
Sep 20 21:55:44.668: INFO: stdout: "e2e-test-crd-publish-openapi-623-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep 20 21:55:44.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 explain e2e-test-crd-publish-openapi-623-crds'
Sep 20 21:55:44.762: INFO: stderr: ""
Sep 20 21:55:44.762: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-623-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:55:47.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1166" for this suite.
Sep 20 21:55:53.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:55:53.460: INFO: namespace crd-publish-openapi-1166 deletion completed in 6.080468254s

• [SLOW TEST:11.488 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:55:53.461: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Sep 20 21:55:53.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 api-versions'
Sep 20 21:55:53.526: INFO: stderr: ""
Sep 20 21:55:53.526: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:55:53.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3503" for this suite.
Sep 20 21:55:59.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:55:59.595: INFO: namespace kubectl-3503 deletion completed in 6.067586979s

• [SLOW TEST:6.134 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:55:59.597: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 20 21:55:59.635: INFO: Number of nodes with available pods: 0
Sep 20 21:55:59.635: INFO: Node controlplane-1 is running more than one daemon pod
Sep 20 21:56:00.645: INFO: Number of nodes with available pods: 0
Sep 20 21:56:00.645: INFO: Node controlplane-1 is running more than one daemon pod
Sep 20 21:56:01.641: INFO: Number of nodes with available pods: 1
Sep 20 21:56:01.641: INFO: Node worker-1 is running more than one daemon pod
Sep 20 21:56:02.649: INFO: Number of nodes with available pods: 3
Sep 20 21:56:02.649: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep 20 21:56:02.687: INFO: Number of nodes with available pods: 2
Sep 20 21:56:02.687: INFO: Node worker-1 is running more than one daemon pod
Sep 20 21:56:03.699: INFO: Number of nodes with available pods: 2
Sep 20 21:56:03.699: INFO: Node worker-1 is running more than one daemon pod
Sep 20 21:56:04.704: INFO: Number of nodes with available pods: 3
Sep 20 21:56:04.704: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7940, will wait for the garbage collector to delete the pods
I0920 21:56:04.721723      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 21:56:04.721770      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 21:56:04.782: INFO: Deleting DaemonSet.extensions daemon-set took: 10.171158ms
Sep 20 21:56:05.090: INFO: Terminating DaemonSet.extensions daemon-set pods took: 308.072982ms
I0920 21:56:05.090253      16 controller_utils.go:810] Ignoring inactive pod daemonsets-7940/daemon-set-vgslm in state Running, deletion time 2019-09-20 21:56:34 +0000 UTC
I0920 21:56:05.090276      16 controller_utils.go:810] Ignoring inactive pod daemonsets-7940/daemon-set-wskgv in state Running, deletion time 2019-09-20 21:56:34 +0000 UTC
I0920 21:56:05.090280      16 controller_utils.go:810] Ignoring inactive pod daemonsets-7940/daemon-set-lkmxg in state Running, deletion time 2019-09-20 21:56:34 +0000 UTC
Sep 20 21:56:13.192: INFO: Number of nodes with available pods: 0
Sep 20 21:56:13.192: INFO: Number of running nodes: 0, number of available pods: 0
Sep 20 21:56:13.194: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7940/daemonsets","resourceVersion":"4287"},"items":null}

Sep 20 21:56:13.196: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7940/pods","resourceVersion":"4287"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:56:13.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7940" for this suite.
Sep 20 21:56:19.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:56:19.295: INFO: namespace daemonsets-7940 deletion completed in 6.092005438s

• [SLOW TEST:19.698 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:56:19.297: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Sep 20 21:56:19.364: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 21:56:21.988: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:56:30.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5671" for this suite.
Sep 20 21:56:36.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:56:36.260: INFO: namespace crd-publish-openapi-5671 deletion completed in 6.101846633s

• [SLOW TEST:16.964 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:56:36.261: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-8e12254e-ea18-42f1-9d63-368b0f81fec6
STEP: Creating a pod to test consume secrets
Sep 20 21:56:36.288: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1b169f87-7757-4b9e-855a-0ed001dc6e23" in namespace "projected-5940" to be "success or failure"
Sep 20 21:56:36.291: INFO: Pod "pod-projected-secrets-1b169f87-7757-4b9e-855a-0ed001dc6e23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.562526ms
Sep 20 21:56:38.299: INFO: Pod "pod-projected-secrets-1b169f87-7757-4b9e-855a-0ed001dc6e23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010433111s
STEP: Saw pod success
Sep 20 21:56:38.299: INFO: Pod "pod-projected-secrets-1b169f87-7757-4b9e-855a-0ed001dc6e23" satisfied condition "success or failure"
Sep 20 21:56:38.300: INFO: Trying to get logs from node worker-2 pod pod-projected-secrets-1b169f87-7757-4b9e-855a-0ed001dc6e23 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 20 21:56:38.314: INFO: Waiting for pod pod-projected-secrets-1b169f87-7757-4b9e-855a-0ed001dc6e23 to disappear
Sep 20 21:56:38.316: INFO: Pod pod-projected-secrets-1b169f87-7757-4b9e-855a-0ed001dc6e23 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:56:38.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5940" for this suite.
Sep 20 21:56:44.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:56:44.422: INFO: namespace projected-5940 deletion completed in 6.103077172s

• [SLOW TEST:8.161 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:56:44.423: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 21:56:44.448: INFO: Waiting up to 5m0s for pod "downwardapi-volume-050f613c-9159-49bb-8ca7-ed8abc8f15f4" in namespace "downward-api-1809" to be "success or failure"
Sep 20 21:56:44.452: INFO: Pod "downwardapi-volume-050f613c-9159-49bb-8ca7-ed8abc8f15f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045946ms
Sep 20 21:56:46.471: INFO: Pod "downwardapi-volume-050f613c-9159-49bb-8ca7-ed8abc8f15f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022932464s
STEP: Saw pod success
Sep 20 21:56:46.471: INFO: Pod "downwardapi-volume-050f613c-9159-49bb-8ca7-ed8abc8f15f4" satisfied condition "success or failure"
Sep 20 21:56:46.473: INFO: Trying to get logs from node worker-2 pod downwardapi-volume-050f613c-9159-49bb-8ca7-ed8abc8f15f4 container client-container: <nil>
STEP: delete the pod
Sep 20 21:56:46.486: INFO: Waiting for pod downwardapi-volume-050f613c-9159-49bb-8ca7-ed8abc8f15f4 to disappear
Sep 20 21:56:46.488: INFO: Pod downwardapi-volume-050f613c-9159-49bb-8ca7-ed8abc8f15f4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:56:46.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1809" for this suite.
Sep 20 21:56:52.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:56:52.577: INFO: namespace downward-api-1809 deletion completed in 6.087042187s

• [SLOW TEST:8.155 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:56:52.579: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:56:58.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1390" for this suite.
Sep 20 21:57:04.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:57:04.762: INFO: namespace namespaces-1390 deletion completed in 6.072021525s
STEP: Destroying namespace "nsdeletetest-9326" for this suite.
Sep 20 21:57:04.763: INFO: Namespace nsdeletetest-9326 was already deleted
STEP: Destroying namespace "nsdeletetest-5980" for this suite.
Sep 20 21:57:10.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:57:10.843: INFO: namespace nsdeletetest-5980 deletion completed in 6.079833455s

• [SLOW TEST:18.263 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:57:10.843: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep 20 21:57:11.176: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Sep 20 21:57:13.201: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704613430, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704613430, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704613430, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704613430, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 21:57:16.226: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 21:57:16.232: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:57:17.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8156" for this suite.
Sep 20 21:57:23.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:57:23.541: INFO: namespace crd-webhook-8156 deletion completed in 6.069340538s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:12.706 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:57:23.549: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 21:57:23.578: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fa51a669-0e74-4595-b566-6143962df1e0" in namespace "downward-api-8394" to be "success or failure"
Sep 20 21:57:23.582: INFO: Pod "downwardapi-volume-fa51a669-0e74-4595-b566-6143962df1e0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.863342ms
Sep 20 21:57:25.584: INFO: Pod "downwardapi-volume-fa51a669-0e74-4595-b566-6143962df1e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005889082s
Sep 20 21:57:27.589: INFO: Pod "downwardapi-volume-fa51a669-0e74-4595-b566-6143962df1e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01153777s
STEP: Saw pod success
Sep 20 21:57:27.590: INFO: Pod "downwardapi-volume-fa51a669-0e74-4595-b566-6143962df1e0" satisfied condition "success or failure"
Sep 20 21:57:27.591: INFO: Trying to get logs from node worker-2 pod downwardapi-volume-fa51a669-0e74-4595-b566-6143962df1e0 container client-container: <nil>
STEP: delete the pod
Sep 20 21:57:27.607: INFO: Waiting for pod downwardapi-volume-fa51a669-0e74-4595-b566-6143962df1e0 to disappear
Sep 20 21:57:27.609: INFO: Pod downwardapi-volume-fa51a669-0e74-4595-b566-6143962df1e0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:57:27.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8394" for this suite.
Sep 20 21:57:33.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:57:33.716: INFO: namespace downward-api-8394 deletion completed in 6.104944292s

• [SLOW TEST:10.167 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:57:33.716: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 21:57:34.002: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 21:57:36.025: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704613453, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704613453, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704613453, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704613453, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 21:57:39.088: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 21:57:39.090: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:57:40.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5430" for this suite.
Sep 20 21:57:46.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:57:46.335: INFO: namespace webhook-5430 deletion completed in 6.114644071s
STEP: Destroying namespace "webhook-5430-markers" for this suite.
Sep 20 21:57:52.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:57:52.425: INFO: namespace webhook-5430-markers deletion completed in 6.090326006s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.716 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:57:52.432: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 21:57:52.916: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 21:57:54.932: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704613472, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704613472, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704613472, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704613472, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 21:57:57.966: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:57:58.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8178" for this suite.
Sep 20 21:58:04.105: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:58:04.169: INFO: namespace webhook-8178 deletion completed in 6.07110126s
STEP: Destroying namespace "webhook-8178-markers" for this suite.
Sep 20 21:58:10.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:58:10.257: INFO: namespace webhook-8178-markers deletion completed in 6.087924133s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.831 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:58:10.263: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with configMap that has name projected-configmap-test-upd-b94d3035-d8a4-4036-b9de-4de577cb41c3
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-b94d3035-d8a4-4036-b9de-4de577cb41c3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 21:59:41.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4360" for this suite.
Sep 20 21:59:55.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 21:59:55.388: INFO: namespace projected-4360 deletion completed in 14.084822758s

• [SLOW TEST:105.124 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 21:59:55.389: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-1289, will wait for the garbage collector to delete the pods
I0920 22:00:01.442012      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 22:00:01.442286      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 22:00:01.505: INFO: Deleting Job.batch foo took: 13.012884ms
Sep 20 22:00:01.805: INFO: Terminating Job.batch foo pods took: 300.388293ms
I0920 22:00:01.805807      16 controller_utils.go:810] Ignoring inactive pod job-1289/foo-7h9l6 in state Running, deletion time 2019-09-20 22:00:31 +0000 UTC
I0920 22:00:01.805842      16 controller_utils.go:810] Ignoring inactive pod job-1289/foo-tf56h in state Running, deletion time 2019-09-20 22:00:31 +0000 UTC
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:00:43.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1289" for this suite.
Sep 20 22:00:49.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:00:49.268: INFO: namespace job-1289 deletion completed in 6.056536933s

• [SLOW TEST:53.880 seconds]
[sig-apps] Job
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:00:49.268: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 20 22:00:53.319: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 20 22:00:53.321: INFO: Pod pod-with-prestop-http-hook still exists
Sep 20 22:00:55.322: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 20 22:00:55.324: INFO: Pod pod-with-prestop-http-hook still exists
Sep 20 22:00:57.321: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 20 22:00:57.324: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:00:57.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6126" for this suite.
Sep 20 22:01:25.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:01:25.426: INFO: namespace container-lifecycle-hook-6126 deletion completed in 28.093380635s

• [SLOW TEST:36.158 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:01:25.426: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2649
I0920 22:01:25.452349      16 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2649, replica count: 1
I0920 22:01:25.452417      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 22:01:25.452467      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 22:01:26.503012      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 22:01:27.503268      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 22:01:28.503812      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 22:01:28.505082      16 reflector.go:120] Starting reflector *v1.Endpoints (0s) from k8s.io/kubernetes/test/e2e/network/service_latency.go:323
I0920 22:01:28.505777      16 reflector.go:158] Listing and watching *v1.Endpoints from k8s.io/kubernetes/test/e2e/network/service_latency.go:323
Sep 20 22:01:28.633: INFO: Created: latency-svc-rwn4c
Sep 20 22:01:28.645: INFO: Got endpoints: latency-svc-rwn4c [40.849099ms]
Sep 20 22:01:28.661: INFO: Created: latency-svc-lxbxw
Sep 20 22:01:28.669: INFO: Got endpoints: latency-svc-lxbxw [24.051459ms]
Sep 20 22:01:28.677: INFO: Created: latency-svc-c4f66
Sep 20 22:01:28.683: INFO: Got endpoints: latency-svc-c4f66 [37.548689ms]
Sep 20 22:01:28.692: INFO: Created: latency-svc-cpvk5
Sep 20 22:01:28.694: INFO: Got endpoints: latency-svc-cpvk5 [48.729697ms]
Sep 20 22:01:28.697: INFO: Created: latency-svc-6jfwp
Sep 20 22:01:28.698: INFO: Got endpoints: latency-svc-6jfwp [51.985189ms]
Sep 20 22:01:28.711: INFO: Created: latency-svc-mkx54
Sep 20 22:01:28.711: INFO: Got endpoints: latency-svc-mkx54 [65.307983ms]
Sep 20 22:01:28.719: INFO: Created: latency-svc-xst2k
Sep 20 22:01:28.724: INFO: Got endpoints: latency-svc-xst2k [78.630929ms]
Sep 20 22:01:28.736: INFO: Created: latency-svc-qmcrb
Sep 20 22:01:28.739: INFO: Got endpoints: latency-svc-qmcrb [93.616799ms]
Sep 20 22:01:28.746: INFO: Created: latency-svc-d5kvn
Sep 20 22:01:28.757: INFO: Got endpoints: latency-svc-d5kvn [111.446229ms]
Sep 20 22:01:28.763: INFO: Created: latency-svc-rchg5
Sep 20 22:01:28.767: INFO: Got endpoints: latency-svc-rchg5 [120.838636ms]
Sep 20 22:01:28.770: INFO: Created: latency-svc-rxnq7
Sep 20 22:01:28.773: INFO: Got endpoints: latency-svc-rxnq7 [126.582317ms]
Sep 20 22:01:28.776: INFO: Created: latency-svc-qj96k
Sep 20 22:01:28.781: INFO: Got endpoints: latency-svc-qj96k [134.87262ms]
Sep 20 22:01:28.785: INFO: Created: latency-svc-nmm94
Sep 20 22:01:28.789: INFO: Got endpoints: latency-svc-nmm94 [142.772924ms]
Sep 20 22:01:28.791: INFO: Created: latency-svc-g88kt
Sep 20 22:01:28.795: INFO: Got endpoints: latency-svc-g88kt [149.096377ms]
Sep 20 22:01:28.798: INFO: Created: latency-svc-q29n5
Sep 20 22:01:28.804: INFO: Got endpoints: latency-svc-q29n5 [157.951199ms]
Sep 20 22:01:28.809: INFO: Created: latency-svc-ppdbc
Sep 20 22:01:28.812: INFO: Got endpoints: latency-svc-ppdbc [165.868131ms]
Sep 20 22:01:28.816: INFO: Created: latency-svc-dh5kq
Sep 20 22:01:28.819: INFO: Got endpoints: latency-svc-dh5kq [149.503826ms]
Sep 20 22:01:28.840: INFO: Created: latency-svc-ksm5m
Sep 20 22:01:28.845: INFO: Got endpoints: latency-svc-ksm5m [161.835002ms]
Sep 20 22:01:28.850: INFO: Created: latency-svc-n7gqr
Sep 20 22:01:28.850: INFO: Got endpoints: latency-svc-n7gqr [155.47632ms]
Sep 20 22:01:28.858: INFO: Created: latency-svc-g4k9q
Sep 20 22:01:28.861: INFO: Got endpoints: latency-svc-g4k9q [163.040175ms]
Sep 20 22:01:28.872: INFO: Created: latency-svc-thss9
Sep 20 22:01:28.880: INFO: Got endpoints: latency-svc-thss9 [167.93083ms]
Sep 20 22:01:28.890: INFO: Created: latency-svc-rtp7b
Sep 20 22:01:28.893: INFO: Got endpoints: latency-svc-rtp7b [168.737914ms]
Sep 20 22:01:28.900: INFO: Created: latency-svc-bpstk
Sep 20 22:01:28.902: INFO: Got endpoints: latency-svc-bpstk [162.193367ms]
Sep 20 22:01:28.909: INFO: Created: latency-svc-7l5vl
Sep 20 22:01:28.914: INFO: Got endpoints: latency-svc-7l5vl [156.046665ms]
Sep 20 22:01:28.919: INFO: Created: latency-svc-gw98l
Sep 20 22:01:28.921: INFO: Got endpoints: latency-svc-gw98l [154.158191ms]
Sep 20 22:01:28.933: INFO: Created: latency-svc-k8vvv
Sep 20 22:01:28.938: INFO: Created: latency-svc-ggm8c
Sep 20 22:01:28.950: INFO: Got endpoints: latency-svc-k8vvv [36.353215ms]
Sep 20 22:01:28.953: INFO: Got endpoints: latency-svc-ggm8c [179.878872ms]
Sep 20 22:01:28.958: INFO: Created: latency-svc-ljh6r
Sep 20 22:01:28.963: INFO: Got endpoints: latency-svc-ljh6r [181.572662ms]
Sep 20 22:01:28.970: INFO: Created: latency-svc-zm7wv
Sep 20 22:01:28.975: INFO: Got endpoints: latency-svc-zm7wv [186.507056ms]
Sep 20 22:01:28.980: INFO: Created: latency-svc-4mf9z
Sep 20 22:01:28.987: INFO: Created: latency-svc-w9fqb
Sep 20 22:01:28.990: INFO: Got endpoints: latency-svc-4mf9z [194.933028ms]
Sep 20 22:01:28.994: INFO: Got endpoints: latency-svc-w9fqb [189.035082ms]
Sep 20 22:01:29.001: INFO: Created: latency-svc-4qvll
Sep 20 22:01:29.004: INFO: Got endpoints: latency-svc-4qvll [191.817127ms]
Sep 20 22:01:29.015: INFO: Created: latency-svc-9c29v
Sep 20 22:01:29.016: INFO: Got endpoints: latency-svc-9c29v [196.940208ms]
Sep 20 22:01:29.020: INFO: Created: latency-svc-pxtxx
Sep 20 22:01:29.021: INFO: Got endpoints: latency-svc-pxtxx [174.995673ms]
Sep 20 22:01:29.041: INFO: Created: latency-svc-2gs86
Sep 20 22:01:29.053: INFO: Got endpoints: latency-svc-2gs86 [202.822447ms]
Sep 20 22:01:29.057: INFO: Created: latency-svc-pxrxw
Sep 20 22:01:29.061: INFO: Got endpoints: latency-svc-pxrxw [199.833651ms]
Sep 20 22:01:29.067: INFO: Created: latency-svc-48zg7
Sep 20 22:01:29.068: INFO: Got endpoints: latency-svc-48zg7 [188.204129ms]
Sep 20 22:01:29.075: INFO: Created: latency-svc-j4vj5
Sep 20 22:01:29.078: INFO: Got endpoints: latency-svc-j4vj5 [185.036234ms]
Sep 20 22:01:29.081: INFO: Created: latency-svc-dn54b
Sep 20 22:01:29.086: INFO: Got endpoints: latency-svc-dn54b [183.845622ms]
Sep 20 22:01:29.097: INFO: Created: latency-svc-l6jd5
Sep 20 22:01:29.105: INFO: Created: latency-svc-2bwkw
Sep 20 22:01:29.117: INFO: Created: latency-svc-7ndg2
Sep 20 22:01:29.117: INFO: Created: latency-svc-p8kz7
Sep 20 22:01:29.130: INFO: Created: latency-svc-hd8v4
Sep 20 22:01:29.141: INFO: Created: latency-svc-v6lhg
Sep 20 22:01:29.209: INFO: Got endpoints: latency-svc-l6jd5 [287.810123ms]
Sep 20 22:01:29.212: INFO: Got endpoints: latency-svc-2bwkw [261.374164ms]
Sep 20 22:01:29.218: INFO: Created: latency-svc-ls4sf
Sep 20 22:01:29.226: INFO: Created: latency-svc-w5c4z
Sep 20 22:01:29.235: INFO: Created: latency-svc-q2f45
Sep 20 22:01:29.238: INFO: Got endpoints: latency-svc-p8kz7 [284.293661ms]
Sep 20 22:01:29.253: INFO: Created: latency-svc-q52t7
Sep 20 22:01:29.261: INFO: Created: latency-svc-b4c85
Sep 20 22:01:29.265: INFO: Created: latency-svc-f8tdv
Sep 20 22:01:29.271: INFO: Created: latency-svc-txh6f
Sep 20 22:01:29.281: INFO: Created: latency-svc-7v4fz
Sep 20 22:01:29.287: INFO: Created: latency-svc-ds9nq
Sep 20 22:01:29.292: INFO: Created: latency-svc-94m24
Sep 20 22:01:29.295: INFO: Got endpoints: latency-svc-7ndg2 [331.412828ms]
Sep 20 22:01:29.301: INFO: Created: latency-svc-jg5dz
Sep 20 22:01:29.309: INFO: Created: latency-svc-69fkz
Sep 20 22:01:29.319: INFO: Created: latency-svc-kmthn
Sep 20 22:01:29.334: INFO: Got endpoints: latency-svc-hd8v4 [357.83958ms]
Sep 20 22:01:29.342: INFO: Created: latency-svc-d4tjj
Sep 20 22:01:29.384: INFO: Got endpoints: latency-svc-v6lhg [393.47946ms]
Sep 20 22:01:29.392: INFO: Created: latency-svc-4wbhm
Sep 20 22:01:29.436: INFO: Got endpoints: latency-svc-ls4sf [441.762258ms]
Sep 20 22:01:29.446: INFO: Created: latency-svc-g846q
Sep 20 22:01:29.489: INFO: Got endpoints: latency-svc-w5c4z [484.61984ms]
Sep 20 22:01:29.499: INFO: Created: latency-svc-lrgm2
Sep 20 22:01:29.533: INFO: Got endpoints: latency-svc-q2f45 [516.888524ms]
Sep 20 22:01:29.545: INFO: Created: latency-svc-pwm9z
Sep 20 22:01:29.584: INFO: Got endpoints: latency-svc-q52t7 [562.249015ms]
Sep 20 22:01:29.599: INFO: Created: latency-svc-pts4h
Sep 20 22:01:29.636: INFO: Got endpoints: latency-svc-b4c85 [582.201825ms]
Sep 20 22:01:29.644: INFO: Created: latency-svc-wcx4r
Sep 20 22:01:29.684: INFO: Got endpoints: latency-svc-f8tdv [623.213939ms]
Sep 20 22:01:29.692: INFO: Created: latency-svc-8qj7m
Sep 20 22:01:29.735: INFO: Got endpoints: latency-svc-txh6f [666.884554ms]
Sep 20 22:01:29.742: INFO: Created: latency-svc-qshzw
Sep 20 22:01:29.784: INFO: Got endpoints: latency-svc-7v4fz [704.903457ms]
Sep 20 22:01:29.792: INFO: Created: latency-svc-bblxf
Sep 20 22:01:29.834: INFO: Got endpoints: latency-svc-ds9nq [748.196679ms]
Sep 20 22:01:29.846: INFO: Created: latency-svc-txs8n
Sep 20 22:01:29.884: INFO: Got endpoints: latency-svc-94m24 [675.177849ms]
Sep 20 22:01:29.895: INFO: Created: latency-svc-h594b
Sep 20 22:01:29.935: INFO: Got endpoints: latency-svc-jg5dz [723.497762ms]
Sep 20 22:01:29.943: INFO: Created: latency-svc-jlp9c
Sep 20 22:01:29.984: INFO: Got endpoints: latency-svc-69fkz [746.254815ms]
Sep 20 22:01:29.995: INFO: Created: latency-svc-mb24r
Sep 20 22:01:30.036: INFO: Got endpoints: latency-svc-kmthn [740.575031ms]
Sep 20 22:01:30.045: INFO: Created: latency-svc-sgv4d
Sep 20 22:01:30.085: INFO: Got endpoints: latency-svc-d4tjj [751.58502ms]
Sep 20 22:01:30.100: INFO: Created: latency-svc-qnvfv
Sep 20 22:01:30.134: INFO: Got endpoints: latency-svc-4wbhm [750.333428ms]
Sep 20 22:01:30.145: INFO: Created: latency-svc-v5f75
Sep 20 22:01:30.187: INFO: Got endpoints: latency-svc-g846q [751.550267ms]
Sep 20 22:01:30.201: INFO: Created: latency-svc-hkcrm
Sep 20 22:01:30.234: INFO: Got endpoints: latency-svc-lrgm2 [745.127017ms]
Sep 20 22:01:30.242: INFO: Created: latency-svc-jv4xj
Sep 20 22:01:30.284: INFO: Got endpoints: latency-svc-pwm9z [750.568945ms]
Sep 20 22:01:30.298: INFO: Created: latency-svc-4bz28
Sep 20 22:01:30.334: INFO: Got endpoints: latency-svc-pts4h [750.201018ms]
Sep 20 22:01:30.356: INFO: Created: latency-svc-w5vcj
Sep 20 22:01:30.387: INFO: Got endpoints: latency-svc-wcx4r [751.939657ms]
Sep 20 22:01:30.399: INFO: Created: latency-svc-j8wfq
Sep 20 22:01:30.434: INFO: Got endpoints: latency-svc-8qj7m [749.060074ms]
Sep 20 22:01:30.442: INFO: Created: latency-svc-8hhc5
Sep 20 22:01:30.484: INFO: Got endpoints: latency-svc-qshzw [748.616924ms]
Sep 20 22:01:30.494: INFO: Created: latency-svc-x94wf
Sep 20 22:01:30.535: INFO: Got endpoints: latency-svc-bblxf [750.98332ms]
Sep 20 22:01:30.544: INFO: Created: latency-svc-n6m55
Sep 20 22:01:30.585: INFO: Got endpoints: latency-svc-txs8n [750.437657ms]
Sep 20 22:01:30.594: INFO: Created: latency-svc-qsfbw
Sep 20 22:01:30.636: INFO: Got endpoints: latency-svc-h594b [751.393883ms]
Sep 20 22:01:30.658: INFO: Created: latency-svc-g27mq
Sep 20 22:01:30.684: INFO: Got endpoints: latency-svc-jlp9c [748.229448ms]
Sep 20 22:01:30.691: INFO: Created: latency-svc-xb2hx
Sep 20 22:01:30.735: INFO: Got endpoints: latency-svc-mb24r [750.720569ms]
Sep 20 22:01:30.744: INFO: Created: latency-svc-c7jxk
Sep 20 22:01:30.784: INFO: Got endpoints: latency-svc-sgv4d [748.439094ms]
Sep 20 22:01:30.793: INFO: Created: latency-svc-6cxvm
Sep 20 22:01:30.835: INFO: Got endpoints: latency-svc-qnvfv [749.189162ms]
Sep 20 22:01:30.844: INFO: Created: latency-svc-wrtrf
Sep 20 22:01:30.884: INFO: Got endpoints: latency-svc-v5f75 [749.33663ms]
Sep 20 22:01:30.896: INFO: Created: latency-svc-jhtgd
Sep 20 22:01:30.936: INFO: Got endpoints: latency-svc-hkcrm [748.617909ms]
Sep 20 22:01:30.943: INFO: Created: latency-svc-2j666
Sep 20 22:01:30.984: INFO: Got endpoints: latency-svc-jv4xj [750.018238ms]
Sep 20 22:01:31.000: INFO: Created: latency-svc-lk7kk
Sep 20 22:01:31.034: INFO: Got endpoints: latency-svc-4bz28 [749.961179ms]
Sep 20 22:01:31.044: INFO: Created: latency-svc-m64dq
Sep 20 22:01:31.085: INFO: Got endpoints: latency-svc-w5vcj [750.760229ms]
Sep 20 22:01:31.095: INFO: Created: latency-svc-st7kp
Sep 20 22:01:31.136: INFO: Got endpoints: latency-svc-j8wfq [748.167859ms]
Sep 20 22:01:31.144: INFO: Created: latency-svc-vbs6g
Sep 20 22:01:31.184: INFO: Got endpoints: latency-svc-8hhc5 [749.946013ms]
Sep 20 22:01:31.212: INFO: Created: latency-svc-bbcnl
Sep 20 22:01:31.235: INFO: Got endpoints: latency-svc-x94wf [750.574467ms]
Sep 20 22:01:31.245: INFO: Created: latency-svc-2xkx5
Sep 20 22:01:31.286: INFO: Got endpoints: latency-svc-n6m55 [750.58498ms]
Sep 20 22:01:31.293: INFO: Created: latency-svc-7scj6
Sep 20 22:01:31.334: INFO: Got endpoints: latency-svc-qsfbw [749.000258ms]
Sep 20 22:01:31.342: INFO: Created: latency-svc-gxnq4
Sep 20 22:01:31.385: INFO: Got endpoints: latency-svc-g27mq [749.024407ms]
Sep 20 22:01:31.396: INFO: Created: latency-svc-s2swh
Sep 20 22:01:31.435: INFO: Got endpoints: latency-svc-xb2hx [750.568862ms]
Sep 20 22:01:31.443: INFO: Created: latency-svc-h65x7
Sep 20 22:01:31.485: INFO: Got endpoints: latency-svc-c7jxk [750.24289ms]
Sep 20 22:01:31.500: INFO: Created: latency-svc-f89wv
Sep 20 22:01:31.534: INFO: Got endpoints: latency-svc-6cxvm [749.973121ms]
Sep 20 22:01:31.550: INFO: Created: latency-svc-bwr9n
Sep 20 22:01:31.584: INFO: Got endpoints: latency-svc-wrtrf [748.686765ms]
Sep 20 22:01:31.597: INFO: Created: latency-svc-jjkp7
Sep 20 22:01:31.635: INFO: Got endpoints: latency-svc-jhtgd [750.734903ms]
Sep 20 22:01:31.648: INFO: Created: latency-svc-wpm5c
Sep 20 22:01:31.685: INFO: Got endpoints: latency-svc-2j666 [748.289741ms]
Sep 20 22:01:31.698: INFO: Created: latency-svc-6bszn
Sep 20 22:01:31.735: INFO: Got endpoints: latency-svc-lk7kk [750.279092ms]
Sep 20 22:01:31.748: INFO: Created: latency-svc-r722x
Sep 20 22:01:31.786: INFO: Got endpoints: latency-svc-m64dq [751.898733ms]
Sep 20 22:01:31.807: INFO: Created: latency-svc-rprcc
Sep 20 22:01:31.834: INFO: Got endpoints: latency-svc-st7kp [749.482895ms]
Sep 20 22:01:31.843: INFO: Created: latency-svc-9h69z
Sep 20 22:01:31.885: INFO: Got endpoints: latency-svc-vbs6g [748.922498ms]
Sep 20 22:01:31.895: INFO: Created: latency-svc-5tq6t
Sep 20 22:01:31.936: INFO: Got endpoints: latency-svc-bbcnl [752.23645ms]
Sep 20 22:01:31.946: INFO: Created: latency-svc-fr8vd
Sep 20 22:01:31.984: INFO: Got endpoints: latency-svc-2xkx5 [749.243126ms]
Sep 20 22:01:31.996: INFO: Created: latency-svc-vwmkv
Sep 20 22:01:32.034: INFO: Got endpoints: latency-svc-7scj6 [748.637164ms]
Sep 20 22:01:32.054: INFO: Created: latency-svc-2xhp6
Sep 20 22:01:32.085: INFO: Got endpoints: latency-svc-gxnq4 [750.851491ms]
Sep 20 22:01:32.107: INFO: Created: latency-svc-4m6rb
Sep 20 22:01:32.134: INFO: Got endpoints: latency-svc-s2swh [749.22323ms]
Sep 20 22:01:32.142: INFO: Created: latency-svc-srxm7
Sep 20 22:01:32.186: INFO: Got endpoints: latency-svc-h65x7 [750.880806ms]
Sep 20 22:01:32.193: INFO: Created: latency-svc-sr622
Sep 20 22:01:32.235: INFO: Got endpoints: latency-svc-f89wv [749.988278ms]
Sep 20 22:01:32.246: INFO: Created: latency-svc-vnsz4
Sep 20 22:01:32.284: INFO: Got endpoints: latency-svc-bwr9n [749.763402ms]
Sep 20 22:01:32.293: INFO: Created: latency-svc-qdnm7
Sep 20 22:01:32.334: INFO: Got endpoints: latency-svc-jjkp7 [750.268142ms]
Sep 20 22:01:32.344: INFO: Created: latency-svc-b2ktx
Sep 20 22:01:32.385: INFO: Got endpoints: latency-svc-wpm5c [750.433858ms]
Sep 20 22:01:32.393: INFO: Created: latency-svc-sfvk5
Sep 20 22:01:32.435: INFO: Got endpoints: latency-svc-6bszn [749.365445ms]
Sep 20 22:01:32.451: INFO: Created: latency-svc-nhcpp
Sep 20 22:01:32.486: INFO: Got endpoints: latency-svc-r722x [750.72598ms]
Sep 20 22:01:32.496: INFO: Created: latency-svc-tsnr4
Sep 20 22:01:32.536: INFO: Got endpoints: latency-svc-rprcc [749.177691ms]
Sep 20 22:01:32.543: INFO: Created: latency-svc-922mq
Sep 20 22:01:32.584: INFO: Got endpoints: latency-svc-9h69z [749.549751ms]
Sep 20 22:01:32.655: INFO: Got endpoints: latency-svc-5tq6t [770.087627ms]
Sep 20 22:01:32.655: INFO: Created: latency-svc-79jb6
Sep 20 22:01:32.682: INFO: Created: latency-svc-8xvsn
Sep 20 22:01:32.686: INFO: Got endpoints: latency-svc-fr8vd [749.289408ms]
Sep 20 22:01:32.694: INFO: Created: latency-svc-qthvg
Sep 20 22:01:32.735: INFO: Got endpoints: latency-svc-vwmkv [750.467187ms]
Sep 20 22:01:32.744: INFO: Created: latency-svc-wsrcf
Sep 20 22:01:32.784: INFO: Got endpoints: latency-svc-2xhp6 [749.752029ms]
Sep 20 22:01:32.797: INFO: Created: latency-svc-7c2t2
Sep 20 22:01:32.835: INFO: Got endpoints: latency-svc-4m6rb [750.328274ms]
Sep 20 22:01:32.843: INFO: Created: latency-svc-jhvks
Sep 20 22:01:32.885: INFO: Got endpoints: latency-svc-srxm7 [749.889721ms]
Sep 20 22:01:32.899: INFO: Created: latency-svc-lf4wk
Sep 20 22:01:32.934: INFO: Got endpoints: latency-svc-sr622 [748.214686ms]
Sep 20 22:01:32.944: INFO: Created: latency-svc-8xght
Sep 20 22:01:32.984: INFO: Got endpoints: latency-svc-vnsz4 [748.674695ms]
Sep 20 22:01:32.992: INFO: Created: latency-svc-j94g7
Sep 20 22:01:33.034: INFO: Got endpoints: latency-svc-qdnm7 [749.837971ms]
Sep 20 22:01:33.054: INFO: Created: latency-svc-rsmf2
Sep 20 22:01:33.084: INFO: Got endpoints: latency-svc-b2ktx [749.886691ms]
Sep 20 22:01:33.095: INFO: Created: latency-svc-6hsbq
Sep 20 22:01:33.135: INFO: Got endpoints: latency-svc-sfvk5 [749.607297ms]
Sep 20 22:01:33.146: INFO: Created: latency-svc-46dq9
Sep 20 22:01:33.187: INFO: Got endpoints: latency-svc-nhcpp [751.91036ms]
Sep 20 22:01:33.194: INFO: Created: latency-svc-55spz
Sep 20 22:01:33.235: INFO: Got endpoints: latency-svc-tsnr4 [749.538592ms]
Sep 20 22:01:33.245: INFO: Created: latency-svc-5mb8d
Sep 20 22:01:33.285: INFO: Got endpoints: latency-svc-922mq [749.238128ms]
Sep 20 22:01:33.297: INFO: Created: latency-svc-w5876
Sep 20 22:01:33.334: INFO: Got endpoints: latency-svc-79jb6 [749.94547ms]
Sep 20 22:01:33.343: INFO: Created: latency-svc-pw2j2
Sep 20 22:01:33.384: INFO: Got endpoints: latency-svc-8xvsn [729.052716ms]
Sep 20 22:01:33.391: INFO: Created: latency-svc-9jr5h
Sep 20 22:01:33.434: INFO: Got endpoints: latency-svc-qthvg [748.733488ms]
Sep 20 22:01:33.441: INFO: Created: latency-svc-98q48
Sep 20 22:01:33.486: INFO: Got endpoints: latency-svc-wsrcf [751.181103ms]
Sep 20 22:01:33.497: INFO: Created: latency-svc-8ngrg
Sep 20 22:01:33.533: INFO: Got endpoints: latency-svc-7c2t2 [748.851589ms]
Sep 20 22:01:33.546: INFO: Created: latency-svc-m4rh8
Sep 20 22:01:33.584: INFO: Got endpoints: latency-svc-jhvks [748.721585ms]
Sep 20 22:01:33.596: INFO: Created: latency-svc-s4j6t
Sep 20 22:01:33.635: INFO: Got endpoints: latency-svc-lf4wk [750.394891ms]
Sep 20 22:01:33.643: INFO: Created: latency-svc-mss7q
Sep 20 22:01:33.685: INFO: Got endpoints: latency-svc-8xght [750.300213ms]
Sep 20 22:01:33.694: INFO: Created: latency-svc-26r5x
Sep 20 22:01:33.733: INFO: Got endpoints: latency-svc-j94g7 [749.031681ms]
Sep 20 22:01:33.745: INFO: Created: latency-svc-psl82
Sep 20 22:01:33.785: INFO: Got endpoints: latency-svc-rsmf2 [750.652757ms]
Sep 20 22:01:33.799: INFO: Created: latency-svc-xwqgm
Sep 20 22:01:33.840: INFO: Got endpoints: latency-svc-6hsbq [755.678444ms]
Sep 20 22:01:33.848: INFO: Created: latency-svc-wslmn
Sep 20 22:01:33.884: INFO: Got endpoints: latency-svc-46dq9 [749.13218ms]
Sep 20 22:01:33.891: INFO: Created: latency-svc-mjwg4
Sep 20 22:01:33.935: INFO: Got endpoints: latency-svc-55spz [748.420055ms]
Sep 20 22:01:33.947: INFO: Created: latency-svc-q2xsq
Sep 20 22:01:33.984: INFO: Got endpoints: latency-svc-5mb8d [749.022183ms]
Sep 20 22:01:33.996: INFO: Created: latency-svc-vmf4x
Sep 20 22:01:34.044: INFO: Got endpoints: latency-svc-w5876 [759.351196ms]
Sep 20 22:01:34.073: INFO: Created: latency-svc-8ftxx
Sep 20 22:01:34.084: INFO: Got endpoints: latency-svc-pw2j2 [750.235784ms]
Sep 20 22:01:34.093: INFO: Created: latency-svc-xwwzj
Sep 20 22:01:34.137: INFO: Got endpoints: latency-svc-9jr5h [752.533736ms]
Sep 20 22:01:34.145: INFO: Created: latency-svc-rfdfv
Sep 20 22:01:34.185: INFO: Got endpoints: latency-svc-98q48 [749.915525ms]
Sep 20 22:01:34.195: INFO: Created: latency-svc-l7wd6
Sep 20 22:01:34.235: INFO: Got endpoints: latency-svc-8ngrg [748.33406ms]
Sep 20 22:01:34.244: INFO: Created: latency-svc-4zct6
Sep 20 22:01:34.285: INFO: Got endpoints: latency-svc-m4rh8 [751.064669ms]
Sep 20 22:01:34.295: INFO: Created: latency-svc-fpggj
Sep 20 22:01:34.334: INFO: Got endpoints: latency-svc-s4j6t [749.55177ms]
Sep 20 22:01:34.340: INFO: Created: latency-svc-26xbz
Sep 20 22:01:34.384: INFO: Got endpoints: latency-svc-mss7q [748.823905ms]
Sep 20 22:01:34.392: INFO: Created: latency-svc-qdxwn
Sep 20 22:01:34.434: INFO: Got endpoints: latency-svc-26r5x [749.615542ms]
Sep 20 22:01:34.445: INFO: Created: latency-svc-g9wc8
Sep 20 22:01:34.485: INFO: Got endpoints: latency-svc-psl82 [751.639871ms]
Sep 20 22:01:34.496: INFO: Created: latency-svc-5qpj6
Sep 20 22:01:34.536: INFO: Got endpoints: latency-svc-xwqgm [750.787323ms]
Sep 20 22:01:34.546: INFO: Created: latency-svc-5mtkb
Sep 20 22:01:34.586: INFO: Got endpoints: latency-svc-wslmn [745.502915ms]
Sep 20 22:01:34.594: INFO: Created: latency-svc-d494t
Sep 20 22:01:34.639: INFO: Got endpoints: latency-svc-mjwg4 [754.697697ms]
Sep 20 22:01:34.647: INFO: Created: latency-svc-8dzgp
Sep 20 22:01:34.685: INFO: Got endpoints: latency-svc-q2xsq [749.944268ms]
Sep 20 22:01:34.694: INFO: Created: latency-svc-ttzlq
Sep 20 22:01:34.735: INFO: Got endpoints: latency-svc-vmf4x [750.151491ms]
Sep 20 22:01:34.750: INFO: Created: latency-svc-qswpt
Sep 20 22:01:34.785: INFO: Got endpoints: latency-svc-8ftxx [740.040298ms]
Sep 20 22:01:34.796: INFO: Created: latency-svc-f29z7
Sep 20 22:01:34.836: INFO: Got endpoints: latency-svc-xwwzj [751.389141ms]
Sep 20 22:01:34.848: INFO: Created: latency-svc-s4gkx
Sep 20 22:01:34.884: INFO: Got endpoints: latency-svc-rfdfv [747.601504ms]
Sep 20 22:01:34.900: INFO: Created: latency-svc-7m476
Sep 20 22:01:34.934: INFO: Got endpoints: latency-svc-l7wd6 [749.570836ms]
Sep 20 22:01:34.945: INFO: Created: latency-svc-dzph2
Sep 20 22:01:34.987: INFO: Got endpoints: latency-svc-4zct6 [752.032054ms]
Sep 20 22:01:35.006: INFO: Created: latency-svc-58scj
Sep 20 22:01:35.037: INFO: Got endpoints: latency-svc-fpggj [751.865816ms]
Sep 20 22:01:35.047: INFO: Created: latency-svc-j6tcb
Sep 20 22:01:35.087: INFO: Got endpoints: latency-svc-26xbz [752.805711ms]
Sep 20 22:01:35.099: INFO: Created: latency-svc-564hn
Sep 20 22:01:35.134: INFO: Got endpoints: latency-svc-qdxwn [749.505451ms]
Sep 20 22:01:35.157: INFO: Created: latency-svc-n8zm6
Sep 20 22:01:35.186: INFO: Got endpoints: latency-svc-g9wc8 [751.012655ms]
Sep 20 22:01:35.200: INFO: Created: latency-svc-tt67p
Sep 20 22:01:35.236: INFO: Got endpoints: latency-svc-5qpj6 [751.077993ms]
Sep 20 22:01:35.246: INFO: Created: latency-svc-f66l6
Sep 20 22:01:35.289: INFO: Got endpoints: latency-svc-5mtkb [752.953584ms]
Sep 20 22:01:35.297: INFO: Created: latency-svc-vbslr
Sep 20 22:01:35.335: INFO: Got endpoints: latency-svc-d494t [749.014023ms]
Sep 20 22:01:35.344: INFO: Created: latency-svc-6m4lw
Sep 20 22:01:35.384: INFO: Got endpoints: latency-svc-8dzgp [744.785763ms]
Sep 20 22:01:35.396: INFO: Created: latency-svc-62tbx
Sep 20 22:01:35.435: INFO: Got endpoints: latency-svc-ttzlq [749.846943ms]
Sep 20 22:01:35.445: INFO: Created: latency-svc-c2zz6
Sep 20 22:01:35.485: INFO: Got endpoints: latency-svc-qswpt [750.390677ms]
Sep 20 22:01:35.498: INFO: Created: latency-svc-ddq7d
Sep 20 22:01:35.534: INFO: Got endpoints: latency-svc-f29z7 [749.802415ms]
Sep 20 22:01:35.544: INFO: Created: latency-svc-rrjp2
Sep 20 22:01:35.585: INFO: Got endpoints: latency-svc-s4gkx [749.338495ms]
Sep 20 22:01:35.594: INFO: Created: latency-svc-m94gn
Sep 20 22:01:35.635: INFO: Got endpoints: latency-svc-7m476 [750.046623ms]
Sep 20 22:01:35.643: INFO: Created: latency-svc-zs8p7
Sep 20 22:01:35.686: INFO: Got endpoints: latency-svc-dzph2 [751.246245ms]
Sep 20 22:01:35.696: INFO: Created: latency-svc-k6fg4
Sep 20 22:01:35.735: INFO: Got endpoints: latency-svc-58scj [748.307815ms]
Sep 20 22:01:35.743: INFO: Created: latency-svc-f29kj
Sep 20 22:01:35.784: INFO: Got endpoints: latency-svc-j6tcb [746.820981ms]
Sep 20 22:01:35.796: INFO: Created: latency-svc-vv5nd
Sep 20 22:01:35.836: INFO: Got endpoints: latency-svc-564hn [748.850529ms]
Sep 20 22:01:35.846: INFO: Created: latency-svc-rj2wl
Sep 20 22:01:35.885: INFO: Got endpoints: latency-svc-n8zm6 [750.469393ms]
Sep 20 22:01:35.893: INFO: Created: latency-svc-ldxwz
Sep 20 22:01:35.934: INFO: Got endpoints: latency-svc-tt67p [748.710913ms]
Sep 20 22:01:35.946: INFO: Created: latency-svc-ht8z4
Sep 20 22:01:35.985: INFO: Got endpoints: latency-svc-f66l6 [748.690771ms]
Sep 20 22:01:35.993: INFO: Created: latency-svc-vpntz
Sep 20 22:01:36.034: INFO: Got endpoints: latency-svc-vbslr [745.00849ms]
Sep 20 22:01:36.044: INFO: Created: latency-svc-tqzzh
Sep 20 22:01:36.085: INFO: Got endpoints: latency-svc-6m4lw [749.903448ms]
Sep 20 22:01:36.094: INFO: Created: latency-svc-x8w95
Sep 20 22:01:36.135: INFO: Got endpoints: latency-svc-62tbx [751.029502ms]
Sep 20 22:01:36.145: INFO: Created: latency-svc-76nqm
Sep 20 22:01:36.187: INFO: Got endpoints: latency-svc-c2zz6 [752.144032ms]
Sep 20 22:01:36.198: INFO: Created: latency-svc-jg9jc
Sep 20 22:01:36.235: INFO: Got endpoints: latency-svc-ddq7d [749.967909ms]
Sep 20 22:01:36.249: INFO: Created: latency-svc-djzjj
Sep 20 22:01:36.284: INFO: Got endpoints: latency-svc-rrjp2 [749.816191ms]
Sep 20 22:01:36.295: INFO: Created: latency-svc-jm9ld
Sep 20 22:01:36.336: INFO: Got endpoints: latency-svc-m94gn [750.679741ms]
Sep 20 22:01:36.348: INFO: Created: latency-svc-h9zpw
Sep 20 22:01:36.386: INFO: Got endpoints: latency-svc-zs8p7 [750.624709ms]
Sep 20 22:01:36.395: INFO: Created: latency-svc-5r9zt
Sep 20 22:01:36.436: INFO: Got endpoints: latency-svc-k6fg4 [750.074504ms]
Sep 20 22:01:36.487: INFO: Created: latency-svc-psn49
Sep 20 22:01:36.489: INFO: Got endpoints: latency-svc-f29kj [754.052889ms]
Sep 20 22:01:36.546: INFO: Got endpoints: latency-svc-vv5nd [762.18696ms]
Sep 20 22:01:36.585: INFO: Got endpoints: latency-svc-rj2wl [749.072018ms]
Sep 20 22:01:36.634: INFO: Got endpoints: latency-svc-ldxwz [749.084918ms]
Sep 20 22:01:36.684: INFO: Got endpoints: latency-svc-ht8z4 [748.848134ms]
Sep 20 22:01:36.734: INFO: Got endpoints: latency-svc-vpntz [748.783204ms]
Sep 20 22:01:36.786: INFO: Got endpoints: latency-svc-tqzzh [751.541283ms]
Sep 20 22:01:36.834: INFO: Got endpoints: latency-svc-x8w95 [748.711078ms]
Sep 20 22:01:36.884: INFO: Got endpoints: latency-svc-76nqm [747.943464ms]
Sep 20 22:01:36.934: INFO: Got endpoints: latency-svc-jg9jc [746.545617ms]
Sep 20 22:01:36.984: INFO: Got endpoints: latency-svc-djzjj [748.771683ms]
Sep 20 22:01:37.034: INFO: Got endpoints: latency-svc-jm9ld [749.073252ms]
Sep 20 22:01:37.088: INFO: Got endpoints: latency-svc-h9zpw [751.66684ms]
Sep 20 22:01:37.134: INFO: Got endpoints: latency-svc-5r9zt [748.554232ms]
Sep 20 22:01:37.185: INFO: Got endpoints: latency-svc-psn49 [749.309437ms]
Sep 20 22:01:37.185: INFO: Latencies: [24.051459ms 36.353215ms 37.548689ms 48.729697ms 51.985189ms 65.307983ms 78.630929ms 93.616799ms 111.446229ms 120.838636ms 126.582317ms 134.87262ms 142.772924ms 149.096377ms 149.503826ms 154.158191ms 155.47632ms 156.046665ms 157.951199ms 161.835002ms 162.193367ms 163.040175ms 165.868131ms 167.93083ms 168.737914ms 174.995673ms 179.878872ms 181.572662ms 183.845622ms 185.036234ms 186.507056ms 188.204129ms 189.035082ms 191.817127ms 194.933028ms 196.940208ms 199.833651ms 202.822447ms 261.374164ms 284.293661ms 287.810123ms 331.412828ms 357.83958ms 393.47946ms 441.762258ms 484.61984ms 516.888524ms 562.249015ms 582.201825ms 623.213939ms 666.884554ms 675.177849ms 704.903457ms 723.497762ms 729.052716ms 740.040298ms 740.575031ms 744.785763ms 745.00849ms 745.127017ms 745.502915ms 746.254815ms 746.545617ms 746.820981ms 747.601504ms 747.943464ms 748.167859ms 748.196679ms 748.214686ms 748.229448ms 748.289741ms 748.307815ms 748.33406ms 748.420055ms 748.439094ms 748.554232ms 748.616924ms 748.617909ms 748.637164ms 748.674695ms 748.686765ms 748.690771ms 748.710913ms 748.711078ms 748.721585ms 748.733488ms 748.771683ms 748.783204ms 748.823905ms 748.848134ms 748.850529ms 748.851589ms 748.922498ms 749.000258ms 749.014023ms 749.022183ms 749.024407ms 749.031681ms 749.060074ms 749.072018ms 749.073252ms 749.084918ms 749.13218ms 749.177691ms 749.189162ms 749.22323ms 749.238128ms 749.243126ms 749.289408ms 749.309437ms 749.33663ms 749.338495ms 749.365445ms 749.482895ms 749.505451ms 749.538592ms 749.549751ms 749.55177ms 749.570836ms 749.607297ms 749.615542ms 749.752029ms 749.763402ms 749.802415ms 749.816191ms 749.837971ms 749.846943ms 749.886691ms 749.889721ms 749.903448ms 749.915525ms 749.944268ms 749.94547ms 749.946013ms 749.961179ms 749.967909ms 749.973121ms 749.988278ms 750.018238ms 750.046623ms 750.074504ms 750.151491ms 750.201018ms 750.235784ms 750.24289ms 750.268142ms 750.279092ms 750.300213ms 750.328274ms 750.333428ms 750.390677ms 750.394891ms 750.433858ms 750.437657ms 750.467187ms 750.469393ms 750.568862ms 750.568945ms 750.574467ms 750.58498ms 750.624709ms 750.652757ms 750.679741ms 750.720569ms 750.72598ms 750.734903ms 750.760229ms 750.787323ms 750.851491ms 750.880806ms 750.98332ms 751.012655ms 751.029502ms 751.064669ms 751.077993ms 751.181103ms 751.246245ms 751.389141ms 751.393883ms 751.541283ms 751.550267ms 751.58502ms 751.639871ms 751.66684ms 751.865816ms 751.898733ms 751.91036ms 751.939657ms 752.032054ms 752.144032ms 752.23645ms 752.533736ms 752.805711ms 752.953584ms 754.052889ms 754.697697ms 755.678444ms 759.351196ms 762.18696ms 770.087627ms]
Sep 20 22:01:37.186: INFO: 50 %ile: 749.073252ms
Sep 20 22:01:37.186: INFO: 90 %ile: 751.550267ms
Sep 20 22:01:37.186: INFO: 99 %ile: 762.18696ms
Sep 20 22:01:37.186: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:01:37.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-2649" for this suite.
Sep 20 22:01:47.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:01:47.313: INFO: namespace svc-latency-2649 deletion completed in 10.124013877s

• [SLOW TEST:21.886 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:01:47.313: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-6e644daf-0aff-4b63-99b7-728024a0cbeb
STEP: Creating a pod to test consume secrets
Sep 20 22:01:47.364: INFO: Waiting up to 5m0s for pod "pod-secrets-18d88d68-dd17-4787-9083-2865e0cf86f5" in namespace "secrets-793" to be "success or failure"
Sep 20 22:01:47.367: INFO: Pod "pod-secrets-18d88d68-dd17-4787-9083-2865e0cf86f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.580563ms
Sep 20 22:01:49.369: INFO: Pod "pod-secrets-18d88d68-dd17-4787-9083-2865e0cf86f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004905068s
STEP: Saw pod success
Sep 20 22:01:49.369: INFO: Pod "pod-secrets-18d88d68-dd17-4787-9083-2865e0cf86f5" satisfied condition "success or failure"
Sep 20 22:01:49.371: INFO: Trying to get logs from node worker-2 pod pod-secrets-18d88d68-dd17-4787-9083-2865e0cf86f5 container secret-volume-test: <nil>
STEP: delete the pod
Sep 20 22:01:49.386: INFO: Waiting for pod pod-secrets-18d88d68-dd17-4787-9083-2865e0cf86f5 to disappear
Sep 20 22:01:49.388: INFO: Pod pod-secrets-18d88d68-dd17-4787-9083-2865e0cf86f5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:01:49.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-793" for this suite.
Sep 20 22:01:55.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:01:55.477: INFO: namespace secrets-793 deletion completed in 6.087728029s
STEP: Destroying namespace "secret-namespace-3285" for this suite.
Sep 20 22:02:01.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:02:01.527: INFO: namespace secret-namespace-3285 deletion completed in 6.049808315s

• [SLOW TEST:14.215 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:02:01.527: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 20 22:02:01.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-9208'
Sep 20 22:02:01.609: INFO: stderr: ""
Sep 20 22:02:01.609: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Sep 20 22:02:01.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 delete pods e2e-test-httpd-pod --namespace=kubectl-9208'
Sep 20 22:02:13.146: INFO: stderr: ""
Sep 20 22:02:13.146: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:02:13.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9208" for this suite.
Sep 20 22:02:19.162: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:02:19.220: INFO: namespace kubectl-9208 deletion completed in 6.070142948s

• [SLOW TEST:17.693 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:02:19.224: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Sep 20 22:02:19.253: INFO: Waiting up to 5m0s for pod "var-expansion-63ff41aa-b8ad-4c15-a544-a202ff98f8a9" in namespace "var-expansion-699" to be "success or failure"
Sep 20 22:02:19.261: INFO: Pod "var-expansion-63ff41aa-b8ad-4c15-a544-a202ff98f8a9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.24205ms
Sep 20 22:02:21.263: INFO: Pod "var-expansion-63ff41aa-b8ad-4c15-a544-a202ff98f8a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00930604s
STEP: Saw pod success
Sep 20 22:02:21.263: INFO: Pod "var-expansion-63ff41aa-b8ad-4c15-a544-a202ff98f8a9" satisfied condition "success or failure"
Sep 20 22:02:21.264: INFO: Trying to get logs from node worker-2 pod var-expansion-63ff41aa-b8ad-4c15-a544-a202ff98f8a9 container dapi-container: <nil>
STEP: delete the pod
Sep 20 22:02:21.280: INFO: Waiting for pod var-expansion-63ff41aa-b8ad-4c15-a544-a202ff98f8a9 to disappear
Sep 20 22:02:21.282: INFO: Pod var-expansion-63ff41aa-b8ad-4c15-a544-a202ff98f8a9 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:02:21.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-699" for this suite.
Sep 20 22:02:27.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:02:27.369: INFO: namespace var-expansion-699 deletion completed in 6.085871917s

• [SLOW TEST:8.146 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:02:27.369: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
I0920 22:02:27.576418      16 request.go:538] Throttling request took 52.925033ms, request: POST:https://10.96.0.1:443/api/v1/namespaces/emptydir-wrapper-5138/replicationcontrollers
Sep 20 22:02:27.627: INFO: Pod name wrapped-volume-race-fcbe3530-ea36-43f6-9921-73106864e815: Found 5 pods out of 5
STEP: Ensuring each pod is running
I0920 22:02:27.701116      16 request.go:538] Throttling request took 73.164209ms, request: GET:https://10.96.0.1:443/api/v1/namespaces/emptydir-wrapper-5138/pods/wrapped-volume-race-fcbe3530-ea36-43f6-9921-73106864e815-df9jz
STEP: deleting ReplicationController wrapped-volume-race-fcbe3530-ea36-43f6-9921-73106864e815 in namespace emptydir-wrapper-5138, will wait for the garbage collector to delete the pods
I0920 22:02:49.716547      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 22:02:49.716620      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 22:02:49.784: INFO: Deleting ReplicationController wrapped-volume-race-fcbe3530-ea36-43f6-9921-73106864e815 took: 16.537909ms
I0920 22:02:50.085133      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-5138/wrapped-volume-race-fcbe3530-ea36-43f6-9921-73106864e815-l9fvc in state Running, deletion time 2019-09-20 22:03:19 +0000 UTC
I0920 22:02:50.085163      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-5138/wrapped-volume-race-fcbe3530-ea36-43f6-9921-73106864e815-df9jz in state Running, deletion time 2019-09-20 22:03:19 +0000 UTC
I0920 22:02:50.085180      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-5138/wrapped-volume-race-fcbe3530-ea36-43f6-9921-73106864e815-hbsbk in state Running, deletion time 2019-09-20 22:03:19 +0000 UTC
I0920 22:02:50.085185      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-5138/wrapped-volume-race-fcbe3530-ea36-43f6-9921-73106864e815-r8bvr in state Running, deletion time 2019-09-20 22:03:19 +0000 UTC
I0920 22:02:50.085194      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-5138/wrapped-volume-race-fcbe3530-ea36-43f6-9921-73106864e815-kgbd4 in state Running, deletion time 2019-09-20 22:03:19 +0000 UTC
Sep 20 22:02:50.085: INFO: Terminating ReplicationController wrapped-volume-race-fcbe3530-ea36-43f6-9921-73106864e815 pods took: 300.819477ms
STEP: Creating RC which spawns configmap-volume pods
Sep 20 22:03:26.501: INFO: Pod name wrapped-volume-race-961f355a-3640-480a-8d00-8c4cc2967c58: Found 0 pods out of 5
Sep 20 22:03:31.505: INFO: Pod name wrapped-volume-race-961f355a-3640-480a-8d00-8c4cc2967c58: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-961f355a-3640-480a-8d00-8c4cc2967c58 in namespace emptydir-wrapper-5138, will wait for the garbage collector to delete the pods
I0920 22:03:43.544740      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 22:03:43.544793      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 22:03:43.624: INFO: Deleting ReplicationController wrapped-volume-race-961f355a-3640-480a-8d00-8c4cc2967c58 took: 4.010682ms
Sep 20 22:03:43.925: INFO: Terminating ReplicationController wrapped-volume-race-961f355a-3640-480a-8d00-8c4cc2967c58 pods took: 300.530866ms
I0920 22:03:43.925099      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-5138/wrapped-volume-race-961f355a-3640-480a-8d00-8c4cc2967c58-4mn7z in state Running, deletion time 2019-09-20 22:04:13 +0000 UTC
I0920 22:03:43.925121      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-5138/wrapped-volume-race-961f355a-3640-480a-8d00-8c4cc2967c58-mq7bw in state Running, deletion time 2019-09-20 22:04:13 +0000 UTC
I0920 22:03:43.925126      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-5138/wrapped-volume-race-961f355a-3640-480a-8d00-8c4cc2967c58-qcbv9 in state Running, deletion time 2019-09-20 22:04:13 +0000 UTC
I0920 22:03:43.925129      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-5138/wrapped-volume-race-961f355a-3640-480a-8d00-8c4cc2967c58-2lvnt in state Running, deletion time 2019-09-20 22:04:13 +0000 UTC
I0920 22:03:43.925133      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-5138/wrapped-volume-race-961f355a-3640-480a-8d00-8c4cc2967c58-p6dwp in state Running, deletion time 2019-09-20 22:04:13 +0000 UTC
STEP: Creating RC which spawns configmap-volume pods
Sep 20 22:04:19.633: INFO: Pod name wrapped-volume-race-de7e6681-4e30-4059-bce1-5ec374d39073: Found 0 pods out of 5
Sep 20 22:04:24.638: INFO: Pod name wrapped-volume-race-de7e6681-4e30-4059-bce1-5ec374d39073: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-de7e6681-4e30-4059-bce1-5ec374d39073 in namespace emptydir-wrapper-5138, will wait for the garbage collector to delete the pods
I0920 22:04:36.691353      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 22:04:36.691378      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 22:04:36.755: INFO: Deleting ReplicationController wrapped-volume-race-de7e6681-4e30-4059-bce1-5ec374d39073 took: 11.069115ms
I0920 22:04:37.055502      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-5138/wrapped-volume-race-de7e6681-4e30-4059-bce1-5ec374d39073-pjbtc in state Running, deletion time 2019-09-20 22:05:06 +0000 UTC
I0920 22:04:37.055633      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-5138/wrapped-volume-race-de7e6681-4e30-4059-bce1-5ec374d39073-7nz8k in state Running, deletion time 2019-09-20 22:05:06 +0000 UTC
I0920 22:04:37.055674      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-5138/wrapped-volume-race-de7e6681-4e30-4059-bce1-5ec374d39073-kxf4b in state Running, deletion time 2019-09-20 22:05:06 +0000 UTC
I0920 22:04:37.055721      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-5138/wrapped-volume-race-de7e6681-4e30-4059-bce1-5ec374d39073-vrk5d in state Running, deletion time 2019-09-20 22:05:06 +0000 UTC
I0920 22:04:37.055754      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-5138/wrapped-volume-race-de7e6681-4e30-4059-bce1-5ec374d39073-pvkv7 in state Running, deletion time 2019-09-20 22:05:06 +0000 UTC
Sep 20 22:04:37.055: INFO: Terminating ReplicationController wrapped-volume-race-de7e6681-4e30-4059-bce1-5ec374d39073 pods took: 300.659316ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:05:17.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5138" for this suite.
Sep 20 22:05:23.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:05:23.366: INFO: namespace emptydir-wrapper-5138 deletion completed in 6.065022001s

• [SLOW TEST:175.997 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:05:23.370: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-723cd74f-a38b-4275-ad69-d4e9deccab07
STEP: Creating a pod to test consume configMaps
Sep 20 22:05:23.400: INFO: Waiting up to 5m0s for pod "pod-configmaps-72df349d-3acd-45ee-a99a-977edea07bbd" in namespace "configmap-7685" to be "success or failure"
Sep 20 22:05:23.402: INFO: Pod "pod-configmaps-72df349d-3acd-45ee-a99a-977edea07bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.812635ms
Sep 20 22:05:25.406: INFO: Pod "pod-configmaps-72df349d-3acd-45ee-a99a-977edea07bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006319433s
Sep 20 22:05:27.412: INFO: Pod "pod-configmaps-72df349d-3acd-45ee-a99a-977edea07bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011800498s
STEP: Saw pod success
Sep 20 22:05:27.412: INFO: Pod "pod-configmaps-72df349d-3acd-45ee-a99a-977edea07bbd" satisfied condition "success or failure"
Sep 20 22:05:27.418: INFO: Trying to get logs from node worker-2 pod pod-configmaps-72df349d-3acd-45ee-a99a-977edea07bbd container configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 22:05:27.478: INFO: Waiting for pod pod-configmaps-72df349d-3acd-45ee-a99a-977edea07bbd to disappear
Sep 20 22:05:27.481: INFO: Pod pod-configmaps-72df349d-3acd-45ee-a99a-977edea07bbd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:05:27.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7685" for this suite.
Sep 20 22:05:33.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:05:33.575: INFO: namespace configmap-7685 deletion completed in 6.090466871s

• [SLOW TEST:10.205 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:05:33.575: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Sep 20 22:05:33.644: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 22:05:35.288: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:05:44.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7222" for this suite.
Sep 20 22:05:50.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:05:50.485: INFO: namespace crd-publish-openapi-7222 deletion completed in 6.054242167s

• [SLOW TEST:16.911 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:05:50.487: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:06:01.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4192" for this suite.
Sep 20 22:06:07.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:06:07.638: INFO: namespace resourcequota-4192 deletion completed in 6.081793544s

• [SLOW TEST:17.152 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:06:07.642: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 22:06:07.685: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b96275fd-ff46-4df0-a126-a8a71ef60cf3" in namespace "projected-4369" to be "success or failure"
Sep 20 22:06:07.686: INFO: Pod "downwardapi-volume-b96275fd-ff46-4df0-a126-a8a71ef60cf3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.46546ms
Sep 20 22:06:09.696: INFO: Pod "downwardapi-volume-b96275fd-ff46-4df0-a126-a8a71ef60cf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011921172s
STEP: Saw pod success
Sep 20 22:06:09.697: INFO: Pod "downwardapi-volume-b96275fd-ff46-4df0-a126-a8a71ef60cf3" satisfied condition "success or failure"
Sep 20 22:06:09.702: INFO: Trying to get logs from node worker-2 pod downwardapi-volume-b96275fd-ff46-4df0-a126-a8a71ef60cf3 container client-container: <nil>
STEP: delete the pod
Sep 20 22:06:09.742: INFO: Waiting for pod downwardapi-volume-b96275fd-ff46-4df0-a126-a8a71ef60cf3 to disappear
Sep 20 22:06:09.747: INFO: Pod downwardapi-volume-b96275fd-ff46-4df0-a126-a8a71ef60cf3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:06:09.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4369" for this suite.
Sep 20 22:06:15.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:06:15.849: INFO: namespace projected-4369 deletion completed in 6.096685005s

• [SLOW TEST:8.208 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:06:15.850: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Sep 20 22:06:15.872: INFO: PodSpec: initContainers in spec.initContainers
Sep 20 22:07:01.868: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-87ff4b72-e43b-4446-8f1e-8b4348cdfa37", GenerateName:"", Namespace:"init-container-0", SelfLink:"/api/v1/namespaces/init-container-0/pods/pod-init-87ff4b72-e43b-4446-8f1e-8b4348cdfa37", UID:"fdd0a800-d911-482d-b28d-53513307fe5e", ResourceVersion:"8113", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63704613975, loc:(*time.Location)(0x84be2c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"872710846"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-j5667", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0066472c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-j5667", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-j5667", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-j5667", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004086d98), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"worker-2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002d241e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004086eb0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004086ed0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004086ed8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004086edc), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704613975, loc:(*time.Location)(0x84be2c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704613975, loc:(*time.Location)(0x84be2c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704613975, loc:(*time.Location)(0x84be2c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704613975, loc:(*time.Location)(0x84be2c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.5.102", PodIP:"10.40.0.3", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.40.0.3"}}, StartTime:(*v1.Time)(0xc007eaac80), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0035c8310)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0035c8380)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://2016413962cb1e01c59922adba119e014c72afa9385072fd0b99db1fdafe545e", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc007eaacc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc007eaaca0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc004086fef)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:07:01.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-0" for this suite.
Sep 20 22:07:29.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:07:29.958: INFO: namespace init-container-0 deletion completed in 28.086113755s

• [SLOW TEST:74.108 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:07:29.959: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 22:07:30.036: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-9dc25458-ab7d-41d2-9396-f547b4476b6d" in namespace "security-context-test-0" to be "success or failure"
Sep 20 22:07:30.038: INFO: Pod "busybox-readonly-false-9dc25458-ab7d-41d2-9396-f547b4476b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.477169ms
Sep 20 22:07:32.041: INFO: Pod "busybox-readonly-false-9dc25458-ab7d-41d2-9396-f547b4476b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005366103s
Sep 20 22:07:34.057: INFO: Pod "busybox-readonly-false-9dc25458-ab7d-41d2-9396-f547b4476b6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021176055s
Sep 20 22:07:34.057: INFO: Pod "busybox-readonly-false-9dc25458-ab7d-41d2-9396-f547b4476b6d" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:07:34.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-0" for this suite.
Sep 20 22:07:40.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:07:40.135: INFO: namespace security-context-test-0 deletion completed in 6.068632004s

• [SLOW TEST:10.176 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:07:40.136: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0920 22:07:40.739106      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 20 22:07:40.739: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:07:40.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1890" for this suite.
Sep 20 22:07:46.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:07:46.834: INFO: namespace gc-1890 deletion completed in 6.087227419s

• [SLOW TEST:6.698 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:07:46.834: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 22:07:46.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 version'
Sep 20 22:07:46.903: INFO: stderr: ""
Sep 20 22:07:46.903: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.0\", GitCommit:\"2bd9643cee5b3b3a5ecbd3af49d09018f0773c77\", GitTreeState:\"clean\", BuildDate:\"2019-09-18T14:36:53Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.0\", GitCommit:\"2bd9643cee5b3b3a5ecbd3af49d09018f0773c77\", GitTreeState:\"clean\", BuildDate:\"2019-09-18T14:27:17Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:07:46.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9228" for this suite.
Sep 20 22:07:52.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:07:52.964: INFO: namespace kubectl-9228 deletion completed in 6.058806015s

• [SLOW TEST:6.130 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:07:52.965: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:08:06.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7131" for this suite.
Sep 20 22:08:12.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:08:12.199: INFO: namespace namespaces-7131 deletion completed in 6.081267287s
STEP: Destroying namespace "nsdeletetest-8332" for this suite.
Sep 20 22:08:12.201: INFO: Namespace nsdeletetest-8332 was already deleted
STEP: Destroying namespace "nsdeletetest-6959" for this suite.
Sep 20 22:08:18.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:08:18.283: INFO: namespace nsdeletetest-6959 deletion completed in 6.082263593s

• [SLOW TEST:25.319 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:08:18.284: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2658.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2658.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2658.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2658.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2658.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2658.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2658.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2658.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2658.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2658.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2658.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2658.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2658.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2658.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2658.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2658.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2658.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2658.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 20 22:09:18.395: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2658.svc.cluster.local from pod dns-2658/dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8: the server could not find the requested resource (get pods dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8)
Sep 20 22:09:18.403: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2658.svc.cluster.local from pod dns-2658/dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8: the server could not find the requested resource (get pods dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8)
Sep 20 22:09:18.411: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2658.svc.cluster.local from pod dns-2658/dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8: the server could not find the requested resource (get pods dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8)
Sep 20 22:09:18.415: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2658.svc.cluster.local from pod dns-2658/dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8: the server could not find the requested resource (get pods dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8)
Sep 20 22:09:18.425: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2658.svc.cluster.local from pod dns-2658/dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8: the server could not find the requested resource (get pods dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8)
Sep 20 22:09:18.428: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2658.svc.cluster.local from pod dns-2658/dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8: the server could not find the requested resource (get pods dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8)
Sep 20 22:09:18.430: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2658.svc.cluster.local from pod dns-2658/dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8: the server could not find the requested resource (get pods dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8)
Sep 20 22:09:18.433: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2658.svc.cluster.local from pod dns-2658/dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8: the server could not find the requested resource (get pods dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8)
Sep 20 22:09:18.438: INFO: Lookups using dns-2658/dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2658.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2658.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2658.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2658.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2658.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2658.svc.cluster.local jessie_udp@dns-test-service-2.dns-2658.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2658.svc.cluster.local]

Sep 20 22:09:23.441: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2658.svc.cluster.local from pod dns-2658/dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8: the server could not find the requested resource (get pods dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8)
Sep 20 22:09:23.443: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2658.svc.cluster.local from pod dns-2658/dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8: the server could not find the requested resource (get pods dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8)
Sep 20 22:09:23.446: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2658.svc.cluster.local from pod dns-2658/dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8: the server could not find the requested resource (get pods dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8)
Sep 20 22:09:23.448: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2658.svc.cluster.local from pod dns-2658/dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8: the server could not find the requested resource (get pods dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8)
Sep 20 22:09:23.455: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2658.svc.cluster.local from pod dns-2658/dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8: the server could not find the requested resource (get pods dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8)
Sep 20 22:09:23.458: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2658.svc.cluster.local from pod dns-2658/dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8: the server could not find the requested resource (get pods dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8)
Sep 20 22:09:23.460: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2658.svc.cluster.local from pod dns-2658/dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8: the server could not find the requested resource (get pods dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8)
Sep 20 22:09:23.462: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2658.svc.cluster.local from pod dns-2658/dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8: the server could not find the requested resource (get pods dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8)
Sep 20 22:09:23.467: INFO: Lookups using dns-2658/dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2658.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2658.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2658.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2658.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2658.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2658.svc.cluster.local jessie_udp@dns-test-service-2.dns-2658.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2658.svc.cluster.local]

Sep 20 22:09:28.540: INFO: DNS probes using dns-2658/dns-test-67ec7f07-9bc5-4999-9691-1d214b1f3ea8 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:09:28.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2658" for this suite.
Sep 20 22:09:34.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:09:34.669: INFO: namespace dns-2658 deletion completed in 6.096480039s

• [SLOW TEST:76.384 seconds]
[sig-network] DNS
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:09:34.669: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 22:09:34.691: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Sep 20 22:09:36.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 --namespace=crd-publish-openapi-7464 create -f -'
Sep 20 22:09:36.963: INFO: stderr: ""
Sep 20 22:09:36.963: INFO: stdout: "e2e-test-crd-publish-openapi-8526-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 20 22:09:36.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 --namespace=crd-publish-openapi-7464 delete e2e-test-crd-publish-openapi-8526-crds test-foo'
Sep 20 22:09:37.019: INFO: stderr: ""
Sep 20 22:09:37.019: INFO: stdout: "e2e-test-crd-publish-openapi-8526-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Sep 20 22:09:37.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 --namespace=crd-publish-openapi-7464 apply -f -'
Sep 20 22:09:37.118: INFO: stderr: ""
Sep 20 22:09:37.118: INFO: stdout: "e2e-test-crd-publish-openapi-8526-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 20 22:09:37.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 --namespace=crd-publish-openapi-7464 delete e2e-test-crd-publish-openapi-8526-crds test-foo'
Sep 20 22:09:37.177: INFO: stderr: ""
Sep 20 22:09:37.178: INFO: stdout: "e2e-test-crd-publish-openapi-8526-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Sep 20 22:09:37.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 --namespace=crd-publish-openapi-7464 create -f -'
Sep 20 22:09:37.265: INFO: rc: 1
Sep 20 22:09:37.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 --namespace=crd-publish-openapi-7464 apply -f -'
Sep 20 22:09:37.350: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Sep 20 22:09:37.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 --namespace=crd-publish-openapi-7464 create -f -'
Sep 20 22:09:37.439: INFO: rc: 1
Sep 20 22:09:37.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 --namespace=crd-publish-openapi-7464 apply -f -'
Sep 20 22:09:37.561: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Sep 20 22:09:37.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 explain e2e-test-crd-publish-openapi-8526-crds'
Sep 20 22:09:37.664: INFO: stderr: ""
Sep 20 22:09:37.664: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8526-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Sep 20 22:09:37.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 explain e2e-test-crd-publish-openapi-8526-crds.metadata'
Sep 20 22:09:37.758: INFO: stderr: ""
Sep 20 22:09:37.758: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8526-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Sep 20 22:09:37.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 explain e2e-test-crd-publish-openapi-8526-crds.spec'
Sep 20 22:09:37.863: INFO: stderr: ""
Sep 20 22:09:37.863: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8526-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Sep 20 22:09:37.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 explain e2e-test-crd-publish-openapi-8526-crds.spec.bars'
Sep 20 22:09:37.954: INFO: stderr: ""
Sep 20 22:09:37.954: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8526-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Sep 20 22:09:37.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 explain e2e-test-crd-publish-openapi-8526-crds.spec.bars2'
Sep 20 22:09:38.059: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:09:39.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7464" for this suite.
Sep 20 22:09:45.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:09:45.719: INFO: namespace crd-publish-openapi-7464 deletion completed in 6.081803146s

• [SLOW TEST:11.050 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:09:45.720: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-8c67c3b4-cd4e-401e-b612-0ad9992cbaeb
STEP: Creating secret with name secret-projected-all-test-volume-9c9daa72-7481-42a8-82fa-cd7e14154faf
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep 20 22:09:45.757: INFO: Waiting up to 5m0s for pod "projected-volume-19c2482e-13bc-4e93-b2e8-f6c13813e1ad" in namespace "projected-7083" to be "success or failure"
Sep 20 22:09:45.762: INFO: Pod "projected-volume-19c2482e-13bc-4e93-b2e8-f6c13813e1ad": Phase="Pending", Reason="", readiness=false. Elapsed: 5.693663ms
Sep 20 22:09:47.765: INFO: Pod "projected-volume-19c2482e-13bc-4e93-b2e8-f6c13813e1ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007982944s
STEP: Saw pod success
Sep 20 22:09:47.765: INFO: Pod "projected-volume-19c2482e-13bc-4e93-b2e8-f6c13813e1ad" satisfied condition "success or failure"
Sep 20 22:09:47.766: INFO: Trying to get logs from node worker-2 pod projected-volume-19c2482e-13bc-4e93-b2e8-f6c13813e1ad container projected-all-volume-test: <nil>
STEP: delete the pod
Sep 20 22:09:47.786: INFO: Waiting for pod projected-volume-19c2482e-13bc-4e93-b2e8-f6c13813e1ad to disappear
Sep 20 22:09:47.789: INFO: Pod projected-volume-19c2482e-13bc-4e93-b2e8-f6c13813e1ad no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:09:47.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7083" for this suite.
Sep 20 22:09:53.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:09:53.866: INFO: namespace projected-7083 deletion completed in 6.075734865s

• [SLOW TEST:8.147 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:09:53.868: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-c5625e0d-17c0-4c23-83e4-5b1ccf723709
Sep 20 22:09:53.898: INFO: Pod name my-hostname-basic-c5625e0d-17c0-4c23-83e4-5b1ccf723709: Found 1 pods out of 1
Sep 20 22:09:53.898: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-c5625e0d-17c0-4c23-83e4-5b1ccf723709" are running
Sep 20 22:09:55.922: INFO: Pod "my-hostname-basic-c5625e0d-17c0-4c23-83e4-5b1ccf723709-ftrql" is running (conditions: [])
Sep 20 22:09:55.922: INFO: Trying to dial the pod
Sep 20 22:10:00.944: INFO: Controller my-hostname-basic-c5625e0d-17c0-4c23-83e4-5b1ccf723709: Got expected result from replica 1 [my-hostname-basic-c5625e0d-17c0-4c23-83e4-5b1ccf723709-ftrql]: "my-hostname-basic-c5625e0d-17c0-4c23-83e4-5b1ccf723709-ftrql", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:10:00.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5625" for this suite.
Sep 20 22:10:06.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:10:07.044: INFO: namespace replication-controller-5625 deletion completed in 6.094011091s

• [SLOW TEST:13.177 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:10:07.047: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 22:10:07.309: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 22:10:09.321: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704614206, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704614206, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704614206, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704614206, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 22:10:12.349: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:10:22.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6309" for this suite.
Sep 20 22:10:28.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:10:28.718: INFO: namespace webhook-6309 deletion completed in 6.052659858s
STEP: Destroying namespace "webhook-6309-markers" for this suite.
Sep 20 22:10:34.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:10:34.803: INFO: namespace webhook-6309-markers deletion completed in 6.08483025s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.764 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:10:34.812: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Sep 20 22:10:34.836: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 20 22:10:34.842: INFO: Waiting for terminating namespaces to be deleted...
Sep 20 22:10:34.843: INFO: 
Logging pods the kubelet thinks is on node controlplane-1 before test
Sep 20 22:10:34.853: INFO: kube-proxy-62wrx from kube-system started at 2019-09-20 21:29:38 +0000 UTC (1 container statuses recorded)
Sep 20 22:10:34.853: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 22:10:34.853: INFO: etcd-controlplane-1 from kube-system started at 2019-09-20 21:29:15 +0000 UTC (1 container statuses recorded)
Sep 20 22:10:34.853: INFO: 	Container etcd ready: true, restart count 0
Sep 20 22:10:34.853: INFO: kube-apiserver-controlplane-1 from kube-system started at 2019-09-20 21:29:15 +0000 UTC (1 container statuses recorded)
Sep 20 22:10:34.853: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep 20 22:10:34.853: INFO: weave-net-n52f9 from kube-system started at 2019-09-20 21:41:30 +0000 UTC (2 container statuses recorded)
Sep 20 22:10:34.854: INFO: 	Container weave ready: true, restart count 0
Sep 20 22:10:34.854: INFO: 	Container weave-npc ready: true, restart count 0
Sep 20 22:10:34.854: INFO: sonobuoy-systemd-logs-daemon-set-63bbc52511204dd7-g8gl8 from sonobuoy started at 2019-09-20 21:43:06 +0000 UTC (2 container statuses recorded)
Sep 20 22:10:34.854: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 22:10:34.854: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 22:10:34.854: INFO: kube-controller-manager-controlplane-1 from kube-system started at 2019-09-20 21:29:15 +0000 UTC (1 container statuses recorded)
Sep 20 22:10:34.854: INFO: 	Container kube-controller-manager ready: true, restart count 0
Sep 20 22:10:34.854: INFO: kube-scheduler-controlplane-1 from kube-system started at 2019-09-20 21:29:15 +0000 UTC (1 container statuses recorded)
Sep 20 22:10:34.854: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep 20 22:10:34.854: INFO: 
Logging pods the kubelet thinks is on node worker-1 before test
Sep 20 22:10:34.864: INFO: weave-net-bnv9z from kube-system started at 2019-09-20 21:41:30 +0000 UTC (2 container statuses recorded)
Sep 20 22:10:34.864: INFO: 	Container weave ready: true, restart count 0
Sep 20 22:10:34.864: INFO: 	Container weave-npc ready: true, restart count 0
Sep 20 22:10:34.864: INFO: coredns-5644d7b6d9-pvh6f from kube-system started at 2019-09-20 21:42:42 +0000 UTC (1 container statuses recorded)
Sep 20 22:10:34.864: INFO: 	Container coredns ready: true, restart count 0
Sep 20 22:10:34.865: INFO: sonobuoy-e2e-job-558ae789799345e3 from sonobuoy started at 2019-09-20 21:43:07 +0000 UTC (2 container statuses recorded)
Sep 20 22:10:34.865: INFO: 	Container e2e ready: true, restart count 0
Sep 20 22:10:34.865: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 22:10:34.865: INFO: sonobuoy-systemd-logs-daemon-set-63bbc52511204dd7-7p8gp from sonobuoy started at 2019-09-20 21:43:07 +0000 UTC (2 container statuses recorded)
Sep 20 22:10:34.865: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 22:10:34.865: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 22:10:34.865: INFO: kube-proxy-dxbgr from kube-system started at 2019-09-20 21:29:43 +0000 UTC (1 container statuses recorded)
Sep 20 22:10:34.865: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 22:10:34.865: INFO: 
Logging pods the kubelet thinks is on node worker-2 before test
Sep 20 22:10:34.869: INFO: sonobuoy-systemd-logs-daemon-set-63bbc52511204dd7-4bfzz from sonobuoy started at 2019-09-20 21:43:07 +0000 UTC (2 container statuses recorded)
Sep 20 22:10:34.869: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 22:10:34.869: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 22:10:34.869: INFO: kube-proxy-dcssj from kube-system started at 2019-09-20 21:29:43 +0000 UTC (1 container statuses recorded)
Sep 20 22:10:34.869: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 22:10:34.869: INFO: weave-net-cwtjw from kube-system started at 2019-09-20 21:41:30 +0000 UTC (2 container statuses recorded)
Sep 20 22:10:34.869: INFO: 	Container weave ready: true, restart count 0
Sep 20 22:10:34.869: INFO: 	Container weave-npc ready: true, restart count 0
Sep 20 22:10:34.869: INFO: sonobuoy from sonobuoy started at 2019-09-20 21:42:42 +0000 UTC (1 container statuses recorded)
Sep 20 22:10:34.869: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 20 22:10:34.869: INFO: coredns-5644d7b6d9-47g5v from kube-system started at 2019-09-20 21:42:42 +0000 UTC (1 container statuses recorded)
Sep 20 22:10:34.869: INFO: 	Container coredns ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node controlplane-1
STEP: verifying the node has the label node worker-1
STEP: verifying the node has the label node worker-2
Sep 20 22:10:34.894: INFO: Pod coredns-5644d7b6d9-47g5v requesting resource cpu=100m on Node worker-2
Sep 20 22:10:34.894: INFO: Pod coredns-5644d7b6d9-pvh6f requesting resource cpu=100m on Node worker-1
Sep 20 22:10:34.894: INFO: Pod etcd-controlplane-1 requesting resource cpu=0m on Node controlplane-1
Sep 20 22:10:34.894: INFO: Pod kube-apiserver-controlplane-1 requesting resource cpu=250m on Node controlplane-1
Sep 20 22:10:34.894: INFO: Pod kube-controller-manager-controlplane-1 requesting resource cpu=200m on Node controlplane-1
Sep 20 22:10:34.894: INFO: Pod kube-proxy-62wrx requesting resource cpu=0m on Node controlplane-1
Sep 20 22:10:34.894: INFO: Pod kube-proxy-dcssj requesting resource cpu=0m on Node worker-2
Sep 20 22:10:34.894: INFO: Pod kube-proxy-dxbgr requesting resource cpu=0m on Node worker-1
Sep 20 22:10:34.894: INFO: Pod kube-scheduler-controlplane-1 requesting resource cpu=100m on Node controlplane-1
Sep 20 22:10:34.894: INFO: Pod weave-net-bnv9z requesting resource cpu=20m on Node worker-1
Sep 20 22:10:34.894: INFO: Pod weave-net-cwtjw requesting resource cpu=20m on Node worker-2
Sep 20 22:10:34.894: INFO: Pod weave-net-n52f9 requesting resource cpu=20m on Node controlplane-1
Sep 20 22:10:34.894: INFO: Pod sonobuoy requesting resource cpu=0m on Node worker-2
Sep 20 22:10:34.894: INFO: Pod sonobuoy-e2e-job-558ae789799345e3 requesting resource cpu=0m on Node worker-1
Sep 20 22:10:34.894: INFO: Pod sonobuoy-systemd-logs-daemon-set-63bbc52511204dd7-4bfzz requesting resource cpu=0m on Node worker-2
Sep 20 22:10:34.894: INFO: Pod sonobuoy-systemd-logs-daemon-set-63bbc52511204dd7-7p8gp requesting resource cpu=0m on Node worker-1
Sep 20 22:10:34.894: INFO: Pod sonobuoy-systemd-logs-daemon-set-63bbc52511204dd7-g8gl8 requesting resource cpu=0m on Node controlplane-1
STEP: Starting Pods to consume most of the cluster CPU.
Sep 20 22:10:34.894: INFO: Creating a pod which consumes cpu=1001m on Node controlplane-1
Sep 20 22:10:34.898: INFO: Creating a pod which consumes cpu=1316m on Node worker-1
Sep 20 22:10:34.904: INFO: Creating a pod which consumes cpu=1316m on Node worker-2
STEP: Creating another pod that requires unavailable amount of CPU.
I0920 22:10:38.947680      16 reflector.go:120] Starting reflector *v1.Event (0s) from k8s.io/kubernetes/test/e2e/common/events.go:136
I0920 22:10:38.947859      16 reflector.go:158] Listing and watching *v1.Event from k8s.io/kubernetes/test/e2e/common/events.go:136
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-312bf828-3ae2-43bb-a98a-419dfdde422e.15c6450b1ecd61db], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3653/filler-pod-312bf828-3ae2-43bb-a98a-419dfdde422e to worker-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-312bf828-3ae2-43bb-a98a-419dfdde422e.15c6450b7eccc3c7], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-312bf828-3ae2-43bb-a98a-419dfdde422e.15c6450b83132b99], Reason = [Created], Message = [Created container filler-pod-312bf828-3ae2-43bb-a98a-419dfdde422e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-312bf828-3ae2-43bb-a98a-419dfdde422e.15c6450b8c203568], Reason = [Started], Message = [Started container filler-pod-312bf828-3ae2-43bb-a98a-419dfdde422e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-adf67b85-6985-46f0-9d45-9ad3a8a50ba7.15c6450b1dd0cce1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3653/filler-pod-adf67b85-6985-46f0-9d45-9ad3a8a50ba7 to controlplane-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-adf67b85-6985-46f0-9d45-9ad3a8a50ba7.15c6450b8f5231f0], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-adf67b85-6985-46f0-9d45-9ad3a8a50ba7.15c6450b92c7ac54], Reason = [Created], Message = [Created container filler-pod-adf67b85-6985-46f0-9d45-9ad3a8a50ba7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-adf67b85-6985-46f0-9d45-9ad3a8a50ba7.15c6450ba12dce95], Reason = [Started], Message = [Started container filler-pod-adf67b85-6985-46f0-9d45-9ad3a8a50ba7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ea08ac8d-fcfa-4e70-a18e-75c252d3106c.15c6450b1df43459], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3653/filler-pod-ea08ac8d-fcfa-4e70-a18e-75c252d3106c to worker-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ea08ac8d-fcfa-4e70-a18e-75c252d3106c.15c6450b7e68c0f4], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ea08ac8d-fcfa-4e70-a18e-75c252d3106c.15c6450b826d6a14], Reason = [Created], Message = [Created container filler-pod-ea08ac8d-fcfa-4e70-a18e-75c252d3106c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ea08ac8d-fcfa-4e70-a18e-75c252d3106c.15c6450b8d279964], Reason = [Started], Message = [Started container filler-pod-ea08ac8d-fcfa-4e70-a18e-75c252d3106c]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15c6450c0f2e54c5], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node controlplane-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker-2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:10:40.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3653" for this suite.
Sep 20 22:10:46.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:10:46.109: INFO: namespace sched-pred-3653 deletion completed in 6.089998054s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
I0920 22:10:46.109653      16 request.go:706] Error in request: resource name may not be empty

• [SLOW TEST:11.298 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:10:46.109: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Sep 20 22:10:46.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 create -f - --namespace=kubectl-3560'
Sep 20 22:10:46.260: INFO: stderr: ""
Sep 20 22:10:46.260: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep 20 22:10:47.270: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 22:10:47.270: INFO: Found 0 / 1
Sep 20 22:10:48.264: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 22:10:48.264: INFO: Found 0 / 1
Sep 20 22:10:49.267: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 22:10:49.267: INFO: Found 0 / 1
Sep 20 22:10:50.279: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 22:10:50.279: INFO: Found 0 / 1
Sep 20 22:10:51.268: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 22:10:51.268: INFO: Found 0 / 1
Sep 20 22:10:52.262: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 22:10:52.263: INFO: Found 0 / 1
Sep 20 22:10:53.267: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 22:10:53.267: INFO: Found 1 / 1
Sep 20 22:10:53.268: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep 20 22:10:53.273: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 22:10:53.273: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 20 22:10:53.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 patch pod redis-master-4g2d5 --namespace=kubectl-3560 -p {"metadata":{"annotations":{"x":"y"}}}'
Sep 20 22:10:53.351: INFO: stderr: ""
Sep 20 22:10:53.351: INFO: stdout: "pod/redis-master-4g2d5 patched\n"
STEP: checking annotations
Sep 20 22:10:53.353: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 22:10:53.353: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:10:53.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3560" for this suite.
Sep 20 22:11:05.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:11:05.454: INFO: namespace kubectl-3560 deletion completed in 12.099381751s

• [SLOW TEST:19.344 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:11:05.455: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-8667/configmap-test-5cefdc92-147a-43e7-8b7d-edc77bb6064c
STEP: Creating a pod to test consume configMaps
Sep 20 22:11:05.478: INFO: Waiting up to 5m0s for pod "pod-configmaps-8ef59a97-a07d-49b2-88a3-c05dd4c21cca" in namespace "configmap-8667" to be "success or failure"
Sep 20 22:11:05.489: INFO: Pod "pod-configmaps-8ef59a97-a07d-49b2-88a3-c05dd4c21cca": Phase="Pending", Reason="", readiness=false. Elapsed: 10.823646ms
Sep 20 22:11:07.492: INFO: Pod "pod-configmaps-8ef59a97-a07d-49b2-88a3-c05dd4c21cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013261774s
STEP: Saw pod success
Sep 20 22:11:07.492: INFO: Pod "pod-configmaps-8ef59a97-a07d-49b2-88a3-c05dd4c21cca" satisfied condition "success or failure"
Sep 20 22:11:07.495: INFO: Trying to get logs from node worker-2 pod pod-configmaps-8ef59a97-a07d-49b2-88a3-c05dd4c21cca container env-test: <nil>
STEP: delete the pod
Sep 20 22:11:07.508: INFO: Waiting for pod pod-configmaps-8ef59a97-a07d-49b2-88a3-c05dd4c21cca to disappear
Sep 20 22:11:07.510: INFO: Pod pod-configmaps-8ef59a97-a07d-49b2-88a3-c05dd4c21cca no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:11:07.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8667" for this suite.
Sep 20 22:11:13.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:11:13.645: INFO: namespace configmap-8667 deletion completed in 6.132462574s

• [SLOW TEST:8.190 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:11:13.646: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 20 22:11:15.705: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:11:15.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6638" for this suite.
Sep 20 22:11:21.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:11:21.816: INFO: namespace container-runtime-6638 deletion completed in 6.099594275s

• [SLOW TEST:8.170 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:11:21.816: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-1d05a334-d63a-4fe4-b293-eee22369117e
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:11:23.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1631" for this suite.
Sep 20 22:11:35.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:11:36.017: INFO: namespace configmap-1631 deletion completed in 12.102789883s

• [SLOW TEST:14.201 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:11:36.018: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:11:44.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-266" for this suite.
Sep 20 22:11:50.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:11:50.135: INFO: namespace job-266 deletion completed in 6.075440419s

• [SLOW TEST:14.117 seconds]
[sig-apps] Job
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:11:50.139: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 20 22:11:52.190: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:11:52.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9078" for this suite.
Sep 20 22:11:58.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:11:58.316: INFO: namespace container-runtime-9078 deletion completed in 6.101834178s

• [SLOW TEST:8.178 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:11:58.317: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 22:11:58.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 create -f - --namespace=kubectl-4530'
Sep 20 22:11:58.448: INFO: stderr: ""
Sep 20 22:11:58.448: INFO: stdout: "replicationcontroller/redis-master created\n"
Sep 20 22:11:58.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 create -f - --namespace=kubectl-4530'
Sep 20 22:11:58.551: INFO: stderr: ""
Sep 20 22:11:58.551: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep 20 22:11:59.554: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 22:11:59.554: INFO: Found 0 / 1
Sep 20 22:12:00.557: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 22:12:00.557: INFO: Found 1 / 1
Sep 20 22:12:00.557: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 20 22:12:00.563: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 22:12:00.563: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 20 22:12:00.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 describe pod redis-master-88ndq --namespace=kubectl-4530'
Sep 20 22:12:00.648: INFO: stderr: ""
Sep 20 22:12:00.648: INFO: stdout: "Name:         redis-master-88ndq\nNamespace:    kubectl-4530\nPriority:     0\nNode:         worker-2/192.168.5.102\nStart Time:   Fri, 20 Sep 2019 22:11:58 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nStatus:       Running\nIP:           10.40.0.4\nIPs:\n  IP:           10.40.0.4\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://9789528aaae53daab62054c75769273a768cc65db29371812b0e5c27aa8253a9\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 20 Sep 2019 22:11:59 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-mqdp6 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-mqdp6:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-mqdp6\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From               Message\n  ----    ------     ----       ----               -------\n  Normal  Scheduled  <unknown>  default-scheduler  Successfully assigned kubectl-4530/redis-master-88ndq to worker-2\n  Normal  Pulled     1s         kubelet, worker-2  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    1s         kubelet, worker-2  Created container redis-master\n  Normal  Started    1s         kubelet, worker-2  Started container redis-master\n"
Sep 20 22:12:00.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 describe rc redis-master --namespace=kubectl-4530'
Sep 20 22:12:00.713: INFO: stderr: ""
Sep 20 22:12:00.713: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-4530\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-88ndq\n"
Sep 20 22:12:00.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 describe service redis-master --namespace=kubectl-4530'
Sep 20 22:12:00.774: INFO: stderr: ""
Sep 20 22:12:00.774: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-4530\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.111.244.43\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.40.0.4:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep 20 22:12:00.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 describe node controlplane-1'
Sep 20 22:12:00.840: INFO: stderr: ""
Sep 20 22:12:00.840: INFO: stdout: "Name:               controlplane-1\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=controlplane-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 20 Sep 2019 21:29:19 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 20 Sep 2019 21:42:25 +0000   Fri, 20 Sep 2019 21:42:25 +0000   WeaveIsUp                    Weave pod has set this\n  MemoryPressure       False   Fri, 20 Sep 2019 22:11:52 +0000   Fri, 20 Sep 2019 21:29:18 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 20 Sep 2019 22:11:52 +0000   Fri, 20 Sep 2019 21:29:18 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 20 Sep 2019 22:11:52 +0000   Fri, 20 Sep 2019 21:29:18 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 20 Sep 2019 22:11:52 +0000   Fri, 20 Sep 2019 21:42:40 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.5.11\n  Hostname:    controlplane-1\nCapacity:\n cpu:                2\n ephemeral-storage:  64800356Ki\n hugepages-2Mi:      0\n memory:             2041132Ki\n pods:               110\nAllocatable:\n cpu:                2\n ephemeral-storage:  59720007991\n hugepages-2Mi:      0\n memory:             1938732Ki\n pods:               110\nSystem Info:\n Machine ID:                 bc7fb9af754f424788ca4a5e4bab7dec\n System UUID:                383861F3-0CE0-481E-A3F8-8B5553C29214\n Boot ID:                    5329fa91-4c18-4b7c-8ebf-35fdd4b2db91\n Kernel Version:             4.15.0-29-generic\n OS Image:                   Ubuntu 18.04.1 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.7\n Kubelet Version:            v1.16.0\n Kube-Proxy Version:         v1.16.0\nNon-terminated Pods:         (7 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                etcd-controlplane-1                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         41m\n  kube-system                kube-apiserver-controlplane-1                              250m (12%)    0 (0%)      0 (0%)           0 (0%)         41m\n  kube-system                kube-controller-manager-controlplane-1                     200m (10%)    0 (0%)      0 (0%)           0 (0%)         41m\n  kube-system                kube-proxy-62wrx                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         42m\n  kube-system                kube-scheduler-controlplane-1                              100m (5%)     0 (0%)      0 (0%)           0 (0%)         41m\n  kube-system                weave-net-n52f9                                            20m (1%)      0 (0%)      0 (0%)           0 (0%)         30m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-63bbc52511204dd7-g8gl8    0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                570m (28%)  0 (0%)\n  memory             0 (0%)      0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:\n  Type    Reason                   Age                From                        Message\n  ----    ------                   ----               ----                        -------\n  Normal  NodeHasSufficientMemory  42m (x8 over 42m)  kubelet, controlplane-1     Node controlplane-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    42m (x7 over 42m)  kubelet, controlplane-1     Node controlplane-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     42m (x8 over 42m)  kubelet, controlplane-1     Node controlplane-1 status is now: NodeHasSufficientPID\n  Normal  Starting                 42m                kube-proxy, controlplane-1  Starting kube-proxy.\n  Normal  NodeReady                29m                kubelet, controlplane-1     Node controlplane-1 status is now: NodeReady\n"
Sep 20 22:12:00.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 describe namespace kubectl-4530'
Sep 20 22:12:00.899: INFO: stderr: ""
Sep 20 22:12:00.899: INFO: stdout: "Name:         kubectl-4530\nLabels:       e2e-framework=kubectl\n              e2e-run=4bfd9ca6-3642-4c8a-99a9-b535da36cab7\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:12:00.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4530" for this suite.
Sep 20 22:12:28.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:12:28.963: INFO: namespace kubectl-4530 deletion completed in 28.062192848s

• [SLOW TEST:30.646 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:12:28.963: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-641d77d4-59db-42a1-8d4f-6a5f447e7a24
STEP: Creating a pod to test consume configMaps
Sep 20 22:12:29.045: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3b3d8816-9f52-4902-b518-5c26f6a2032c" in namespace "projected-8641" to be "success or failure"
Sep 20 22:12:29.052: INFO: Pod "pod-projected-configmaps-3b3d8816-9f52-4902-b518-5c26f6a2032c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.869959ms
Sep 20 22:12:31.055: INFO: Pod "pod-projected-configmaps-3b3d8816-9f52-4902-b518-5c26f6a2032c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010081495s
Sep 20 22:12:33.057: INFO: Pod "pod-projected-configmaps-3b3d8816-9f52-4902-b518-5c26f6a2032c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01200913s
STEP: Saw pod success
Sep 20 22:12:33.057: INFO: Pod "pod-projected-configmaps-3b3d8816-9f52-4902-b518-5c26f6a2032c" satisfied condition "success or failure"
Sep 20 22:12:33.058: INFO: Trying to get logs from node worker-2 pod pod-projected-configmaps-3b3d8816-9f52-4902-b518-5c26f6a2032c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 22:12:33.072: INFO: Waiting for pod pod-projected-configmaps-3b3d8816-9f52-4902-b518-5c26f6a2032c to disappear
Sep 20 22:12:33.075: INFO: Pod pod-projected-configmaps-3b3d8816-9f52-4902-b518-5c26f6a2032c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:12:33.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8641" for this suite.
Sep 20 22:12:39.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:12:39.174: INFO: namespace projected-8641 deletion completed in 6.096262722s

• [SLOW TEST:10.211 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:12:39.174: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-c9tf
STEP: Creating a pod to test atomic-volume-subpath
Sep 20 22:12:39.205: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-c9tf" in namespace "subpath-3826" to be "success or failure"
Sep 20 22:12:39.206: INFO: Pod "pod-subpath-test-secret-c9tf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.367628ms
Sep 20 22:12:41.212: INFO: Pod "pod-subpath-test-secret-c9tf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007348464s
Sep 20 22:12:43.217: INFO: Pod "pod-subpath-test-secret-c9tf": Phase="Running", Reason="", readiness=true. Elapsed: 4.012143393s
Sep 20 22:12:45.220: INFO: Pod "pod-subpath-test-secret-c9tf": Phase="Running", Reason="", readiness=true. Elapsed: 6.015316004s
Sep 20 22:12:47.226: INFO: Pod "pod-subpath-test-secret-c9tf": Phase="Running", Reason="", readiness=true. Elapsed: 8.020996114s
Sep 20 22:12:49.257: INFO: Pod "pod-subpath-test-secret-c9tf": Phase="Running", Reason="", readiness=true. Elapsed: 10.052537921s
Sep 20 22:12:51.300: INFO: Pod "pod-subpath-test-secret-c9tf": Phase="Running", Reason="", readiness=true. Elapsed: 12.095201726s
Sep 20 22:12:53.307: INFO: Pod "pod-subpath-test-secret-c9tf": Phase="Running", Reason="", readiness=true. Elapsed: 14.101823561s
Sep 20 22:12:55.309: INFO: Pod "pod-subpath-test-secret-c9tf": Phase="Running", Reason="", readiness=true. Elapsed: 16.10388327s
Sep 20 22:12:57.315: INFO: Pod "pod-subpath-test-secret-c9tf": Phase="Running", Reason="", readiness=true. Elapsed: 18.109956298s
Sep 20 22:12:59.321: INFO: Pod "pod-subpath-test-secret-c9tf": Phase="Running", Reason="", readiness=true. Elapsed: 20.116061519s
Sep 20 22:13:01.328: INFO: Pod "pod-subpath-test-secret-c9tf": Phase="Running", Reason="", readiness=true. Elapsed: 22.122869914s
Sep 20 22:13:03.337: INFO: Pod "pod-subpath-test-secret-c9tf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.132244878s
STEP: Saw pod success
Sep 20 22:13:03.337: INFO: Pod "pod-subpath-test-secret-c9tf" satisfied condition "success or failure"
Sep 20 22:13:03.343: INFO: Trying to get logs from node worker-2 pod pod-subpath-test-secret-c9tf container test-container-subpath-secret-c9tf: <nil>
STEP: delete the pod
Sep 20 22:13:03.384: INFO: Waiting for pod pod-subpath-test-secret-c9tf to disappear
Sep 20 22:13:03.388: INFO: Pod pod-subpath-test-secret-c9tf no longer exists
STEP: Deleting pod pod-subpath-test-secret-c9tf
Sep 20 22:13:03.388: INFO: Deleting pod "pod-subpath-test-secret-c9tf" in namespace "subpath-3826"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:13:03.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3826" for this suite.
Sep 20 22:13:09.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:13:09.473: INFO: namespace subpath-3826 deletion completed in 6.079127567s

• [SLOW TEST:30.300 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:13:09.474: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep 20 22:13:12.042: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3189 pod-service-account-68f97b5e-6c27-4c48-a291-d4aa42cb5965 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep 20 22:13:12.203: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3189 pod-service-account-68f97b5e-6c27-4c48-a291-d4aa42cb5965 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep 20 22:13:12.327: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3189 pod-service-account-68f97b5e-6c27-4c48-a291-d4aa42cb5965 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:13:12.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3189" for this suite.
Sep 20 22:13:18.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:13:18.513: INFO: namespace svcaccounts-3189 deletion completed in 6.04839359s

• [SLOW TEST:9.039 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:13:18.514: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-f021a6ec-c2e3-45b3-90d7-409c5ce55836
STEP: Creating a pod to test consume configMaps
Sep 20 22:13:18.543: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-64486657-c54e-4755-908b-dec91e4ef15d" in namespace "projected-4813" to be "success or failure"
Sep 20 22:13:18.546: INFO: Pod "pod-projected-configmaps-64486657-c54e-4755-908b-dec91e4ef15d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.3981ms
Sep 20 22:13:20.552: INFO: Pod "pod-projected-configmaps-64486657-c54e-4755-908b-dec91e4ef15d": Phase="Running", Reason="", readiness=true. Elapsed: 2.009059161s
Sep 20 22:13:22.559: INFO: Pod "pod-projected-configmaps-64486657-c54e-4755-908b-dec91e4ef15d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015899128s
STEP: Saw pod success
Sep 20 22:13:22.559: INFO: Pod "pod-projected-configmaps-64486657-c54e-4755-908b-dec91e4ef15d" satisfied condition "success or failure"
Sep 20 22:13:22.564: INFO: Trying to get logs from node worker-2 pod pod-projected-configmaps-64486657-c54e-4755-908b-dec91e4ef15d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 22:13:22.600: INFO: Waiting for pod pod-projected-configmaps-64486657-c54e-4755-908b-dec91e4ef15d to disappear
Sep 20 22:13:22.604: INFO: Pod pod-projected-configmaps-64486657-c54e-4755-908b-dec91e4ef15d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:13:22.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4813" for this suite.
Sep 20 22:13:28.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:13:28.664: INFO: namespace projected-4813 deletion completed in 6.056821112s

• [SLOW TEST:10.150 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:13:28.664: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 22:13:28.695: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c71a6403-be2c-47a5-8654-5ac588523b80" in namespace "projected-5075" to be "success or failure"
Sep 20 22:13:28.700: INFO: Pod "downwardapi-volume-c71a6403-be2c-47a5-8654-5ac588523b80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.84785ms
Sep 20 22:13:30.709: INFO: Pod "downwardapi-volume-c71a6403-be2c-47a5-8654-5ac588523b80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013710079s
STEP: Saw pod success
Sep 20 22:13:30.709: INFO: Pod "downwardapi-volume-c71a6403-be2c-47a5-8654-5ac588523b80" satisfied condition "success or failure"
Sep 20 22:13:30.711: INFO: Trying to get logs from node worker-2 pod downwardapi-volume-c71a6403-be2c-47a5-8654-5ac588523b80 container client-container: <nil>
STEP: delete the pod
Sep 20 22:13:30.729: INFO: Waiting for pod downwardapi-volume-c71a6403-be2c-47a5-8654-5ac588523b80 to disappear
Sep 20 22:13:30.731: INFO: Pod downwardapi-volume-c71a6403-be2c-47a5-8654-5ac588523b80 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:13:30.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5075" for this suite.
Sep 20 22:13:36.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:13:36.790: INFO: namespace projected-5075 deletion completed in 6.05618009s

• [SLOW TEST:8.126 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:13:36.790: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:13:36.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2692" for this suite.
Sep 20 22:13:42.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:13:42.978: INFO: namespace custom-resource-definition-2692 deletion completed in 6.092572745s

• [SLOW TEST:6.189 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:13:42.979: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
I0920 22:13:43.004179      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/client-go/tools/watch/informerwatcher.go:146
I0920 22:13:43.004271      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/client-go/tools/watch/informerwatcher.go:146
Sep 20 22:13:43.005: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Sep 20 22:13:52.070: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:13:52.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6952" for this suite.
Sep 20 22:13:58.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:13:58.179: INFO: namespace pods-6952 deletion completed in 6.09888725s

• [SLOW TEST:15.200 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:13:58.183: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 22:13:58.205: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:14:03.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-895" for this suite.
Sep 20 22:14:09.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:14:09.107: INFO: namespace custom-resource-definition-895 deletion completed in 6.102761525s

• [SLOW TEST:10.925 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:14:09.108: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:14:13.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1611" for this suite.
Sep 20 22:14:57.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:14:57.233: INFO: namespace kubelet-test-1611 deletion completed in 44.076533308s

• [SLOW TEST:48.125 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:14:57.234: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 22:14:57.275: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d8dd5368-ebf4-4047-9ce7-5fe6ed65fa64" in namespace "downward-api-8513" to be "success or failure"
Sep 20 22:14:57.278: INFO: Pod "downwardapi-volume-d8dd5368-ebf4-4047-9ce7-5fe6ed65fa64": Phase="Pending", Reason="", readiness=false. Elapsed: 3.587966ms
Sep 20 22:14:59.280: INFO: Pod "downwardapi-volume-d8dd5368-ebf4-4047-9ce7-5fe6ed65fa64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005829872s
Sep 20 22:15:01.295: INFO: Pod "downwardapi-volume-d8dd5368-ebf4-4047-9ce7-5fe6ed65fa64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020120112s
STEP: Saw pod success
Sep 20 22:15:01.295: INFO: Pod "downwardapi-volume-d8dd5368-ebf4-4047-9ce7-5fe6ed65fa64" satisfied condition "success or failure"
Sep 20 22:15:01.297: INFO: Trying to get logs from node worker-2 pod downwardapi-volume-d8dd5368-ebf4-4047-9ce7-5fe6ed65fa64 container client-container: <nil>
STEP: delete the pod
Sep 20 22:15:01.323: INFO: Waiting for pod downwardapi-volume-d8dd5368-ebf4-4047-9ce7-5fe6ed65fa64 to disappear
Sep 20 22:15:01.325: INFO: Pod downwardapi-volume-d8dd5368-ebf4-4047-9ce7-5fe6ed65fa64 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:15:01.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8513" for this suite.
Sep 20 22:15:07.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:15:07.427: INFO: namespace downward-api-8513 deletion completed in 6.097858459s

• [SLOW TEST:10.193 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:15:07.427: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-9891
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Sep 20 22:15:07.468: INFO: Found 0 stateful pods, waiting for 3
Sep 20 22:15:17.476: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 22:15:17.476: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 22:15:17.476: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Sep 20 22:15:17.521: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep 20 22:15:27.567: INFO: Updating stateful set ss2
Sep 20 22:15:27.594: INFO: Waiting for Pod statefulset-9891/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Sep 20 22:15:37.674: INFO: Found 1 stateful pods, waiting for 3
Sep 20 22:15:47.700: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 22:15:47.700: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 22:15:47.700: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 20 22:15:57.682: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 22:15:57.682: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 22:15:57.682: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep 20 22:15:57.719: INFO: Updating stateful set ss2
Sep 20 22:15:57.750: INFO: Waiting for Pod statefulset-9891/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 20 22:16:07.775: INFO: Updating stateful set ss2
Sep 20 22:16:07.779: INFO: Waiting for StatefulSet statefulset-9891/ss2 to complete update
Sep 20 22:16:07.779: INFO: Waiting for Pod statefulset-9891/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 20 22:16:17.790: INFO: Waiting for StatefulSet statefulset-9891/ss2 to complete update
Sep 20 22:16:17.791: INFO: Waiting for Pod statefulset-9891/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 20 22:16:27.784: INFO: Waiting for StatefulSet statefulset-9891/ss2 to complete update
Sep 20 22:16:27.785: INFO: Waiting for Pod statefulset-9891/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 20 22:16:37.786: INFO: Waiting for StatefulSet statefulset-9891/ss2 to complete update
Sep 20 22:16:37.787: INFO: Waiting for Pod statefulset-9891/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 20 22:16:47.783: INFO: Waiting for StatefulSet statefulset-9891/ss2 to complete update
Sep 20 22:16:57.792: INFO: Waiting for StatefulSet statefulset-9891/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Sep 20 22:17:07.798: INFO: Deleting all statefulset in ns statefulset-9891
Sep 20 22:17:07.799: INFO: Scaling statefulset ss2 to 0
Sep 20 22:17:27.814: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 22:17:27.818: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:17:27.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9891" for this suite.
Sep 20 22:17:33.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:17:33.946: INFO: namespace statefulset-9891 deletion completed in 6.096179619s

• [SLOW TEST:146.519 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:17:33.947: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-6223
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-6223
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6223
Sep 20 22:17:33.985: INFO: Found 0 stateful pods, waiting for 1
Sep 20 22:17:43.993: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep 20 22:17:44.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 22:17:44.157: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 22:17:44.157: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 22:17:44.157: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 22:17:44.158: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 20 22:17:54.165: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 20 22:17:54.165: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 22:17:54.202: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Sep 20 22:17:54.202: INFO: ss-0  worker-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:33 +0000 UTC  }]
Sep 20 22:17:54.202: INFO: ss-1            Pending         []
Sep 20 22:17:54.202: INFO: 
Sep 20 22:17:54.202: INFO: StatefulSet ss has not reached scale 3, at 2
Sep 20 22:17:55.206: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.980261377s
Sep 20 22:17:56.213: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.977164658s
Sep 20 22:17:57.221: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.969073652s
Sep 20 22:17:58.227: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.961636454s
Sep 20 22:17:59.233: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.956700638s
Sep 20 22:18:00.243: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.947163745s
Sep 20 22:18:01.250: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.940334454s
Sep 20 22:18:02.256: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.933450286s
Sep 20 22:18:03.258: INFO: Verifying statefulset ss doesn't scale past 3 for another 927.341699ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6223
Sep 20 22:18:04.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:18:04.445: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 20 22:18:04.445: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 22:18:04.445: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 20 22:18:04.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:18:04.601: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 20 22:18:04.601: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 22:18:04.601: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 20 22:18:04.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:18:04.792: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 20 22:18:04.792: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 22:18:04.792: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 20 22:18:04.794: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 22:18:04.794: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 22:18:04.794: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep 20 22:18:04.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 22:18:04.939: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 22:18:04.939: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 22:18:04.939: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 22:18:04.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 22:18:05.083: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 22:18:05.083: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 22:18:05.083: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 22:18:05.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 22:18:05.239: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 22:18:05.239: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 22:18:05.239: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 22:18:05.239: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 22:18:05.241: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep 20 22:18:15.259: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 20 22:18:15.259: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 20 22:18:15.259: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 20 22:18:15.284: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 20 22:18:15.284: INFO: ss-0  worker-2        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:33 +0000 UTC  }]
Sep 20 22:18:15.285: INFO: ss-1  worker-1        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:53 +0000 UTC  }]
Sep 20 22:18:15.285: INFO: ss-2  controlplane-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:53 +0000 UTC  }]
Sep 20 22:18:15.285: INFO: 
Sep 20 22:18:15.285: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 20 22:18:16.302: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 20 22:18:16.302: INFO: ss-0  worker-2        Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:33 +0000 UTC  }]
Sep 20 22:18:16.302: INFO: ss-1  worker-1        Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:53 +0000 UTC  }]
Sep 20 22:18:16.302: INFO: ss-2  controlplane-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:53 +0000 UTC  }]
Sep 20 22:18:16.302: INFO: 
Sep 20 22:18:16.302: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 20 22:18:17.311: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 20 22:18:17.311: INFO: ss-0  worker-2        Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:33 +0000 UTC  }]
Sep 20 22:18:17.312: INFO: ss-1  worker-1        Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:53 +0000 UTC  }]
Sep 20 22:18:17.312: INFO: ss-2  controlplane-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:53 +0000 UTC  }]
Sep 20 22:18:17.312: INFO: 
Sep 20 22:18:17.312: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 20 22:18:18.318: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 20 22:18:18.319: INFO: ss-0  worker-2        Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:33 +0000 UTC  }]
Sep 20 22:18:18.319: INFO: ss-2  controlplane-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:53 +0000 UTC  }]
Sep 20 22:18:18.320: INFO: 
Sep 20 22:18:18.320: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 20 22:18:19.327: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 20 22:18:19.327: INFO: ss-0  worker-2        Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:33 +0000 UTC  }]
Sep 20 22:18:19.328: INFO: ss-2  controlplane-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:53 +0000 UTC  }]
Sep 20 22:18:19.328: INFO: 
Sep 20 22:18:19.328: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 20 22:18:20.379: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 20 22:18:20.379: INFO: ss-0  worker-2        Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:33 +0000 UTC  }]
Sep 20 22:18:20.380: INFO: ss-2  controlplane-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:53 +0000 UTC  }]
Sep 20 22:18:20.380: INFO: 
Sep 20 22:18:20.380: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 20 22:18:21.392: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 20 22:18:21.392: INFO: ss-0  worker-2        Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:33 +0000 UTC  }]
Sep 20 22:18:21.392: INFO: ss-2  controlplane-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:53 +0000 UTC  }]
Sep 20 22:18:21.393: INFO: 
Sep 20 22:18:21.393: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 20 22:18:22.401: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 20 22:18:22.401: INFO: ss-0  worker-2        Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:33 +0000 UTC  }]
Sep 20 22:18:22.401: INFO: ss-2  controlplane-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:53 +0000 UTC  }]
Sep 20 22:18:22.402: INFO: 
Sep 20 22:18:22.402: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 20 22:18:23.416: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 20 22:18:23.416: INFO: ss-2  controlplane-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:53 +0000 UTC  }]
Sep 20 22:18:23.416: INFO: 
Sep 20 22:18:23.416: INFO: StatefulSet ss has not reached scale 0, at 1
Sep 20 22:18:24.422: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 20 22:18:24.422: INFO: ss-2  controlplane-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 22:17:53 +0000 UTC  }]
Sep 20 22:18:24.422: INFO: 
Sep 20 22:18:24.422: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6223
Sep 20 22:18:25.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:18:25.496: INFO: rc: 1
Sep 20 22:18:25.496: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  error: unable to upgrade connection: container not found ("webserver")
 [] <nil> 0xc00421e5a0 exit status 1 <nil> <nil> true [0xc0039a0808 0xc0039a0820 0xc0039a0838] [0xc0039a0808 0xc0039a0820 0xc0039a0838] [0xc0039a0818 0xc0039a0830] [0x10ef310 0x10ef310] 0xc0030a7560 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Sep 20 22:18:35.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:18:35.555: INFO: rc: 1
Sep 20 22:18:35.555: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00421ea20 exit status 1 <nil> <nil> true [0xc0039a0840 0xc0039a0858 0xc0039a0870] [0xc0039a0840 0xc0039a0858 0xc0039a0870] [0xc0039a0850 0xc0039a0868] [0x10ef310 0x10ef310] 0xc0030a78c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:18:45.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:18:45.634: INFO: rc: 1
Sep 20 22:18:45.634: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f87d70 exit status 1 <nil> <nil> true [0xc002dab1e0 0xc002dab248 0xc002dab2e0] [0xc002dab1e0 0xc002dab248 0xc002dab2e0] [0xc002dab210 0xc002dab2a0] [0x10ef310 0x10ef310] 0xc002fd70e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:18:55.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:18:55.729: INFO: rc: 1
Sep 20 22:18:55.729: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0041ea180 exit status 1 <nil> <nil> true [0xc002dab338 0xc002dab380 0xc002dab398] [0xc002dab338 0xc002dab380 0xc002dab398] [0xc002dab378 0xc002dab390] [0x10ef310 0x10ef310] 0xc002fd7440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:19:05.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:19:05.807: INFO: rc: 1
Sep 20 22:19:05.807: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00421ef30 exit status 1 <nil> <nil> true [0xc0039a0878 0xc0039a0890 0xc0039a08a8] [0xc0039a0878 0xc0039a0890 0xc0039a08a8] [0xc0039a0888 0xc0039a08a0] [0x10ef310 0x10ef310] 0xc0030a7c80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:19:15.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:19:15.897: INFO: rc: 1
Sep 20 22:19:15.898: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00421f320 exit status 1 <nil> <nil> true [0xc0039a08b0 0xc0039a08c8 0xc0039a08e0] [0xc0039a08b0 0xc0039a08c8 0xc0039a08e0] [0xc0039a08c0 0xc0039a08d8] [0x10ef310 0x10ef310] 0xc0033c0000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:19:25.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:19:25.977: INFO: rc: 1
Sep 20 22:19:25.977: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0061f2330 exit status 1 <nil> <nil> true [0xc000010098 0xc0000103c0 0xc000010730] [0xc000010098 0xc0000103c0 0xc000010730] [0xc000010338 0xc000010678] [0x10ef310 0x10ef310] 0xc0039382a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:19:35.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:19:36.036: INFO: rc: 1
Sep 20 22:19:36.036: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000cd2330 exit status 1 <nil> <nil> true [0xc0010360e0 0xc001036308 0xc001036708] [0xc0010360e0 0xc001036308 0xc001036708] [0xc001036258 0xc001036378] [0x10ef310 0x10ef310] 0xc0032cc2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:19:46.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:19:46.620: INFO: rc: 1
Sep 20 22:19:46.620: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0061f26f0 exit status 1 <nil> <nil> true [0xc000010818 0xc000010dd8 0xc002daa000] [0xc000010818 0xc000010dd8 0xc002daa000] [0xc000010c18 0xc000011128] [0x10ef310 0x10ef310] 0xc003938600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:19:56.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:19:56.706: INFO: rc: 1
Sep 20 22:19:56.706: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0061f2a80 exit status 1 <nil> <nil> true [0xc002daa008 0xc002daa020 0xc002daa038] [0xc002daa008 0xc002daa020 0xc002daa038] [0xc002daa018 0xc002daa030] [0x10ef310 0x10ef310] 0xc003938b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:20:06.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:20:06.761: INFO: rc: 1
Sep 20 22:20:06.761: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000cd26c0 exit status 1 <nil> <nil> true [0xc001036918 0xc001036ec0 0xc001037060] [0xc001036918 0xc001036ec0 0xc001037060] [0xc001036e98 0xc001037040] [0x10ef310 0x10ef310] 0xc0032cc600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:20:16.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:20:16.854: INFO: rc: 1
Sep 20 22:20:16.854: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0061f2e10 exit status 1 <nil> <nil> true [0xc002daa040 0xc002daa058 0xc002daa070] [0xc002daa040 0xc002daa058 0xc002daa070] [0xc002daa050 0xc002daa068] [0x10ef310 0x10ef310] 0xc003938ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:20:26.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:20:26.930: INFO: rc: 1
Sep 20 22:20:26.930: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000cd2a80 exit status 1 <nil> <nil> true [0xc0010370e0 0xc0010372b0 0xc0010373d0] [0xc0010370e0 0xc0010372b0 0xc0010373d0] [0xc001037290 0xc001037338] [0x10ef310 0x10ef310] 0xc0032cc9c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:20:36.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:20:36.991: INFO: rc: 1
Sep 20 22:20:36.991: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000cd2de0 exit status 1 <nil> <nil> true [0xc0010374b0 0xc0010376c8 0xc0010378c0] [0xc0010374b0 0xc0010376c8 0xc0010378c0] [0xc0010376b0 0xc001037860] [0x10ef310 0x10ef310] 0xc0032cd080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:20:46.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:20:47.075: INFO: rc: 1
Sep 20 22:20:47.075: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000cd31d0 exit status 1 <nil> <nil> true [0xc001037928 0xc001037ac0 0xc001037c68] [0xc001037928 0xc001037ac0 0xc001037c68] [0xc001037ab0 0xc001037c00] [0x10ef310 0x10ef310] 0xc0032cdc20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:20:57.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:20:57.180: INFO: rc: 1
Sep 20 22:20:57.180: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0061f33e0 exit status 1 <nil> <nil> true [0xc002daa078 0xc002daa090 0xc002daa0a8] [0xc002daa078 0xc002daa090 0xc002daa0a8] [0xc002daa088 0xc002daa0a0] [0x10ef310 0x10ef310] 0xc003939200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:21:07.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:21:07.239: INFO: rc: 1
Sep 20 22:21:07.239: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0061f37a0 exit status 1 <nil> <nil> true [0xc002daa0b0 0xc002daa0c8 0xc002daa0e0] [0xc002daa0b0 0xc002daa0c8 0xc002daa0e0] [0xc002daa0c0 0xc002daa0d8] [0x10ef310 0x10ef310] 0xc003939800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:21:17.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:21:17.336: INFO: rc: 1
Sep 20 22:21:17.336: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0061f3b00 exit status 1 <nil> <nil> true [0xc002daa0e8 0xc002daa100 0xc002daa118] [0xc002daa0e8 0xc002daa100 0xc002daa118] [0xc002daa0f8 0xc002daa110] [0x10ef310 0x10ef310] 0xc003939e00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:21:27.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:21:27.383: INFO: rc: 1
Sep 20 22:21:27.383: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0061f2360 exit status 1 <nil> <nil> true [0xc000010098 0xc0000103c0 0xc000010730] [0xc000010098 0xc0000103c0 0xc000010730] [0xc000010338 0xc000010678] [0x10ef310 0x10ef310] 0xc0039382a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:21:37.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:21:37.461: INFO: rc: 1
Sep 20 22:21:37.462: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000cd2360 exit status 1 <nil> <nil> true [0xc002daa000 0xc002daa018 0xc002daa030] [0xc002daa000 0xc002daa018 0xc002daa030] [0xc002daa010 0xc002daa028] [0x10ef310 0x10ef310] 0xc0032cc2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:21:47.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:21:47.545: INFO: rc: 1
Sep 20 22:21:47.545: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0061f2720 exit status 1 <nil> <nil> true [0xc000010818 0xc000010dd8 0xc0010360e0] [0xc000010818 0xc000010dd8 0xc0010360e0] [0xc000010c18 0xc000011128] [0x10ef310 0x10ef310] 0xc003938600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:21:57.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:21:57.629: INFO: rc: 1
Sep 20 22:21:57.629: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0061f2ae0 exit status 1 <nil> <nil> true [0xc0010361d8 0xc001036358 0xc001036918] [0xc0010361d8 0xc001036358 0xc001036918] [0xc001036308 0xc001036708] [0x10ef310 0x10ef310] 0xc003938b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:22:07.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:22:07.688: INFO: rc: 1
Sep 20 22:22:07.688: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000cd2720 exit status 1 <nil> <nil> true [0xc002daa038 0xc002daa050 0xc002daa068] [0xc002daa038 0xc002daa050 0xc002daa068] [0xc002daa048 0xc002daa060] [0x10ef310 0x10ef310] 0xc0032cc600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:22:17.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:22:17.768: INFO: rc: 1
Sep 20 22:22:17.768: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000cd2ae0 exit status 1 <nil> <nil> true [0xc002daa070 0xc002daa088 0xc002daa0a0] [0xc002daa070 0xc002daa088 0xc002daa0a0] [0xc002daa080 0xc002daa098] [0x10ef310 0x10ef310] 0xc0032cc9c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:22:27.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:22:27.849: INFO: rc: 1
Sep 20 22:22:27.849: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0061f2ea0 exit status 1 <nil> <nil> true [0xc001036b08 0xc001036fa8 0xc0010370e0] [0xc001036b08 0xc001036fa8 0xc0010370e0] [0xc001036ec0 0xc001037060] [0x10ef310 0x10ef310] 0xc003938ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:22:37.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:22:37.936: INFO: rc: 1
Sep 20 22:22:37.936: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0061f34d0 exit status 1 <nil> <nil> true [0xc001037120 0xc0010372f0 0xc0010374b0] [0xc001037120 0xc0010372f0 0xc0010374b0] [0xc0010372b0 0xc0010373d0] [0x10ef310 0x10ef310] 0xc003939200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:22:47.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:22:48.009: INFO: rc: 1
Sep 20 22:22:48.009: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000cd2ed0 exit status 1 <nil> <nil> true [0xc002daa0a8 0xc002daa0c0 0xc002daa0d8] [0xc002daa0a8 0xc002daa0c0 0xc002daa0d8] [0xc002daa0b8 0xc002daa0d0] [0x10ef310 0x10ef310] 0xc0032cd080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:22:58.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:22:58.065: INFO: rc: 1
Sep 20 22:22:58.066: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000cd3290 exit status 1 <nil> <nil> true [0xc002daa0e0 0xc002daa0f8 0xc002daa110] [0xc002daa0e0 0xc002daa0f8 0xc002daa110] [0xc002daa0f0 0xc002daa108] [0x10ef310 0x10ef310] 0xc0032cdc20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:23:08.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:23:08.159: INFO: rc: 1
Sep 20 22:23:08.159: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000cd3620 exit status 1 <nil> <nil> true [0xc002daa118 0xc002daa130 0xc002daa148] [0xc002daa118 0xc002daa130 0xc002daa148] [0xc002daa128 0xc002daa140] [0x10ef310 0x10ef310] 0xc002a02060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:23:18.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:23:18.238: INFO: rc: 1
Sep 20 22:23:18.238: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000cd39b0 exit status 1 <nil> <nil> true [0xc002daa150 0xc002daa180 0xc002daa198] [0xc002daa150 0xc002daa180 0xc002daa198] [0xc002daa178 0xc002daa190] [0x10ef310 0x10ef310] 0xc002a023c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 22:23:28.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-6223 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:23:28.320: INFO: rc: 1
Sep 20 22:23:28.320: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Sep 20 22:23:28.320: INFO: Scaling statefulset ss to 0
Sep 20 22:23:28.325: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Sep 20 22:23:28.326: INFO: Deleting all statefulset in ns statefulset-6223
Sep 20 22:23:28.327: INFO: Scaling statefulset ss to 0
Sep 20 22:23:28.331: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 22:23:28.332: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:23:28.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6223" for this suite.
Sep 20 22:23:34.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:23:34.436: INFO: namespace statefulset-6223 deletion completed in 6.093290981s

• [SLOW TEST:360.489 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:23:34.436: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9521
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9521
STEP: creating replication controller externalsvc in namespace services-9521
I0920 22:23:34.484220      16 runners.go:184] Created replication controller with name: externalsvc, namespace: services-9521, replica count: 2
I0920 22:23:34.484302      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 22:23:34.484567      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 22:23:37.537215      16 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Sep 20 22:23:37.578: INFO: Creating new exec pod
Sep 20 22:23:41.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=services-9521 execpodjfjf8 -- /bin/sh -x -c nslookup nodeport-service'
Sep 20 22:23:41.801: INFO: stderr: "+ nslookup nodeport-service\n"
Sep 20 22:23:41.801: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-9521.svc.cluster.local\tcanonical name = externalsvc.services-9521.svc.cluster.local.\nName:\texternalsvc.services-9521.svc.cluster.local\nAddress: 10.96.171.35\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9521, will wait for the garbage collector to delete the pods
I0920 22:23:41.803943      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 22:23:41.803960      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 22:23:41.877: INFO: Deleting ReplicationController externalsvc took: 16.279175ms
Sep 20 22:23:41.977: INFO: Terminating ReplicationController externalsvc pods took: 100.408231ms
I0920 22:23:41.977581      16 controller_utils.go:810] Ignoring inactive pod services-9521/externalsvc-w575c in state Running, deletion time 2019-09-20 22:23:42 +0000 UTC
I0920 22:23:41.977603      16 controller_utils.go:810] Ignoring inactive pod services-9521/externalsvc-jj5x6 in state Running, deletion time 2019-09-20 22:23:42 +0000 UTC
Sep 20 22:23:53.197: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:23:53.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9521" for this suite.
Sep 20 22:23:59.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:23:59.308: INFO: namespace services-9521 deletion completed in 6.090057339s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:24.872 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:23:59.312: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 22:23:59.334: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79c580cd-d2e2-4aec-827b-1cbd9399e094" in namespace "downward-api-2523" to be "success or failure"
Sep 20 22:23:59.337: INFO: Pod "downwardapi-volume-79c580cd-d2e2-4aec-827b-1cbd9399e094": Phase="Pending", Reason="", readiness=false. Elapsed: 3.652938ms
Sep 20 22:24:01.340: INFO: Pod "downwardapi-volume-79c580cd-d2e2-4aec-827b-1cbd9399e094": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00690302s
STEP: Saw pod success
Sep 20 22:24:01.340: INFO: Pod "downwardapi-volume-79c580cd-d2e2-4aec-827b-1cbd9399e094" satisfied condition "success or failure"
Sep 20 22:24:01.342: INFO: Trying to get logs from node worker-2 pod downwardapi-volume-79c580cd-d2e2-4aec-827b-1cbd9399e094 container client-container: <nil>
STEP: delete the pod
Sep 20 22:24:01.366: INFO: Waiting for pod downwardapi-volume-79c580cd-d2e2-4aec-827b-1cbd9399e094 to disappear
Sep 20 22:24:01.369: INFO: Pod downwardapi-volume-79c580cd-d2e2-4aec-827b-1cbd9399e094 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:24:01.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2523" for this suite.
Sep 20 22:24:07.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:24:07.457: INFO: namespace downward-api-2523 deletion completed in 6.085907327s

• [SLOW TEST:8.146 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:24:07.460: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 22:24:07.482: INFO: Creating deployment "webserver-deployment"
Sep 20 22:24:07.486: INFO: Waiting for observed generation 1
Sep 20 22:24:09.494: INFO: Waiting for all required pods to come up
Sep 20 22:24:09.497: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep 20 22:24:11.523: INFO: Waiting for deployment "webserver-deployment" to complete
Sep 20 22:24:11.533: INFO: Updating deployment "webserver-deployment" with a non-existent image
Sep 20 22:24:11.543: INFO: Updating deployment webserver-deployment
Sep 20 22:24:11.543: INFO: Waiting for observed generation 2
Sep 20 22:24:13.548: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep 20 22:24:13.550: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep 20 22:24:13.551: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 20 22:24:13.557: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep 20 22:24:13.557: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep 20 22:24:13.560: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 20 22:24:13.563: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Sep 20 22:24:13.563: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Sep 20 22:24:13.568: INFO: Updating deployment webserver-deployment
Sep 20 22:24:13.568: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Sep 20 22:24:13.590: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep 20 22:24:15.629: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Sep 20 22:24:15.654: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1864 /apis/apps/v1/namespaces/deployment-1864/deployments/webserver-deployment ae3fc002-5e1b-4928-a429-ff62a502b4f5 11478 3 2019-09-20 22:24:07 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00424fdb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:9,UnavailableReplicas:24,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2019-09-20 22:24:13 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2019-09-20 22:24:15 +0000 UTC,LastTransitionTime:2019-09-20 22:24:07 +0000 UTC,},},ReadyReplicas:9,CollisionCount:nil,},}

Sep 20 22:24:15.659: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-1864 /apis/apps/v1/namespaces/deployment-1864/replicasets/webserver-deployment-c7997dcc8 42e90331-1331-45e0-8702-8008c61f19b4 11413 3 2019-09-20 22:24:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment ae3fc002-5e1b-4928-a429-ff62a502b4f5 0xc00415a4b7 0xc00415a4b8}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00415a528 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 20 22:24:15.659: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Sep 20 22:24:15.659: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-1864 /apis/apps/v1/namespaces/deployment-1864/replicasets/webserver-deployment-595b5b9587 def5c400-a1e7-4ec8-99a4-0a496892e34e 11476 3 2019-09-20 22:24:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment ae3fc002-5e1b-4928-a429-ff62a502b4f5 0xc00415a377 0xc00415a378}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00415a458 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:9,AvailableReplicas:9,Conditions:[]ReplicaSetCondition{},},}
Sep 20 22:24:15.670: INFO: Pod "webserver-deployment-595b5b9587-7r9l4" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-7r9l4 webserver-deployment-595b5b9587- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-595b5b9587-7r9l4 8d6eb03c-db50-4479-a2b3-a94d31d14985 11236 0 2019-09-20 22:24:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 def5c400-a1e7-4ec8-99a4-0a496892e34e 0xc00415b017 0xc00415b018}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.102,PodIP:10.40.0.5,StartTime:2019-09-20 22:24:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 22:24:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f6314672cbb52c49f2e96460a58870f8924d4234fec770eeffcd514f5a32307f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.40.0.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.670: INFO: Pod "webserver-deployment-595b5b9587-85v46" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-85v46 webserver-deployment-595b5b9587- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-595b5b9587-85v46 b34c5189-651e-4f2e-b3c9-58634c9955e7 11474 0 2019-09-20 22:24:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 def5c400-a1e7-4ec8-99a4-0a496892e34e 0xc00415b320 0xc00415b321}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.102,PodIP:10.40.0.9,StartTime:2019-09-20 22:24:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 22:24:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://737c72a93ffec28bc5e90e3b9b2208b416e13eaf871a21f6b1854b693dce632d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.40.0.9,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.670: INFO: Pod "webserver-deployment-595b5b9587-h2wxq" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-h2wxq webserver-deployment-595b5b9587- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-595b5b9587-h2wxq bdce1ad9-4f79-49c5-a87a-b945473ecab7 11427 0 2019-09-20 22:24:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 def5c400-a1e7-4ec8-99a4-0a496892e34e 0xc00415b560 0xc00415b561}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.102,PodIP:,StartTime:2019-09-20 22:24:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.671: INFO: Pod "webserver-deployment-595b5b9587-j2vlm" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-j2vlm webserver-deployment-595b5b9587- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-595b5b9587-j2vlm ab87f5b7-6ded-46c5-ab5b-df59152045bc 11248 0 2019-09-20 22:24:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 def5c400-a1e7-4ec8-99a4-0a496892e34e 0xc00415b6e7 0xc00415b6e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.101,PodIP:10.32.0.4,StartTime:2019-09-20 22:24:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 22:24:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://8b81f471f3171d8b568da7781e9be8c7911bc6394013466438888daaf2f0ffc6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.0.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.671: INFO: Pod "webserver-deployment-595b5b9587-jd76p" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jd76p webserver-deployment-595b5b9587- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-595b5b9587-jd76p 3ca69264-c529-4a2d-9a91-e3e4fbb0c2bb 11259 0 2019-09-20 22:24:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 def5c400-a1e7-4ec8-99a4-0a496892e34e 0xc00415b8a0 0xc00415b8a1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.11,PodIP:10.38.0.2,StartTime:2019-09-20 22:24:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 22:24:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://e2ef963ba2dc7260ffaeafcda3bb0f229bb42bcccc5407e2662f966db6d0925a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.38.0.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.671: INFO: Pod "webserver-deployment-595b5b9587-jr7bv" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jr7bv webserver-deployment-595b5b9587- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-595b5b9587-jr7bv c6495be1-797d-4890-9905-2be043302f73 11426 0 2019-09-20 22:24:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 def5c400-a1e7-4ec8-99a4-0a496892e34e 0xc00415bbb0 0xc00415bbb1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.101,PodIP:,StartTime:2019-09-20 22:24:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.671: INFO: Pod "webserver-deployment-595b5b9587-ln6h5" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-ln6h5 webserver-deployment-595b5b9587- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-595b5b9587-ln6h5 a3f85949-6a7d-4965-a59b-782880371c58 11254 0 2019-09-20 22:24:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 def5c400-a1e7-4ec8-99a4-0a496892e34e 0xc00415bd47 0xc00415bd48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.101,PodIP:10.32.0.6,StartTime:2019-09-20 22:24:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 22:24:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://1cc1319b3dbdb7a0d11fbed36b1863b3771f8f896a03ae4a60edab5dd9d9af1a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.0.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.671: INFO: Pod "webserver-deployment-595b5b9587-lsskn" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lsskn webserver-deployment-595b5b9587- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-595b5b9587-lsskn aff6d5d8-e648-4814-934a-e799886d97fd 11368 0 2019-09-20 22:24:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 def5c400-a1e7-4ec8-99a4-0a496892e34e 0xc0040f2080 0xc0040f2081}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.101,PodIP:,StartTime:2019-09-20 22:24:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.671: INFO: Pod "webserver-deployment-595b5b9587-n7vls" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-n7vls webserver-deployment-595b5b9587- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-595b5b9587-n7vls 7b5999ad-76b9-409d-92d5-63f1ccd129f0 11397 0 2019-09-20 22:24:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 def5c400-a1e7-4ec8-99a4-0a496892e34e 0xc0040f21d7 0xc0040f21d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.11,PodIP:,StartTime:2019-09-20 22:24:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.671: INFO: Pod "webserver-deployment-595b5b9587-np268" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-np268 webserver-deployment-595b5b9587- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-595b5b9587-np268 30c63690-63ba-4e26-8e2e-edd1bc704031 11418 0 2019-09-20 22:24:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 def5c400-a1e7-4ec8-99a4-0a496892e34e 0xc0040f2337 0xc0040f2338}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.101,PodIP:,StartTime:2019-09-20 22:24:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.672: INFO: Pod "webserver-deployment-595b5b9587-qgb9t" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qgb9t webserver-deployment-595b5b9587- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-595b5b9587-qgb9t cc69020b-fbf4-4b28-a36f-e946d45c92cb 11439 0 2019-09-20 22:24:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 def5c400-a1e7-4ec8-99a4-0a496892e34e 0xc0040f2497 0xc0040f2498}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.11,PodIP:,StartTime:2019-09-20 22:24:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.672: INFO: Pod "webserver-deployment-595b5b9587-qq829" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qq829 webserver-deployment-595b5b9587- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-595b5b9587-qq829 4924c5c4-abfe-46af-9c2d-9fa76ab49cd8 11228 0 2019-09-20 22:24:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 def5c400-a1e7-4ec8-99a4-0a496892e34e 0xc0040f25f7 0xc0040f25f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.102,PodIP:10.40.0.4,StartTime:2019-09-20 22:24:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 22:24:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://8d345a868a0e38a7ce6934961fb2438866721abb565187ba160c9dc3e496f258,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.40.0.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.672: INFO: Pod "webserver-deployment-595b5b9587-rrvbv" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rrvbv webserver-deployment-595b5b9587- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-595b5b9587-rrvbv 414205ce-d64b-4b66-b82a-75fa9a3f58ea 11424 0 2019-09-20 22:24:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 def5c400-a1e7-4ec8-99a4-0a496892e34e 0xc0040f2770 0xc0040f2771}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.102,PodIP:,StartTime:2019-09-20 22:24:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.672: INFO: Pod "webserver-deployment-595b5b9587-s67gp" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-s67gp webserver-deployment-595b5b9587- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-595b5b9587-s67gp 14955ef9-2634-4932-8cf7-94e065231d37 11232 0 2019-09-20 22:24:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 def5c400-a1e7-4ec8-99a4-0a496892e34e 0xc0040f28e7 0xc0040f28e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.11,PodIP:10.38.0.3,StartTime:2019-09-20 22:24:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 22:24:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://78f2c9b3146b35c21148be2985aa39e9eea258ec6f377c9d26d52b2a786b340e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.38.0.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.672: INFO: Pod "webserver-deployment-595b5b9587-vcrdd" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-vcrdd webserver-deployment-595b5b9587- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-595b5b9587-vcrdd a339dab3-46f2-45a7-a123-99a488be6a1d 11383 0 2019-09-20 22:24:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 def5c400-a1e7-4ec8-99a4-0a496892e34e 0xc0040f2a60 0xc0040f2a61}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.102,PodIP:,StartTime:2019-09-20 22:24:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.672: INFO: Pod "webserver-deployment-595b5b9587-wg268" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-wg268 webserver-deployment-595b5b9587- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-595b5b9587-wg268 011a14d5-d307-4ebe-b3ec-55345cb91e31 11231 0 2019-09-20 22:24:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 def5c400-a1e7-4ec8-99a4-0a496892e34e 0xc0040f2bb7 0xc0040f2bb8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.102,PodIP:10.40.0.3,StartTime:2019-09-20 22:24:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 22:24:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://e57c6f36c301d9e6dbe267bcbb9c15e55837bb15769c1a1ca265c7a7a5a63d4f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.40.0.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.672: INFO: Pod "webserver-deployment-595b5b9587-wqmfj" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-wqmfj webserver-deployment-595b5b9587- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-595b5b9587-wqmfj e07953c9-9609-4234-b4ee-604d96c39d1d 11425 0 2019-09-20 22:24:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 def5c400-a1e7-4ec8-99a4-0a496892e34e 0xc0040f2d30 0xc0040f2d31}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.11,PodIP:,StartTime:2019-09-20 22:24:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.673: INFO: Pod "webserver-deployment-595b5b9587-xjb8z" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xjb8z webserver-deployment-595b5b9587- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-595b5b9587-xjb8z c8117d51-8990-4589-8d61-eb4dfb7be84d 11225 0 2019-09-20 22:24:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 def5c400-a1e7-4ec8-99a4-0a496892e34e 0xc0040f2e87 0xc0040f2e88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.11,PodIP:10.38.0.1,StartTime:2019-09-20 22:24:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 22:24:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://2a967122ea7783113e9a524abeab1035661c828bdb40d84f4136b27a834cb9df,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.38.0.1,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.673: INFO: Pod "webserver-deployment-595b5b9587-xm4ts" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xm4ts webserver-deployment-595b5b9587- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-595b5b9587-xm4ts 6310f594-1d9f-4259-8a6c-3eb4c7ebc609 11422 0 2019-09-20 22:24:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 def5c400-a1e7-4ec8-99a4-0a496892e34e 0xc0040f3000 0xc0040f3001}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.101,PodIP:,StartTime:2019-09-20 22:24:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.673: INFO: Pod "webserver-deployment-595b5b9587-xmp7h" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xmp7h webserver-deployment-595b5b9587- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-595b5b9587-xmp7h 5c9aed7d-29cd-4caa-8e87-2e6473fb19d7 11416 0 2019-09-20 22:24:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 def5c400-a1e7-4ec8-99a4-0a496892e34e 0xc0040f3157 0xc0040f3158}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.11,PodIP:,StartTime:2019-09-20 22:24:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.673: INFO: Pod "webserver-deployment-c7997dcc8-6v5f5" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-6v5f5 webserver-deployment-c7997dcc8- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-c7997dcc8-6v5f5 a8b616ef-28cf-4da8-91f1-8a318f242630 11457 0 2019-09-20 22:24:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 42e90331-1331-45e0-8702-8008c61f19b4 0xc0040f32b7 0xc0040f32b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.102,PodIP:10.40.0.6,StartTime:2019-09-20 22:24:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login',},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.40.0.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.673: INFO: Pod "webserver-deployment-c7997dcc8-7j5r8" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-7j5r8 webserver-deployment-c7997dcc8- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-c7997dcc8-7j5r8 5601f312-c74f-483b-bdcb-7f4a2d45ad01 11407 0 2019-09-20 22:24:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 42e90331-1331-45e0-8702-8008c61f19b4 0xc0040f3460 0xc0040f3461}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.101,PodIP:,StartTime:2019-09-20 22:24:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.674: INFO: Pod "webserver-deployment-c7997dcc8-95mj5" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-95mj5 webserver-deployment-c7997dcc8- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-c7997dcc8-95mj5 32c7f236-6a22-4e47-b194-62d324cea4bd 11441 0 2019-09-20 22:24:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 42e90331-1331-45e0-8702-8008c61f19b4 0xc0040f35d7 0xc0040f35d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.102,PodIP:,StartTime:2019-09-20 22:24:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.674: INFO: Pod "webserver-deployment-c7997dcc8-g9tzm" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-g9tzm webserver-deployment-c7997dcc8- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-c7997dcc8-g9tzm 9b255269-ceb3-4525-b9c0-25cb6b0636e3 11460 0 2019-09-20 22:24:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 42e90331-1331-45e0-8702-8008c61f19b4 0xc0040f3757 0xc0040f3758}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.11,PodIP:,StartTime:2019-09-20 22:24:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.674: INFO: Pod "webserver-deployment-c7997dcc8-h8h9d" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-h8h9d webserver-deployment-c7997dcc8- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-c7997dcc8-h8h9d faaade99-7e5c-4667-aaee-4bc673486f09 11467 0 2019-09-20 22:24:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 42e90331-1331-45e0-8702-8008c61f19b4 0xc0040f38d7 0xc0040f38d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.11,PodIP:10.38.0.4,StartTime:2019-09-20 22:24:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login',},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.38.0.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.674: INFO: Pod "webserver-deployment-c7997dcc8-hxcnl" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-hxcnl webserver-deployment-c7997dcc8- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-c7997dcc8-hxcnl 7efa23a1-4d80-46e1-b15a-841ff9364d05 11483 0 2019-09-20 22:24:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 42e90331-1331-45e0-8702-8008c61f19b4 0xc0040f3a80 0xc0040f3a81}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.101,PodIP:10.32.0.7,StartTime:2019-09-20 22:24:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.0.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.674: INFO: Pod "webserver-deployment-c7997dcc8-lcg28" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-lcg28 webserver-deployment-c7997dcc8- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-c7997dcc8-lcg28 1a409529-9428-41e4-91fa-0cfa8cd2e7b0 11423 0 2019-09-20 22:24:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 42e90331-1331-45e0-8702-8008c61f19b4 0xc0040f3c20 0xc0040f3c21}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.11,PodIP:,StartTime:2019-09-20 22:24:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.674: INFO: Pod "webserver-deployment-c7997dcc8-lrj4b" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-lrj4b webserver-deployment-c7997dcc8- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-c7997dcc8-lrj4b bc21b901-526b-4a03-bbed-edaa76430b78 11479 0 2019-09-20 22:24:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 42e90331-1331-45e0-8702-8008c61f19b4 0xc004014437 0xc004014438}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.102,PodIP:10.40.0.7,StartTime:2019-09-20 22:24:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login',},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.40.0.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.675: INFO: Pod "webserver-deployment-c7997dcc8-msbr9" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-msbr9 webserver-deployment-c7997dcc8- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-c7997dcc8-msbr9 1904294d-8e2d-43af-b18d-dd2a1df03e6c 11294 0 2019-09-20 22:24:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 42e90331-1331-45e0-8702-8008c61f19b4 0xc0040147f0 0xc0040147f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.101,PodIP:,StartTime:2019-09-20 22:24:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.675: INFO: Pod "webserver-deployment-c7997dcc8-nkzqn" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-nkzqn webserver-deployment-c7997dcc8- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-c7997dcc8-nkzqn ed61e43e-1bef-48ab-8557-373f2b005d52 11412 0 2019-09-20 22:24:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 42e90331-1331-45e0-8702-8008c61f19b4 0xc004014967 0xc004014968}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.102,PodIP:,StartTime:2019-09-20 22:24:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.675: INFO: Pod "webserver-deployment-c7997dcc8-tvrlb" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-tvrlb webserver-deployment-c7997dcc8- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-c7997dcc8-tvrlb 329c235c-dcf2-4289-9199-0a059c49f2c0 11367 0 2019-09-20 22:24:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 42e90331-1331-45e0-8702-8008c61f19b4 0xc004014af7 0xc004014af8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.11,PodIP:,StartTime:2019-09-20 22:24:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.675: INFO: Pod "webserver-deployment-c7997dcc8-vhs9v" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-vhs9v webserver-deployment-c7997dcc8- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-c7997dcc8-vhs9v 917aba94-d0a3-4b32-a15b-674a0584ac22 11438 0 2019-09-20 22:24:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 42e90331-1331-45e0-8702-8008c61f19b4 0xc004014c77 0xc004014c78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.101,PodIP:,StartTime:2019-09-20 22:24:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 22:24:15.675: INFO: Pod "webserver-deployment-c7997dcc8-xcmqr" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-xcmqr webserver-deployment-c7997dcc8- deployment-1864 /api/v1/namespaces/deployment-1864/pods/webserver-deployment-c7997dcc8-xcmqr f9d9a7bc-881f-476b-b0fc-6c9cc6030fef 11455 0 2019-09-20 22:24:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 42e90331-1331-45e0-8702-8008c61f19b4 0xc004014df7 0xc004014df8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pxn6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pxn6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pxn6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:24:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.11,PodIP:,StartTime:2019-09-20 22:24:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:24:15.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1864" for this suite.
Sep 20 22:24:23.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:24:23.742: INFO: namespace deployment-1864 deletion completed in 8.059049387s

• [SLOW TEST:16.282 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:24:23.742: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Sep 20 22:24:23.762: INFO: namespace kubectl-5546
Sep 20 22:24:23.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 create -f - --namespace=kubectl-5546'
Sep 20 22:24:23.906: INFO: stderr: ""
Sep 20 22:24:23.906: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep 20 22:24:24.913: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 22:24:24.913: INFO: Found 0 / 1
Sep 20 22:24:25.912: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 22:24:25.912: INFO: Found 0 / 1
Sep 20 22:24:26.908: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 22:24:26.908: INFO: Found 1 / 1
Sep 20 22:24:26.908: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 20 22:24:26.910: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 22:24:26.910: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 20 22:24:26.910: INFO: wait on redis-master startup in kubectl-5546 
Sep 20 22:24:26.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 logs redis-master-ztb7z redis-master --namespace=kubectl-5546'
Sep 20 22:24:26.977: INFO: stderr: ""
Sep 20 22:24:26.977: INFO: stdout: "1:C 20 Sep 2019 22:24:25.288 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 20 Sep 2019 22:24:25.288 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 20 Sep 2019 22:24:25.288 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 20 Sep 2019 22:24:25.289 * Running mode=standalone, port=6379.\n1:M 20 Sep 2019 22:24:25.289 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 20 Sep 2019 22:24:25.289 # Server initialized\n1:M 20 Sep 2019 22:24:25.289 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 20 Sep 2019 22:24:25.289 * Ready to accept connections\n"
STEP: exposing RC
Sep 20 22:24:26.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-5546'
Sep 20 22:24:27.045: INFO: stderr: ""
Sep 20 22:24:27.045: INFO: stdout: "service/rm2 exposed\n"
Sep 20 22:24:27.050: INFO: Service rm2 in namespace kubectl-5546 found.
STEP: exposing service
Sep 20 22:24:29.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-5546'
Sep 20 22:24:29.181: INFO: stderr: ""
Sep 20 22:24:29.181: INFO: stdout: "service/rm3 exposed\n"
Sep 20 22:24:29.188: INFO: Service rm3 in namespace kubectl-5546 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:24:31.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5546" for this suite.
Sep 20 22:24:59.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:24:59.300: INFO: namespace kubectl-5546 deletion completed in 28.095516111s

• [SLOW TEST:35.558 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:24:59.301: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 22:24:59.335: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep 20 22:24:59.346: INFO: Number of nodes with available pods: 0
Sep 20 22:24:59.346: INFO: Node controlplane-1 is running more than one daemon pod
Sep 20 22:25:00.353: INFO: Number of nodes with available pods: 0
Sep 20 22:25:00.353: INFO: Node controlplane-1 is running more than one daemon pod
Sep 20 22:25:01.361: INFO: Number of nodes with available pods: 2
Sep 20 22:25:01.361: INFO: Node worker-1 is running more than one daemon pod
Sep 20 22:25:02.360: INFO: Number of nodes with available pods: 3
Sep 20 22:25:02.360: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep 20 22:25:02.422: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:02.422: INFO: Wrong image for pod: daemon-set-ghc74. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:02.422: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:03.427: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:03.427: INFO: Wrong image for pod: daemon-set-ghc74. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:03.427: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:04.433: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:04.433: INFO: Wrong image for pod: daemon-set-ghc74. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:04.433: INFO: Pod daemon-set-ghc74 is not available
Sep 20 22:25:04.433: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:05.432: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:05.432: INFO: Wrong image for pod: daemon-set-ghc74. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:05.432: INFO: Pod daemon-set-ghc74 is not available
Sep 20 22:25:05.432: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:06.442: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:06.442: INFO: Wrong image for pod: daemon-set-ghc74. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:06.442: INFO: Pod daemon-set-ghc74 is not available
Sep 20 22:25:06.442: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:07.431: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:07.431: INFO: Wrong image for pod: daemon-set-ghc74. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:07.431: INFO: Pod daemon-set-ghc74 is not available
Sep 20 22:25:07.431: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:08.432: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:08.432: INFO: Wrong image for pod: daemon-set-ghc74. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:08.432: INFO: Pod daemon-set-ghc74 is not available
Sep 20 22:25:08.432: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:09.428: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:09.428: INFO: Wrong image for pod: daemon-set-ghc74. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:09.428: INFO: Pod daemon-set-ghc74 is not available
Sep 20 22:25:09.428: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:10.434: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:10.434: INFO: Wrong image for pod: daemon-set-ghc74. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:10.434: INFO: Pod daemon-set-ghc74 is not available
Sep 20 22:25:10.434: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:11.428: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:11.428: INFO: Wrong image for pod: daemon-set-ghc74. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:11.428: INFO: Pod daemon-set-ghc74 is not available
Sep 20 22:25:11.428: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:12.433: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:12.433: INFO: Wrong image for pod: daemon-set-ghc74. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:12.433: INFO: Pod daemon-set-ghc74 is not available
Sep 20 22:25:12.433: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:13.433: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:13.433: INFO: Wrong image for pod: daemon-set-ghc74. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:13.433: INFO: Pod daemon-set-ghc74 is not available
Sep 20 22:25:13.433: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:14.433: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:14.433: INFO: Wrong image for pod: daemon-set-ghc74. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:14.433: INFO: Pod daemon-set-ghc74 is not available
Sep 20 22:25:14.433: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:15.435: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:15.435: INFO: Wrong image for pod: daemon-set-ghc74. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:15.436: INFO: Pod daemon-set-ghc74 is not available
Sep 20 22:25:15.436: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:16.433: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:16.434: INFO: Pod daemon-set-bdg5s is not available
Sep 20 22:25:16.434: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:17.428: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:17.428: INFO: Pod daemon-set-bdg5s is not available
Sep 20 22:25:17.428: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:18.429: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:18.429: INFO: Pod daemon-set-bdg5s is not available
Sep 20 22:25:18.429: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:19.434: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:19.434: INFO: Pod daemon-set-bdg5s is not available
Sep 20 22:25:19.434: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:20.427: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:20.427: INFO: Pod daemon-set-bdg5s is not available
Sep 20 22:25:20.427: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:21.434: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:21.435: INFO: Pod daemon-set-bdg5s is not available
Sep 20 22:25:21.435: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:22.430: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:22.430: INFO: Pod daemon-set-bdg5s is not available
Sep 20 22:25:22.430: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:23.427: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:23.427: INFO: Pod daemon-set-bdg5s is not available
Sep 20 22:25:23.428: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:24.433: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:24.433: INFO: Pod daemon-set-bdg5s is not available
Sep 20 22:25:24.433: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:25.432: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:25.432: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:26.431: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:26.431: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:26.431: INFO: Pod daemon-set-vmn2v is not available
Sep 20 22:25:27.432: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:27.432: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:27.433: INFO: Pod daemon-set-vmn2v is not available
Sep 20 22:25:28.427: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:28.427: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:28.427: INFO: Pod daemon-set-vmn2v is not available
Sep 20 22:25:29.432: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:29.432: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:29.432: INFO: Pod daemon-set-vmn2v is not available
Sep 20 22:25:30.433: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:30.433: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:30.433: INFO: Pod daemon-set-vmn2v is not available
Sep 20 22:25:31.427: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:31.427: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:31.427: INFO: Pod daemon-set-vmn2v is not available
Sep 20 22:25:32.428: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:32.428: INFO: Wrong image for pod: daemon-set-vmn2v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:32.429: INFO: Pod daemon-set-vmn2v is not available
Sep 20 22:25:33.432: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:33.432: INFO: Pod daemon-set-xsq9r is not available
Sep 20 22:25:34.427: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:34.427: INFO: Pod daemon-set-xsq9r is not available
Sep 20 22:25:35.427: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:36.433: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:37.430: INFO: Wrong image for pod: daemon-set-4d5sk. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 22:25:37.430: INFO: Pod daemon-set-4d5sk is not available
Sep 20 22:25:38.432: INFO: Pod daemon-set-2x6rm is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Sep 20 22:25:38.447: INFO: Number of nodes with available pods: 2
Sep 20 22:25:38.447: INFO: Node worker-1 is running more than one daemon pod
Sep 20 22:25:39.461: INFO: Number of nodes with available pods: 2
Sep 20 22:25:39.461: INFO: Node worker-1 is running more than one daemon pod
Sep 20 22:25:40.458: INFO: Number of nodes with available pods: 2
Sep 20 22:25:40.458: INFO: Node worker-1 is running more than one daemon pod
Sep 20 22:25:41.461: INFO: Number of nodes with available pods: 2
Sep 20 22:25:41.461: INFO: Node worker-1 is running more than one daemon pod
Sep 20 22:25:42.452: INFO: Number of nodes with available pods: 2
Sep 20 22:25:42.452: INFO: Node worker-1 is running more than one daemon pod
Sep 20 22:25:43.458: INFO: Number of nodes with available pods: 2
Sep 20 22:25:43.458: INFO: Node worker-1 is running more than one daemon pod
Sep 20 22:25:44.461: INFO: Number of nodes with available pods: 2
Sep 20 22:25:44.461: INFO: Node worker-1 is running more than one daemon pod
Sep 20 22:25:45.460: INFO: Number of nodes with available pods: 3
Sep 20 22:25:45.460: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3890, will wait for the garbage collector to delete the pods
I0920 22:25:45.492814      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 22:25:45.492846      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 22:25:45.560: INFO: Deleting DaemonSet.extensions daemon-set took: 12.25601ms
I0920 22:25:45.860590      16 controller_utils.go:810] Ignoring inactive pod daemonsets-3890/daemon-set-2x6rm in state Running, deletion time 2019-09-20 22:26:15 +0000 UTC
I0920 22:25:45.860748      16 controller_utils.go:810] Ignoring inactive pod daemonsets-3890/daemon-set-xsq9r in state Running, deletion time 2019-09-20 22:26:15 +0000 UTC
I0920 22:25:45.860774      16 controller_utils.go:810] Ignoring inactive pod daemonsets-3890/daemon-set-bdg5s in state Running, deletion time 2019-09-20 22:26:15 +0000 UTC
Sep 20 22:25:45.861: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.796704ms
Sep 20 22:25:53.163: INFO: Number of nodes with available pods: 0
Sep 20 22:25:53.163: INFO: Number of running nodes: 0, number of available pods: 0
Sep 20 22:25:53.165: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3890/daemonsets","resourceVersion":"12061"},"items":null}

Sep 20 22:25:53.167: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3890/pods","resourceVersion":"12061"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:25:53.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3890" for this suite.
Sep 20 22:25:59.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:25:59.253: INFO: namespace daemonsets-3890 deletion completed in 6.078057087s

• [SLOW TEST:59.952 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:25:59.253: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 22:25:59.275: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 20 22:26:01.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 --namespace=crd-publish-openapi-6489 create -f -'
Sep 20 22:26:02.083: INFO: stderr: ""
Sep 20 22:26:02.083: INFO: stdout: "e2e-test-crd-publish-openapi-2347-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 20 22:26:02.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 --namespace=crd-publish-openapi-6489 delete e2e-test-crd-publish-openapi-2347-crds test-cr'
Sep 20 22:26:02.138: INFO: stderr: ""
Sep 20 22:26:02.138: INFO: stdout: "e2e-test-crd-publish-openapi-2347-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Sep 20 22:26:02.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 --namespace=crd-publish-openapi-6489 apply -f -'
Sep 20 22:26:02.238: INFO: stderr: ""
Sep 20 22:26:02.238: INFO: stdout: "e2e-test-crd-publish-openapi-2347-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 20 22:26:02.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 --namespace=crd-publish-openapi-6489 delete e2e-test-crd-publish-openapi-2347-crds test-cr'
Sep 20 22:26:02.293: INFO: stderr: ""
Sep 20 22:26:02.293: INFO: stdout: "e2e-test-crd-publish-openapi-2347-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Sep 20 22:26:02.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 explain e2e-test-crd-publish-openapi-2347-crds'
Sep 20 22:26:02.388: INFO: stderr: ""
Sep 20 22:26:02.388: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2347-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:26:03.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6489" for this suite.
Sep 20 22:26:09.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:26:10.027: INFO: namespace crd-publish-openapi-6489 deletion completed in 6.07289723s

• [SLOW TEST:10.773 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:26:10.027: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-641fe57c-c728-46ff-b416-03a164078f33
STEP: Creating a pod to test consume secrets
Sep 20 22:26:10.075: INFO: Waiting up to 5m0s for pod "pod-secrets-26caa551-0e65-40a4-b0d7-b86a2ae3c59e" in namespace "secrets-8710" to be "success or failure"
Sep 20 22:26:10.081: INFO: Pod "pod-secrets-26caa551-0e65-40a4-b0d7-b86a2ae3c59e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013698ms
Sep 20 22:26:12.084: INFO: Pod "pod-secrets-26caa551-0e65-40a4-b0d7-b86a2ae3c59e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008806937s
STEP: Saw pod success
Sep 20 22:26:12.084: INFO: Pod "pod-secrets-26caa551-0e65-40a4-b0d7-b86a2ae3c59e" satisfied condition "success or failure"
Sep 20 22:26:12.086: INFO: Trying to get logs from node worker-2 pod pod-secrets-26caa551-0e65-40a4-b0d7-b86a2ae3c59e container secret-volume-test: <nil>
STEP: delete the pod
Sep 20 22:26:12.106: INFO: Waiting for pod pod-secrets-26caa551-0e65-40a4-b0d7-b86a2ae3c59e to disappear
Sep 20 22:26:12.108: INFO: Pod pod-secrets-26caa551-0e65-40a4-b0d7-b86a2ae3c59e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:26:12.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8710" for this suite.
Sep 20 22:26:18.123: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:26:18.186: INFO: namespace secrets-8710 deletion completed in 6.076350383s

• [SLOW TEST:8.159 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:26:18.189: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 22:26:18.219: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:26:18.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9476" for this suite.
Sep 20 22:26:24.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:26:24.803: INFO: namespace custom-resource-definition-9476 deletion completed in 6.060513818s

• [SLOW TEST:6.614 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:26:24.803: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-33ce6f1c-8736-48a7-a0cb-64c4f61a2ed2
STEP: Creating a pod to test consume secrets
Sep 20 22:26:24.831: INFO: Waiting up to 5m0s for pod "pod-secrets-26c609ea-b46a-45b7-8e03-5bd40a840017" in namespace "secrets-677" to be "success or failure"
Sep 20 22:26:24.835: INFO: Pod "pod-secrets-26c609ea-b46a-45b7-8e03-5bd40a840017": Phase="Pending", Reason="", readiness=false. Elapsed: 3.292884ms
Sep 20 22:26:26.837: INFO: Pod "pod-secrets-26c609ea-b46a-45b7-8e03-5bd40a840017": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005478701s
STEP: Saw pod success
Sep 20 22:26:26.837: INFO: Pod "pod-secrets-26c609ea-b46a-45b7-8e03-5bd40a840017" satisfied condition "success or failure"
Sep 20 22:26:26.839: INFO: Trying to get logs from node worker-2 pod pod-secrets-26c609ea-b46a-45b7-8e03-5bd40a840017 container secret-volume-test: <nil>
STEP: delete the pod
Sep 20 22:26:26.851: INFO: Waiting for pod pod-secrets-26c609ea-b46a-45b7-8e03-5bd40a840017 to disappear
Sep 20 22:26:26.852: INFO: Pod pod-secrets-26c609ea-b46a-45b7-8e03-5bd40a840017 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:26:26.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-677" for this suite.
Sep 20 22:26:32.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:26:32.932: INFO: namespace secrets-677 deletion completed in 6.076936589s

• [SLOW TEST:8.129 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:26:32.932: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 22:26:32.971: INFO: (0) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 17.136641ms)
Sep 20 22:26:32.973: INFO: (1) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.944071ms)
Sep 20 22:26:32.975: INFO: (2) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.885263ms)
Sep 20 22:26:32.978: INFO: (3) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.190776ms)
Sep 20 22:26:32.979: INFO: (4) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.852425ms)
Sep 20 22:26:32.981: INFO: (5) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.748987ms)
Sep 20 22:26:32.983: INFO: (6) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.710054ms)
Sep 20 22:26:32.985: INFO: (7) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.601158ms)
Sep 20 22:26:32.986: INFO: (8) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.686838ms)
Sep 20 22:26:32.988: INFO: (9) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.802658ms)
Sep 20 22:26:32.990: INFO: (10) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.343859ms)
Sep 20 22:26:32.992: INFO: (11) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.911196ms)
Sep 20 22:26:32.995: INFO: (12) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.387178ms)
Sep 20 22:26:32.997: INFO: (13) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.376515ms)
Sep 20 22:26:32.999: INFO: (14) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.994969ms)
Sep 20 22:26:33.001: INFO: (15) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.752931ms)
Sep 20 22:26:33.003: INFO: (16) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.971706ms)
Sep 20 22:26:33.005: INFO: (17) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.957172ms)
Sep 20 22:26:33.007: INFO: (18) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.937024ms)
Sep 20 22:26:33.009: INFO: (19) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.673497ms)
[AfterEach] version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:26:33.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8674" for this suite.
Sep 20 22:26:39.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:26:39.099: INFO: namespace proxy-8674 deletion completed in 6.08863837s

• [SLOW TEST:6.167 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:26:39.099: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Sep 20 22:26:39.122: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Sep 20 22:26:39.848: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep 20 22:26:41.914: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 22:26:43.973: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 22:26:45.919: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 22:26:47.916: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 22:26:49.921: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 22:26:51.920: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 22:26:53.920: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 22:26:55.942: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 22:26:57.918: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 22:26:59.929: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 22:27:01.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 22:27:03.919: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 22:27:05.920: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 22:27:07.920: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 22:27:09.920: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 22:27:11.916: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 22:27:13.917: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615199, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 22:27:16.339: INFO: Waited 416.367053ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
I0920 22:27:16.734571      16 request.go:538] Throttling request took 56.672566ms, request: DELETE:https://10.96.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/wardler:aggregator-9452:sample-apiserver-reader
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:27:16.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9452" for this suite.
Sep 20 22:27:22.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:27:22.967: INFO: namespace aggregator-9452 deletion completed in 6.189518532s

• [SLOW TEST:43.868 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:27:22.967: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 20 22:27:25.018: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:27:25.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8978" for this suite.
Sep 20 22:27:31.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:27:31.096: INFO: namespace container-runtime-8978 deletion completed in 6.063360713s

• [SLOW TEST:8.129 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:27:31.096: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 22:27:31.524: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615251, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615251, loc:(*time.Location)(0x84be2c0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-86d95b659d\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615251, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615251, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 22:27:34.537: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:27:34.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-525" for this suite.
Sep 20 22:27:40.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:27:40.667: INFO: namespace webhook-525 deletion completed in 6.083184508s
STEP: Destroying namespace "webhook-525-markers" for this suite.
Sep 20 22:27:46.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:27:46.745: INFO: namespace webhook-525-markers deletion completed in 6.077824569s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:15.657 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:27:46.753: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 20 22:27:46.784: INFO: Waiting up to 5m0s for pod "pod-29bbbd2c-e406-4b78-89f9-d3e03c0c8e83" in namespace "emptydir-2309" to be "success or failure"
Sep 20 22:27:46.789: INFO: Pod "pod-29bbbd2c-e406-4b78-89f9-d3e03c0c8e83": Phase="Pending", Reason="", readiness=false. Elapsed: 4.890828ms
Sep 20 22:27:48.797: INFO: Pod "pod-29bbbd2c-e406-4b78-89f9-d3e03c0c8e83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012920205s
STEP: Saw pod success
Sep 20 22:27:48.797: INFO: Pod "pod-29bbbd2c-e406-4b78-89f9-d3e03c0c8e83" satisfied condition "success or failure"
Sep 20 22:27:48.799: INFO: Trying to get logs from node worker-2 pod pod-29bbbd2c-e406-4b78-89f9-d3e03c0c8e83 container test-container: <nil>
STEP: delete the pod
Sep 20 22:27:48.810: INFO: Waiting for pod pod-29bbbd2c-e406-4b78-89f9-d3e03c0c8e83 to disappear
Sep 20 22:27:48.813: INFO: Pod pod-29bbbd2c-e406-4b78-89f9-d3e03c0c8e83 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:27:48.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2309" for this suite.
Sep 20 22:27:54.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:27:54.893: INFO: namespace emptydir-2309 deletion completed in 6.077868585s

• [SLOW TEST:8.139 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:27:54.893: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-588972ce-8cdd-43f2-87de-128dfd0df996
STEP: Creating a pod to test consume configMaps
Sep 20 22:27:54.970: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4a4158a7-ed12-4fa5-998d-7ceda4bfc8d2" in namespace "projected-1004" to be "success or failure"
Sep 20 22:27:54.980: INFO: Pod "pod-projected-configmaps-4a4158a7-ed12-4fa5-998d-7ceda4bfc8d2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.845382ms
Sep 20 22:27:56.989: INFO: Pod "pod-projected-configmaps-4a4158a7-ed12-4fa5-998d-7ceda4bfc8d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019530047s
STEP: Saw pod success
Sep 20 22:27:56.989: INFO: Pod "pod-projected-configmaps-4a4158a7-ed12-4fa5-998d-7ceda4bfc8d2" satisfied condition "success or failure"
Sep 20 22:27:56.993: INFO: Trying to get logs from node worker-2 pod pod-projected-configmaps-4a4158a7-ed12-4fa5-998d-7ceda4bfc8d2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 22:27:57.026: INFO: Waiting for pod pod-projected-configmaps-4a4158a7-ed12-4fa5-998d-7ceda4bfc8d2 to disappear
Sep 20 22:27:57.031: INFO: Pod pod-projected-configmaps-4a4158a7-ed12-4fa5-998d-7ceda4bfc8d2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:27:57.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1004" for this suite.
Sep 20 22:28:03.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:28:03.129: INFO: namespace projected-1004 deletion completed in 6.095393295s

• [SLOW TEST:8.236 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:28:03.130: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:28:05.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5169" for this suite.
Sep 20 22:28:55.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:28:55.277: INFO: namespace kubelet-test-5169 deletion completed in 50.088042593s

• [SLOW TEST:52.147 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:28:55.277: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-650c7b9e-1850-4bb4-8f91-d581c01fd865
STEP: Creating configMap with name cm-test-opt-upd-80220722-7716-4fef-9a31-5871f1f810b1
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-650c7b9e-1850-4bb4-8f91-d581c01fd865
STEP: Updating configmap cm-test-opt-upd-80220722-7716-4fef-9a31-5871f1f810b1
STEP: Creating configMap with name cm-test-opt-create-e0876f70-78c9-42db-aa2a-bfbf1fa425bf
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:28:59.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8172" for this suite.
Sep 20 22:29:17.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:29:17.529: INFO: namespace configmap-8172 deletion completed in 18.098650616s

• [SLOW TEST:22.252 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:29:17.530: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 20 22:29:25.667: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 20 22:29:25.671: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 20 22:29:27.685: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 20 22:29:27.690: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 20 22:29:29.700: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 20 22:29:29.705: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 20 22:29:31.671: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 20 22:29:31.677: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 20 22:29:33.671: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 20 22:29:33.676: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:29:33.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6503" for this suite.
Sep 20 22:29:45.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:29:45.762: INFO: namespace container-lifecycle-hook-6503 deletion completed in 12.077753367s

• [SLOW TEST:28.232 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:29:45.762: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Sep 20 22:29:45.783: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-786974626 proxy --unix-socket=/tmp/kubectl-proxy-unix214096893/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:29:45.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9983" for this suite.
Sep 20 22:29:51.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:29:51.907: INFO: namespace kubectl-9983 deletion completed in 6.085398644s

• [SLOW TEST:6.145 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:29:51.907: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8666.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8666.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8666.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8666.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8666.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8666.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8666.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8666.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8666.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8666.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 71.49.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.49.71_udp@PTR;check="$$(dig +tcp +noall +answer +search 71.49.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.49.71_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8666.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8666.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8666.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8666.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8666.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8666.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8666.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8666.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8666.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8666.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8666.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 71.49.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.49.71_udp@PTR;check="$$(dig +tcp +noall +answer +search 71.49.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.49.71_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 20 22:29:56.077: INFO: Unable to read wheezy_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:29:56.083: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:29:56.087: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:29:56.090: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:29:56.119: INFO: Unable to read jessie_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:29:56.123: INFO: Unable to read jessie_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:29:56.126: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:29:56.130: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:29:56.152: INFO: Lookups using dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091 failed for: [wheezy_udp@dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local jessie_udp@dns-test-service.dns-8666.svc.cluster.local jessie_tcp@dns-test-service.dns-8666.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local]

Sep 20 22:30:01.163: INFO: Unable to read wheezy_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:01.168: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:01.173: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:01.178: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:01.204: INFO: Unable to read jessie_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:01.207: INFO: Unable to read jessie_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:01.211: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:01.214: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:01.234: INFO: Lookups using dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091 failed for: [wheezy_udp@dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local jessie_udp@dns-test-service.dns-8666.svc.cluster.local jessie_tcp@dns-test-service.dns-8666.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local]

Sep 20 22:30:06.163: INFO: Unable to read wheezy_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:06.171: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:06.179: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:06.187: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:06.225: INFO: Unable to read jessie_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:06.228: INFO: Unable to read jessie_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:06.230: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:06.233: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:06.245: INFO: Lookups using dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091 failed for: [wheezy_udp@dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local jessie_udp@dns-test-service.dns-8666.svc.cluster.local jessie_tcp@dns-test-service.dns-8666.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local]

Sep 20 22:30:11.156: INFO: Unable to read wheezy_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:11.160: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:11.163: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:11.168: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:11.188: INFO: Unable to read jessie_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:11.191: INFO: Unable to read jessie_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:11.193: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:11.197: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:11.219: INFO: Lookups using dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091 failed for: [wheezy_udp@dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local jessie_udp@dns-test-service.dns-8666.svc.cluster.local jessie_tcp@dns-test-service.dns-8666.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local]

Sep 20 22:30:16.162: INFO: Unable to read wheezy_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:16.170: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:16.177: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:16.185: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:16.236: INFO: Unable to read jessie_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:16.241: INFO: Unable to read jessie_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:16.245: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:16.249: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:16.270: INFO: Lookups using dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091 failed for: [wheezy_udp@dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local jessie_udp@dns-test-service.dns-8666.svc.cluster.local jessie_tcp@dns-test-service.dns-8666.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local]

Sep 20 22:30:21.155: INFO: Unable to read wheezy_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:21.157: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:21.160: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:21.162: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:21.177: INFO: Unable to read jessie_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:21.180: INFO: Unable to read jessie_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:21.183: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:21.185: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091: the server could not find the requested resource (get pods dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091)
Sep 20 22:30:21.198: INFO: Lookups using dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091 failed for: [wheezy_udp@dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local jessie_udp@dns-test-service.dns-8666.svc.cluster.local jessie_tcp@dns-test-service.dns-8666.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local]

Sep 20 22:30:26.249: INFO: DNS probes using dns-8666/dns-test-a1dc6e24-79e4-49cd-9bdb-3a3a6998c091 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:30:26.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8666" for this suite.
Sep 20 22:30:32.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:30:32.401: INFO: namespace dns-8666 deletion completed in 6.090117542s

• [SLOW TEST:40.494 seconds]
[sig-network] DNS
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:30:32.402: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Sep 20 22:30:34.982: INFO: Successfully updated pod "annotationupdateaaa8b320-4a39-4ec0-ad12-0ff7fcc989b0"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:30:39.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6442" for this suite.
Sep 20 22:30:57.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:30:57.172: INFO: namespace projected-6442 deletion completed in 18.127150781s

• [SLOW TEST:24.770 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:30:57.173: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:31:01.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2827" for this suite.
Sep 20 22:31:19.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:31:19.323: INFO: namespace containers-2827 deletion completed in 18.080147972s

• [SLOW TEST:22.150 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:31:19.325: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Sep 20 22:31:19.426: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:31:32.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1630" for this suite.
Sep 20 22:31:38.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:31:38.544: INFO: namespace crd-publish-openapi-1630 deletion completed in 6.073891046s

• [SLOW TEST:19.220 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:31:38.545: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6299.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6299.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6299.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6299.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6299.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6299.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 20 22:31:42.635: INFO: Unable to read jessie_hosts@dns-querier-2.dns-test-service-2.dns-6299.svc.cluster.local from pod dns-6299/dns-test-4504f2f2-a7e1-44ac-aa27-4aa3912ef734: the server could not find the requested resource (get pods dns-test-4504f2f2-a7e1-44ac-aa27-4aa3912ef734)
Sep 20 22:31:42.639: INFO: Unable to read jessie_hosts@dns-querier-2 from pod dns-6299/dns-test-4504f2f2-a7e1-44ac-aa27-4aa3912ef734: the server could not find the requested resource (get pods dns-test-4504f2f2-a7e1-44ac-aa27-4aa3912ef734)
Sep 20 22:31:42.642: INFO: Unable to read jessie_udp@PodARecord from pod dns-6299/dns-test-4504f2f2-a7e1-44ac-aa27-4aa3912ef734: the server could not find the requested resource (get pods dns-test-4504f2f2-a7e1-44ac-aa27-4aa3912ef734)
Sep 20 22:31:42.645: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6299/dns-test-4504f2f2-a7e1-44ac-aa27-4aa3912ef734: the server could not find the requested resource (get pods dns-test-4504f2f2-a7e1-44ac-aa27-4aa3912ef734)
Sep 20 22:31:42.645: INFO: Lookups using dns-6299/dns-test-4504f2f2-a7e1-44ac-aa27-4aa3912ef734 failed for: [jessie_hosts@dns-querier-2.dns-test-service-2.dns-6299.svc.cluster.local jessie_hosts@dns-querier-2 jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep 20 22:31:47.691: INFO: Unable to read jessie_hosts@dns-querier-2 from pod dns-6299/dns-test-4504f2f2-a7e1-44ac-aa27-4aa3912ef734: the server could not find the requested resource (get pods dns-test-4504f2f2-a7e1-44ac-aa27-4aa3912ef734)
Sep 20 22:31:47.699: INFO: Unable to read jessie_udp@PodARecord from pod dns-6299/dns-test-4504f2f2-a7e1-44ac-aa27-4aa3912ef734: the server could not find the requested resource (get pods dns-test-4504f2f2-a7e1-44ac-aa27-4aa3912ef734)
Sep 20 22:31:47.705: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6299/dns-test-4504f2f2-a7e1-44ac-aa27-4aa3912ef734: the server could not find the requested resource (get pods dns-test-4504f2f2-a7e1-44ac-aa27-4aa3912ef734)
Sep 20 22:31:47.705: INFO: Lookups using dns-6299/dns-test-4504f2f2-a7e1-44ac-aa27-4aa3912ef734 failed for: [jessie_hosts@dns-querier-2 jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep 20 22:31:52.704: INFO: DNS probes using dns-6299/dns-test-4504f2f2-a7e1-44ac-aa27-4aa3912ef734 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:31:52.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6299" for this suite.
Sep 20 22:31:58.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:31:58.826: INFO: namespace dns-6299 deletion completed in 6.080394785s

• [SLOW TEST:20.281 seconds]
[sig-network] DNS
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:31:58.828: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 22:31:58.851: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:32:00.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4752" for this suite.
Sep 20 22:32:44.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:32:45.069: INFO: namespace pods-4752 deletion completed in 44.093148371s

• [SLOW TEST:46.241 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:32:45.069: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Sep 20 22:32:45.096: INFO: Waiting up to 5m0s for pod "downward-api-7f131512-ffb9-40a5-a484-16a47e5eec10" in namespace "downward-api-846" to be "success or failure"
Sep 20 22:32:45.100: INFO: Pod "downward-api-7f131512-ffb9-40a5-a484-16a47e5eec10": Phase="Pending", Reason="", readiness=false. Elapsed: 4.228765ms
Sep 20 22:32:47.102: INFO: Pod "downward-api-7f131512-ffb9-40a5-a484-16a47e5eec10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006301392s
STEP: Saw pod success
Sep 20 22:32:47.102: INFO: Pod "downward-api-7f131512-ffb9-40a5-a484-16a47e5eec10" satisfied condition "success or failure"
Sep 20 22:32:47.104: INFO: Trying to get logs from node worker-2 pod downward-api-7f131512-ffb9-40a5-a484-16a47e5eec10 container dapi-container: <nil>
STEP: delete the pod
Sep 20 22:32:47.127: INFO: Waiting for pod downward-api-7f131512-ffb9-40a5-a484-16a47e5eec10 to disappear
Sep 20 22:32:47.130: INFO: Pod downward-api-7f131512-ffb9-40a5-a484-16a47e5eec10 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:32:47.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-846" for this suite.
Sep 20 22:32:53.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:32:53.186: INFO: namespace downward-api-846 deletion completed in 6.053127242s

• [SLOW TEST:8.116 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:32:53.186: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:32:57.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3915" for this suite.
Sep 20 22:33:03.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:33:03.359: INFO: namespace emptydir-wrapper-3915 deletion completed in 6.101377664s

• [SLOW TEST:10.173 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:33:03.359: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:33:08.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6084" for this suite.
Sep 20 22:33:20.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:33:20.531: INFO: namespace replication-controller-6084 deletion completed in 12.110210057s

• [SLOW TEST:17.172 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:33:20.532: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Sep 20 22:33:25.080: INFO: Successfully updated pod "labelsupdate29941c6a-bcd6-4f1d-b323-7f7e1d08e585"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:33:27.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6139" for this suite.
Sep 20 22:33:45.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:33:45.169: INFO: namespace projected-6139 deletion completed in 18.059709054s

• [SLOW TEST:24.638 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:33:45.169: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-fn4p
STEP: Creating a pod to test atomic-volume-subpath
Sep 20 22:33:45.203: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-fn4p" in namespace "subpath-756" to be "success or failure"
Sep 20 22:33:45.206: INFO: Pod "pod-subpath-test-downwardapi-fn4p": Phase="Pending", Reason="", readiness=false. Elapsed: 3.128582ms
Sep 20 22:33:47.213: INFO: Pod "pod-subpath-test-downwardapi-fn4p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009882849s
Sep 20 22:33:49.217: INFO: Pod "pod-subpath-test-downwardapi-fn4p": Phase="Running", Reason="", readiness=true. Elapsed: 4.013957326s
Sep 20 22:33:51.227: INFO: Pod "pod-subpath-test-downwardapi-fn4p": Phase="Running", Reason="", readiness=true. Elapsed: 6.024315389s
Sep 20 22:33:53.233: INFO: Pod "pod-subpath-test-downwardapi-fn4p": Phase="Running", Reason="", readiness=true. Elapsed: 8.029967798s
Sep 20 22:33:55.235: INFO: Pod "pod-subpath-test-downwardapi-fn4p": Phase="Running", Reason="", readiness=true. Elapsed: 10.032283845s
Sep 20 22:33:57.268: INFO: Pod "pod-subpath-test-downwardapi-fn4p": Phase="Running", Reason="", readiness=true. Elapsed: 12.064606832s
Sep 20 22:33:59.273: INFO: Pod "pod-subpath-test-downwardapi-fn4p": Phase="Running", Reason="", readiness=true. Elapsed: 14.069718949s
Sep 20 22:34:01.282: INFO: Pod "pod-subpath-test-downwardapi-fn4p": Phase="Running", Reason="", readiness=true. Elapsed: 16.079096444s
Sep 20 22:34:03.299: INFO: Pod "pod-subpath-test-downwardapi-fn4p": Phase="Running", Reason="", readiness=true. Elapsed: 18.095549404s
Sep 20 22:34:05.305: INFO: Pod "pod-subpath-test-downwardapi-fn4p": Phase="Running", Reason="", readiness=true. Elapsed: 20.101487675s
Sep 20 22:34:07.316: INFO: Pod "pod-subpath-test-downwardapi-fn4p": Phase="Running", Reason="", readiness=true. Elapsed: 22.11254153s
Sep 20 22:34:09.328: INFO: Pod "pod-subpath-test-downwardapi-fn4p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.124573933s
STEP: Saw pod success
Sep 20 22:34:09.328: INFO: Pod "pod-subpath-test-downwardapi-fn4p" satisfied condition "success or failure"
Sep 20 22:34:09.332: INFO: Trying to get logs from node worker-2 pod pod-subpath-test-downwardapi-fn4p container test-container-subpath-downwardapi-fn4p: <nil>
STEP: delete the pod
Sep 20 22:34:09.373: INFO: Waiting for pod pod-subpath-test-downwardapi-fn4p to disappear
Sep 20 22:34:09.380: INFO: Pod pod-subpath-test-downwardapi-fn4p no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-fn4p
Sep 20 22:34:09.380: INFO: Deleting pod "pod-subpath-test-downwardapi-fn4p" in namespace "subpath-756"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:34:09.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-756" for this suite.
Sep 20 22:34:15.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:34:15.469: INFO: namespace subpath-756 deletion completed in 6.082769585s

• [SLOW TEST:30.300 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:34:15.469: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3398.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3398.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3398.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3398.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3398.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3398.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 20 22:34:19.531: INFO: Unable to read jessie_hosts@dns-querier-1.dns-test-service.dns-3398.svc.cluster.local from pod dns-3398/dns-test-ea18b705-32c3-4cb6-a87b-6a1e13e9954a: the server could not find the requested resource (get pods dns-test-ea18b705-32c3-4cb6-a87b-6a1e13e9954a)
Sep 20 22:34:19.534: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-3398/dns-test-ea18b705-32c3-4cb6-a87b-6a1e13e9954a: the server could not find the requested resource (get pods dns-test-ea18b705-32c3-4cb6-a87b-6a1e13e9954a)
Sep 20 22:34:19.536: INFO: Unable to read jessie_udp@PodARecord from pod dns-3398/dns-test-ea18b705-32c3-4cb6-a87b-6a1e13e9954a: the server could not find the requested resource (get pods dns-test-ea18b705-32c3-4cb6-a87b-6a1e13e9954a)
Sep 20 22:34:19.538: INFO: Unable to read jessie_tcp@PodARecord from pod dns-3398/dns-test-ea18b705-32c3-4cb6-a87b-6a1e13e9954a: the server could not find the requested resource (get pods dns-test-ea18b705-32c3-4cb6-a87b-6a1e13e9954a)
Sep 20 22:34:19.538: INFO: Lookups using dns-3398/dns-test-ea18b705-32c3-4cb6-a87b-6a1e13e9954a failed for: [jessie_hosts@dns-querier-1.dns-test-service.dns-3398.svc.cluster.local jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep 20 22:34:24.583: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-3398/dns-test-ea18b705-32c3-4cb6-a87b-6a1e13e9954a: the server could not find the requested resource (get pods dns-test-ea18b705-32c3-4cb6-a87b-6a1e13e9954a)
Sep 20 22:34:24.586: INFO: Unable to read jessie_udp@PodARecord from pod dns-3398/dns-test-ea18b705-32c3-4cb6-a87b-6a1e13e9954a: the server could not find the requested resource (get pods dns-test-ea18b705-32c3-4cb6-a87b-6a1e13e9954a)
Sep 20 22:34:24.589: INFO: Unable to read jessie_tcp@PodARecord from pod dns-3398/dns-test-ea18b705-32c3-4cb6-a87b-6a1e13e9954a: the server could not find the requested resource (get pods dns-test-ea18b705-32c3-4cb6-a87b-6a1e13e9954a)
Sep 20 22:34:24.589: INFO: Lookups using dns-3398/dns-test-ea18b705-32c3-4cb6-a87b-6a1e13e9954a failed for: [jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep 20 22:34:29.578: INFO: DNS probes using dns-3398/dns-test-ea18b705-32c3-4cb6-a87b-6a1e13e9954a succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:34:29.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3398" for this suite.
Sep 20 22:34:35.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:34:35.709: INFO: namespace dns-3398 deletion completed in 6.105505371s

• [SLOW TEST:20.240 seconds]
[sig-network] DNS
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:34:35.710: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 22:34:35.751: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5953c52c-db94-4e7d-8584-160b5f80955b" in namespace "projected-1084" to be "success or failure"
Sep 20 22:34:35.755: INFO: Pod "downwardapi-volume-5953c52c-db94-4e7d-8584-160b5f80955b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.561164ms
Sep 20 22:34:37.762: INFO: Pod "downwardapi-volume-5953c52c-db94-4e7d-8584-160b5f80955b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011064191s
STEP: Saw pod success
Sep 20 22:34:37.762: INFO: Pod "downwardapi-volume-5953c52c-db94-4e7d-8584-160b5f80955b" satisfied condition "success or failure"
Sep 20 22:34:37.767: INFO: Trying to get logs from node worker-2 pod downwardapi-volume-5953c52c-db94-4e7d-8584-160b5f80955b container client-container: <nil>
STEP: delete the pod
Sep 20 22:34:37.790: INFO: Waiting for pod downwardapi-volume-5953c52c-db94-4e7d-8584-160b5f80955b to disappear
Sep 20 22:34:37.792: INFO: Pod downwardapi-volume-5953c52c-db94-4e7d-8584-160b5f80955b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:34:37.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1084" for this suite.
Sep 20 22:34:43.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:34:43.873: INFO: namespace projected-1084 deletion completed in 6.078744313s

• [SLOW TEST:8.163 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:34:43.873: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
I0920 22:34:44.337942      16 request.go:538] Throttling request took 59.065248ms, request: POST:https://10.96.0.1:443/api/v1/namespaces/watch-9247/configmaps
I0920 22:34:46.279236      16 request.go:538] Throttling request took 50.060428ms, request: POST:https://10.96.0.1:443/api/v1/namespaces/watch-9247/configmaps
I0920 22:34:47.199139      16 request.go:538] Throttling request took 58.157005ms, request: PUT:https://10.96.0.1:443/api/v1/namespaces/watch-9247/configmaps/cm-33
I0920 22:34:47.329250      16 request.go:538] Throttling request took 90.439594ms, request: POST:https://10.96.0.1:443/api/v1/namespaces/watch-9247/configmaps
I0920 22:34:47.403615      16 request.go:538] Throttling request took 66.333906ms, request: PUT:https://10.96.0.1:443/api/v1/namespaces/watch-9247/configmaps/cm-39
I0920 22:34:47.721067      16 request.go:538] Throttling request took 83.199199ms, request: POST:https://10.96.0.1:443/api/v1/namespaces/watch-9247/configmaps
I0920 22:34:47.883635      16 request.go:538] Throttling request took 50.869338ms, request: POST:https://10.96.0.1:443/api/v1/namespaces/watch-9247/configmaps
I0920 22:34:48.135775      16 request.go:538] Throttling request took 55.573384ms, request: POST:https://10.96.0.1:443/api/v1/namespaces/watch-9247/configmaps
I0920 22:34:48.843783      16 request.go:538] Throttling request took 60.643848ms, request: PUT:https://10.96.0.1:443/api/v1/namespaces/watch-9247/configmaps/cm-46
I0920 22:34:49.032744      16 request.go:538] Throttling request took 52.04031ms, request: DELETE:https://10.96.0.1:443/api/v1/namespaces/watch-9247/configmaps/cm-48
I0920 22:34:49.244907      16 request.go:538] Throttling request took 58.766299ms, request: POST:https://10.96.0.1:443/api/v1/namespaces/watch-9247/configmaps
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:34:49.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9247" for this suite.
I0920 22:34:49.332772      16 request.go:538] Throttling request took 51.368202ms, request: DELETE:https://10.96.0.1:443/api/v1/namespaces/watch-9247
Sep 20 22:34:55.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:34:55.457: INFO: namespace watch-9247 deletion completed in 6.176156436s

• [SLOW TEST:11.584 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:34:55.458: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep 20 22:34:55.800: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Sep 20 22:34:57.820: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615695, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615695, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615695, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704615695, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 22:35:00.840: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 22:35:00.842: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:35:01.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9227" for this suite.
Sep 20 22:35:07.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:35:08.064: INFO: namespace crd-webhook-9227 deletion completed in 6.110122796s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:12.615 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:35:08.074: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Sep 20 22:35:08.093: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 20 22:35:08.099: INFO: Waiting for terminating namespaces to be deleted...
Sep 20 22:35:08.100: INFO: 
Logging pods the kubelet thinks is on node controlplane-1 before test
Sep 20 22:35:08.109: INFO: weave-net-n52f9 from kube-system started at 2019-09-20 21:41:30 +0000 UTC (2 container statuses recorded)
Sep 20 22:35:08.109: INFO: 	Container weave ready: true, restart count 0
Sep 20 22:35:08.109: INFO: 	Container weave-npc ready: true, restart count 0
Sep 20 22:35:08.109: INFO: kube-proxy-62wrx from kube-system started at 2019-09-20 21:29:38 +0000 UTC (1 container statuses recorded)
Sep 20 22:35:08.110: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 22:35:08.110: INFO: etcd-controlplane-1 from kube-system started at 2019-09-20 21:29:15 +0000 UTC (1 container statuses recorded)
Sep 20 22:35:08.110: INFO: 	Container etcd ready: true, restart count 0
Sep 20 22:35:08.110: INFO: kube-scheduler-controlplane-1 from kube-system started at 2019-09-20 21:29:15 +0000 UTC (1 container statuses recorded)
Sep 20 22:35:08.110: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep 20 22:35:08.110: INFO: sonobuoy-systemd-logs-daemon-set-63bbc52511204dd7-g8gl8 from sonobuoy started at 2019-09-20 21:43:06 +0000 UTC (2 container statuses recorded)
Sep 20 22:35:08.110: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 22:35:08.110: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 22:35:08.110: INFO: kube-controller-manager-controlplane-1 from kube-system started at 2019-09-20 21:29:15 +0000 UTC (1 container statuses recorded)
Sep 20 22:35:08.110: INFO: 	Container kube-controller-manager ready: true, restart count 0
Sep 20 22:35:08.110: INFO: kube-apiserver-controlplane-1 from kube-system started at 2019-09-20 21:29:15 +0000 UTC (1 container statuses recorded)
Sep 20 22:35:08.110: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep 20 22:35:08.110: INFO: 
Logging pods the kubelet thinks is on node worker-1 before test
Sep 20 22:35:08.119: INFO: weave-net-bnv9z from kube-system started at 2019-09-20 21:41:30 +0000 UTC (2 container statuses recorded)
Sep 20 22:35:08.119: INFO: 	Container weave ready: true, restart count 0
Sep 20 22:35:08.119: INFO: 	Container weave-npc ready: true, restart count 0
Sep 20 22:35:08.119: INFO: coredns-5644d7b6d9-pvh6f from kube-system started at 2019-09-20 21:42:42 +0000 UTC (1 container statuses recorded)
Sep 20 22:35:08.119: INFO: 	Container coredns ready: true, restart count 0
Sep 20 22:35:08.119: INFO: sonobuoy-e2e-job-558ae789799345e3 from sonobuoy started at 2019-09-20 21:43:07 +0000 UTC (2 container statuses recorded)
Sep 20 22:35:08.119: INFO: 	Container e2e ready: true, restart count 0
Sep 20 22:35:08.119: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 22:35:08.119: INFO: sonobuoy-systemd-logs-daemon-set-63bbc52511204dd7-7p8gp from sonobuoy started at 2019-09-20 21:43:07 +0000 UTC (2 container statuses recorded)
Sep 20 22:35:08.119: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 22:35:08.119: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 22:35:08.119: INFO: kube-proxy-dxbgr from kube-system started at 2019-09-20 21:29:43 +0000 UTC (1 container statuses recorded)
Sep 20 22:35:08.119: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 22:35:08.119: INFO: 
Logging pods the kubelet thinks is on node worker-2 before test
Sep 20 22:35:08.122: INFO: coredns-5644d7b6d9-47g5v from kube-system started at 2019-09-20 21:42:42 +0000 UTC (1 container statuses recorded)
Sep 20 22:35:08.122: INFO: 	Container coredns ready: true, restart count 0
Sep 20 22:35:08.122: INFO: kube-proxy-dcssj from kube-system started at 2019-09-20 21:29:43 +0000 UTC (1 container statuses recorded)
Sep 20 22:35:08.122: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 22:35:08.122: INFO: sonobuoy-systemd-logs-daemon-set-63bbc52511204dd7-4bfzz from sonobuoy started at 2019-09-20 21:43:07 +0000 UTC (2 container statuses recorded)
Sep 20 22:35:08.122: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 22:35:08.122: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 22:35:08.122: INFO: sonobuoy from sonobuoy started at 2019-09-20 21:42:42 +0000 UTC (1 container statuses recorded)
Sep 20 22:35:08.123: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 20 22:35:08.123: INFO: weave-net-cwtjw from kube-system started at 2019-09-20 21:41:30 +0000 UTC (2 container statuses recorded)
Sep 20 22:35:08.123: INFO: 	Container weave ready: true, restart count 0
Sep 20 22:35:08.123: INFO: 	Container weave-npc ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-2cb4f9c6-2143-4355-919f-42e6a40023e6 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-2cb4f9c6-2143-4355-919f-42e6a40023e6 off the node worker-2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-2cb4f9c6-2143-4355-919f-42e6a40023e6
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:40:12.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5581" for this suite.
Sep 20 22:40:22.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:40:22.346: INFO: namespace sched-pred-5581 deletion completed in 10.096304544s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:314.272 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
I0920 22:40:22.346429      16 request.go:706] Error in request: resource name may not be empty
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:40:22.348: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 22:40:22.375: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cbb9f705-3138-4db3-9665-90e9995b58a2" in namespace "projected-1549" to be "success or failure"
Sep 20 22:40:22.381: INFO: Pod "downwardapi-volume-cbb9f705-3138-4db3-9665-90e9995b58a2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.520837ms
Sep 20 22:40:24.386: INFO: Pod "downwardapi-volume-cbb9f705-3138-4db3-9665-90e9995b58a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010973794s
STEP: Saw pod success
Sep 20 22:40:24.386: INFO: Pod "downwardapi-volume-cbb9f705-3138-4db3-9665-90e9995b58a2" satisfied condition "success or failure"
Sep 20 22:40:24.391: INFO: Trying to get logs from node worker-2 pod downwardapi-volume-cbb9f705-3138-4db3-9665-90e9995b58a2 container client-container: <nil>
STEP: delete the pod
Sep 20 22:40:24.452: INFO: Waiting for pod downwardapi-volume-cbb9f705-3138-4db3-9665-90e9995b58a2 to disappear
Sep 20 22:40:24.458: INFO: Pod downwardapi-volume-cbb9f705-3138-4db3-9665-90e9995b58a2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:40:24.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1549" for this suite.
Sep 20 22:40:30.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:40:30.550: INFO: namespace projected-1549 deletion completed in 6.088203131s

• [SLOW TEST:8.202 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:40:30.552: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Sep 20 22:40:35.123: INFO: Successfully updated pod "labelsupdateec21f022-3448-4ae8-a768-99ac55cd8ac6"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:40:37.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6771" for this suite.
Sep 20 22:40:55.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:40:55.234: INFO: namespace downward-api-6771 deletion completed in 18.085596092s

• [SLOW TEST:24.682 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:40:55.236: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:40:55.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-8317" for this suite.
Sep 20 22:41:01.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:41:01.364: INFO: namespace tables-8317 deletion completed in 6.100917397s

• [SLOW TEST:6.128 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:41:01.366: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:41:01.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3754" for this suite.
Sep 20 22:41:07.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:41:07.468: INFO: namespace kubelet-test-3754 deletion completed in 6.059567761s

• [SLOW TEST:6.102 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:41:07.468: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 20 22:41:07.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-7625'
Sep 20 22:41:08.018: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 20 22:41:08.018: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Sep 20 22:41:10.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 delete deployment e2e-test-httpd-deployment --namespace=kubectl-7625'
Sep 20 22:41:10.118: INFO: stderr: ""
Sep 20 22:41:10.118: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:41:10.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7625" for this suite.
Sep 20 22:41:16.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:41:16.231: INFO: namespace kubectl-7625 deletion completed in 6.109323787s

• [SLOW TEST:8.763 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:41:16.231: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Sep 20 22:41:16.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 create -f - --namespace=kubectl-5122'
Sep 20 22:41:16.374: INFO: stderr: ""
Sep 20 22:41:16.374: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 20 22:41:16.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5122'
Sep 20 22:41:16.441: INFO: stderr: ""
Sep 20 22:41:16.441: INFO: stdout: "update-demo-nautilus-ggptf update-demo-nautilus-zp6l9 "
Sep 20 22:41:16.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-nautilus-ggptf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5122'
Sep 20 22:41:16.490: INFO: stderr: ""
Sep 20 22:41:16.490: INFO: stdout: ""
Sep 20 22:41:16.490: INFO: update-demo-nautilus-ggptf is created but not running
Sep 20 22:41:21.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5122'
Sep 20 22:41:21.578: INFO: stderr: ""
Sep 20 22:41:21.578: INFO: stdout: "update-demo-nautilus-ggptf update-demo-nautilus-zp6l9 "
Sep 20 22:41:21.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-nautilus-ggptf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5122'
Sep 20 22:41:21.623: INFO: stderr: ""
Sep 20 22:41:21.623: INFO: stdout: "true"
Sep 20 22:41:21.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-nautilus-ggptf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5122'
Sep 20 22:41:21.674: INFO: stderr: ""
Sep 20 22:41:21.674: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 20 22:41:21.674: INFO: validating pod update-demo-nautilus-ggptf
Sep 20 22:41:21.678: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 22:41:21.678: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 22:41:21.678: INFO: update-demo-nautilus-ggptf is verified up and running
Sep 20 22:41:21.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-nautilus-zp6l9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5122'
Sep 20 22:41:21.735: INFO: stderr: ""
Sep 20 22:41:21.735: INFO: stdout: "true"
Sep 20 22:41:21.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-nautilus-zp6l9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5122'
Sep 20 22:41:21.790: INFO: stderr: ""
Sep 20 22:41:21.790: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 20 22:41:21.790: INFO: validating pod update-demo-nautilus-zp6l9
Sep 20 22:41:21.793: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 22:41:21.793: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 22:41:21.793: INFO: update-demo-nautilus-zp6l9 is verified up and running
STEP: using delete to clean up resources
Sep 20 22:41:21.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 delete --grace-period=0 --force -f - --namespace=kubectl-5122'
Sep 20 22:41:21.845: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 22:41:21.845: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 20 22:41:21.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5122'
Sep 20 22:41:21.900: INFO: stderr: "No resources found in kubectl-5122 namespace.\n"
Sep 20 22:41:21.900: INFO: stdout: ""
Sep 20 22:41:21.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods -l name=update-demo --namespace=kubectl-5122 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 20 22:41:21.952: INFO: stderr: ""
Sep 20 22:41:21.952: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:41:21.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5122" for this suite.
Sep 20 22:41:49.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:41:50.042: INFO: namespace kubectl-5122 deletion completed in 28.087586935s

• [SLOW TEST:33.810 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:41:50.042: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8850.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8850.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 20 22:41:52.137: INFO: DNS probes using dns-8850/dns-test-58af588d-4580-490a-aff5-a79795576c27 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:41:52.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8850" for this suite.
Sep 20 22:41:58.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:41:58.195: INFO: namespace dns-8850 deletion completed in 6.046340033s

• [SLOW TEST:8.153 seconds]
[sig-network] DNS
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:41:58.195: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 20 22:42:02.257: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 20 22:42:02.259: INFO: Pod pod-with-poststart-http-hook still exists
Sep 20 22:42:04.262: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 20 22:42:04.268: INFO: Pod pod-with-poststart-http-hook still exists
Sep 20 22:42:06.259: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 20 22:42:06.262: INFO: Pod pod-with-poststart-http-hook still exists
Sep 20 22:42:08.260: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 20 22:42:08.261: INFO: Pod pod-with-poststart-http-hook still exists
Sep 20 22:42:10.259: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 20 22:42:10.262: INFO: Pod pod-with-poststart-http-hook still exists
Sep 20 22:42:12.266: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 20 22:42:12.269: INFO: Pod pod-with-poststart-http-hook still exists
Sep 20 22:42:14.259: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 20 22:42:14.261: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:42:14.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2135" for this suite.
Sep 20 22:42:26.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:42:26.342: INFO: namespace container-lifecycle-hook-2135 deletion completed in 12.078679181s

• [SLOW TEST:28.147 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:42:26.343: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2663
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-2663
I0920 22:42:26.385812      16 runners.go:184] Created replication controller with name: externalname-service, namespace: services-2663, replica count: 2
I0920 22:42:26.385868      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 22:42:26.385878      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 22:42:29.436491      16 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 20 22:42:29.436: INFO: Creating new exec pod
I0920 22:42:31.449495      16 reflector.go:120] Starting reflector *v1.Endpoints (0s) from k8s.io/kubernetes/test/e2e/framework/service/jig.go:389
I0920 22:42:31.449521      16 reflector.go:158] Listing and watching *v1.Endpoints from k8s.io/kubernetes/test/e2e/framework/service/jig.go:389
Sep 20 22:42:32.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=services-2663 execpod4r2cm -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Sep 20 22:42:32.617: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 20 22:42:32.617: INFO: stdout: ""
Sep 20 22:42:32.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=services-2663 execpod4r2cm -- /bin/sh -x -c nc -zv -t -w 2 10.108.70.99 80'
Sep 20 22:42:32.769: INFO: stderr: "+ nc -zv -t -w 2 10.108.70.99 80\nConnection to 10.108.70.99 80 port [tcp/http] succeeded!\n"
Sep 20 22:42:32.769: INFO: stdout: ""
Sep 20 22:42:32.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=services-2663 execpod4r2cm -- /bin/sh -x -c nc -zv -t -w 2 192.168.5.11 32421'
Sep 20 22:42:32.919: INFO: stderr: "+ nc -zv -t -w 2 192.168.5.11 32421\nConnection to 192.168.5.11 32421 port [tcp/32421] succeeded!\n"
Sep 20 22:42:32.919: INFO: stdout: ""
Sep 20 22:42:32.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=services-2663 execpod4r2cm -- /bin/sh -x -c nc -zv -t -w 2 192.168.5.101 32421'
Sep 20 22:42:33.069: INFO: stderr: "+ nc -zv -t -w 2 192.168.5.101 32421\nConnection to 192.168.5.101 32421 port [tcp/32421] succeeded!\n"
Sep 20 22:42:33.069: INFO: stdout: ""
Sep 20 22:42:33.069: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:42:33.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2663" for this suite.
Sep 20 22:42:39.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:42:39.180: INFO: namespace services-2663 deletion completed in 6.090748701s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:12.837 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:42:39.182: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-714d1b55-4cbe-4382-b2c6-7f9b4205f865
STEP: Creating a pod to test consume configMaps
Sep 20 22:42:39.232: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fc3c2933-33f2-4f70-80a3-084867c48577" in namespace "projected-2544" to be "success or failure"
Sep 20 22:42:39.240: INFO: Pod "pod-projected-configmaps-fc3c2933-33f2-4f70-80a3-084867c48577": Phase="Pending", Reason="", readiness=false. Elapsed: 7.804553ms
Sep 20 22:42:41.246: INFO: Pod "pod-projected-configmaps-fc3c2933-33f2-4f70-80a3-084867c48577": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013735759s
Sep 20 22:42:43.251: INFO: Pod "pod-projected-configmaps-fc3c2933-33f2-4f70-80a3-084867c48577": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01953392s
STEP: Saw pod success
Sep 20 22:42:43.251: INFO: Pod "pod-projected-configmaps-fc3c2933-33f2-4f70-80a3-084867c48577" satisfied condition "success or failure"
Sep 20 22:42:43.256: INFO: Trying to get logs from node worker-2 pod pod-projected-configmaps-fc3c2933-33f2-4f70-80a3-084867c48577 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 22:42:43.302: INFO: Waiting for pod pod-projected-configmaps-fc3c2933-33f2-4f70-80a3-084867c48577 to disappear
Sep 20 22:42:43.306: INFO: Pod pod-projected-configmaps-fc3c2933-33f2-4f70-80a3-084867c48577 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:42:43.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2544" for this suite.
Sep 20 22:42:49.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:42:49.359: INFO: namespace projected-2544 deletion completed in 6.048166173s

• [SLOW TEST:10.177 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:42:49.359: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0920 22:42:55.422267      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 20 22:42:55.422: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:42:55.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6102" for this suite.
Sep 20 22:43:01.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:43:01.508: INFO: namespace gc-6102 deletion completed in 6.081203631s

• [SLOW TEST:12.150 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:43:01.509: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-a61c4cdb-3d4e-4043-8363-86d07d2b747c
STEP: Creating a pod to test consume secrets
Sep 20 22:43:01.540: INFO: Waiting up to 5m0s for pod "pod-secrets-490bedfa-f224-4485-8f3b-8edcbcab3413" in namespace "secrets-4888" to be "success or failure"
Sep 20 22:43:01.542: INFO: Pod "pod-secrets-490bedfa-f224-4485-8f3b-8edcbcab3413": Phase="Pending", Reason="", readiness=false. Elapsed: 2.380763ms
Sep 20 22:43:03.548: INFO: Pod "pod-secrets-490bedfa-f224-4485-8f3b-8edcbcab3413": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007897254s
Sep 20 22:43:05.553: INFO: Pod "pod-secrets-490bedfa-f224-4485-8f3b-8edcbcab3413": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012676658s
STEP: Saw pod success
Sep 20 22:43:05.553: INFO: Pod "pod-secrets-490bedfa-f224-4485-8f3b-8edcbcab3413" satisfied condition "success or failure"
Sep 20 22:43:05.557: INFO: Trying to get logs from node worker-2 pod pod-secrets-490bedfa-f224-4485-8f3b-8edcbcab3413 container secret-volume-test: <nil>
STEP: delete the pod
Sep 20 22:43:05.599: INFO: Waiting for pod pod-secrets-490bedfa-f224-4485-8f3b-8edcbcab3413 to disappear
Sep 20 22:43:05.617: INFO: Pod pod-secrets-490bedfa-f224-4485-8f3b-8edcbcab3413 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:43:05.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4888" for this suite.
Sep 20 22:43:11.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:43:11.683: INFO: namespace secrets-4888 deletion completed in 6.062344722s

• [SLOW TEST:10.174 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:43:11.684: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 22:43:11.722: INFO: (0) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 8.657407ms)
Sep 20 22:43:11.724: INFO: (1) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.959249ms)
Sep 20 22:43:11.725: INFO: (2) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.712351ms)
Sep 20 22:43:11.728: INFO: (3) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.391031ms)
Sep 20 22:43:11.730: INFO: (4) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.951836ms)
Sep 20 22:43:11.732: INFO: (5) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.416882ms)
Sep 20 22:43:11.735: INFO: (6) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.419053ms)
Sep 20 22:43:11.737: INFO: (7) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.072041ms)
Sep 20 22:43:11.739: INFO: (8) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.156045ms)
Sep 20 22:43:11.741: INFO: (9) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.774471ms)
Sep 20 22:43:11.743: INFO: (10) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.83252ms)
Sep 20 22:43:11.745: INFO: (11) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.001738ms)
Sep 20 22:43:11.747: INFO: (12) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.962524ms)
Sep 20 22:43:11.749: INFO: (13) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.981762ms)
Sep 20 22:43:11.751: INFO: (14) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.008938ms)
Sep 20 22:43:11.753: INFO: (15) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.893857ms)
Sep 20 22:43:11.756: INFO: (16) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.25176ms)
Sep 20 22:43:11.758: INFO: (17) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.953324ms)
Sep 20 22:43:11.760: INFO: (18) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.999725ms)
Sep 20 22:43:11.762: INFO: (19) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.803926ms)
[AfterEach] version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:43:11.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1442" for this suite.
Sep 20 22:43:17.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:43:17.859: INFO: namespace proxy-1442 deletion completed in 6.095303554s

• [SLOW TEST:6.175 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:43:17.861: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 22:43:17.893: INFO: Create a RollingUpdate DaemonSet
Sep 20 22:43:17.896: INFO: Check that daemon pods launch on every node of the cluster
Sep 20 22:43:17.904: INFO: Number of nodes with available pods: 0
Sep 20 22:43:17.904: INFO: Node controlplane-1 is running more than one daemon pod
Sep 20 22:43:18.913: INFO: Number of nodes with available pods: 0
Sep 20 22:43:18.913: INFO: Node controlplane-1 is running more than one daemon pod
Sep 20 22:43:19.921: INFO: Number of nodes with available pods: 2
Sep 20 22:43:19.921: INFO: Node controlplane-1 is running more than one daemon pod
Sep 20 22:43:20.918: INFO: Number of nodes with available pods: 3
Sep 20 22:43:20.918: INFO: Number of running nodes: 3, number of available pods: 3
Sep 20 22:43:20.919: INFO: Update the DaemonSet to trigger a rollout
Sep 20 22:43:20.933: INFO: Updating DaemonSet daemon-set
Sep 20 22:43:35.950: INFO: Roll back the DaemonSet before rollout is complete
Sep 20 22:43:35.954: INFO: Updating DaemonSet daemon-set
Sep 20 22:43:35.954: INFO: Make sure DaemonSet rollback is complete
Sep 20 22:43:35.958: INFO: Wrong image for pod: daemon-set-lltfl. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 20 22:43:35.958: INFO: Pod daemon-set-lltfl is not available
Sep 20 22:43:36.976: INFO: Wrong image for pod: daemon-set-lltfl. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 20 22:43:36.976: INFO: Pod daemon-set-lltfl is not available
Sep 20 22:43:37.975: INFO: Wrong image for pod: daemon-set-lltfl. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 20 22:43:37.975: INFO: Pod daemon-set-lltfl is not available
Sep 20 22:43:38.975: INFO: Pod daemon-set-4lpf2 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3878, will wait for the garbage collector to delete the pods
I0920 22:43:38.989276      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 22:43:38.989293      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 22:43:39.046: INFO: Deleting DaemonSet.extensions daemon-set took: 5.159225ms
Sep 20 22:43:39.348: INFO: Terminating DaemonSet.extensions daemon-set pods took: 301.880127ms
I0920 22:43:39.348019      16 controller_utils.go:810] Ignoring inactive pod daemonsets-3878/daemon-set-4lpf2 in state Pending, deletion time 2019-09-20 22:44:09 +0000 UTC
I0920 22:43:39.348065      16 controller_utils.go:810] Ignoring inactive pod daemonsets-3878/daemon-set-4g972 in state Running, deletion time 2019-09-20 22:44:09 +0000 UTC
I0920 22:43:39.348074      16 controller_utils.go:810] Ignoring inactive pod daemonsets-3878/daemon-set-6pjdd in state Running, deletion time 2019-09-20 22:44:09 +0000 UTC
Sep 20 22:43:42.651: INFO: Number of nodes with available pods: 0
Sep 20 22:43:42.651: INFO: Number of running nodes: 0, number of available pods: 0
Sep 20 22:43:42.652: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3878/daemonsets","resourceVersion":"15471"},"items":null}

Sep 20 22:43:42.654: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3878/pods","resourceVersion":"15471"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:43:42.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3878" for this suite.
Sep 20 22:43:48.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:43:48.746: INFO: namespace daemonsets-3878 deletion completed in 6.082482248s

• [SLOW TEST:30.884 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:43:48.747: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 22:43:48.772: INFO: Waiting up to 5m0s for pod "downwardapi-volume-962e5790-f58d-49d3-a533-f9d777276619" in namespace "downward-api-9700" to be "success or failure"
Sep 20 22:43:48.775: INFO: Pod "downwardapi-volume-962e5790-f58d-49d3-a533-f9d777276619": Phase="Pending", Reason="", readiness=false. Elapsed: 2.662808ms
Sep 20 22:43:50.777: INFO: Pod "downwardapi-volume-962e5790-f58d-49d3-a533-f9d777276619": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004618298s
STEP: Saw pod success
Sep 20 22:43:50.777: INFO: Pod "downwardapi-volume-962e5790-f58d-49d3-a533-f9d777276619" satisfied condition "success or failure"
Sep 20 22:43:50.778: INFO: Trying to get logs from node worker-2 pod downwardapi-volume-962e5790-f58d-49d3-a533-f9d777276619 container client-container: <nil>
STEP: delete the pod
Sep 20 22:43:50.791: INFO: Waiting for pod downwardapi-volume-962e5790-f58d-49d3-a533-f9d777276619 to disappear
Sep 20 22:43:50.793: INFO: Pod downwardapi-volume-962e5790-f58d-49d3-a533-f9d777276619 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:43:50.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9700" for this suite.
Sep 20 22:43:56.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:43:56.878: INFO: namespace downward-api-9700 deletion completed in 6.08258826s

• [SLOW TEST:8.131 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:43:56.879: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 20 22:43:56.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-7048'
Sep 20 22:43:56.966: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 20 22:43:56.966: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Sep 20 22:43:56.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 delete jobs e2e-test-httpd-job --namespace=kubectl-7048'
Sep 20 22:43:57.027: INFO: stderr: ""
Sep 20 22:43:57.027: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:43:57.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7048" for this suite.
Sep 20 22:44:09.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:44:09.136: INFO: namespace kubectl-7048 deletion completed in 12.106656255s

• [SLOW TEST:12.258 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:44:09.137: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep 20 22:44:09.174: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9075 /api/v1/namespaces/watch-9075/configmaps/e2e-watch-test-watch-closed 6c6f4e1e-91fe-4dbb-8293-1cbed0cdd2a4 15597 0 2019-09-20 22:44:09 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 20 22:44:09.175: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9075 /api/v1/namespaces/watch-9075/configmaps/e2e-watch-test-watch-closed 6c6f4e1e-91fe-4dbb-8293-1cbed0cdd2a4 15598 0 2019-09-20 22:44:09 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep 20 22:44:09.183: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9075 /api/v1/namespaces/watch-9075/configmaps/e2e-watch-test-watch-closed 6c6f4e1e-91fe-4dbb-8293-1cbed0cdd2a4 15599 0 2019-09-20 22:44:09 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 20 22:44:09.183: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9075 /api/v1/namespaces/watch-9075/configmaps/e2e-watch-test-watch-closed 6c6f4e1e-91fe-4dbb-8293-1cbed0cdd2a4 15600 0 2019-09-20 22:44:09 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:44:09.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9075" for this suite.
Sep 20 22:44:15.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:44:15.280: INFO: namespace watch-9075 deletion completed in 6.094768435s

• [SLOW TEST:6.143 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:44:15.283: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-897c29ef-ff1b-4a39-b80f-9041f0e0f322 in namespace container-probe-9948
Sep 20 22:44:19.323: INFO: Started pod busybox-897c29ef-ff1b-4a39-b80f-9041f0e0f322 in namespace container-probe-9948
STEP: checking the pod's current state and verifying that restartCount is present
Sep 20 22:44:19.328: INFO: Initial restart count of pod busybox-897c29ef-ff1b-4a39-b80f-9041f0e0f322 is 0
Sep 20 22:45:09.544: INFO: Restart count of pod container-probe-9948/busybox-897c29ef-ff1b-4a39-b80f-9041f0e0f322 is now 1 (50.216085782s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:45:09.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9948" for this suite.
Sep 20 22:45:15.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:45:15.632: INFO: namespace container-probe-9948 deletion completed in 6.056112147s

• [SLOW TEST:60.349 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:45:15.632: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 20 22:45:15.664: INFO: Waiting up to 5m0s for pod "pod-db4a776e-4865-473f-8b25-8094e0b8c685" in namespace "emptydir-2287" to be "success or failure"
Sep 20 22:45:15.668: INFO: Pod "pod-db4a776e-4865-473f-8b25-8094e0b8c685": Phase="Pending", Reason="", readiness=false. Elapsed: 4.882107ms
Sep 20 22:45:17.678: INFO: Pod "pod-db4a776e-4865-473f-8b25-8094e0b8c685": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014862333s
Sep 20 22:45:19.680: INFO: Pod "pod-db4a776e-4865-473f-8b25-8094e0b8c685": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016742629s
STEP: Saw pod success
Sep 20 22:45:19.680: INFO: Pod "pod-db4a776e-4865-473f-8b25-8094e0b8c685" satisfied condition "success or failure"
Sep 20 22:45:19.682: INFO: Trying to get logs from node worker-2 pod pod-db4a776e-4865-473f-8b25-8094e0b8c685 container test-container: <nil>
STEP: delete the pod
Sep 20 22:45:19.694: INFO: Waiting for pod pod-db4a776e-4865-473f-8b25-8094e0b8c685 to disappear
Sep 20 22:45:19.695: INFO: Pod pod-db4a776e-4865-473f-8b25-8094e0b8c685 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:45:19.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2287" for this suite.
Sep 20 22:45:25.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:45:25.790: INFO: namespace emptydir-2287 deletion completed in 6.093288323s

• [SLOW TEST:10.159 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:45:25.792: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 22:45:25.813: INFO: Creating ReplicaSet my-hostname-basic-e7ce7b42-dc45-47f3-a108-260f32628134
Sep 20 22:45:25.831: INFO: Pod name my-hostname-basic-e7ce7b42-dc45-47f3-a108-260f32628134: Found 1 pods out of 1
Sep 20 22:45:25.831: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-e7ce7b42-dc45-47f3-a108-260f32628134" is running
Sep 20 22:45:29.840: INFO: Pod "my-hostname-basic-e7ce7b42-dc45-47f3-a108-260f32628134-n55xv" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-20 22:45:25 +0000 UTC Reason: Message:}])
Sep 20 22:45:29.841: INFO: Trying to dial the pod
Sep 20 22:45:34.879: INFO: Controller my-hostname-basic-e7ce7b42-dc45-47f3-a108-260f32628134: Got expected result from replica 1 [my-hostname-basic-e7ce7b42-dc45-47f3-a108-260f32628134-n55xv]: "my-hostname-basic-e7ce7b42-dc45-47f3-a108-260f32628134-n55xv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:45:34.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9385" for this suite.
Sep 20 22:45:40.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:45:40.969: INFO: namespace replicaset-9385 deletion completed in 6.084986127s

• [SLOW TEST:15.178 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:45:40.970: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-1abe275b-220d-41bc-a455-93fb5716f713
STEP: Creating a pod to test consume secrets
Sep 20 22:45:41.002: INFO: Waiting up to 5m0s for pod "pod-secrets-0363c02a-c44a-4057-aa97-6be8bc0a807b" in namespace "secrets-2241" to be "success or failure"
Sep 20 22:45:41.006: INFO: Pod "pod-secrets-0363c02a-c44a-4057-aa97-6be8bc0a807b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.736842ms
Sep 20 22:45:43.012: INFO: Pod "pod-secrets-0363c02a-c44a-4057-aa97-6be8bc0a807b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009952864s
STEP: Saw pod success
Sep 20 22:45:43.013: INFO: Pod "pod-secrets-0363c02a-c44a-4057-aa97-6be8bc0a807b" satisfied condition "success or failure"
Sep 20 22:45:43.017: INFO: Trying to get logs from node worker-2 pod pod-secrets-0363c02a-c44a-4057-aa97-6be8bc0a807b container secret-volume-test: <nil>
STEP: delete the pod
Sep 20 22:45:43.065: INFO: Waiting for pod pod-secrets-0363c02a-c44a-4057-aa97-6be8bc0a807b to disappear
Sep 20 22:45:43.068: INFO: Pod pod-secrets-0363c02a-c44a-4057-aa97-6be8bc0a807b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:45:43.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2241" for this suite.
Sep 20 22:45:49.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:45:49.163: INFO: namespace secrets-2241 deletion completed in 6.092513862s

• [SLOW TEST:8.193 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:45:49.163: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Sep 20 22:45:49.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-1939 -- logs-generator --log-lines-total 100 --run-duration 20s'
Sep 20 22:45:49.242: INFO: stderr: ""
Sep 20 22:45:49.242: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Sep 20 22:45:49.242: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Sep 20 22:45:49.242: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-1939" to be "running and ready, or succeeded"
Sep 20 22:45:49.247: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.293797ms
Sep 20 22:45:51.291: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048768506s
Sep 20 22:45:53.297: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.055068783s
Sep 20 22:45:53.298: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Sep 20 22:45:53.298: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Sep 20 22:45:53.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 logs logs-generator logs-generator --namespace=kubectl-1939'
Sep 20 22:45:53.373: INFO: stderr: ""
Sep 20 22:45:53.373: INFO: stdout: "I0920 22:45:50.778003       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/gxm 587\nI0920 22:45:50.980934       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/mpq 311\nI0920 22:45:51.180080       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/dcz7 542\nI0920 22:45:51.382276       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/mgb6 427\nI0920 22:45:51.579980       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/jfd 263\nI0920 22:45:51.797010       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/jr4 266\nI0920 22:45:51.982222       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/xl7p 598\nI0920 22:45:52.183714       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/m9p 254\nI0920 22:45:52.382222       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/pnw 558\nI0920 22:45:52.583900       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/fwbp 270\nI0920 22:45:52.789735       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/wxd 427\nI0920 22:45:52.982313       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/6j4 220\nI0920 22:45:53.182040       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/8fxk 253\n"
STEP: limiting log lines
Sep 20 22:45:53.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 logs logs-generator logs-generator --namespace=kubectl-1939 --tail=1'
Sep 20 22:45:53.434: INFO: stderr: ""
Sep 20 22:45:53.434: INFO: stdout: "I0920 22:45:53.378285       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/6kq 527\n"
STEP: limiting log bytes
Sep 20 22:45:53.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 logs logs-generator logs-generator --namespace=kubectl-1939 --limit-bytes=1'
Sep 20 22:45:53.506: INFO: stderr: ""
Sep 20 22:45:53.506: INFO: stdout: "I"
STEP: exposing timestamps
Sep 20 22:45:53.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 logs logs-generator logs-generator --namespace=kubectl-1939 --tail=1 --timestamps'
Sep 20 22:45:53.562: INFO: stderr: ""
Sep 20 22:45:53.562: INFO: stdout: "2019-09-20T22:45:53.379086718Z I0920 22:45:53.378285       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/6kq 527\n"
STEP: restricting to a time range
Sep 20 22:45:56.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 logs logs-generator logs-generator --namespace=kubectl-1939 --since=1s'
Sep 20 22:45:56.150: INFO: stderr: ""
Sep 20 22:45:56.150: INFO: stdout: "I0920 22:45:55.178480       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/z4t 419\nI0920 22:45:55.378672       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/xqbc 397\nI0920 22:45:55.578287       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/ns/pods/5zl2 427\nI0920 22:45:55.782094       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/npwj 421\nI0920 22:45:55.997457       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/nlpj 509\n"
Sep 20 22:45:56.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 logs logs-generator logs-generator --namespace=kubectl-1939 --since=24h'
Sep 20 22:45:56.208: INFO: stderr: ""
Sep 20 22:45:56.208: INFO: stdout: "I0920 22:45:50.778003       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/gxm 587\nI0920 22:45:50.980934       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/mpq 311\nI0920 22:45:51.180080       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/dcz7 542\nI0920 22:45:51.382276       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/mgb6 427\nI0920 22:45:51.579980       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/jfd 263\nI0920 22:45:51.797010       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/jr4 266\nI0920 22:45:51.982222       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/xl7p 598\nI0920 22:45:52.183714       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/m9p 254\nI0920 22:45:52.382222       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/pnw 558\nI0920 22:45:52.583900       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/fwbp 270\nI0920 22:45:52.789735       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/wxd 427\nI0920 22:45:52.982313       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/6j4 220\nI0920 22:45:53.182040       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/8fxk 253\nI0920 22:45:53.378285       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/6kq 527\nI0920 22:45:53.581963       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/p7h9 576\nI0920 22:45:53.781380       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/4tw 382\nI0920 22:45:53.982288       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/5gc 368\nI0920 22:45:54.182336       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/wtjv 566\nI0920 22:45:54.378170       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/gph 278\nI0920 22:45:54.581122       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/qd7 259\nI0920 22:45:54.779509       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/rntb 444\nI0920 22:45:54.978238       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/cqg 453\nI0920 22:45:55.178480       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/z4t 419\nI0920 22:45:55.378672       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/xqbc 397\nI0920 22:45:55.578287       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/ns/pods/5zl2 427\nI0920 22:45:55.782094       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/npwj 421\nI0920 22:45:55.997457       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/nlpj 509\nI0920 22:45:56.182009       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/kube-system/pods/tnvs 425\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Sep 20 22:45:56.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 delete pod logs-generator --namespace=kubectl-1939'
Sep 20 22:45:57.722: INFO: stderr: ""
Sep 20 22:45:57.722: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:45:57.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1939" for this suite.
Sep 20 22:46:03.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:46:03.804: INFO: namespace kubectl-1939 deletion completed in 6.073381892s

• [SLOW TEST:14.641 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:46:03.805: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-72727aca-2eea-4451-ba6b-8702e45cde1f
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:46:03.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1849" for this suite.
Sep 20 22:46:09.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:46:09.922: INFO: namespace secrets-1849 deletion completed in 6.090847431s

• [SLOW TEST:6.117 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:46:09.924: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-1340
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1340 to expose endpoints map[]
Sep 20 22:46:09.955: INFO: Get endpoints failed (3.528033ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Sep 20 22:46:10.961: INFO: successfully validated that service endpoint-test2 in namespace services-1340 exposes endpoints map[] (1.008853799s elapsed)
STEP: Creating pod pod1 in namespace services-1340
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1340 to expose endpoints map[pod1:[80]]
Sep 20 22:46:13.006: INFO: successfully validated that service endpoint-test2 in namespace services-1340 exposes endpoints map[pod1:[80]] (2.032043597s elapsed)
STEP: Creating pod pod2 in namespace services-1340
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1340 to expose endpoints map[pod1:[80] pod2:[80]]
Sep 20 22:46:15.071: INFO: successfully validated that service endpoint-test2 in namespace services-1340 exposes endpoints map[pod1:[80] pod2:[80]] (2.056209945s elapsed)
STEP: Deleting pod pod1 in namespace services-1340
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1340 to expose endpoints map[pod2:[80]]
Sep 20 22:46:16.128: INFO: successfully validated that service endpoint-test2 in namespace services-1340 exposes endpoints map[pod2:[80]] (1.047761389s elapsed)
STEP: Deleting pod pod2 in namespace services-1340
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1340 to expose endpoints map[]
Sep 20 22:46:16.153: INFO: successfully validated that service endpoint-test2 in namespace services-1340 exposes endpoints map[] (6.480015ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:46:16.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1340" for this suite.
Sep 20 22:46:28.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:46:28.266: INFO: namespace services-1340 deletion completed in 12.08749267s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:18.342 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:46:28.267: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Sep 20 22:46:28.292: INFO: Waiting up to 5m0s for pod "client-containers-28a76a78-00ea-41dc-9fa2-12840cfcf882" in namespace "containers-2505" to be "success or failure"
Sep 20 22:46:28.294: INFO: Pod "client-containers-28a76a78-00ea-41dc-9fa2-12840cfcf882": Phase="Pending", Reason="", readiness=false. Elapsed: 1.951359ms
Sep 20 22:46:30.305: INFO: Pod "client-containers-28a76a78-00ea-41dc-9fa2-12840cfcf882": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013216218s
STEP: Saw pod success
Sep 20 22:46:30.305: INFO: Pod "client-containers-28a76a78-00ea-41dc-9fa2-12840cfcf882" satisfied condition "success or failure"
Sep 20 22:46:30.306: INFO: Trying to get logs from node worker-2 pod client-containers-28a76a78-00ea-41dc-9fa2-12840cfcf882 container test-container: <nil>
STEP: delete the pod
Sep 20 22:46:30.316: INFO: Waiting for pod client-containers-28a76a78-00ea-41dc-9fa2-12840cfcf882 to disappear
Sep 20 22:46:30.318: INFO: Pod client-containers-28a76a78-00ea-41dc-9fa2-12840cfcf882 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:46:30.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2505" for this suite.
Sep 20 22:46:36.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:46:36.407: INFO: namespace containers-2505 deletion completed in 6.087178452s

• [SLOW TEST:8.140 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:46:36.407: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-0c602c30-4ba6-4f89-a352-00517f454267
STEP: Creating secret with name s-test-opt-upd-cb4a8724-bf16-4ef7-a44c-92fc3a2cc272
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-0c602c30-4ba6-4f89-a352-00517f454267
STEP: Updating secret s-test-opt-upd-cb4a8724-bf16-4ef7-a44c-92fc3a2cc272
STEP: Creating secret with name s-test-opt-create-551823cf-c306-4276-878d-c380e7641faa
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:47:59.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5753" for this suite.
Sep 20 22:48:17.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:48:17.412: INFO: namespace projected-5753 deletion completed in 18.091107192s

• [SLOW TEST:101.004 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:48:17.412: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:48:33.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9544" for this suite.
Sep 20 22:48:39.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:48:39.715: INFO: namespace resourcequota-9544 deletion completed in 6.126203353s

• [SLOW TEST:22.302 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:48:39.715: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-0d3ec093-ebef-44ba-9c4a-74c9ad0b00a4
STEP: Creating a pod to test consume secrets
Sep 20 22:48:39.748: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fba0cf05-40eb-4a3e-b2f1-960a08a4a1a9" in namespace "projected-5463" to be "success or failure"
Sep 20 22:48:39.753: INFO: Pod "pod-projected-secrets-fba0cf05-40eb-4a3e-b2f1-960a08a4a1a9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.190807ms
Sep 20 22:48:41.756: INFO: Pod "pod-projected-secrets-fba0cf05-40eb-4a3e-b2f1-960a08a4a1a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007377583s
STEP: Saw pod success
Sep 20 22:48:41.756: INFO: Pod "pod-projected-secrets-fba0cf05-40eb-4a3e-b2f1-960a08a4a1a9" satisfied condition "success or failure"
Sep 20 22:48:41.758: INFO: Trying to get logs from node worker-2 pod pod-projected-secrets-fba0cf05-40eb-4a3e-b2f1-960a08a4a1a9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 20 22:48:41.770: INFO: Waiting for pod pod-projected-secrets-fba0cf05-40eb-4a3e-b2f1-960a08a4a1a9 to disappear
Sep 20 22:48:41.774: INFO: Pod pod-projected-secrets-fba0cf05-40eb-4a3e-b2f1-960a08a4a1a9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:48:41.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5463" for this suite.
Sep 20 22:48:47.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:48:47.836: INFO: namespace projected-5463 deletion completed in 6.060066384s

• [SLOW TEST:8.121 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:48:47.836: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-s8shf in namespace proxy-5801
I0920 22:48:47.951331      16 runners.go:184] Created replication controller with name: proxy-service-s8shf, namespace: proxy-5801, replica count: 1
I0920 22:48:47.951400      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 22:48:47.951412      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 22:48:49.001910      16 runners.go:184] proxy-service-s8shf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 22:48:50.002684      16 runners.go:184] proxy-service-s8shf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0920 22:48:51.023407      16 runners.go:184] proxy-service-s8shf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0920 22:48:52.026209      16 runners.go:184] proxy-service-s8shf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0920 22:48:53.027428      16 runners.go:184] proxy-service-s8shf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0920 22:48:54.027892      16 runners.go:184] proxy-service-s8shf Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 20 22:48:54.033: INFO: setup took 6.103024722s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep 20 22:48:54.051: INFO: (0) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/rewriteme">test</a> (200; 17.995673ms)
Sep 20 22:48:54.061: INFO: (0) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:160/proxy/: foo (200; 26.183343ms)
Sep 20 22:48:54.064: INFO: (0) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:162/proxy/: bar (200; 27.770649ms)
Sep 20 22:48:54.065: INFO: (0) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:160/proxy/: foo (200; 28.363183ms)
Sep 20 22:48:54.065: INFO: (0) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:162/proxy/: bar (200; 29.98264ms)
Sep 20 22:48:54.067: INFO: (0) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/rewriteme">test<... (200; 29.479318ms)
Sep 20 22:48:54.067: INFO: (0) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/rewriteme">... (200; 33.973001ms)
Sep 20 22:48:54.074: INFO: (0) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname2/proxy/: bar (200; 40.092624ms)
Sep 20 22:48:54.074: INFO: (0) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname1/proxy/: foo (200; 39.489812ms)
Sep 20 22:48:54.076: INFO: (0) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:462/proxy/: tls qux (200; 39.765842ms)
Sep 20 22:48:54.077: INFO: (0) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname2/proxy/: bar (200; 40.086812ms)
Sep 20 22:48:54.078: INFO: (0) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname1/proxy/: foo (200; 44.136808ms)
Sep 20 22:48:54.078: INFO: (0) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:460/proxy/: tls baz (200; 43.47721ms)
Sep 20 22:48:54.079: INFO: (0) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname2/proxy/: tls qux (200; 44.192096ms)
Sep 20 22:48:54.080: INFO: (0) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname1/proxy/: tls baz (200; 45.192687ms)
Sep 20 22:48:54.084: INFO: (0) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/tlsrewritem... (200; 46.541887ms)
Sep 20 22:48:54.090: INFO: (1) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:160/proxy/: foo (200; 6.129373ms)
Sep 20 22:48:54.090: INFO: (1) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/rewriteme">... (200; 5.731305ms)
Sep 20 22:48:54.090: INFO: (1) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname2/proxy/: bar (200; 6.410697ms)
Sep 20 22:48:54.090: INFO: (1) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname2/proxy/: bar (200; 6.224398ms)
Sep 20 22:48:54.091: INFO: (1) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:160/proxy/: foo (200; 6.883443ms)
Sep 20 22:48:54.092: INFO: (1) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname1/proxy/: tls baz (200; 7.712234ms)
Sep 20 22:48:54.092: INFO: (1) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname1/proxy/: foo (200; 8.027197ms)
Sep 20 22:48:54.092: INFO: (1) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname1/proxy/: foo (200; 8.215399ms)
Sep 20 22:48:54.093: INFO: (1) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/tlsrewritem... (200; 9.171656ms)
Sep 20 22:48:54.093: INFO: (1) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/rewriteme">test</a> (200; 9.188436ms)
Sep 20 22:48:54.093: INFO: (1) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:162/proxy/: bar (200; 9.157085ms)
Sep 20 22:48:54.093: INFO: (1) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:460/proxy/: tls baz (200; 9.406606ms)
Sep 20 22:48:54.094: INFO: (1) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:162/proxy/: bar (200; 9.834534ms)
Sep 20 22:48:54.094: INFO: (1) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:462/proxy/: tls qux (200; 9.955805ms)
Sep 20 22:48:54.094: INFO: (1) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/rewriteme">test<... (200; 9.857212ms)
Sep 20 22:48:54.095: INFO: (1) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname2/proxy/: tls qux (200; 10.893991ms)
Sep 20 22:48:54.101: INFO: (2) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:462/proxy/: tls qux (200; 5.9017ms)
Sep 20 22:48:54.101: INFO: (2) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/tlsrewritem... (200; 6.349688ms)
Sep 20 22:48:54.102: INFO: (2) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/rewriteme">... (200; 7.148612ms)
Sep 20 22:48:54.102: INFO: (2) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:460/proxy/: tls baz (200; 6.811448ms)
Sep 20 22:48:54.102: INFO: (2) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:160/proxy/: foo (200; 7.021093ms)
Sep 20 22:48:54.102: INFO: (2) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:162/proxy/: bar (200; 6.889853ms)
Sep 20 22:48:54.102: INFO: (2) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:162/proxy/: bar (200; 6.990869ms)
Sep 20 22:48:54.102: INFO: (2) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:160/proxy/: foo (200; 7.211469ms)
Sep 20 22:48:54.103: INFO: (2) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/rewriteme">test</a> (200; 8.158544ms)
Sep 20 22:48:54.103: INFO: (2) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/rewriteme">test<... (200; 8.344756ms)
Sep 20 22:48:54.105: INFO: (2) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname1/proxy/: tls baz (200; 9.251395ms)
Sep 20 22:48:54.107: INFO: (2) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname1/proxy/: foo (200; 11.77083ms)
Sep 20 22:48:54.107: INFO: (2) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname2/proxy/: tls qux (200; 12.148585ms)
Sep 20 22:48:54.108: INFO: (2) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname2/proxy/: bar (200; 12.919293ms)
Sep 20 22:48:54.108: INFO: (2) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname1/proxy/: foo (200; 12.868738ms)
Sep 20 22:48:54.108: INFO: (2) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname2/proxy/: bar (200; 13.144973ms)
Sep 20 22:48:54.113: INFO: (3) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:462/proxy/: tls qux (200; 4.173595ms)
Sep 20 22:48:54.113: INFO: (3) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:460/proxy/: tls baz (200; 4.44605ms)
Sep 20 22:48:54.114: INFO: (3) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:160/proxy/: foo (200; 5.224984ms)
Sep 20 22:48:54.114: INFO: (3) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/rewriteme">test</a> (200; 5.552386ms)
Sep 20 22:48:54.115: INFO: (3) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:160/proxy/: foo (200; 6.436254ms)
Sep 20 22:48:54.115: INFO: (3) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/rewriteme">... (200; 6.135178ms)
Sep 20 22:48:54.117: INFO: (3) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname2/proxy/: bar (200; 8.646397ms)
Sep 20 22:48:54.117: INFO: (3) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname2/proxy/: tls qux (200; 8.57917ms)
Sep 20 22:48:54.118: INFO: (3) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname2/proxy/: bar (200; 9.714446ms)
Sep 20 22:48:54.118: INFO: (3) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname1/proxy/: foo (200; 9.786548ms)
Sep 20 22:48:54.118: INFO: (3) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/rewriteme">test<... (200; 9.927274ms)
Sep 20 22:48:54.119: INFO: (3) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname1/proxy/: foo (200; 11.024772ms)
Sep 20 22:48:54.119: INFO: (3) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname1/proxy/: tls baz (200; 11.075321ms)
Sep 20 22:48:54.120: INFO: (3) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:162/proxy/: bar (200; 12.003694ms)
Sep 20 22:48:54.121: INFO: (3) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:162/proxy/: bar (200; 12.361535ms)
Sep 20 22:48:54.121: INFO: (3) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/tlsrewritem... (200; 12.367989ms)
Sep 20 22:48:54.125: INFO: (4) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/rewriteme">test</a> (200; 3.841153ms)
Sep 20 22:48:54.125: INFO: (4) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:462/proxy/: tls qux (200; 4.059151ms)
Sep 20 22:48:54.125: INFO: (4) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:460/proxy/: tls baz (200; 3.787856ms)
Sep 20 22:48:54.126: INFO: (4) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/tlsrewritem... (200; 4.417554ms)
Sep 20 22:48:54.126: INFO: (4) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:162/proxy/: bar (200; 4.910215ms)
Sep 20 22:48:54.126: INFO: (4) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/rewriteme">test<... (200; 4.836599ms)
Sep 20 22:48:54.126: INFO: (4) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:160/proxy/: foo (200; 5.152847ms)
Sep 20 22:48:54.126: INFO: (4) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:160/proxy/: foo (200; 5.041602ms)
Sep 20 22:48:54.126: INFO: (4) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:162/proxy/: bar (200; 5.250872ms)
Sep 20 22:48:54.126: INFO: (4) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/rewriteme">... (200; 5.193344ms)
Sep 20 22:48:54.129: INFO: (4) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname2/proxy/: bar (200; 7.497072ms)
Sep 20 22:48:54.129: INFO: (4) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname1/proxy/: foo (200; 7.94635ms)
Sep 20 22:48:54.129: INFO: (4) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname1/proxy/: foo (200; 7.874274ms)
Sep 20 22:48:54.129: INFO: (4) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname1/proxy/: tls baz (200; 7.881476ms)
Sep 20 22:48:54.129: INFO: (4) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname2/proxy/: tls qux (200; 7.991038ms)
Sep 20 22:48:54.129: INFO: (4) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname2/proxy/: bar (200; 8.243707ms)
Sep 20 22:48:54.134: INFO: (5) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/tlsrewritem... (200; 3.802665ms)
Sep 20 22:48:54.135: INFO: (5) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:160/proxy/: foo (200; 5.196146ms)
Sep 20 22:48:54.135: INFO: (5) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/rewriteme">... (200; 4.161099ms)
Sep 20 22:48:54.135: INFO: (5) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/rewriteme">test<... (200; 4.532052ms)
Sep 20 22:48:54.135: INFO: (5) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:162/proxy/: bar (200; 4.803693ms)
Sep 20 22:48:54.135: INFO: (5) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:462/proxy/: tls qux (200; 5.06607ms)
Sep 20 22:48:54.135: INFO: (5) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/rewriteme">test</a> (200; 5.20887ms)
Sep 20 22:48:54.135: INFO: (5) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:160/proxy/: foo (200; 5.107036ms)
Sep 20 22:48:54.135: INFO: (5) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:460/proxy/: tls baz (200; 5.612262ms)
Sep 20 22:48:54.136: INFO: (5) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:162/proxy/: bar (200; 6.135729ms)
Sep 20 22:48:54.137: INFO: (5) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname2/proxy/: bar (200; 6.045941ms)
Sep 20 22:48:54.138: INFO: (5) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname1/proxy/: foo (200; 7.243361ms)
Sep 20 22:48:54.138: INFO: (5) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname1/proxy/: foo (200; 8.118049ms)
Sep 20 22:48:54.138: INFO: (5) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname1/proxy/: tls baz (200; 8.181018ms)
Sep 20 22:48:54.138: INFO: (5) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname2/proxy/: bar (200; 7.679323ms)
Sep 20 22:48:54.139: INFO: (5) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname2/proxy/: tls qux (200; 8.778991ms)
Sep 20 22:48:54.145: INFO: (6) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/rewriteme">test<... (200; 5.650755ms)
Sep 20 22:48:54.145: INFO: (6) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:160/proxy/: foo (200; 5.53187ms)
Sep 20 22:48:54.145: INFO: (6) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/rewriteme">... (200; 5.603031ms)
Sep 20 22:48:54.145: INFO: (6) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:160/proxy/: foo (200; 6.269694ms)
Sep 20 22:48:54.145: INFO: (6) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:162/proxy/: bar (200; 5.906082ms)
Sep 20 22:48:54.145: INFO: (6) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/rewriteme">test</a> (200; 6.034993ms)
Sep 20 22:48:54.146: INFO: (6) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:162/proxy/: bar (200; 6.661818ms)
Sep 20 22:48:54.146: INFO: (6) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:462/proxy/: tls qux (200; 6.086353ms)
Sep 20 22:48:54.146: INFO: (6) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/tlsrewritem... (200; 5.979332ms)
Sep 20 22:48:54.145: INFO: (6) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:460/proxy/: tls baz (200; 6.049056ms)
Sep 20 22:48:54.148: INFO: (6) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname1/proxy/: foo (200; 9.188651ms)
Sep 20 22:48:54.148: INFO: (6) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname2/proxy/: bar (200; 9.397616ms)
Sep 20 22:48:54.148: INFO: (6) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname2/proxy/: bar (200; 8.854431ms)
Sep 20 22:48:54.149: INFO: (6) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname1/proxy/: tls baz (200; 9.332377ms)
Sep 20 22:48:54.149: INFO: (6) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname2/proxy/: tls qux (200; 9.378186ms)
Sep 20 22:48:54.149: INFO: (6) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname1/proxy/: foo (200; 9.694306ms)
Sep 20 22:48:54.154: INFO: (7) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:162/proxy/: bar (200; 4.957349ms)
Sep 20 22:48:54.154: INFO: (7) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/rewriteme">... (200; 5.062737ms)
Sep 20 22:48:54.154: INFO: (7) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname2/proxy/: tls qux (200; 5.114524ms)
Sep 20 22:48:54.155: INFO: (7) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/tlsrewritem... (200; 5.747527ms)
Sep 20 22:48:54.155: INFO: (7) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:460/proxy/: tls baz (200; 5.73569ms)
Sep 20 22:48:54.156: INFO: (7) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/rewriteme">test<... (200; 6.497112ms)
Sep 20 22:48:54.156: INFO: (7) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname1/proxy/: tls baz (200; 6.841223ms)
Sep 20 22:48:54.156: INFO: (7) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:160/proxy/: foo (200; 7.243028ms)
Sep 20 22:48:54.156: INFO: (7) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:162/proxy/: bar (200; 7.56483ms)
Sep 20 22:48:54.158: INFO: (7) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname2/proxy/: bar (200; 8.643912ms)
Sep 20 22:48:54.158: INFO: (7) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname1/proxy/: foo (200; 9.235413ms)
Sep 20 22:48:54.158: INFO: (7) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/rewriteme">test</a> (200; 9.673932ms)
Sep 20 22:48:54.158: INFO: (7) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:160/proxy/: foo (200; 9.629952ms)
Sep 20 22:48:54.159: INFO: (7) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname2/proxy/: bar (200; 9.635903ms)
Sep 20 22:48:54.159: INFO: (7) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:462/proxy/: tls qux (200; 10.107585ms)
Sep 20 22:48:54.159: INFO: (7) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname1/proxy/: foo (200; 10.018772ms)
Sep 20 22:48:54.164: INFO: (8) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/tlsrewritem... (200; 4.265353ms)
Sep 20 22:48:54.164: INFO: (8) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/rewriteme">test</a> (200; 4.667864ms)
Sep 20 22:48:54.164: INFO: (8) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:460/proxy/: tls baz (200; 4.718175ms)
Sep 20 22:48:54.164: INFO: (8) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:162/proxy/: bar (200; 4.751104ms)
Sep 20 22:48:54.164: INFO: (8) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/rewriteme">test<... (200; 5.014489ms)
Sep 20 22:48:54.165: INFO: (8) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname2/proxy/: tls qux (200; 5.982506ms)
Sep 20 22:48:54.165: INFO: (8) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname2/proxy/: bar (200; 6.147873ms)
Sep 20 22:48:54.168: INFO: (8) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:160/proxy/: foo (200; 8.373759ms)
Sep 20 22:48:54.168: INFO: (8) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/rewriteme">... (200; 8.588943ms)
Sep 20 22:48:54.169: INFO: (8) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname1/proxy/: foo (200; 9.486751ms)
Sep 20 22:48:54.169: INFO: (8) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:160/proxy/: foo (200; 9.656044ms)
Sep 20 22:48:54.169: INFO: (8) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:462/proxy/: tls qux (200; 9.839978ms)
Sep 20 22:48:54.169: INFO: (8) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname1/proxy/: foo (200; 9.728661ms)
Sep 20 22:48:54.169: INFO: (8) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname2/proxy/: bar (200; 9.826632ms)
Sep 20 22:48:54.169: INFO: (8) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname1/proxy/: tls baz (200; 9.969503ms)
Sep 20 22:48:54.173: INFO: (8) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:162/proxy/: bar (200; 13.878815ms)
Sep 20 22:48:54.178: INFO: (9) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:160/proxy/: foo (200; 4.245613ms)
Sep 20 22:48:54.178: INFO: (9) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:460/proxy/: tls baz (200; 4.596742ms)
Sep 20 22:48:54.178: INFO: (9) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/tlsrewritem... (200; 4.635137ms)
Sep 20 22:48:54.178: INFO: (9) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/rewriteme">test<... (200; 4.632424ms)
Sep 20 22:48:54.179: INFO: (9) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname2/proxy/: bar (200; 5.088688ms)
Sep 20 22:48:54.179: INFO: (9) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname1/proxy/: tls baz (200; 5.155815ms)
Sep 20 22:48:54.179: INFO: (9) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:160/proxy/: foo (200; 5.113786ms)
Sep 20 22:48:54.180: INFO: (9) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:162/proxy/: bar (200; 5.968995ms)
Sep 20 22:48:54.180: INFO: (9) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:162/proxy/: bar (200; 6.281357ms)
Sep 20 22:48:54.180: INFO: (9) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/rewriteme">... (200; 6.33673ms)
Sep 20 22:48:54.180: INFO: (9) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname1/proxy/: foo (200; 6.290714ms)
Sep 20 22:48:54.181: INFO: (9) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname1/proxy/: foo (200; 7.374968ms)
Sep 20 22:48:54.181: INFO: (9) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname2/proxy/: tls qux (200; 7.456291ms)
Sep 20 22:48:54.181: INFO: (9) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:462/proxy/: tls qux (200; 7.427903ms)
Sep 20 22:48:54.181: INFO: (9) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/rewriteme">test</a> (200; 7.449132ms)
Sep 20 22:48:54.183: INFO: (9) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname2/proxy/: bar (200; 8.881991ms)
Sep 20 22:48:54.186: INFO: (10) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:460/proxy/: tls baz (200; 3.508719ms)
Sep 20 22:48:54.187: INFO: (10) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/rewriteme">test</a> (200; 4.122378ms)
Sep 20 22:48:54.187: INFO: (10) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:160/proxy/: foo (200; 3.905417ms)
Sep 20 22:48:54.187: INFO: (10) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/rewriteme">test<... (200; 4.144421ms)
Sep 20 22:48:54.188: INFO: (10) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/rewriteme">... (200; 4.516588ms)
Sep 20 22:48:54.188: INFO: (10) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/tlsrewritem... (200; 4.766992ms)
Sep 20 22:48:54.188: INFO: (10) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:462/proxy/: tls qux (200; 5.036316ms)
Sep 20 22:48:54.189: INFO: (10) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:162/proxy/: bar (200; 5.459257ms)
Sep 20 22:48:54.189: INFO: (10) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:162/proxy/: bar (200; 5.585529ms)
Sep 20 22:48:54.189: INFO: (10) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:160/proxy/: foo (200; 5.402713ms)
Sep 20 22:48:54.190: INFO: (10) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname1/proxy/: tls baz (200; 6.780735ms)
Sep 20 22:48:54.191: INFO: (10) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname1/proxy/: foo (200; 7.137082ms)
Sep 20 22:48:54.191: INFO: (10) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname1/proxy/: foo (200; 7.345385ms)
Sep 20 22:48:54.191: INFO: (10) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname2/proxy/: bar (200; 7.813842ms)
Sep 20 22:48:54.191: INFO: (10) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname2/proxy/: tls qux (200; 7.086321ms)
Sep 20 22:48:54.191: INFO: (10) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname2/proxy/: bar (200; 7.397249ms)
Sep 20 22:48:54.195: INFO: (11) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:162/proxy/: bar (200; 3.355109ms)
Sep 20 22:48:54.195: INFO: (11) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:162/proxy/: bar (200; 3.466889ms)
Sep 20 22:48:54.195: INFO: (11) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:160/proxy/: foo (200; 3.610034ms)
Sep 20 22:48:54.196: INFO: (11) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/rewriteme">test</a> (200; 4.221045ms)
Sep 20 22:48:54.196: INFO: (11) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:462/proxy/: tls qux (200; 4.724933ms)
Sep 20 22:48:54.196: INFO: (11) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/rewriteme">... (200; 4.82757ms)
Sep 20 22:48:54.197: INFO: (11) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/tlsrewritem... (200; 5.153122ms)
Sep 20 22:48:54.197: INFO: (11) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:160/proxy/: foo (200; 5.426467ms)
Sep 20 22:48:54.197: INFO: (11) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/rewriteme">test<... (200; 5.228618ms)
Sep 20 22:48:54.197: INFO: (11) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname2/proxy/: bar (200; 5.348849ms)
Sep 20 22:48:54.197: INFO: (11) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname2/proxy/: tls qux (200; 5.462375ms)
Sep 20 22:48:54.197: INFO: (11) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:460/proxy/: tls baz (200; 5.54269ms)
Sep 20 22:48:54.197: INFO: (11) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname2/proxy/: bar (200; 5.613418ms)
Sep 20 22:48:54.198: INFO: (11) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname1/proxy/: foo (200; 7.038117ms)
Sep 20 22:48:54.198: INFO: (11) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname1/proxy/: foo (200; 6.868271ms)
Sep 20 22:48:54.198: INFO: (11) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname1/proxy/: tls baz (200; 6.845009ms)
Sep 20 22:48:54.202: INFO: (12) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:160/proxy/: foo (200; 3.480419ms)
Sep 20 22:48:54.203: INFO: (12) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/tlsrewritem... (200; 3.841904ms)
Sep 20 22:48:54.203: INFO: (12) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/rewriteme">test</a> (200; 4.232992ms)
Sep 20 22:48:54.203: INFO: (12) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:462/proxy/: tls qux (200; 4.5972ms)
Sep 20 22:48:54.203: INFO: (12) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:460/proxy/: tls baz (200; 4.831078ms)
Sep 20 22:48:54.205: INFO: (12) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname1/proxy/: tls baz (200; 6.810242ms)
Sep 20 22:48:54.206: INFO: (12) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname1/proxy/: foo (200; 6.193627ms)
Sep 20 22:48:54.206: INFO: (12) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname1/proxy/: foo (200; 7.226875ms)
Sep 20 22:48:54.206: INFO: (12) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname2/proxy/: bar (200; 6.331489ms)
Sep 20 22:48:54.206: INFO: (12) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname2/proxy/: tls qux (200; 7.177365ms)
Sep 20 22:48:54.207: INFO: (12) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:160/proxy/: foo (200; 7.497456ms)
Sep 20 22:48:54.207: INFO: (12) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/rewriteme">... (200; 7.693708ms)
Sep 20 22:48:54.207: INFO: (12) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/rewriteme">test<... (200; 7.902073ms)
Sep 20 22:48:54.207: INFO: (12) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:162/proxy/: bar (200; 7.723559ms)
Sep 20 22:48:54.207: INFO: (12) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname2/proxy/: bar (200; 8.49864ms)
Sep 20 22:48:54.208: INFO: (12) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:162/proxy/: bar (200; 8.473444ms)
Sep 20 22:48:54.213: INFO: (13) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/rewriteme">test<... (200; 4.946398ms)
Sep 20 22:48:54.214: INFO: (13) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:462/proxy/: tls qux (200; 5.630436ms)
Sep 20 22:48:54.214: INFO: (13) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname2/proxy/: bar (200; 5.27324ms)
Sep 20 22:48:54.214: INFO: (13) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:162/proxy/: bar (200; 5.57292ms)
Sep 20 22:48:54.214: INFO: (13) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:160/proxy/: foo (200; 5.176702ms)
Sep 20 22:48:54.214: INFO: (13) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/rewriteme">... (200; 5.601441ms)
Sep 20 22:48:54.214: INFO: (13) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/rewriteme">test</a> (200; 6.265148ms)
Sep 20 22:48:54.214: INFO: (13) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname1/proxy/: tls baz (200; 5.429193ms)
Sep 20 22:48:54.214: INFO: (13) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:160/proxy/: foo (200; 6.292591ms)
Sep 20 22:48:54.214: INFO: (13) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/tlsrewritem... (200; 6.159331ms)
Sep 20 22:48:54.215: INFO: (13) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname2/proxy/: tls qux (200; 7.263685ms)
Sep 20 22:48:54.215: INFO: (13) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname1/proxy/: foo (200; 6.458677ms)
Sep 20 22:48:54.215: INFO: (13) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:162/proxy/: bar (200; 7.0623ms)
Sep 20 22:48:54.215: INFO: (13) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:460/proxy/: tls baz (200; 6.562031ms)
Sep 20 22:48:54.216: INFO: (13) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname1/proxy/: foo (200; 7.094867ms)
Sep 20 22:48:54.216: INFO: (13) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname2/proxy/: bar (200; 7.604526ms)
Sep 20 22:48:54.220: INFO: (14) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:160/proxy/: foo (200; 3.664219ms)
Sep 20 22:48:54.221: INFO: (14) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:160/proxy/: foo (200; 3.997241ms)
Sep 20 22:48:54.221: INFO: (14) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/tlsrewritem... (200; 4.562562ms)
Sep 20 22:48:54.221: INFO: (14) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:462/proxy/: tls qux (200; 4.463983ms)
Sep 20 22:48:54.221: INFO: (14) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:162/proxy/: bar (200; 4.166491ms)
Sep 20 22:48:54.221: INFO: (14) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:460/proxy/: tls baz (200; 4.625141ms)
Sep 20 22:48:54.221: INFO: (14) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/rewriteme">test<... (200; 4.888399ms)
Sep 20 22:48:54.221: INFO: (14) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/rewriteme">test</a> (200; 5.053099ms)
Sep 20 22:48:54.221: INFO: (14) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/rewriteme">... (200; 4.67526ms)
Sep 20 22:48:54.222: INFO: (14) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:162/proxy/: bar (200; 5.223112ms)
Sep 20 22:48:54.223: INFO: (14) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname1/proxy/: tls baz (200; 6.813706ms)
Sep 20 22:48:54.224: INFO: (14) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname2/proxy/: bar (200; 7.206147ms)
Sep 20 22:48:54.224: INFO: (14) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname2/proxy/: tls qux (200; 7.106444ms)
Sep 20 22:48:54.224: INFO: (14) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname1/proxy/: foo (200; 7.495038ms)
Sep 20 22:48:54.224: INFO: (14) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname2/proxy/: bar (200; 7.566086ms)
Sep 20 22:48:54.224: INFO: (14) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname1/proxy/: foo (200; 7.663502ms)
Sep 20 22:48:54.228: INFO: (15) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:160/proxy/: foo (200; 3.416679ms)
Sep 20 22:48:54.228: INFO: (15) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:160/proxy/: foo (200; 3.474577ms)
Sep 20 22:48:54.229: INFO: (15) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/rewriteme">... (200; 3.812996ms)
Sep 20 22:48:54.230: INFO: (15) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/rewriteme">test<... (200; 4.761184ms)
Sep 20 22:48:54.230: INFO: (15) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:462/proxy/: tls qux (200; 5.227099ms)
Sep 20 22:48:54.230: INFO: (15) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:460/proxy/: tls baz (200; 5.379142ms)
Sep 20 22:48:54.230: INFO: (15) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/rewriteme">test</a> (200; 5.313131ms)
Sep 20 22:48:54.230: INFO: (15) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:162/proxy/: bar (200; 5.130729ms)
Sep 20 22:48:54.231: INFO: (15) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/tlsrewritem... (200; 5.381235ms)
Sep 20 22:48:54.231: INFO: (15) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:162/proxy/: bar (200; 5.510984ms)
Sep 20 22:48:54.231: INFO: (15) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname1/proxy/: tls baz (200; 6.488587ms)
Sep 20 22:48:54.232: INFO: (15) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname2/proxy/: tls qux (200; 6.886283ms)
Sep 20 22:48:54.232: INFO: (15) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname1/proxy/: foo (200; 7.475709ms)
Sep 20 22:48:54.232: INFO: (15) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname1/proxy/: foo (200; 7.31216ms)
Sep 20 22:48:54.232: INFO: (15) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname2/proxy/: bar (200; 6.727357ms)
Sep 20 22:48:54.233: INFO: (15) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname2/proxy/: bar (200; 8.637757ms)
Sep 20 22:48:54.240: INFO: (16) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:460/proxy/: tls baz (200; 6.858455ms)
Sep 20 22:48:54.241: INFO: (16) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname2/proxy/: bar (200; 6.417663ms)
Sep 20 22:48:54.241: INFO: (16) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname2/proxy/: bar (200; 6.902403ms)
Sep 20 22:48:54.241: INFO: (16) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/rewriteme">test</a> (200; 7.077969ms)
Sep 20 22:48:54.241: INFO: (16) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname1/proxy/: foo (200; 6.750946ms)
Sep 20 22:48:54.241: INFO: (16) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:160/proxy/: foo (200; 7.604166ms)
Sep 20 22:48:54.245: INFO: (16) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:162/proxy/: bar (200; 11.432524ms)
Sep 20 22:48:54.245: INFO: (16) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:162/proxy/: bar (200; 11.454482ms)
Sep 20 22:48:54.245: INFO: (16) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/tlsrewritem... (200; 11.438561ms)
Sep 20 22:48:54.245: INFO: (16) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname1/proxy/: foo (200; 12.094547ms)
Sep 20 22:48:54.245: INFO: (16) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname1/proxy/: tls baz (200; 12.064785ms)
Sep 20 22:48:54.246: INFO: (16) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:462/proxy/: tls qux (200; 11.878031ms)
Sep 20 22:48:54.246: INFO: (16) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname2/proxy/: tls qux (200; 12.063161ms)
Sep 20 22:48:54.246: INFO: (16) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:160/proxy/: foo (200; 11.732378ms)
Sep 20 22:48:54.246: INFO: (16) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/rewriteme">test<... (200; 11.86663ms)
Sep 20 22:48:54.246: INFO: (16) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/rewriteme">... (200; 11.842918ms)
Sep 20 22:48:54.252: INFO: (17) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:460/proxy/: tls baz (200; 5.77561ms)
Sep 20 22:48:54.252: INFO: (17) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:160/proxy/: foo (200; 6.229881ms)
Sep 20 22:48:54.252: INFO: (17) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/rewriteme">test</a> (200; 6.253545ms)
Sep 20 22:48:54.253: INFO: (17) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:160/proxy/: foo (200; 6.677244ms)
Sep 20 22:48:54.253: INFO: (17) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:462/proxy/: tls qux (200; 6.738918ms)
Sep 20 22:48:54.253: INFO: (17) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/rewriteme">... (200; 6.463562ms)
Sep 20 22:48:54.253: INFO: (17) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:162/proxy/: bar (200; 6.629317ms)
Sep 20 22:48:54.253: INFO: (17) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname1/proxy/: tls baz (200; 6.891155ms)
Sep 20 22:48:54.253: INFO: (17) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/rewriteme">test<... (200; 6.755593ms)
Sep 20 22:48:54.253: INFO: (17) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:162/proxy/: bar (200; 6.93078ms)
Sep 20 22:48:54.253: INFO: (17) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/tlsrewritem... (200; 6.857828ms)
Sep 20 22:48:54.253: INFO: (17) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname1/proxy/: foo (200; 7.203415ms)
Sep 20 22:48:54.254: INFO: (17) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname1/proxy/: foo (200; 7.918711ms)
Sep 20 22:48:54.255: INFO: (17) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname2/proxy/: bar (200; 8.555558ms)
Sep 20 22:48:54.256: INFO: (17) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname2/proxy/: bar (200; 9.683616ms)
Sep 20 22:48:54.257: INFO: (17) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname2/proxy/: tls qux (200; 10.68762ms)
Sep 20 22:48:54.262: INFO: (18) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/rewriteme">test<... (200; 4.45468ms)
Sep 20 22:48:54.263: INFO: (18) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/rewriteme">... (200; 5.729294ms)
Sep 20 22:48:54.264: INFO: (18) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:462/proxy/: tls qux (200; 7.148959ms)
Sep 20 22:48:54.264: INFO: (18) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/rewriteme">test</a> (200; 7.516787ms)
Sep 20 22:48:54.265: INFO: (18) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:460/proxy/: tls baz (200; 7.764147ms)
Sep 20 22:48:54.265: INFO: (18) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/tlsrewritem... (200; 8.352604ms)
Sep 20 22:48:54.266: INFO: (18) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:162/proxy/: bar (200; 9.171375ms)
Sep 20 22:48:54.266: INFO: (18) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:162/proxy/: bar (200; 9.584264ms)
Sep 20 22:48:54.267: INFO: (18) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:160/proxy/: foo (200; 10.037778ms)
Sep 20 22:48:54.267: INFO: (18) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:160/proxy/: foo (200; 9.953729ms)
Sep 20 22:48:54.270: INFO: (18) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname2/proxy/: bar (200; 12.614497ms)
Sep 20 22:48:54.270: INFO: (18) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname1/proxy/: tls baz (200; 13.62908ms)
Sep 20 22:48:54.271: INFO: (18) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname1/proxy/: foo (200; 13.35997ms)
Sep 20 22:48:54.271: INFO: (18) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname1/proxy/: foo (200; 14.124822ms)
Sep 20 22:48:54.271: INFO: (18) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname2/proxy/: tls qux (200; 14.031642ms)
Sep 20 22:48:54.272: INFO: (18) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname2/proxy/: bar (200; 14.585181ms)
Sep 20 22:48:54.278: INFO: (19) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname1/proxy/: foo (200; 6.508418ms)
Sep 20 22:48:54.280: INFO: (19) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:460/proxy/: tls baz (200; 7.950526ms)
Sep 20 22:48:54.280: INFO: (19) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:1080/proxy/rewriteme">... (200; 8.123487ms)
Sep 20 22:48:54.280: INFO: (19) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:1080/proxy/rewriteme">test<... (200; 8.174281ms)
Sep 20 22:48:54.281: INFO: (19) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname2/proxy/: tls qux (200; 8.757294ms)
Sep 20 22:48:54.281: INFO: (19) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname2/proxy/: bar (200; 8.70632ms)
Sep 20 22:48:54.282: INFO: (19) /api/v1/namespaces/proxy-5801/services/https:proxy-service-s8shf:tlsportname1/proxy/: tls baz (200; 10.075254ms)
Sep 20 22:48:54.282: INFO: (19) /api/v1/namespaces/proxy-5801/services/proxy-service-s8shf:portname1/proxy/: foo (200; 10.226812ms)
Sep 20 22:48:54.282: INFO: (19) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:160/proxy/: foo (200; 10.577393ms)
Sep 20 22:48:54.283: INFO: (19) /api/v1/namespaces/proxy-5801/services/http:proxy-service-s8shf:portname2/proxy/: bar (200; 10.661746ms)
Sep 20 22:48:54.283: INFO: (19) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g/proxy/rewriteme">test</a> (200; 11.084963ms)
Sep 20 22:48:54.283: INFO: (19) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:462/proxy/: tls qux (200; 11.34129ms)
Sep 20 22:48:54.284: INFO: (19) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:162/proxy/: bar (200; 11.831903ms)
Sep 20 22:48:54.284: INFO: (19) /api/v1/namespaces/proxy-5801/pods/proxy-service-s8shf-wg44g:160/proxy/: foo (200; 12.498528ms)
Sep 20 22:48:54.285: INFO: (19) /api/v1/namespaces/proxy-5801/pods/http:proxy-service-s8shf-wg44g:162/proxy/: bar (200; 12.513843ms)
Sep 20 22:48:54.285: INFO: (19) /api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/: <a href="/api/v1/namespaces/proxy-5801/pods/https:proxy-service-s8shf-wg44g:443/proxy/tlsrewritem... (200; 13.026244ms)
STEP: deleting ReplicationController proxy-service-s8shf in namespace proxy-5801, will wait for the garbage collector to delete the pods
I0920 22:48:54.286910      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 22:48:54.286950      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 22:48:54.390: INFO: Deleting ReplicationController proxy-service-s8shf took: 20.99753ms
Sep 20 22:48:54.691: INFO: Terminating ReplicationController proxy-service-s8shf pods took: 301.394437ms
I0920 22:48:54.691539      16 controller_utils.go:810] Ignoring inactive pod proxy-5801/proxy-service-s8shf-wg44g in state Running, deletion time 2019-09-20 22:48:55 +0000 UTC
[AfterEach] version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:49:03.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5801" for this suite.
Sep 20 22:49:09.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:49:09.297: INFO: namespace proxy-5801 deletion completed in 6.102538039s

• [SLOW TEST:21.460 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:49:09.297: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-eb1632ed-8533-490e-bfbe-d58054dfcf44
STEP: Creating a pod to test consume configMaps
Sep 20 22:49:09.325: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3d6b643d-7184-4be1-af35-ec32582e78fa" in namespace "projected-5283" to be "success or failure"
Sep 20 22:49:09.331: INFO: Pod "pod-projected-configmaps-3d6b643d-7184-4be1-af35-ec32582e78fa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.410838ms
Sep 20 22:49:11.337: INFO: Pod "pod-projected-configmaps-3d6b643d-7184-4be1-af35-ec32582e78fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012274479s
Sep 20 22:49:13.340: INFO: Pod "pod-projected-configmaps-3d6b643d-7184-4be1-af35-ec32582e78fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014554824s
STEP: Saw pod success
Sep 20 22:49:13.340: INFO: Pod "pod-projected-configmaps-3d6b643d-7184-4be1-af35-ec32582e78fa" satisfied condition "success or failure"
Sep 20 22:49:13.341: INFO: Trying to get logs from node worker-2 pod pod-projected-configmaps-3d6b643d-7184-4be1-af35-ec32582e78fa container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 22:49:13.353: INFO: Waiting for pod pod-projected-configmaps-3d6b643d-7184-4be1-af35-ec32582e78fa to disappear
Sep 20 22:49:13.354: INFO: Pod pod-projected-configmaps-3d6b643d-7184-4be1-af35-ec32582e78fa no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:49:13.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5283" for this suite.
Sep 20 22:49:19.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:49:19.446: INFO: namespace projected-5283 deletion completed in 6.090721194s

• [SLOW TEST:10.150 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:49:19.447: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7271
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Sep 20 22:49:19.477: INFO: Found 0 stateful pods, waiting for 3
Sep 20 22:49:29.484: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 22:49:29.484: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 22:49:29.484: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 22:49:29.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-7271 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 22:49:29.678: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 22:49:29.678: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 22:49:29.678: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Sep 20 22:49:39.723: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep 20 22:49:49.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-7271 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:49:49.917: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 20 22:49:49.917: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 22:49:49.917: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Sep 20 22:50:09.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-7271 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 22:50:10.118: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 22:50:10.118: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 22:50:10.118: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 22:50:20.140: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep 20 22:50:30.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=statefulset-7271 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 22:50:30.330: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 20 22:50:30.330: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 22:50:30.330: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 20 22:50:50.341: INFO: Waiting for StatefulSet statefulset-7271/ss2 to complete update
Sep 20 22:50:50.341: INFO: Waiting for Pod statefulset-7271/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Sep 20 22:51:00.354: INFO: Deleting all statefulset in ns statefulset-7271
Sep 20 22:51:00.359: INFO: Scaling statefulset ss2 to 0
Sep 20 22:51:10.383: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 22:51:10.385: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:51:10.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7271" for this suite.
Sep 20 22:51:16.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:51:16.481: INFO: namespace statefulset-7271 deletion completed in 6.07675627s

• [SLOW TEST:117.034 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:51:16.481: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-8645
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-8645
STEP: Deleting pre-stop pod
Sep 20 22:51:25.571: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:51:25.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-8645" for this suite.
Sep 20 22:52:09.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:52:09.690: INFO: namespace prestop-8645 deletion completed in 44.093902533s

• [SLOW TEST:53.209 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:52:09.690: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 22:52:10.415: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 22:52:12.420: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704616730, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704616730, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704616730, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704616730, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 22:52:15.443: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:52:27.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2356" for this suite.
Sep 20 22:52:33.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:52:33.736: INFO: namespace webhook-2356 deletion completed in 6.085965493s
STEP: Destroying namespace "webhook-2356-markers" for this suite.
Sep 20 22:52:39.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:52:39.818: INFO: namespace webhook-2356-markers deletion completed in 6.082016153s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:30.139 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:52:39.829: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-8732/secret-test-c1a5b732-8fdf-4fc2-be30-19f806c8ff2b
STEP: Creating a pod to test consume secrets
Sep 20 22:52:39.857: INFO: Waiting up to 5m0s for pod "pod-configmaps-980f0743-6397-483f-94a2-25583e220849" in namespace "secrets-8732" to be "success or failure"
Sep 20 22:52:39.866: INFO: Pod "pod-configmaps-980f0743-6397-483f-94a2-25583e220849": Phase="Pending", Reason="", readiness=false. Elapsed: 8.333519ms
Sep 20 22:52:41.873: INFO: Pod "pod-configmaps-980f0743-6397-483f-94a2-25583e220849": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015141895s
Sep 20 22:52:43.875: INFO: Pod "pod-configmaps-980f0743-6397-483f-94a2-25583e220849": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017133447s
STEP: Saw pod success
Sep 20 22:52:43.875: INFO: Pod "pod-configmaps-980f0743-6397-483f-94a2-25583e220849" satisfied condition "success or failure"
Sep 20 22:52:43.877: INFO: Trying to get logs from node worker-2 pod pod-configmaps-980f0743-6397-483f-94a2-25583e220849 container env-test: <nil>
STEP: delete the pod
Sep 20 22:52:43.894: INFO: Waiting for pod pod-configmaps-980f0743-6397-483f-94a2-25583e220849 to disappear
Sep 20 22:52:43.896: INFO: Pod pod-configmaps-980f0743-6397-483f-94a2-25583e220849 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:52:43.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8732" for this suite.
Sep 20 22:52:49.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:52:50.004: INFO: namespace secrets-8732 deletion completed in 6.104850388s

• [SLOW TEST:10.175 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:52:50.005: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-6334
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-6334
Sep 20 22:52:50.044: INFO: Found 0 stateful pods, waiting for 1
Sep 20 22:53:00.050: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Sep 20 22:53:00.094: INFO: Deleting all statefulset in ns statefulset-6334
Sep 20 22:53:00.109: INFO: Scaling statefulset ss to 0
Sep 20 22:53:10.130: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 22:53:10.131: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:53:10.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6334" for this suite.
Sep 20 22:53:16.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:53:16.251: INFO: namespace statefulset-6334 deletion completed in 6.106161002s

• [SLOW TEST:26.246 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:53:16.252: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Sep 20 22:53:16.273: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 20 22:53:16.278: INFO: Waiting for terminating namespaces to be deleted...
Sep 20 22:53:16.279: INFO: 
Logging pods the kubelet thinks is on node controlplane-1 before test
Sep 20 22:53:16.287: INFO: weave-net-n52f9 from kube-system started at 2019-09-20 21:41:30 +0000 UTC (2 container statuses recorded)
Sep 20 22:53:16.287: INFO: 	Container weave ready: true, restart count 0
Sep 20 22:53:16.287: INFO: 	Container weave-npc ready: true, restart count 0
Sep 20 22:53:16.287: INFO: etcd-controlplane-1 from kube-system started at 2019-09-20 21:29:15 +0000 UTC (1 container statuses recorded)
Sep 20 22:53:16.287: INFO: 	Container etcd ready: true, restart count 0
Sep 20 22:53:16.287: INFO: kube-proxy-62wrx from kube-system started at 2019-09-20 21:29:38 +0000 UTC (1 container statuses recorded)
Sep 20 22:53:16.287: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 22:53:16.287: INFO: kube-controller-manager-controlplane-1 from kube-system started at 2019-09-20 21:29:15 +0000 UTC (1 container statuses recorded)
Sep 20 22:53:16.287: INFO: 	Container kube-controller-manager ready: true, restart count 0
Sep 20 22:53:16.287: INFO: kube-scheduler-controlplane-1 from kube-system started at 2019-09-20 21:29:15 +0000 UTC (1 container statuses recorded)
Sep 20 22:53:16.287: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep 20 22:53:16.287: INFO: sonobuoy-systemd-logs-daemon-set-63bbc52511204dd7-g8gl8 from sonobuoy started at 2019-09-20 21:43:06 +0000 UTC (2 container statuses recorded)
Sep 20 22:53:16.287: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 20 22:53:16.287: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 20 22:53:16.287: INFO: kube-apiserver-controlplane-1 from kube-system started at 2019-09-20 21:29:15 +0000 UTC (1 container statuses recorded)
Sep 20 22:53:16.287: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep 20 22:53:16.287: INFO: 
Logging pods the kubelet thinks is on node worker-1 before test
Sep 20 22:53:16.295: INFO: sonobuoy-systemd-logs-daemon-set-63bbc52511204dd7-7p8gp from sonobuoy started at 2019-09-20 21:43:07 +0000 UTC (2 container statuses recorded)
Sep 20 22:53:16.295: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 20 22:53:16.295: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 20 22:53:16.295: INFO: kube-proxy-dxbgr from kube-system started at 2019-09-20 21:29:43 +0000 UTC (1 container statuses recorded)
Sep 20 22:53:16.295: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 22:53:16.295: INFO: weave-net-bnv9z from kube-system started at 2019-09-20 21:41:30 +0000 UTC (2 container statuses recorded)
Sep 20 22:53:16.295: INFO: 	Container weave ready: true, restart count 0
Sep 20 22:53:16.295: INFO: 	Container weave-npc ready: true, restart count 0
Sep 20 22:53:16.295: INFO: coredns-5644d7b6d9-pvh6f from kube-system started at 2019-09-20 21:42:42 +0000 UTC (1 container statuses recorded)
Sep 20 22:53:16.295: INFO: 	Container coredns ready: true, restart count 0
Sep 20 22:53:16.295: INFO: sonobuoy-e2e-job-558ae789799345e3 from sonobuoy started at 2019-09-20 21:43:07 +0000 UTC (2 container statuses recorded)
Sep 20 22:53:16.295: INFO: 	Container e2e ready: true, restart count 0
Sep 20 22:53:16.295: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 22:53:16.295: INFO: 
Logging pods the kubelet thinks is on node worker-2 before test
Sep 20 22:53:16.298: INFO: weave-net-cwtjw from kube-system started at 2019-09-20 21:41:30 +0000 UTC (2 container statuses recorded)
Sep 20 22:53:16.298: INFO: 	Container weave ready: true, restart count 0
Sep 20 22:53:16.298: INFO: 	Container weave-npc ready: true, restart count 0
Sep 20 22:53:16.298: INFO: coredns-5644d7b6d9-47g5v from kube-system started at 2019-09-20 21:42:42 +0000 UTC (1 container statuses recorded)
Sep 20 22:53:16.298: INFO: 	Container coredns ready: true, restart count 0
Sep 20 22:53:16.298: INFO: kube-proxy-dcssj from kube-system started at 2019-09-20 21:29:43 +0000 UTC (1 container statuses recorded)
Sep 20 22:53:16.298: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 22:53:16.298: INFO: sonobuoy-systemd-logs-daemon-set-63bbc52511204dd7-4bfzz from sonobuoy started at 2019-09-20 21:43:07 +0000 UTC (2 container statuses recorded)
Sep 20 22:53:16.298: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 20 22:53:16.298: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 20 22:53:16.298: INFO: sonobuoy from sonobuoy started at 2019-09-20 21:42:42 +0000 UTC (1 container statuses recorded)
Sep 20 22:53:16.298: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-ad4c0ea5-ab83-4608-9c24-cb3d41c58c69 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-ad4c0ea5-ab83-4608-9c24-cb3d41c58c69 off the node worker-2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ad4c0ea5-ab83-4608-9c24-cb3d41c58c69
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:53:30.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6144" for this suite.
Sep 20 22:53:48.467: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:53:48.529: INFO: namespace sched-pred-6144 deletion completed in 18.075171626s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
I0920 22:53:48.529562      16 request.go:706] Error in request: resource name may not be empty

• [SLOW TEST:32.278 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:53:48.529: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 22:53:48.581: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"75ce25c2-ab4f-4e02-8546-05c03e84026a", Controller:(*bool)(0xc0037de536), BlockOwnerDeletion:(*bool)(0xc0037de537)}}
Sep 20 22:53:48.589: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"165aced4-04e9-41ed-8af8-c0199efe2db5", Controller:(*bool)(0xc004d2b5ea), BlockOwnerDeletion:(*bool)(0xc004d2b5eb)}}
Sep 20 22:53:48.597: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"6eed0544-5a85-4346-886a-6829f08a165c", Controller:(*bool)(0xc004d2b7f6), BlockOwnerDeletion:(*bool)(0xc004d2b7f7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:53:53.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7575" for this suite.
Sep 20 22:53:59.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:53:59.684: INFO: namespace gc-7575 deletion completed in 6.06121719s

• [SLOW TEST:11.155 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:53:59.685: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Sep 20 22:54:04.249: INFO: Successfully updated pod "adopt-release-54866"
STEP: Checking that the Job readopts the Pod
Sep 20 22:54:04.249: INFO: Waiting up to 15m0s for pod "adopt-release-54866" in namespace "job-2343" to be "adopted"
Sep 20 22:54:04.257: INFO: Pod "adopt-release-54866": Phase="Running", Reason="", readiness=true. Elapsed: 7.416784ms
Sep 20 22:54:06.263: INFO: Pod "adopt-release-54866": Phase="Running", Reason="", readiness=true. Elapsed: 2.013825229s
Sep 20 22:54:06.263: INFO: Pod "adopt-release-54866" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Sep 20 22:54:06.791: INFO: Successfully updated pod "adopt-release-54866"
STEP: Checking that the Job releases the Pod
Sep 20 22:54:06.792: INFO: Waiting up to 15m0s for pod "adopt-release-54866" in namespace "job-2343" to be "released"
Sep 20 22:54:06.807: INFO: Pod "adopt-release-54866": Phase="Running", Reason="", readiness=true. Elapsed: 14.93385ms
Sep 20 22:54:06.807: INFO: Pod "adopt-release-54866" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:54:06.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2343" for this suite.
Sep 20 22:54:44.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:54:44.883: INFO: namespace job-2343 deletion completed in 38.071983259s

• [SLOW TEST:45.198 seconds]
[sig-apps] Job
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:54:44.884: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-5da9c41e-d07f-4d16-9832-def512d925d5 in namespace container-probe-6786
Sep 20 22:54:46.915: INFO: Started pod liveness-5da9c41e-d07f-4d16-9832-def512d925d5 in namespace container-probe-6786
STEP: checking the pod's current state and verifying that restartCount is present
Sep 20 22:54:46.916: INFO: Initial restart count of pod liveness-5da9c41e-d07f-4d16-9832-def512d925d5 is 0
Sep 20 22:55:00.944: INFO: Restart count of pod container-probe-6786/liveness-5da9c41e-d07f-4d16-9832-def512d925d5 is now 1 (14.027400326s elapsed)
Sep 20 22:55:21.005: INFO: Restart count of pod container-probe-6786/liveness-5da9c41e-d07f-4d16-9832-def512d925d5 is now 2 (34.089240717s elapsed)
Sep 20 22:55:41.086: INFO: Restart count of pod container-probe-6786/liveness-5da9c41e-d07f-4d16-9832-def512d925d5 is now 3 (54.169416663s elapsed)
Sep 20 22:56:01.147: INFO: Restart count of pod container-probe-6786/liveness-5da9c41e-d07f-4d16-9832-def512d925d5 is now 4 (1m14.230430542s elapsed)
Sep 20 22:57:09.450: INFO: Restart count of pod container-probe-6786/liveness-5da9c41e-d07f-4d16-9832-def512d925d5 is now 5 (2m22.533489754s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:57:09.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6786" for this suite.
Sep 20 22:57:15.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:57:15.541: INFO: namespace container-probe-6786 deletion completed in 6.074642935s

• [SLOW TEST:150.658 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:57:15.542: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5990.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5990.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5990.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5990.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 20 22:57:19.605: INFO: DNS probes using dns-test-629ea366-f663-4451-b91a-064a785ab96b succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5990.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5990.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5990.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5990.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 20 22:57:23.679: INFO: File wheezy_udp@dns-test-service-3.dns-5990.svc.cluster.local from pod  dns-5990/dns-test-6bc1496a-4d9b-4c0d-8d85-2730fec37237 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 20 22:57:23.686: INFO: File jessie_udp@dns-test-service-3.dns-5990.svc.cluster.local from pod  dns-5990/dns-test-6bc1496a-4d9b-4c0d-8d85-2730fec37237 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 20 22:57:23.686: INFO: Lookups using dns-5990/dns-test-6bc1496a-4d9b-4c0d-8d85-2730fec37237 failed for: [wheezy_udp@dns-test-service-3.dns-5990.svc.cluster.local jessie_udp@dns-test-service-3.dns-5990.svc.cluster.local]

Sep 20 22:57:28.698: INFO: File wheezy_udp@dns-test-service-3.dns-5990.svc.cluster.local from pod  dns-5990/dns-test-6bc1496a-4d9b-4c0d-8d85-2730fec37237 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 20 22:57:28.706: INFO: File jessie_udp@dns-test-service-3.dns-5990.svc.cluster.local from pod  dns-5990/dns-test-6bc1496a-4d9b-4c0d-8d85-2730fec37237 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 20 22:57:28.706: INFO: Lookups using dns-5990/dns-test-6bc1496a-4d9b-4c0d-8d85-2730fec37237 failed for: [wheezy_udp@dns-test-service-3.dns-5990.svc.cluster.local jessie_udp@dns-test-service-3.dns-5990.svc.cluster.local]

Sep 20 22:57:33.696: INFO: File wheezy_udp@dns-test-service-3.dns-5990.svc.cluster.local from pod  dns-5990/dns-test-6bc1496a-4d9b-4c0d-8d85-2730fec37237 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 20 22:57:33.704: INFO: File jessie_udp@dns-test-service-3.dns-5990.svc.cluster.local from pod  dns-5990/dns-test-6bc1496a-4d9b-4c0d-8d85-2730fec37237 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 20 22:57:33.704: INFO: Lookups using dns-5990/dns-test-6bc1496a-4d9b-4c0d-8d85-2730fec37237 failed for: [wheezy_udp@dns-test-service-3.dns-5990.svc.cluster.local jessie_udp@dns-test-service-3.dns-5990.svc.cluster.local]

Sep 20 22:57:38.696: INFO: File wheezy_udp@dns-test-service-3.dns-5990.svc.cluster.local from pod  dns-5990/dns-test-6bc1496a-4d9b-4c0d-8d85-2730fec37237 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 20 22:57:38.703: INFO: File jessie_udp@dns-test-service-3.dns-5990.svc.cluster.local from pod  dns-5990/dns-test-6bc1496a-4d9b-4c0d-8d85-2730fec37237 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 20 22:57:38.703: INFO: Lookups using dns-5990/dns-test-6bc1496a-4d9b-4c0d-8d85-2730fec37237 failed for: [wheezy_udp@dns-test-service-3.dns-5990.svc.cluster.local jessie_udp@dns-test-service-3.dns-5990.svc.cluster.local]

Sep 20 22:57:43.695: INFO: File wheezy_udp@dns-test-service-3.dns-5990.svc.cluster.local from pod  dns-5990/dns-test-6bc1496a-4d9b-4c0d-8d85-2730fec37237 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 20 22:57:43.702: INFO: File jessie_udp@dns-test-service-3.dns-5990.svc.cluster.local from pod  dns-5990/dns-test-6bc1496a-4d9b-4c0d-8d85-2730fec37237 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 20 22:57:43.702: INFO: Lookups using dns-5990/dns-test-6bc1496a-4d9b-4c0d-8d85-2730fec37237 failed for: [wheezy_udp@dns-test-service-3.dns-5990.svc.cluster.local jessie_udp@dns-test-service-3.dns-5990.svc.cluster.local]

Sep 20 22:57:48.741: INFO: DNS probes using dns-test-6bc1496a-4d9b-4c0d-8d85-2730fec37237 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5990.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5990.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5990.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5990.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 20 22:57:52.837: INFO: DNS probes using dns-test-b0e626bc-aa08-45cb-8d99-8d8d6917d994 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:57:52.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5990" for this suite.
Sep 20 22:57:58.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:57:59.009: INFO: namespace dns-5990 deletion completed in 6.12666099s

• [SLOW TEST:43.467 seconds]
[sig-network] DNS
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:57:59.011: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 22:57:59.031: INFO: Creating deployment "test-recreate-deployment"
Sep 20 22:57:59.033: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep 20 22:57:59.036: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Sep 20 22:58:01.044: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep 20 22:58:01.047: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep 20 22:58:01.054: INFO: Updating deployment test-recreate-deployment
Sep 20 22:58:01.054: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Sep 20 22:58:01.100: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-454 /apis/apps/v1/namespaces/deployment-454/deployments/test-recreate-deployment 6345882a-569d-478a-a861-02eb298032b3 18286 2 2019-09-20 22:57:59 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005dec968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2019-09-20 22:58:01 +0000 UTC,LastTransitionTime:2019-09-20 22:58:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2019-09-20 22:58:01 +0000 UTC,LastTransitionTime:2019-09-20 22:57:59 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Sep 20 22:58:01.107: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-454 /apis/apps/v1/namespaces/deployment-454/replicasets/test-recreate-deployment-5f94c574ff 147190c0-a4fd-45d9-9937-e1965b08905f 18283 1 2019-09-20 22:58:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 6345882a-569d-478a-a861-02eb298032b3 0xc005decd47 0xc005decd48}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005decda8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 20 22:58:01.107: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep 20 22:58:01.107: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-454 /apis/apps/v1/namespaces/deployment-454/replicasets/test-recreate-deployment-68fc85c7bb fa94a5ec-91ac-4fda-95b6-4271fe21c24c 18275 2 2019-09-20 22:57:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 6345882a-569d-478a-a861-02eb298032b3 0xc005dece17 0xc005dece18}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005dece78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 20 22:58:01.109: INFO: Pod "test-recreate-deployment-5f94c574ff-cbkst" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-cbkst test-recreate-deployment-5f94c574ff- deployment-454 /api/v1/namespaces/deployment-454/pods/test-recreate-deployment-5f94c574ff-cbkst 4c0b1b3e-19ba-44d4-8867-15e944a26871 18287 0 2019-09-20 22:58:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 147190c0-a4fd-45d9-9937-e1965b08905f 0xc005ded327 0xc005ded328}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-58vgq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-58vgq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-58vgq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:58:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:58:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:58:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 22:58:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.102,PodIP:,StartTime:2019-09-20 22:58:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:58:01.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-454" for this suite.
Sep 20 22:58:07.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:58:07.192: INFO: namespace deployment-454 deletion completed in 6.080400266s

• [SLOW TEST:8.181 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:58:07.192: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 22:58:07.267: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-5a4bd0fc-31a3-4739-b451-e37cfa86e362" in namespace "security-context-test-9509" to be "success or failure"
Sep 20 22:58:07.271: INFO: Pod "busybox-privileged-false-5a4bd0fc-31a3-4739-b451-e37cfa86e362": Phase="Pending", Reason="", readiness=false. Elapsed: 3.232056ms
Sep 20 22:58:09.273: INFO: Pod "busybox-privileged-false-5a4bd0fc-31a3-4739-b451-e37cfa86e362": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005610065s
Sep 20 22:58:09.273: INFO: Pod "busybox-privileged-false-5a4bd0fc-31a3-4739-b451-e37cfa86e362" satisfied condition "success or failure"
Sep 20 22:58:09.284: INFO: Got logs for pod "busybox-privileged-false-5a4bd0fc-31a3-4739-b451-e37cfa86e362": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:58:09.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9509" for this suite.
Sep 20 22:58:15.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:58:15.377: INFO: namespace security-context-test-9509 deletion completed in 6.090941591s

• [SLOW TEST:8.185 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:58:15.378: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:58:19.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7699" for this suite.
Sep 20 22:58:25.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:58:25.510: INFO: namespace kubelet-test-7699 deletion completed in 6.068882271s

• [SLOW TEST:10.132 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:58:25.511: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Sep 20 22:58:25.539: INFO: Waiting up to 5m0s for pod "downward-api-7165fd46-477f-46a2-9926-6d319207321a" in namespace "downward-api-1227" to be "success or failure"
Sep 20 22:58:25.548: INFO: Pod "downward-api-7165fd46-477f-46a2-9926-6d319207321a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.460158ms
Sep 20 22:58:27.561: INFO: Pod "downward-api-7165fd46-477f-46a2-9926-6d319207321a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02116433s
Sep 20 22:58:29.565: INFO: Pod "downward-api-7165fd46-477f-46a2-9926-6d319207321a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025036848s
STEP: Saw pod success
Sep 20 22:58:29.565: INFO: Pod "downward-api-7165fd46-477f-46a2-9926-6d319207321a" satisfied condition "success or failure"
Sep 20 22:58:29.569: INFO: Trying to get logs from node worker-2 pod downward-api-7165fd46-477f-46a2-9926-6d319207321a container dapi-container: <nil>
STEP: delete the pod
Sep 20 22:58:29.607: INFO: Waiting for pod downward-api-7165fd46-477f-46a2-9926-6d319207321a to disappear
Sep 20 22:58:29.610: INFO: Pod downward-api-7165fd46-477f-46a2-9926-6d319207321a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:58:29.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1227" for this suite.
Sep 20 22:58:35.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:58:35.671: INFO: namespace downward-api-1227 deletion completed in 6.057162978s

• [SLOW TEST:10.160 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:58:35.671: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Sep 20 22:58:35.761: INFO: Waiting up to 5m0s for pod "pod-2145afa0-ec29-4831-921b-0abdc34bcffd" in namespace "emptydir-3646" to be "success or failure"
Sep 20 22:58:35.766: INFO: Pod "pod-2145afa0-ec29-4831-921b-0abdc34bcffd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.188798ms
Sep 20 22:58:37.781: INFO: Pod "pod-2145afa0-ec29-4831-921b-0abdc34bcffd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01933529s
Sep 20 22:58:39.787: INFO: Pod "pod-2145afa0-ec29-4831-921b-0abdc34bcffd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025268804s
STEP: Saw pod success
Sep 20 22:58:39.787: INFO: Pod "pod-2145afa0-ec29-4831-921b-0abdc34bcffd" satisfied condition "success or failure"
Sep 20 22:58:39.791: INFO: Trying to get logs from node worker-2 pod pod-2145afa0-ec29-4831-921b-0abdc34bcffd container test-container: <nil>
STEP: delete the pod
Sep 20 22:58:39.830: INFO: Waiting for pod pod-2145afa0-ec29-4831-921b-0abdc34bcffd to disappear
Sep 20 22:58:39.834: INFO: Pod pod-2145afa0-ec29-4831-921b-0abdc34bcffd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:58:39.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3646" for this suite.
Sep 20 22:58:45.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:58:45.893: INFO: namespace emptydir-3646 deletion completed in 6.055555329s

• [SLOW TEST:10.223 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:58:45.894: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Sep 20 22:58:45.916: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:58:56.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4322" for this suite.
Sep 20 22:59:02.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:59:03.025: INFO: namespace crd-publish-openapi-4322 deletion completed in 6.091981644s

• [SLOW TEST:17.130 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:59:03.025: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 22:59:03.052: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2253a7fd-c340-4d68-9315-31303f18dc1b" in namespace "projected-8248" to be "success or failure"
Sep 20 22:59:03.057: INFO: Pod "downwardapi-volume-2253a7fd-c340-4d68-9315-31303f18dc1b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.980338ms
Sep 20 22:59:05.064: INFO: Pod "downwardapi-volume-2253a7fd-c340-4d68-9315-31303f18dc1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01110549s
STEP: Saw pod success
Sep 20 22:59:05.064: INFO: Pod "downwardapi-volume-2253a7fd-c340-4d68-9315-31303f18dc1b" satisfied condition "success or failure"
Sep 20 22:59:05.069: INFO: Trying to get logs from node worker-2 pod downwardapi-volume-2253a7fd-c340-4d68-9315-31303f18dc1b container client-container: <nil>
STEP: delete the pod
Sep 20 22:59:05.101: INFO: Waiting for pod downwardapi-volume-2253a7fd-c340-4d68-9315-31303f18dc1b to disappear
Sep 20 22:59:05.103: INFO: Pod downwardapi-volume-2253a7fd-c340-4d68-9315-31303f18dc1b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:59:05.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8248" for this suite.
Sep 20 22:59:11.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:59:11.164: INFO: namespace projected-8248 deletion completed in 6.057868721s

• [SLOW TEST:8.139 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:59:11.166: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Sep 20 22:59:11.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 create -f - --namespace=kubectl-9099'
Sep 20 22:59:11.813: INFO: stderr: ""
Sep 20 22:59:11.813: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 20 22:59:11.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9099'
Sep 20 22:59:11.881: INFO: stderr: ""
Sep 20 22:59:11.881: INFO: stdout: "update-demo-nautilus-mq97g update-demo-nautilus-xr67p "
Sep 20 22:59:11.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-nautilus-mq97g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9099'
Sep 20 22:59:11.932: INFO: stderr: ""
Sep 20 22:59:11.932: INFO: stdout: ""
Sep 20 22:59:11.932: INFO: update-demo-nautilus-mq97g is created but not running
Sep 20 22:59:16.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9099'
Sep 20 22:59:16.982: INFO: stderr: ""
Sep 20 22:59:16.982: INFO: stdout: "update-demo-nautilus-mq97g update-demo-nautilus-xr67p "
Sep 20 22:59:16.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-nautilus-mq97g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9099'
Sep 20 22:59:17.030: INFO: stderr: ""
Sep 20 22:59:17.030: INFO: stdout: "true"
Sep 20 22:59:17.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-nautilus-mq97g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9099'
Sep 20 22:59:17.084: INFO: stderr: ""
Sep 20 22:59:17.084: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 20 22:59:17.084: INFO: validating pod update-demo-nautilus-mq97g
Sep 20 22:59:17.087: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 22:59:17.087: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 22:59:17.087: INFO: update-demo-nautilus-mq97g is verified up and running
Sep 20 22:59:17.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-nautilus-xr67p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9099'
Sep 20 22:59:17.148: INFO: stderr: ""
Sep 20 22:59:17.148: INFO: stdout: "true"
Sep 20 22:59:17.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-nautilus-xr67p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9099'
Sep 20 22:59:17.193: INFO: stderr: ""
Sep 20 22:59:17.193: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 20 22:59:17.193: INFO: validating pod update-demo-nautilus-xr67p
Sep 20 22:59:17.197: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 22:59:17.197: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 22:59:17.197: INFO: update-demo-nautilus-xr67p is verified up and running
STEP: scaling down the replication controller
Sep 20 22:59:17.198: INFO: scanned /root for discovery docs: <nil>
Sep 20 22:59:17.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-9099'
Sep 20 22:59:18.271: INFO: stderr: ""
Sep 20 22:59:18.272: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 20 22:59:18.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9099'
Sep 20 22:59:18.337: INFO: stderr: ""
Sep 20 22:59:18.337: INFO: stdout: "update-demo-nautilus-mq97g update-demo-nautilus-xr67p "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep 20 22:59:23.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9099'
Sep 20 22:59:23.438: INFO: stderr: ""
Sep 20 22:59:23.438: INFO: stdout: "update-demo-nautilus-mq97g "
Sep 20 22:59:23.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-nautilus-mq97g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9099'
Sep 20 22:59:23.496: INFO: stderr: ""
Sep 20 22:59:23.496: INFO: stdout: "true"
Sep 20 22:59:23.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-nautilus-mq97g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9099'
Sep 20 22:59:23.547: INFO: stderr: ""
Sep 20 22:59:23.547: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 20 22:59:23.547: INFO: validating pod update-demo-nautilus-mq97g
Sep 20 22:59:23.550: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 22:59:23.550: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 22:59:23.550: INFO: update-demo-nautilus-mq97g is verified up and running
STEP: scaling up the replication controller
Sep 20 22:59:23.551: INFO: scanned /root for discovery docs: <nil>
Sep 20 22:59:23.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-9099'
Sep 20 22:59:24.617: INFO: stderr: ""
Sep 20 22:59:24.617: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 20 22:59:24.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9099'
Sep 20 22:59:24.698: INFO: stderr: ""
Sep 20 22:59:24.698: INFO: stdout: "update-demo-nautilus-2kt7g update-demo-nautilus-mq97g "
Sep 20 22:59:24.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-nautilus-2kt7g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9099'
Sep 20 22:59:24.759: INFO: stderr: ""
Sep 20 22:59:24.759: INFO: stdout: ""
Sep 20 22:59:24.759: INFO: update-demo-nautilus-2kt7g is created but not running
Sep 20 22:59:29.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9099'
Sep 20 22:59:29.817: INFO: stderr: ""
Sep 20 22:59:29.817: INFO: stdout: "update-demo-nautilus-2kt7g update-demo-nautilus-mq97g "
Sep 20 22:59:29.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-nautilus-2kt7g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9099'
Sep 20 22:59:29.866: INFO: stderr: ""
Sep 20 22:59:29.866: INFO: stdout: "true"
Sep 20 22:59:29.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-nautilus-2kt7g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9099'
Sep 20 22:59:29.917: INFO: stderr: ""
Sep 20 22:59:29.917: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 20 22:59:29.917: INFO: validating pod update-demo-nautilus-2kt7g
Sep 20 22:59:29.921: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 22:59:29.921: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 22:59:29.921: INFO: update-demo-nautilus-2kt7g is verified up and running
Sep 20 22:59:29.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-nautilus-mq97g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9099'
Sep 20 22:59:29.972: INFO: stderr: ""
Sep 20 22:59:29.972: INFO: stdout: "true"
Sep 20 22:59:29.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-nautilus-mq97g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9099'
Sep 20 22:59:30.019: INFO: stderr: ""
Sep 20 22:59:30.019: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 20 22:59:30.019: INFO: validating pod update-demo-nautilus-mq97g
Sep 20 22:59:30.022: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 22:59:30.022: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 22:59:30.022: INFO: update-demo-nautilus-mq97g is verified up and running
STEP: using delete to clean up resources
Sep 20 22:59:30.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 delete --grace-period=0 --force -f - --namespace=kubectl-9099'
Sep 20 22:59:30.087: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 22:59:30.087: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 20 22:59:30.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9099'
Sep 20 22:59:30.139: INFO: stderr: "No resources found in kubectl-9099 namespace.\n"
Sep 20 22:59:30.139: INFO: stdout: ""
Sep 20 22:59:30.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods -l name=update-demo --namespace=kubectl-9099 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 20 22:59:30.200: INFO: stderr: ""
Sep 20 22:59:30.200: INFO: stdout: "update-demo-nautilus-2kt7g\nupdate-demo-nautilus-mq97g\n"
Sep 20 22:59:30.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9099'
Sep 20 22:59:30.792: INFO: stderr: "No resources found in kubectl-9099 namespace.\n"
Sep 20 22:59:30.792: INFO: stdout: ""
Sep 20 22:59:30.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods -l name=update-demo --namespace=kubectl-9099 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 20 22:59:30.840: INFO: stderr: ""
Sep 20 22:59:30.840: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 22:59:30.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9099" for this suite.
Sep 20 22:59:58.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 22:59:58.902: INFO: namespace kubectl-9099 deletion completed in 28.059362665s

• [SLOW TEST:47.735 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 22:59:58.902: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-1eeb7dc4-f1ee-4c57-8288-2306f1b61c87
STEP: Creating a pod to test consume secrets
Sep 20 22:59:58.984: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e3627de3-59d6-4b3d-b0d8-47b3a3f27cc5" in namespace "projected-8950" to be "success or failure"
Sep 20 22:59:58.990: INFO: Pod "pod-projected-secrets-e3627de3-59d6-4b3d-b0d8-47b3a3f27cc5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.356446ms
Sep 20 23:00:00.993: INFO: Pod "pod-projected-secrets-e3627de3-59d6-4b3d-b0d8-47b3a3f27cc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008731009s
STEP: Saw pod success
Sep 20 23:00:00.993: INFO: Pod "pod-projected-secrets-e3627de3-59d6-4b3d-b0d8-47b3a3f27cc5" satisfied condition "success or failure"
Sep 20 23:00:00.995: INFO: Trying to get logs from node worker-2 pod pod-projected-secrets-e3627de3-59d6-4b3d-b0d8-47b3a3f27cc5 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 20 23:00:01.008: INFO: Waiting for pod pod-projected-secrets-e3627de3-59d6-4b3d-b0d8-47b3a3f27cc5 to disappear
Sep 20 23:00:01.011: INFO: Pod pod-projected-secrets-e3627de3-59d6-4b3d-b0d8-47b3a3f27cc5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:00:01.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8950" for this suite.
Sep 20 23:00:07.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:00:07.098: INFO: namespace projected-8950 deletion completed in 6.084207134s

• [SLOW TEST:8.196 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:00:07.099: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:00:24.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6041" for this suite.
Sep 20 23:00:30.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:00:30.375: INFO: namespace resourcequota-6041 deletion completed in 6.108235403s

• [SLOW TEST:23.276 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:00:30.377: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:00:30.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4185" for this suite.
Sep 20 23:00:36.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:00:36.478: INFO: namespace services-4185 deletion completed in 6.077690454s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:6.102 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:00:36.478: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-42a6f6be-4ca6-43c8-8c1f-cd38ee0533de
STEP: Creating configMap with name cm-test-opt-upd-7e0753e5-edaa-45ea-9747-dfdd8633f006
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-42a6f6be-4ca6-43c8-8c1f-cd38ee0533de
STEP: Updating configmap cm-test-opt-upd-7e0753e5-edaa-45ea-9747-dfdd8633f006
STEP: Creating configMap with name cm-test-opt-create-e104254f-c1e5-40fb-8727-d168cf1b7d71
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:02:09.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1018" for this suite.
Sep 20 23:02:27.556: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:02:27.604: INFO: namespace projected-1018 deletion completed in 18.061937153s

• [SLOW TEST:111.126 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:02:27.605: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 23:02:28.098: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 23:02:30.116: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704617348, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704617348, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704617348, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704617348, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 23:02:33.150: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:02:33.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8818" for this suite.
Sep 20 23:02:45.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:02:45.257: INFO: namespace webhook-8818 deletion completed in 12.059671615s
STEP: Destroying namespace "webhook-8818-markers" for this suite.
Sep 20 23:02:51.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:02:51.349: INFO: namespace webhook-8818-markers deletion completed in 6.092322642s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:23.751 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:02:51.356: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Sep 20 23:02:51.384: INFO: Waiting up to 5m0s for pod "client-containers-64f6dd42-9fa6-46bc-becb-b811a82c6541" in namespace "containers-5187" to be "success or failure"
Sep 20 23:02:51.386: INFO: Pod "client-containers-64f6dd42-9fa6-46bc-becb-b811a82c6541": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055971ms
Sep 20 23:02:53.392: INFO: Pod "client-containers-64f6dd42-9fa6-46bc-becb-b811a82c6541": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008335559s
Sep 20 23:02:55.455: INFO: Pod "client-containers-64f6dd42-9fa6-46bc-becb-b811a82c6541": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.070947946s
STEP: Saw pod success
Sep 20 23:02:55.455: INFO: Pod "client-containers-64f6dd42-9fa6-46bc-becb-b811a82c6541" satisfied condition "success or failure"
Sep 20 23:02:55.458: INFO: Trying to get logs from node worker-2 pod client-containers-64f6dd42-9fa6-46bc-becb-b811a82c6541 container test-container: <nil>
STEP: delete the pod
Sep 20 23:02:55.476: INFO: Waiting for pod client-containers-64f6dd42-9fa6-46bc-becb-b811a82c6541 to disappear
Sep 20 23:02:55.479: INFO: Pod client-containers-64f6dd42-9fa6-46bc-becb-b811a82c6541 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:02:55.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5187" for this suite.
Sep 20 23:03:01.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:03:01.569: INFO: namespace containers-5187 deletion completed in 6.087501787s

• [SLOW TEST:10.213 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:03:01.570: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-663
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-663
STEP: Creating statefulset with conflicting port in namespace statefulset-663
STEP: Waiting until pod test-pod will start running in namespace statefulset-663
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-663
Sep 20 23:03:03.632: INFO: Observed stateful pod in namespace: statefulset-663, name: ss-0, uid: f8d31cbb-ab1e-49bb-a45f-151bfe86b4ab, status phase: Pending. Waiting for statefulset controller to delete.
Sep 20 23:03:05.401: INFO: Observed stateful pod in namespace: statefulset-663, name: ss-0, uid: f8d31cbb-ab1e-49bb-a45f-151bfe86b4ab, status phase: Failed. Waiting for statefulset controller to delete.
Sep 20 23:03:05.409: INFO: Observed stateful pod in namespace: statefulset-663, name: ss-0, uid: f8d31cbb-ab1e-49bb-a45f-151bfe86b4ab, status phase: Failed. Waiting for statefulset controller to delete.
Sep 20 23:03:05.416: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-663
STEP: Removing pod with conflicting port in namespace statefulset-663
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-663 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Sep 20 23:03:09.462: INFO: Deleting all statefulset in ns statefulset-663
Sep 20 23:03:09.465: INFO: Scaling statefulset ss to 0
Sep 20 23:03:19.474: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 23:03:19.475: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:03:19.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-663" for this suite.
Sep 20 23:03:25.498: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:03:25.579: INFO: namespace statefulset-663 deletion completed in 6.094501396s

• [SLOW TEST:24.009 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:03:25.579: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Sep 20 23:03:25.599: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Sep 20 23:03:25.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 create -f - --namespace=kubectl-5127'
Sep 20 23:03:25.698: INFO: stderr: ""
Sep 20 23:03:25.698: INFO: stdout: "service/redis-slave created\n"
Sep 20 23:03:25.698: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Sep 20 23:03:25.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 create -f - --namespace=kubectl-5127'
Sep 20 23:03:25.814: INFO: stderr: ""
Sep 20 23:03:25.814: INFO: stdout: "service/redis-master created\n"
Sep 20 23:03:25.814: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep 20 23:03:25.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 create -f - --namespace=kubectl-5127'
Sep 20 23:03:25.933: INFO: stderr: ""
Sep 20 23:03:25.933: INFO: stdout: "service/frontend created\n"
Sep 20 23:03:25.933: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Sep 20 23:03:25.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 create -f - --namespace=kubectl-5127'
Sep 20 23:03:26.042: INFO: stderr: ""
Sep 20 23:03:26.042: INFO: stdout: "deployment.apps/frontend created\n"
Sep 20 23:03:26.042: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 20 23:03:26.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 create -f - --namespace=kubectl-5127'
Sep 20 23:03:26.153: INFO: stderr: ""
Sep 20 23:03:26.153: INFO: stdout: "deployment.apps/redis-master created\n"
Sep 20 23:03:26.153: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Sep 20 23:03:26.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 create -f - --namespace=kubectl-5127'
Sep 20 23:03:26.267: INFO: stderr: ""
Sep 20 23:03:26.267: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Sep 20 23:03:26.267: INFO: Waiting for all frontend pods to be Running.
I0920 23:03:26.267808      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 23:03:26.267830      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 23:05:46.342: INFO: Waiting for frontend to serve content.
Sep 20 23:05:46.374: INFO: Trying to add a new entry to the guestbook.
Sep 20 23:05:51.408: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Sep 20 23:05:51.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 delete --grace-period=0 --force -f - --namespace=kubectl-5127'
Sep 20 23:05:51.518: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 23:05:51.518: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Sep 20 23:05:51.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 delete --grace-period=0 --force -f - --namespace=kubectl-5127'
Sep 20 23:05:51.585: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 23:05:51.585: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep 20 23:05:51.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 delete --grace-period=0 --force -f - --namespace=kubectl-5127'
Sep 20 23:05:51.679: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 23:05:51.679: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 20 23:05:51.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 delete --grace-period=0 --force -f - --namespace=kubectl-5127'
Sep 20 23:05:51.772: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 23:05:51.772: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 20 23:05:51.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 delete --grace-period=0 --force -f - --namespace=kubectl-5127'
Sep 20 23:05:51.833: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 23:05:51.833: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep 20 23:05:51.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 delete --grace-period=0 --force -f - --namespace=kubectl-5127'
Sep 20 23:05:51.891: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 23:05:51.891: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:05:51.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5127" for this suite.
Sep 20 23:06:19.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:06:19.997: INFO: namespace kubectl-5127 deletion completed in 28.102462207s

• [SLOW TEST:174.418 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:06:19.998: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 20 23:06:22.568: INFO: Successfully updated pod "pod-update-d89bd9de-9f22-4170-af91-a077412a7586"
STEP: verifying the updated pod is in kubernetes
Sep 20 23:06:22.580: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:06:22.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-649" for this suite.
Sep 20 23:06:50.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:06:50.673: INFO: namespace pods-649 deletion completed in 28.084905498s

• [SLOW TEST:30.676 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:06:50.673: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Sep 20 23:06:50.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 create -f - --namespace=kubectl-8233'
Sep 20 23:06:50.843: INFO: stderr: ""
Sep 20 23:06:50.843: INFO: stdout: "pod/pause created\n"
Sep 20 23:06:50.843: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep 20 23:06:50.843: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8233" to be "running and ready"
Sep 20 23:06:50.846: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.885065ms
Sep 20 23:06:52.868: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.025283398s
Sep 20 23:06:52.868: INFO: Pod "pause" satisfied condition "running and ready"
Sep 20 23:06:52.868: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Sep 20 23:06:52.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 label pods pause testing-label=testing-label-value --namespace=kubectl-8233'
Sep 20 23:06:52.932: INFO: stderr: ""
Sep 20 23:06:52.932: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep 20 23:06:52.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pod pause -L testing-label --namespace=kubectl-8233'
Sep 20 23:06:52.979: INFO: stderr: ""
Sep 20 23:06:52.979: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep 20 23:06:52.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 label pods pause testing-label- --namespace=kubectl-8233'
Sep 20 23:06:53.033: INFO: stderr: ""
Sep 20 23:06:53.033: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep 20 23:06:53.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pod pause -L testing-label --namespace=kubectl-8233'
Sep 20 23:06:53.080: INFO: stderr: ""
Sep 20 23:06:53.080: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Sep 20 23:06:53.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 delete --grace-period=0 --force -f - --namespace=kubectl-8233'
Sep 20 23:06:53.147: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 23:06:53.147: INFO: stdout: "pod \"pause\" force deleted\n"
Sep 20 23:06:53.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get rc,svc -l name=pause --no-headers --namespace=kubectl-8233'
Sep 20 23:06:53.209: INFO: stderr: "No resources found in kubectl-8233 namespace.\n"
Sep 20 23:06:53.209: INFO: stdout: ""
Sep 20 23:06:53.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods -l name=pause --namespace=kubectl-8233 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 20 23:06:53.266: INFO: stderr: ""
Sep 20 23:06:53.266: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:06:53.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8233" for this suite.
Sep 20 23:06:59.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:06:59.342: INFO: namespace kubectl-8233 deletion completed in 6.072875764s

• [SLOW TEST:8.668 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:06:59.342: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:07:06.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7530" for this suite.
Sep 20 23:07:12.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:07:12.485: INFO: namespace resourcequota-7530 deletion completed in 6.083313943s

• [SLOW TEST:13.143 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:07:12.485: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-92e618b7-413a-41d4-97f4-b844d27a20b8
STEP: Creating a pod to test consume configMaps
Sep 20 23:07:12.515: INFO: Waiting up to 5m0s for pod "pod-configmaps-3f2c24c1-8d6d-46e3-958a-ace5dc1ca3f0" in namespace "configmap-1095" to be "success or failure"
Sep 20 23:07:12.520: INFO: Pod "pod-configmaps-3f2c24c1-8d6d-46e3-958a-ace5dc1ca3f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.753809ms
Sep 20 23:07:14.522: INFO: Pod "pod-configmaps-3f2c24c1-8d6d-46e3-958a-ace5dc1ca3f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007068579s
STEP: Saw pod success
Sep 20 23:07:14.522: INFO: Pod "pod-configmaps-3f2c24c1-8d6d-46e3-958a-ace5dc1ca3f0" satisfied condition "success or failure"
Sep 20 23:07:14.524: INFO: Trying to get logs from node worker-2 pod pod-configmaps-3f2c24c1-8d6d-46e3-958a-ace5dc1ca3f0 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 23:07:14.543: INFO: Waiting for pod pod-configmaps-3f2c24c1-8d6d-46e3-958a-ace5dc1ca3f0 to disappear
Sep 20 23:07:14.545: INFO: Pod pod-configmaps-3f2c24c1-8d6d-46e3-958a-ace5dc1ca3f0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:07:14.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1095" for this suite.
Sep 20 23:07:20.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:07:20.617: INFO: namespace configmap-1095 deletion completed in 6.06818625s

• [SLOW TEST:8.132 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:07:20.619: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 23:07:20.644: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9431e10e-0577-49c7-8f9a-341cd8c998cc" in namespace "projected-2988" to be "success or failure"
Sep 20 23:07:20.654: INFO: Pod "downwardapi-volume-9431e10e-0577-49c7-8f9a-341cd8c998cc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.232117ms
Sep 20 23:07:22.664: INFO: Pod "downwardapi-volume-9431e10e-0577-49c7-8f9a-341cd8c998cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020230462s
STEP: Saw pod success
Sep 20 23:07:22.664: INFO: Pod "downwardapi-volume-9431e10e-0577-49c7-8f9a-341cd8c998cc" satisfied condition "success or failure"
Sep 20 23:07:22.666: INFO: Trying to get logs from node worker-2 pod downwardapi-volume-9431e10e-0577-49c7-8f9a-341cd8c998cc container client-container: <nil>
STEP: delete the pod
Sep 20 23:07:22.681: INFO: Waiting for pod downwardapi-volume-9431e10e-0577-49c7-8f9a-341cd8c998cc to disappear
Sep 20 23:07:22.683: INFO: Pod downwardapi-volume-9431e10e-0577-49c7-8f9a-341cd8c998cc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:07:22.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2988" for this suite.
Sep 20 23:07:28.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:07:28.771: INFO: namespace projected-2988 deletion completed in 6.0858969s

• [SLOW TEST:8.152 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:07:28.772: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 20 23:07:28.802: INFO: Waiting up to 5m0s for pod "pod-31cd7903-ebad-4941-8aeb-1f3e072cdf9e" in namespace "emptydir-8795" to be "success or failure"
Sep 20 23:07:28.805: INFO: Pod "pod-31cd7903-ebad-4941-8aeb-1f3e072cdf9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.858095ms
Sep 20 23:07:30.818: INFO: Pod "pod-31cd7903-ebad-4941-8aeb-1f3e072cdf9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01591394s
Sep 20 23:07:32.824: INFO: Pod "pod-31cd7903-ebad-4941-8aeb-1f3e072cdf9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02203684s
STEP: Saw pod success
Sep 20 23:07:32.824: INFO: Pod "pod-31cd7903-ebad-4941-8aeb-1f3e072cdf9e" satisfied condition "success or failure"
Sep 20 23:07:32.828: INFO: Trying to get logs from node worker-2 pod pod-31cd7903-ebad-4941-8aeb-1f3e072cdf9e container test-container: <nil>
STEP: delete the pod
Sep 20 23:07:32.878: INFO: Waiting for pod pod-31cd7903-ebad-4941-8aeb-1f3e072cdf9e to disappear
Sep 20 23:07:32.882: INFO: Pod pod-31cd7903-ebad-4941-8aeb-1f3e072cdf9e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:07:32.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8795" for this suite.
Sep 20 23:07:38.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:07:38.946: INFO: namespace emptydir-8795 deletion completed in 6.060977053s

• [SLOW TEST:10.174 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:07:38.947: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:07:50.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9664" for this suite.
Sep 20 23:07:56.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:07:56.167: INFO: namespace resourcequota-9664 deletion completed in 6.060563426s

• [SLOW TEST:17.220 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:07:56.167: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:07:56.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7862" for this suite.
Sep 20 23:08:24.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:08:24.295: INFO: namespace pods-7862 deletion completed in 28.097515349s

• [SLOW TEST:28.128 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:08:24.295: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-7aebeb38-01bc-484c-8383-2a37df1cd048
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-7aebeb38-01bc-484c-8383-2a37df1cd048
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:08:28.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8216" for this suite.
Sep 20 23:08:40.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:08:40.475: INFO: namespace configmap-8216 deletion completed in 12.073759731s

• [SLOW TEST:16.179 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:08:40.475: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0920 23:08:50.586272      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 20 23:08:50.586: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:08:50.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5062" for this suite.
Sep 20 23:08:56.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:08:56.683: INFO: namespace gc-5062 deletion completed in 6.090266941s

• [SLOW TEST:16.209 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:08:56.685: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Sep 20 23:09:00.737: INFO: Pod pod-hostip-8a256e2d-a56c-4624-b6f9-529e39fa9d46 has hostIP: 192.168.5.102
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:09:00.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4959" for this suite.
Sep 20 23:09:12.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:09:12.846: INFO: namespace pods-4959 deletion completed in 12.101820156s

• [SLOW TEST:16.162 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:09:12.848: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-2a0e4939-e082-4e2e-9f79-aec2772e2be7
STEP: Creating a pod to test consume secrets
Sep 20 23:09:12.879: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dfe2afe2-fb1c-48f2-9fd0-35776d7f9659" in namespace "projected-7816" to be "success or failure"
Sep 20 23:09:12.881: INFO: Pod "pod-projected-secrets-dfe2afe2-fb1c-48f2-9fd0-35776d7f9659": Phase="Pending", Reason="", readiness=false. Elapsed: 2.095843ms
Sep 20 23:09:14.883: INFO: Pod "pod-projected-secrets-dfe2afe2-fb1c-48f2-9fd0-35776d7f9659": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004335619s
Sep 20 23:09:16.890: INFO: Pod "pod-projected-secrets-dfe2afe2-fb1c-48f2-9fd0-35776d7f9659": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011138325s
STEP: Saw pod success
Sep 20 23:09:16.891: INFO: Pod "pod-projected-secrets-dfe2afe2-fb1c-48f2-9fd0-35776d7f9659" satisfied condition "success or failure"
Sep 20 23:09:16.897: INFO: Trying to get logs from node worker-2 pod pod-projected-secrets-dfe2afe2-fb1c-48f2-9fd0-35776d7f9659 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 20 23:09:16.939: INFO: Waiting for pod pod-projected-secrets-dfe2afe2-fb1c-48f2-9fd0-35776d7f9659 to disappear
Sep 20 23:09:16.941: INFO: Pod pod-projected-secrets-dfe2afe2-fb1c-48f2-9fd0-35776d7f9659 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:09:16.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7816" for this suite.
Sep 20 23:09:22.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:09:23.031: INFO: namespace projected-7816 deletion completed in 6.086522522s

• [SLOW TEST:10.183 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:09:23.032: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 23:09:23.131: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep 20 23:09:23.139: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 20 23:09:28.145: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 20 23:09:28.145: INFO: Creating deployment "test-rolling-update-deployment"
Sep 20 23:09:28.150: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep 20 23:09:28.161: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Sep 20 23:09:30.174: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep 20 23:09:30.179: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Sep 20 23:09:30.194: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5504 /apis/apps/v1/namespaces/deployment-5504/deployments/test-rolling-update-deployment 7d86842a-9802-4555-b4d0-4bdc402276e6 20566 1 2019-09-20 23:09:28 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc000578ba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-09-20 23:09:28 +0000 UTC,LastTransitionTime:2019-09-20 23:09:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2019-09-20 23:09:29 +0000 UTC,LastTransitionTime:2019-09-20 23:09:28 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 20 23:09:30.200: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-5504 /apis/apps/v1/namespaces/deployment-5504/replicasets/test-rolling-update-deployment-55d946486 acb84046-c27f-442c-8296-5af372da493b 20555 1 2019-09-20 23:09:28 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 7d86842a-9802-4555-b4d0-4bdc402276e6 0xc000579420 0xc000579421}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0005794b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 20 23:09:30.200: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep 20 23:09:30.200: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5504 /apis/apps/v1/namespaces/deployment-5504/replicasets/test-rolling-update-controller eea06367-f386-459e-8d37-66aea04e45bf 20565 2 2019-09-20 23:09:23 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 7d86842a-9802-4555-b4d0-4bdc402276e6 0xc000579317 0xc000579318}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0005793a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 20 23:09:30.206: INFO: Pod "test-rolling-update-deployment-55d946486-6hx6l" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-6hx6l test-rolling-update-deployment-55d946486- deployment-5504 /api/v1/namespaces/deployment-5504/pods/test-rolling-update-deployment-55d946486-6hx6l e6239788-703d-4ea8-86ec-17d81ca9adf3 20554 0 2019-09-20 23:09:28 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 acb84046-c27f-442c-8296-5af372da493b 0xc000579c50 0xc000579c51}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2cn7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2cn7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2cn7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 23:09:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 23:09:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 23:09:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 23:09:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.101,PodIP:10.32.0.4,StartTime:2019-09-20 23:09:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 23:09:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://254333f09f91e01bd29f375625f916fde3f8d74874d01d30e66e3cad668d0e3c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.0.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:09:30.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5504" for this suite.
Sep 20 23:09:36.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:09:36.346: INFO: namespace deployment-5504 deletion completed in 6.133174461s

• [SLOW TEST:13.314 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:09:36.347: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-72084b20-3123-45b5-951d-e27d23a33378
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:09:36.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5825" for this suite.
Sep 20 23:09:42.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:09:42.424: INFO: namespace configmap-5825 deletion completed in 6.05022693s

• [SLOW TEST:6.077 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:09:42.424: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Sep 20 23:09:42.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 create -f - --namespace=kubectl-6193'
Sep 20 23:09:43.037: INFO: stderr: ""
Sep 20 23:09:43.037: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 20 23:09:43.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6193'
Sep 20 23:09:43.114: INFO: stderr: ""
Sep 20 23:09:43.114: INFO: stdout: "update-demo-nautilus-6hfsp update-demo-nautilus-fkp5s "
Sep 20 23:09:43.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-nautilus-6hfsp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6193'
Sep 20 23:09:43.179: INFO: stderr: ""
Sep 20 23:09:43.179: INFO: stdout: ""
Sep 20 23:09:43.179: INFO: update-demo-nautilus-6hfsp is created but not running
Sep 20 23:09:48.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6193'
Sep 20 23:09:48.259: INFO: stderr: ""
Sep 20 23:09:48.259: INFO: stdout: "update-demo-nautilus-6hfsp update-demo-nautilus-fkp5s "
Sep 20 23:09:48.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-nautilus-6hfsp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6193'
Sep 20 23:09:48.328: INFO: stderr: ""
Sep 20 23:09:48.328: INFO: stdout: "true"
Sep 20 23:09:48.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-nautilus-6hfsp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6193'
Sep 20 23:09:48.376: INFO: stderr: ""
Sep 20 23:09:48.376: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 20 23:09:48.376: INFO: validating pod update-demo-nautilus-6hfsp
Sep 20 23:09:48.386: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 23:09:48.386: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 23:09:48.386: INFO: update-demo-nautilus-6hfsp is verified up and running
Sep 20 23:09:48.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-nautilus-fkp5s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6193'
Sep 20 23:09:48.437: INFO: stderr: ""
Sep 20 23:09:48.437: INFO: stdout: "true"
Sep 20 23:09:48.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-nautilus-fkp5s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6193'
Sep 20 23:09:48.490: INFO: stderr: ""
Sep 20 23:09:48.490: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 20 23:09:48.490: INFO: validating pod update-demo-nautilus-fkp5s
Sep 20 23:09:48.494: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 23:09:48.494: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 23:09:48.494: INFO: update-demo-nautilus-fkp5s is verified up and running
STEP: rolling-update to new replication controller
Sep 20 23:09:48.495: INFO: scanned /root for discovery docs: <nil>
Sep 20 23:09:48.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-6193'
Sep 20 23:10:10.946: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep 20 23:10:10.946: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 20 23:10:10.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6193'
Sep 20 23:10:10.995: INFO: stderr: ""
Sep 20 23:10:10.995: INFO: stdout: "update-demo-kitten-2r9hc update-demo-kitten-pk699 update-demo-nautilus-6hfsp "
STEP: Replicas for name=update-demo: expected=2 actual=3
Sep 20 23:10:15.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6193'
Sep 20 23:10:16.081: INFO: stderr: ""
Sep 20 23:10:16.081: INFO: stdout: "update-demo-kitten-2r9hc update-demo-kitten-pk699 "
Sep 20 23:10:16.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-kitten-2r9hc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6193'
Sep 20 23:10:16.146: INFO: stderr: ""
Sep 20 23:10:16.146: INFO: stdout: "true"
Sep 20 23:10:16.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-kitten-2r9hc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6193'
Sep 20 23:10:16.193: INFO: stderr: ""
Sep 20 23:10:16.193: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep 20 23:10:16.193: INFO: validating pod update-demo-kitten-2r9hc
Sep 20 23:10:16.196: INFO: got data: {
  "image": "kitten.jpg"
}

Sep 20 23:10:16.196: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep 20 23:10:16.196: INFO: update-demo-kitten-2r9hc is verified up and running
Sep 20 23:10:16.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-kitten-pk699 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6193'
Sep 20 23:10:16.252: INFO: stderr: ""
Sep 20 23:10:16.253: INFO: stdout: "true"
Sep 20 23:10:16.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods update-demo-kitten-pk699 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6193'
Sep 20 23:10:16.307: INFO: stderr: ""
Sep 20 23:10:16.307: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep 20 23:10:16.307: INFO: validating pod update-demo-kitten-pk699
Sep 20 23:10:16.311: INFO: got data: {
  "image": "kitten.jpg"
}

Sep 20 23:10:16.311: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep 20 23:10:16.311: INFO: update-demo-kitten-pk699 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:10:16.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6193" for this suite.
Sep 20 23:10:28.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:10:28.407: INFO: namespace kubectl-6193 deletion completed in 12.094098677s

• [SLOW TEST:45.983 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:10:28.409: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 23:10:28.431: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Creating first CR 
Sep 20 23:10:28.493: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-09-20T23:10:28Z generation:1 name:name1 resourceVersion:20833 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:0cf94dcb-fd20-4a8a-9bb9-f0c1eb27d013] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Sep 20 23:10:38.501: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-09-20T23:10:38Z generation:1 name:name2 resourceVersion:20848 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:21496252-4f9c-4724-a53f-56d95a3f5bc7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Sep 20 23:10:48.512: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-09-20T23:10:28Z generation:2 name:name1 resourceVersion:20863 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:0cf94dcb-fd20-4a8a-9bb9-f0c1eb27d013] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Sep 20 23:10:58.520: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-09-20T23:10:38Z generation:2 name:name2 resourceVersion:20879 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:21496252-4f9c-4724-a53f-56d95a3f5bc7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Sep 20 23:11:08.533: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-09-20T23:10:28Z generation:2 name:name1 resourceVersion:20894 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:0cf94dcb-fd20-4a8a-9bb9-f0c1eb27d013] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Sep 20 23:11:18.550: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-09-20T23:10:38Z generation:2 name:name2 resourceVersion:20909 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:21496252-4f9c-4724-a53f-56d95a3f5bc7] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:11:29.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-8736" for this suite.
Sep 20 23:11:35.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:11:35.151: INFO: namespace crd-watch-8736 deletion completed in 6.076552467s

• [SLOW TEST:66.742 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:11:35.151: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 20 23:11:35.176: INFO: Waiting up to 5m0s for pod "pod-7b31db9f-db6d-4049-8242-9b6895dda2bf" in namespace "emptydir-9292" to be "success or failure"
Sep 20 23:11:35.181: INFO: Pod "pod-7b31db9f-db6d-4049-8242-9b6895dda2bf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.619753ms
Sep 20 23:11:37.193: INFO: Pod "pod-7b31db9f-db6d-4049-8242-9b6895dda2bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01786655s
STEP: Saw pod success
Sep 20 23:11:37.194: INFO: Pod "pod-7b31db9f-db6d-4049-8242-9b6895dda2bf" satisfied condition "success or failure"
Sep 20 23:11:37.199: INFO: Trying to get logs from node worker-2 pod pod-7b31db9f-db6d-4049-8242-9b6895dda2bf container test-container: <nil>
STEP: delete the pod
Sep 20 23:11:37.247: INFO: Waiting for pod pod-7b31db9f-db6d-4049-8242-9b6895dda2bf to disappear
Sep 20 23:11:37.250: INFO: Pod pod-7b31db9f-db6d-4049-8242-9b6895dda2bf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:11:37.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9292" for this suite.
Sep 20 23:11:43.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:11:43.343: INFO: namespace emptydir-9292 deletion completed in 6.090618705s

• [SLOW TEST:8.192 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:11:43.344: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 23:11:43.842: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 23:11:45.864: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704617903, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704617903, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704617903, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704617903, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 23:11:48.883: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:11:48.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1124" for this suite.
Sep 20 23:11:54.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:11:55.049: INFO: namespace webhook-1124 deletion completed in 6.090975685s
STEP: Destroying namespace "webhook-1124-markers" for this suite.
Sep 20 23:12:01.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:12:01.153: INFO: namespace webhook-1124-markers deletion completed in 6.104260579s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.815 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:12:01.159: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Sep 20 23:12:01.182: INFO: Waiting up to 5m0s for pod "downward-api-c4157f1f-831b-4e18-acaf-b72b45e9cfc3" in namespace "downward-api-8800" to be "success or failure"
Sep 20 23:12:01.184: INFO: Pod "downward-api-c4157f1f-831b-4e18-acaf-b72b45e9cfc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.283727ms
Sep 20 23:12:03.201: INFO: Pod "downward-api-c4157f1f-831b-4e18-acaf-b72b45e9cfc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018714524s
STEP: Saw pod success
Sep 20 23:12:03.201: INFO: Pod "downward-api-c4157f1f-831b-4e18-acaf-b72b45e9cfc3" satisfied condition "success or failure"
Sep 20 23:12:03.205: INFO: Trying to get logs from node worker-2 pod downward-api-c4157f1f-831b-4e18-acaf-b72b45e9cfc3 container dapi-container: <nil>
STEP: delete the pod
Sep 20 23:12:03.241: INFO: Waiting for pod downward-api-c4157f1f-831b-4e18-acaf-b72b45e9cfc3 to disappear
Sep 20 23:12:03.246: INFO: Pod downward-api-c4157f1f-831b-4e18-acaf-b72b45e9cfc3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:12:03.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8800" for this suite.
Sep 20 23:12:09.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:12:09.328: INFO: namespace downward-api-8800 deletion completed in 6.077945427s

• [SLOW TEST:8.168 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:12:09.328: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 23:12:09.353: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Sep 20 23:12:10.371: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:12:10.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-165" for this suite.
Sep 20 23:12:16.391: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:12:16.461: INFO: namespace replication-controller-165 deletion completed in 6.083136006s

• [SLOW TEST:7.133 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:12:16.461: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 23:12:17.090: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 23:12:19.107: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704617937, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704617937, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704617937, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704617937, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 23:12:22.149: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:12:22.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1380" for this suite.
Sep 20 23:12:28.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:12:28.353: INFO: namespace webhook-1380 deletion completed in 6.093142275s
STEP: Destroying namespace "webhook-1380-markers" for this suite.
Sep 20 23:12:34.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:12:34.465: INFO: namespace webhook-1380-markers deletion completed in 6.111973526s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.014 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:12:34.477: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 20 23:12:34.505: INFO: Waiting up to 5m0s for pod "pod-ea56b053-5db8-4d4d-aa76-02463bb01719" in namespace "emptydir-6777" to be "success or failure"
Sep 20 23:12:34.510: INFO: Pod "pod-ea56b053-5db8-4d4d-aa76-02463bb01719": Phase="Pending", Reason="", readiness=false. Elapsed: 5.261837ms
Sep 20 23:12:36.524: INFO: Pod "pod-ea56b053-5db8-4d4d-aa76-02463bb01719": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018488758s
Sep 20 23:12:38.530: INFO: Pod "pod-ea56b053-5db8-4d4d-aa76-02463bb01719": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024701659s
STEP: Saw pod success
Sep 20 23:12:38.530: INFO: Pod "pod-ea56b053-5db8-4d4d-aa76-02463bb01719" satisfied condition "success or failure"
Sep 20 23:12:38.535: INFO: Trying to get logs from node worker-2 pod pod-ea56b053-5db8-4d4d-aa76-02463bb01719 container test-container: <nil>
STEP: delete the pod
Sep 20 23:12:38.582: INFO: Waiting for pod pod-ea56b053-5db8-4d4d-aa76-02463bb01719 to disappear
Sep 20 23:12:38.586: INFO: Pod pod-ea56b053-5db8-4d4d-aa76-02463bb01719 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:12:38.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6777" for this suite.
Sep 20 23:12:44.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:12:44.691: INFO: namespace emptydir-6777 deletion completed in 6.102674387s

• [SLOW TEST:10.214 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:12:44.692: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 20 23:12:46.770: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:12:46.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7794" for this suite.
Sep 20 23:12:52.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:12:52.870: INFO: namespace container-runtime-7794 deletion completed in 6.060756851s

• [SLOW TEST:8.178 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:12:52.870: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-2ad7c638-e9de-4e43-bac7-3250ec70b830
STEP: Creating a pod to test consume configMaps
Sep 20 23:12:52.898: INFO: Waiting up to 5m0s for pod "pod-configmaps-d9fdd661-f2e3-42c4-b7bb-22e88e9c0f6f" in namespace "configmap-3505" to be "success or failure"
Sep 20 23:12:52.905: INFO: Pod "pod-configmaps-d9fdd661-f2e3-42c4-b7bb-22e88e9c0f6f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.361138ms
Sep 20 23:12:54.914: INFO: Pod "pod-configmaps-d9fdd661-f2e3-42c4-b7bb-22e88e9c0f6f": Phase="Running", Reason="", readiness=true. Elapsed: 2.01640764s
Sep 20 23:12:56.921: INFO: Pod "pod-configmaps-d9fdd661-f2e3-42c4-b7bb-22e88e9c0f6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023085029s
STEP: Saw pod success
Sep 20 23:12:56.921: INFO: Pod "pod-configmaps-d9fdd661-f2e3-42c4-b7bb-22e88e9c0f6f" satisfied condition "success or failure"
Sep 20 23:12:56.924: INFO: Trying to get logs from node worker-2 pod pod-configmaps-d9fdd661-f2e3-42c4-b7bb-22e88e9c0f6f container configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 23:12:56.938: INFO: Waiting for pod pod-configmaps-d9fdd661-f2e3-42c4-b7bb-22e88e9c0f6f to disappear
Sep 20 23:12:56.941: INFO: Pod pod-configmaps-d9fdd661-f2e3-42c4-b7bb-22e88e9c0f6f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:12:56.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3505" for this suite.
Sep 20 23:13:02.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:13:03.017: INFO: namespace configmap-3505 deletion completed in 6.074334328s

• [SLOW TEST:10.147 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:13:03.017: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Sep 20 23:13:03.049: INFO: Waiting up to 5m0s for pod "var-expansion-059d6e09-a10f-41d1-a28d-78daefdaa8ea" in namespace "var-expansion-9855" to be "success or failure"
Sep 20 23:13:03.051: INFO: Pod "var-expansion-059d6e09-a10f-41d1-a28d-78daefdaa8ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.423132ms
Sep 20 23:13:05.060: INFO: Pod "var-expansion-059d6e09-a10f-41d1-a28d-78daefdaa8ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011361026s
STEP: Saw pod success
Sep 20 23:13:05.060: INFO: Pod "var-expansion-059d6e09-a10f-41d1-a28d-78daefdaa8ea" satisfied condition "success or failure"
Sep 20 23:13:05.065: INFO: Trying to get logs from node worker-2 pod var-expansion-059d6e09-a10f-41d1-a28d-78daefdaa8ea container dapi-container: <nil>
STEP: delete the pod
Sep 20 23:13:05.110: INFO: Waiting for pod var-expansion-059d6e09-a10f-41d1-a28d-78daefdaa8ea to disappear
Sep 20 23:13:05.117: INFO: Pod var-expansion-059d6e09-a10f-41d1-a28d-78daefdaa8ea no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:13:05.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9855" for this suite.
Sep 20 23:13:11.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:13:11.172: INFO: namespace var-expansion-9855 deletion completed in 6.050647262s

• [SLOW TEST:8.154 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:13:11.172: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0920 23:13:51.249100      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 20 23:13:51.249: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:13:51.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5435" for this suite.
Sep 20 23:13:57.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:13:57.327: INFO: namespace gc-5435 deletion completed in 6.071304436s

• [SLOW TEST:46.155 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:13:57.327: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Sep 20 23:13:57.363: INFO: Waiting up to 5m0s for pod "var-expansion-a44af1ba-2632-4260-9e38-d86fa96d8ed5" in namespace "var-expansion-6425" to be "success or failure"
Sep 20 23:13:57.366: INFO: Pod "var-expansion-a44af1ba-2632-4260-9e38-d86fa96d8ed5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.872202ms
Sep 20 23:13:59.376: INFO: Pod "var-expansion-a44af1ba-2632-4260-9e38-d86fa96d8ed5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013229459s
STEP: Saw pod success
Sep 20 23:13:59.376: INFO: Pod "var-expansion-a44af1ba-2632-4260-9e38-d86fa96d8ed5" satisfied condition "success or failure"
Sep 20 23:13:59.381: INFO: Trying to get logs from node worker-2 pod var-expansion-a44af1ba-2632-4260-9e38-d86fa96d8ed5 container dapi-container: <nil>
STEP: delete the pod
Sep 20 23:13:59.424: INFO: Waiting for pod var-expansion-a44af1ba-2632-4260-9e38-d86fa96d8ed5 to disappear
Sep 20 23:13:59.429: INFO: Pod var-expansion-a44af1ba-2632-4260-9e38-d86fa96d8ed5 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:13:59.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6425" for this suite.
Sep 20 23:14:05.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:14:05.487: INFO: namespace var-expansion-6425 deletion completed in 6.056345502s

• [SLOW TEST:8.160 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:14:05.488: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Sep 20 23:14:15.550: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0920 23:14:15.550909      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 20 23:14:15.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6612" for this suite.
Sep 20 23:14:21.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:14:21.646: INFO: namespace gc-6612 deletion completed in 6.089744872s

• [SLOW TEST:16.158 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:14:21.647: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-aa49454e-923f-4f5c-a9ac-063867cfbd5b
STEP: Creating a pod to test consume secrets
Sep 20 23:14:21.683: INFO: Waiting up to 5m0s for pod "pod-secrets-26eee7b5-9c24-484b-8fec-58777da33e40" in namespace "secrets-7134" to be "success or failure"
Sep 20 23:14:21.686: INFO: Pod "pod-secrets-26eee7b5-9c24-484b-8fec-58777da33e40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.68543ms
Sep 20 23:14:23.692: INFO: Pod "pod-secrets-26eee7b5-9c24-484b-8fec-58777da33e40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009042996s
Sep 20 23:14:25.697: INFO: Pod "pod-secrets-26eee7b5-9c24-484b-8fec-58777da33e40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014000102s
STEP: Saw pod success
Sep 20 23:14:25.697: INFO: Pod "pod-secrets-26eee7b5-9c24-484b-8fec-58777da33e40" satisfied condition "success or failure"
Sep 20 23:14:25.702: INFO: Trying to get logs from node worker-2 pod pod-secrets-26eee7b5-9c24-484b-8fec-58777da33e40 container secret-volume-test: <nil>
STEP: delete the pod
Sep 20 23:14:25.741: INFO: Waiting for pod pod-secrets-26eee7b5-9c24-484b-8fec-58777da33e40 to disappear
Sep 20 23:14:25.749: INFO: Pod pod-secrets-26eee7b5-9c24-484b-8fec-58777da33e40 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:14:25.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7134" for this suite.
Sep 20 23:14:31.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:14:31.805: INFO: namespace secrets-7134 deletion completed in 6.050359994s

• [SLOW TEST:10.158 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:14:31.805: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:14:47.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1561" for this suite.
Sep 20 23:14:53.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:14:53.967: INFO: namespace resourcequota-1561 deletion completed in 6.089577784s

• [SLOW TEST:22.163 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:14:53.969: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 23:14:54.000: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep 20 23:14:59.008: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 20 23:14:59.008: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep 20 23:15:01.014: INFO: Creating deployment "test-rollover-deployment"
Sep 20 23:15:01.027: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep 20 23:15:03.052: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep 20 23:15:03.062: INFO: Ensure that both replica sets have 1 created replica
Sep 20 23:15:03.072: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep 20 23:15:03.084: INFO: Updating deployment test-rollover-deployment
Sep 20 23:15:03.085: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep 20 23:15:05.096: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep 20 23:15:05.107: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep 20 23:15:05.114: INFO: all replica sets need to contain the pod-template-hash label
Sep 20 23:15:05.114: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618101, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618101, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618103, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618101, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 23:15:07.119: INFO: all replica sets need to contain the pod-template-hash label
Sep 20 23:15:07.119: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618101, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618101, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618105, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618101, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 23:15:09.127: INFO: all replica sets need to contain the pod-template-hash label
Sep 20 23:15:09.128: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618101, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618101, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618105, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618101, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 23:15:11.127: INFO: all replica sets need to contain the pod-template-hash label
Sep 20 23:15:11.127: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618101, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618101, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618105, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618101, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 23:15:13.129: INFO: all replica sets need to contain the pod-template-hash label
Sep 20 23:15:13.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618101, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618101, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618105, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618101, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 23:15:15.128: INFO: all replica sets need to contain the pod-template-hash label
Sep 20 23:15:15.128: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618101, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618101, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618105, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618101, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 23:15:17.120: INFO: 
Sep 20 23:15:17.120: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Sep 20 23:15:17.125: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6984 /apis/apps/v1/namespaces/deployment-6984/deployments/test-rollover-deployment 129a3946-8923-4eee-a953-da380f653277 22000 2 2019-09-20 23:15:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0050151f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-09-20 23:15:01 +0000 UTC,LastTransitionTime:2019-09-20 23:15:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2019-09-20 23:15:15 +0000 UTC,LastTransitionTime:2019-09-20 23:15:01 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 20 23:15:17.127: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-6984 /apis/apps/v1/namespaces/deployment-6984/replicasets/test-rollover-deployment-7d7dc6548c 7a027733-8622-4534-9c2f-053f6103f37f 21989 2 2019-09-20 23:15:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 129a3946-8923-4eee-a953-da380f653277 0xc0050156f7 0xc0050156f8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005015758 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 20 23:15:17.127: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep 20 23:15:17.128: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6984 /apis/apps/v1/namespaces/deployment-6984/replicasets/test-rollover-controller 4c32af41-ca1c-4261-a0ff-5a09e65c9c95 21999 2 2019-09-20 23:14:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 129a3946-8923-4eee-a953-da380f653277 0xc005015617 0xc005015618}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005015678 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 20 23:15:17.128: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-6984 /apis/apps/v1/namespaces/deployment-6984/replicasets/test-rollover-deployment-f6c94f66c 2f6475dc-23c7-49dc-a227-fa28a2068d0b 21961 2 2019-09-20 23:15:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 129a3946-8923-4eee-a953-da380f653277 0xc0050157c0 0xc0050157c1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005015838 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 20 23:15:17.130: INFO: Pod "test-rollover-deployment-7d7dc6548c-rsgcd" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-rsgcd test-rollover-deployment-7d7dc6548c- deployment-6984 /api/v1/namespaces/deployment-6984/pods/test-rollover-deployment-7d7dc6548c-rsgcd 9ecb366d-6322-4544-bb32-99dc980875c0 21972 0 2019-09-20 23:15:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c 7a027733-8622-4534-9c2f-053f6103f37f 0xc001e95e87 0xc001e95e88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dp7tv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dp7tv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dp7tv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 23:15:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 23:15:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 23:15:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 23:15:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.101,PodIP:10.32.0.4,StartTime:2019-09-20 23:15:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 23:15:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://2008f6ce6ade2764e4db96738bb19cadedee8190cde69624e18155045628171b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.0.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:15:17.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6984" for this suite.
Sep 20 23:15:23.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:15:23.193: INFO: namespace deployment-6984 deletion completed in 6.061091515s

• [SLOW TEST:29.224 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:15:23.193: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 23:15:23.219: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f9371872-55d5-42f8-aaef-f9c40b2bf7d4" in namespace "projected-2247" to be "success or failure"
Sep 20 23:15:23.222: INFO: Pod "downwardapi-volume-f9371872-55d5-42f8-aaef-f9c40b2bf7d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.864438ms
Sep 20 23:15:25.227: INFO: Pod "downwardapi-volume-f9371872-55d5-42f8-aaef-f9c40b2bf7d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007196528s
STEP: Saw pod success
Sep 20 23:15:25.227: INFO: Pod "downwardapi-volume-f9371872-55d5-42f8-aaef-f9c40b2bf7d4" satisfied condition "success or failure"
Sep 20 23:15:25.228: INFO: Trying to get logs from node worker-2 pod downwardapi-volume-f9371872-55d5-42f8-aaef-f9c40b2bf7d4 container client-container: <nil>
STEP: delete the pod
Sep 20 23:15:25.243: INFO: Waiting for pod downwardapi-volume-f9371872-55d5-42f8-aaef-f9c40b2bf7d4 to disappear
Sep 20 23:15:25.245: INFO: Pod downwardapi-volume-f9371872-55d5-42f8-aaef-f9c40b2bf7d4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:15:25.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2247" for this suite.
Sep 20 23:15:31.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:15:31.325: INFO: namespace projected-2247 deletion completed in 6.077313625s

• [SLOW TEST:8.131 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:15:31.325: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Sep 20 23:15:33.934: INFO: Successfully updated pod "annotationupdatee6ea5c52-1332-4b02-b214-51f166362195"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:15:35.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-771" for this suite.
Sep 20 23:15:48.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:15:48.080: INFO: namespace downward-api-771 deletion completed in 12.106658419s

• [SLOW TEST:16.755 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:15:48.083: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Sep 20 23:15:48.157: INFO: Waiting up to 5m0s for pod "downward-api-0eb3f4a8-0183-49f7-af6c-aeb2c1135726" in namespace "downward-api-4234" to be "success or failure"
Sep 20 23:15:48.159: INFO: Pod "downward-api-0eb3f4a8-0183-49f7-af6c-aeb2c1135726": Phase="Pending", Reason="", readiness=false. Elapsed: 2.094219ms
Sep 20 23:15:50.162: INFO: Pod "downward-api-0eb3f4a8-0183-49f7-af6c-aeb2c1135726": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004778s
Sep 20 23:15:52.169: INFO: Pod "downward-api-0eb3f4a8-0183-49f7-af6c-aeb2c1135726": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01124674s
STEP: Saw pod success
Sep 20 23:15:52.169: INFO: Pod "downward-api-0eb3f4a8-0183-49f7-af6c-aeb2c1135726" satisfied condition "success or failure"
Sep 20 23:15:52.170: INFO: Trying to get logs from node worker-2 pod downward-api-0eb3f4a8-0183-49f7-af6c-aeb2c1135726 container dapi-container: <nil>
STEP: delete the pod
Sep 20 23:15:52.191: INFO: Waiting for pod downward-api-0eb3f4a8-0183-49f7-af6c-aeb2c1135726 to disappear
Sep 20 23:15:52.193: INFO: Pod downward-api-0eb3f4a8-0183-49f7-af6c-aeb2c1135726 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:15:52.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4234" for this suite.
Sep 20 23:15:58.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:15:58.271: INFO: namespace downward-api-4234 deletion completed in 6.074850117s

• [SLOW TEST:10.189 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:15:58.272: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 20 23:16:00.815: INFO: Successfully updated pod "pod-update-activedeadlineseconds-766280bb-2bf5-440a-81cb-2bd130b16d3b"
Sep 20 23:16:00.815: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-766280bb-2bf5-440a-81cb-2bd130b16d3b" in namespace "pods-2230" to be "terminated due to deadline exceeded"
Sep 20 23:16:00.820: INFO: Pod "pod-update-activedeadlineseconds-766280bb-2bf5-440a-81cb-2bd130b16d3b": Phase="Running", Reason="", readiness=true. Elapsed: 5.078558ms
Sep 20 23:16:02.829: INFO: Pod "pod-update-activedeadlineseconds-766280bb-2bf5-440a-81cb-2bd130b16d3b": Phase="Running", Reason="", readiness=true. Elapsed: 2.014225073s
Sep 20 23:16:04.870: INFO: Pod "pod-update-activedeadlineseconds-766280bb-2bf5-440a-81cb-2bd130b16d3b": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.055249886s
Sep 20 23:16:04.870: INFO: Pod "pod-update-activedeadlineseconds-766280bb-2bf5-440a-81cb-2bd130b16d3b" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:16:04.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2230" for this suite.
Sep 20 23:16:10.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:16:10.968: INFO: namespace pods-2230 deletion completed in 6.092105635s

• [SLOW TEST:12.696 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:16:10.969: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-1198
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 20 23:16:10.993: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 20 23:16:33.140: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.40.0.4:8080/dial?request=hostName&protocol=http&host=10.32.0.4&port=8080&tries=1'] Namespace:pod-network-test-1198 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 23:16:33.140: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 23:16:33.276: INFO: Waiting for endpoints: map[]
Sep 20 23:16:33.278: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.40.0.4:8080/dial?request=hostName&protocol=http&host=10.38.0.1&port=8080&tries=1'] Namespace:pod-network-test-1198 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 23:16:33.278: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 23:16:33.378: INFO: Waiting for endpoints: map[]
Sep 20 23:16:33.379: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.40.0.4:8080/dial?request=hostName&protocol=http&host=10.40.0.3&port=8080&tries=1'] Namespace:pod-network-test-1198 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 23:16:33.379: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 23:16:33.468: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:16:33.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1198" for this suite.
Sep 20 23:16:45.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:16:45.546: INFO: namespace pod-network-test-1198 deletion completed in 12.07580765s

• [SLOW TEST:34.577 seconds]
[sig-network] Networking
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:16:45.546: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-jn2j
STEP: Creating a pod to test atomic-volume-subpath
Sep 20 23:16:45.576: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jn2j" in namespace "subpath-1008" to be "success or failure"
Sep 20 23:16:45.580: INFO: Pod "pod-subpath-test-configmap-jn2j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.086708ms
Sep 20 23:16:47.590: INFO: Pod "pod-subpath-test-configmap-jn2j": Phase="Running", Reason="", readiness=true. Elapsed: 2.014266994s
Sep 20 23:16:49.599: INFO: Pod "pod-subpath-test-configmap-jn2j": Phase="Running", Reason="", readiness=true. Elapsed: 4.023196517s
Sep 20 23:16:51.610: INFO: Pod "pod-subpath-test-configmap-jn2j": Phase="Running", Reason="", readiness=true. Elapsed: 6.03448516s
Sep 20 23:16:53.616: INFO: Pod "pod-subpath-test-configmap-jn2j": Phase="Running", Reason="", readiness=true. Elapsed: 8.040475901s
Sep 20 23:16:55.619: INFO: Pod "pod-subpath-test-configmap-jn2j": Phase="Running", Reason="", readiness=true. Elapsed: 10.04322649s
Sep 20 23:16:57.630: INFO: Pod "pod-subpath-test-configmap-jn2j": Phase="Running", Reason="", readiness=true. Elapsed: 12.054269974s
Sep 20 23:16:59.632: INFO: Pod "pod-subpath-test-configmap-jn2j": Phase="Running", Reason="", readiness=true. Elapsed: 14.056247474s
Sep 20 23:17:01.648: INFO: Pod "pod-subpath-test-configmap-jn2j": Phase="Running", Reason="", readiness=true. Elapsed: 16.072612695s
Sep 20 23:17:03.655: INFO: Pod "pod-subpath-test-configmap-jn2j": Phase="Running", Reason="", readiness=true. Elapsed: 18.079274062s
Sep 20 23:17:05.657: INFO: Pod "pod-subpath-test-configmap-jn2j": Phase="Running", Reason="", readiness=true. Elapsed: 20.081718081s
Sep 20 23:17:07.662: INFO: Pod "pod-subpath-test-configmap-jn2j": Phase="Running", Reason="", readiness=true. Elapsed: 22.086872969s
Sep 20 23:17:09.670: INFO: Pod "pod-subpath-test-configmap-jn2j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.094029487s
STEP: Saw pod success
Sep 20 23:17:09.670: INFO: Pod "pod-subpath-test-configmap-jn2j" satisfied condition "success or failure"
Sep 20 23:17:09.675: INFO: Trying to get logs from node worker-2 pod pod-subpath-test-configmap-jn2j container test-container-subpath-configmap-jn2j: <nil>
STEP: delete the pod
Sep 20 23:17:09.710: INFO: Waiting for pod pod-subpath-test-configmap-jn2j to disappear
Sep 20 23:17:09.713: INFO: Pod pod-subpath-test-configmap-jn2j no longer exists
STEP: Deleting pod pod-subpath-test-configmap-jn2j
Sep 20 23:17:09.713: INFO: Deleting pod "pod-subpath-test-configmap-jn2j" in namespace "subpath-1008"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:17:09.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1008" for this suite.
Sep 20 23:17:15.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:17:15.820: INFO: namespace subpath-1008 deletion completed in 6.103040721s

• [SLOW TEST:30.274 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:17:15.821: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Sep 20 23:17:15.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 --namespace=kubectl-3284 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Sep 20 23:17:18.373: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Sep 20 23:17:18.373: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:17:20.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3284" for this suite.
Sep 20 23:17:26.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:17:26.497: INFO: namespace kubectl-3284 deletion completed in 6.098548406s

• [SLOW TEST:10.677 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:17:26.498: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-11d34fa5-50da-4300-8df1-59aa94c7732f
STEP: Creating a pod to test consume configMaps
Sep 20 23:17:26.525: INFO: Waiting up to 5m0s for pod "pod-configmaps-4cd94092-4a60-444f-bbf5-0fc5be2b8dfe" in namespace "configmap-3229" to be "success or failure"
Sep 20 23:17:26.527: INFO: Pod "pod-configmaps-4cd94092-4a60-444f-bbf5-0fc5be2b8dfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1.858657ms
Sep 20 23:17:28.540: INFO: Pod "pod-configmaps-4cd94092-4a60-444f-bbf5-0fc5be2b8dfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015269595s
STEP: Saw pod success
Sep 20 23:17:28.540: INFO: Pod "pod-configmaps-4cd94092-4a60-444f-bbf5-0fc5be2b8dfe" satisfied condition "success or failure"
Sep 20 23:17:28.547: INFO: Trying to get logs from node worker-2 pod pod-configmaps-4cd94092-4a60-444f-bbf5-0fc5be2b8dfe container configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 23:17:28.576: INFO: Waiting for pod pod-configmaps-4cd94092-4a60-444f-bbf5-0fc5be2b8dfe to disappear
Sep 20 23:17:28.578: INFO: Pod pod-configmaps-4cd94092-4a60-444f-bbf5-0fc5be2b8dfe no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:17:28.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3229" for this suite.
Sep 20 23:17:34.590: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:17:34.646: INFO: namespace configmap-3229 deletion completed in 6.064078745s

• [SLOW TEST:8.147 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:17:34.646: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 23:17:34.902: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 23:17:36.920: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618254, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618254, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618254, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618254, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 23:17:39.932: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 23:17:39.938: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4764-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:17:41.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2028" for this suite.
Sep 20 23:17:47.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:17:47.134: INFO: namespace webhook-2028 deletion completed in 6.087645943s
STEP: Destroying namespace "webhook-2028-markers" for this suite.
Sep 20 23:17:53.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:17:53.190: INFO: namespace webhook-2028-markers deletion completed in 6.056035691s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.552 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:17:53.198: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Sep 20 23:17:53.221: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 20 23:17:53.227: INFO: Waiting for terminating namespaces to be deleted...
Sep 20 23:17:53.228: INFO: 
Logging pods the kubelet thinks is on node controlplane-1 before test
Sep 20 23:17:53.237: INFO: etcd-controlplane-1 from kube-system started at 2019-09-20 21:29:15 +0000 UTC (1 container statuses recorded)
Sep 20 23:17:53.237: INFO: 	Container etcd ready: true, restart count 0
Sep 20 23:17:53.237: INFO: kube-proxy-62wrx from kube-system started at 2019-09-20 21:29:38 +0000 UTC (1 container statuses recorded)
Sep 20 23:17:53.237: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 23:17:53.237: INFO: kube-controller-manager-controlplane-1 from kube-system started at 2019-09-20 21:29:15 +0000 UTC (1 container statuses recorded)
Sep 20 23:17:53.237: INFO: 	Container kube-controller-manager ready: true, restart count 0
Sep 20 23:17:53.237: INFO: kube-scheduler-controlplane-1 from kube-system started at 2019-09-20 21:29:15 +0000 UTC (1 container statuses recorded)
Sep 20 23:17:53.237: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep 20 23:17:53.237: INFO: sonobuoy-systemd-logs-daemon-set-63bbc52511204dd7-g8gl8 from sonobuoy started at 2019-09-20 21:43:06 +0000 UTC (2 container statuses recorded)
Sep 20 23:17:53.237: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 20 23:17:53.237: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 20 23:17:53.237: INFO: kube-apiserver-controlplane-1 from kube-system started at 2019-09-20 21:29:15 +0000 UTC (1 container statuses recorded)
Sep 20 23:17:53.237: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep 20 23:17:53.237: INFO: weave-net-n52f9 from kube-system started at 2019-09-20 21:41:30 +0000 UTC (2 container statuses recorded)
Sep 20 23:17:53.237: INFO: 	Container weave ready: true, restart count 0
Sep 20 23:17:53.237: INFO: 	Container weave-npc ready: true, restart count 0
Sep 20 23:17:53.237: INFO: 
Logging pods the kubelet thinks is on node worker-1 before test
Sep 20 23:17:53.247: INFO: kube-proxy-dxbgr from kube-system started at 2019-09-20 21:29:43 +0000 UTC (1 container statuses recorded)
Sep 20 23:17:53.247: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 23:17:53.247: INFO: coredns-5644d7b6d9-pvh6f from kube-system started at 2019-09-20 21:42:42 +0000 UTC (1 container statuses recorded)
Sep 20 23:17:53.247: INFO: 	Container coredns ready: true, restart count 0
Sep 20 23:17:53.247: INFO: sonobuoy-e2e-job-558ae789799345e3 from sonobuoy started at 2019-09-20 21:43:07 +0000 UTC (2 container statuses recorded)
Sep 20 23:17:53.247: INFO: 	Container e2e ready: true, restart count 0
Sep 20 23:17:53.247: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 23:17:53.247: INFO: weave-net-bnv9z from kube-system started at 2019-09-20 21:41:30 +0000 UTC (2 container statuses recorded)
Sep 20 23:17:53.247: INFO: 	Container weave ready: true, restart count 0
Sep 20 23:17:53.247: INFO: 	Container weave-npc ready: true, restart count 0
Sep 20 23:17:53.247: INFO: sonobuoy-systemd-logs-daemon-set-63bbc52511204dd7-7p8gp from sonobuoy started at 2019-09-20 21:43:07 +0000 UTC (2 container statuses recorded)
Sep 20 23:17:53.247: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 20 23:17:53.247: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 20 23:17:53.247: INFO: 
Logging pods the kubelet thinks is on node worker-2 before test
Sep 20 23:17:53.259: INFO: coredns-5644d7b6d9-47g5v from kube-system started at 2019-09-20 21:42:42 +0000 UTC (1 container statuses recorded)
Sep 20 23:17:53.259: INFO: 	Container coredns ready: true, restart count 0
Sep 20 23:17:53.259: INFO: kube-proxy-dcssj from kube-system started at 2019-09-20 21:29:43 +0000 UTC (1 container statuses recorded)
Sep 20 23:17:53.259: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 23:17:53.259: INFO: sonobuoy-systemd-logs-daemon-set-63bbc52511204dd7-4bfzz from sonobuoy started at 2019-09-20 21:43:07 +0000 UTC (2 container statuses recorded)
Sep 20 23:17:53.259: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 20 23:17:53.260: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 20 23:17:53.260: INFO: sonobuoy from sonobuoy started at 2019-09-20 21:42:42 +0000 UTC (1 container statuses recorded)
Sep 20 23:17:53.260: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 20 23:17:53.260: INFO: weave-net-cwtjw from kube-system started at 2019-09-20 21:41:30 +0000 UTC (2 container statuses recorded)
Sep 20 23:17:53.260: INFO: 	Container weave ready: true, restart count 0
Sep 20 23:17:53.260: INFO: 	Container weave-npc ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
I0920 23:17:53.264715      16 reflector.go:120] Starting reflector *v1.Event (0s) from k8s.io/kubernetes/test/e2e/common/events.go:136
I0920 23:17:53.264731      16 reflector.go:158] Listing and watching *v1.Event from k8s.io/kubernetes/test/e2e/common/events.go:136
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15c648b785b96d1b], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15c648b7861494c8], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:17:54.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-359" for this suite.
Sep 20 23:18:00.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:18:00.340: INFO: namespace sched-pred-359 deletion completed in 6.062852794s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
I0920 23:18:00.340533      16 request.go:706] Error in request: resource name may not be empty

• [SLOW TEST:7.142 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:18:00.340: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep 20 23:18:00.367: INFO: Waiting up to 5m0s for pod "pod-2db73eb7-7893-4ef6-86e3-5c4d2b2fb7e6" in namespace "emptydir-6325" to be "success or failure"
Sep 20 23:18:00.373: INFO: Pod "pod-2db73eb7-7893-4ef6-86e3-5c4d2b2fb7e6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.777036ms
Sep 20 23:18:02.377: INFO: Pod "pod-2db73eb7-7893-4ef6-86e3-5c4d2b2fb7e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010060511s
STEP: Saw pod success
Sep 20 23:18:02.377: INFO: Pod "pod-2db73eb7-7893-4ef6-86e3-5c4d2b2fb7e6" satisfied condition "success or failure"
Sep 20 23:18:02.380: INFO: Trying to get logs from node worker-2 pod pod-2db73eb7-7893-4ef6-86e3-5c4d2b2fb7e6 container test-container: <nil>
STEP: delete the pod
Sep 20 23:18:02.398: INFO: Waiting for pod pod-2db73eb7-7893-4ef6-86e3-5c4d2b2fb7e6 to disappear
Sep 20 23:18:02.401: INFO: Pod pod-2db73eb7-7893-4ef6-86e3-5c4d2b2fb7e6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:18:02.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6325" for this suite.
Sep 20 23:18:08.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:18:08.492: INFO: namespace emptydir-6325 deletion completed in 6.089154617s

• [SLOW TEST:8.152 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:18:08.493: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-20a1c41f-becf-4665-9a37-e98dbe3335ad in namespace container-probe-148
Sep 20 23:18:10.526: INFO: Started pod liveness-20a1c41f-becf-4665-9a37-e98dbe3335ad in namespace container-probe-148
STEP: checking the pod's current state and verifying that restartCount is present
Sep 20 23:18:10.528: INFO: Initial restart count of pod liveness-20a1c41f-becf-4665-9a37-e98dbe3335ad is 0
Sep 20 23:18:28.606: INFO: Restart count of pod container-probe-148/liveness-20a1c41f-becf-4665-9a37-e98dbe3335ad is now 1 (18.078550428s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:18:28.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-148" for this suite.
Sep 20 23:18:34.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:18:34.732: INFO: namespace container-probe-148 deletion completed in 6.08817633s

• [SLOW TEST:26.239 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:18:34.733: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-fb1a8c20-9d57-46d9-9857-676e09243760
STEP: Creating a pod to test consume configMaps
Sep 20 23:18:34.764: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d886483d-edc6-4bf0-b43c-eb1397643378" in namespace "projected-7154" to be "success or failure"
Sep 20 23:18:34.770: INFO: Pod "pod-projected-configmaps-d886483d-edc6-4bf0-b43c-eb1397643378": Phase="Pending", Reason="", readiness=false. Elapsed: 5.841724ms
Sep 20 23:18:36.772: INFO: Pod "pod-projected-configmaps-d886483d-edc6-4bf0-b43c-eb1397643378": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007872069s
Sep 20 23:18:38.775: INFO: Pod "pod-projected-configmaps-d886483d-edc6-4bf0-b43c-eb1397643378": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01081344s
STEP: Saw pod success
Sep 20 23:18:38.775: INFO: Pod "pod-projected-configmaps-d886483d-edc6-4bf0-b43c-eb1397643378" satisfied condition "success or failure"
Sep 20 23:18:38.777: INFO: Trying to get logs from node worker-2 pod pod-projected-configmaps-d886483d-edc6-4bf0-b43c-eb1397643378 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 23:18:38.791: INFO: Waiting for pod pod-projected-configmaps-d886483d-edc6-4bf0-b43c-eb1397643378 to disappear
Sep 20 23:18:38.792: INFO: Pod pod-projected-configmaps-d886483d-edc6-4bf0-b43c-eb1397643378 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:18:38.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7154" for this suite.
Sep 20 23:18:44.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:18:44.875: INFO: namespace projected-7154 deletion completed in 6.080879411s

• [SLOW TEST:10.142 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:18:44.878: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Sep 20 23:18:44.902: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Sep 20 23:18:53.978: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 23:18:56.110: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:19:04.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-761" for this suite.
Sep 20 23:19:10.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:19:10.248: INFO: namespace crd-publish-openapi-761 deletion completed in 6.056933589s

• [SLOW TEST:25.370 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:19:10.250: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Sep 20 23:19:14.293: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-786974626 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Sep 20 23:19:24.404: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:19:24.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7833" for this suite.
Sep 20 23:19:30.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:19:30.532: INFO: namespace pods-7833 deletion completed in 6.114249235s

• [SLOW TEST:20.282 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:19:30.532: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 23:19:30.920: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 23:19:32.934: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618370, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618370, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618370, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618370, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 23:19:36.009: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 23:19:36.016: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1104-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:19:37.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4477" for this suite.
Sep 20 23:19:43.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:19:43.196: INFO: namespace webhook-4477 deletion completed in 6.069675568s
STEP: Destroying namespace "webhook-4477-markers" for this suite.
Sep 20 23:19:49.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:19:49.303: INFO: namespace webhook-4477-markers deletion completed in 6.107125091s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.778 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:19:49.312: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 20 23:19:49.337: INFO: Waiting up to 5m0s for pod "pod-4b740514-95e1-4e93-a28e-a21c0b2ab3e5" in namespace "emptydir-9356" to be "success or failure"
Sep 20 23:19:49.343: INFO: Pod "pod-4b740514-95e1-4e93-a28e-a21c0b2ab3e5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.170443ms
Sep 20 23:19:51.346: INFO: Pod "pod-4b740514-95e1-4e93-a28e-a21c0b2ab3e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008299737s
STEP: Saw pod success
Sep 20 23:19:51.346: INFO: Pod "pod-4b740514-95e1-4e93-a28e-a21c0b2ab3e5" satisfied condition "success or failure"
Sep 20 23:19:51.347: INFO: Trying to get logs from node worker-2 pod pod-4b740514-95e1-4e93-a28e-a21c0b2ab3e5 container test-container: <nil>
STEP: delete the pod
Sep 20 23:19:51.360: INFO: Waiting for pod pod-4b740514-95e1-4e93-a28e-a21c0b2ab3e5 to disappear
Sep 20 23:19:51.362: INFO: Pod pod-4b740514-95e1-4e93-a28e-a21c0b2ab3e5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:19:51.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9356" for this suite.
Sep 20 23:19:57.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:19:57.436: INFO: namespace emptydir-9356 deletion completed in 6.072274781s

• [SLOW TEST:8.124 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:19:57.436: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-69c8b81c-ed48-41ef-b31b-8885ac94ac98
STEP: Creating a pod to test consume secrets
Sep 20 23:19:57.466: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a9ef2aee-3c07-4cc7-8af7-01a3cfb7c1ef" in namespace "projected-4163" to be "success or failure"
Sep 20 23:19:57.468: INFO: Pod "pod-projected-secrets-a9ef2aee-3c07-4cc7-8af7-01a3cfb7c1ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.156403ms
Sep 20 23:19:59.494: INFO: Pod "pod-projected-secrets-a9ef2aee-3c07-4cc7-8af7-01a3cfb7c1ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027589364s
Sep 20 23:20:01.503: INFO: Pod "pod-projected-secrets-a9ef2aee-3c07-4cc7-8af7-01a3cfb7c1ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037326309s
STEP: Saw pod success
Sep 20 23:20:01.504: INFO: Pod "pod-projected-secrets-a9ef2aee-3c07-4cc7-8af7-01a3cfb7c1ef" satisfied condition "success or failure"
Sep 20 23:20:01.509: INFO: Trying to get logs from node worker-2 pod pod-projected-secrets-a9ef2aee-3c07-4cc7-8af7-01a3cfb7c1ef container secret-volume-test: <nil>
STEP: delete the pod
Sep 20 23:20:01.546: INFO: Waiting for pod pod-projected-secrets-a9ef2aee-3c07-4cc7-8af7-01a3cfb7c1ef to disappear
Sep 20 23:20:01.550: INFO: Pod pod-projected-secrets-a9ef2aee-3c07-4cc7-8af7-01a3cfb7c1ef no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:20:01.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4163" for this suite.
Sep 20 23:20:07.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:20:07.660: INFO: namespace projected-4163 deletion completed in 6.103968081s

• [SLOW TEST:10.224 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:20:07.661: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Sep 20 23:20:07.693: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-786974626 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:20:07.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1653" for this suite.
Sep 20 23:20:13.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:20:13.827: INFO: namespace kubectl-1653 deletion completed in 6.086913557s

• [SLOW TEST:6.167 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:20:13.829: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 23:20:13.849: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 20 23:20:16.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 --namespace=crd-publish-openapi-9545 create -f -'
Sep 20 23:20:17.130: INFO: stderr: ""
Sep 20 23:20:17.130: INFO: stdout: "e2e-test-crd-publish-openapi-3604-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 20 23:20:17.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 --namespace=crd-publish-openapi-9545 delete e2e-test-crd-publish-openapi-3604-crds test-cr'
Sep 20 23:20:17.186: INFO: stderr: ""
Sep 20 23:20:17.186: INFO: stdout: "e2e-test-crd-publish-openapi-3604-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Sep 20 23:20:17.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 --namespace=crd-publish-openapi-9545 apply -f -'
Sep 20 23:20:17.301: INFO: stderr: ""
Sep 20 23:20:17.301: INFO: stdout: "e2e-test-crd-publish-openapi-3604-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 20 23:20:17.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 --namespace=crd-publish-openapi-9545 delete e2e-test-crd-publish-openapi-3604-crds test-cr'
Sep 20 23:20:17.354: INFO: stderr: ""
Sep 20 23:20:17.354: INFO: stdout: "e2e-test-crd-publish-openapi-3604-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep 20 23:20:17.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 explain e2e-test-crd-publish-openapi-3604-crds'
Sep 20 23:20:17.455: INFO: stderr: ""
Sep 20 23:20:17.455: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3604-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:20:19.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9545" for this suite.
Sep 20 23:20:25.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:20:25.091: INFO: namespace crd-publish-openapi-9545 deletion completed in 6.058247075s

• [SLOW TEST:11.263 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:20:25.093: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-4684
STEP: creating replication controller nodeport-test in namespace services-4684
I0920 23:20:25.130514      16 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-4684, replica count: 2
I0920 23:20:25.130771      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 23:20:25.130787      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 23:20:28.201: INFO: Creating new exec pod
I0920 23:20:28.201181      16 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 23:20:30.239012      16 reflector.go:120] Starting reflector *v1.Endpoints (0s) from k8s.io/kubernetes/test/e2e/framework/service/jig.go:389
I0920 23:20:30.239111      16 reflector.go:158] Listing and watching *v1.Endpoints from k8s.io/kubernetes/test/e2e/framework/service/jig.go:389
Sep 20 23:20:31.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=services-4684 execpodfc92c -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Sep 20 23:20:31.427: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep 20 23:20:31.427: INFO: stdout: ""
Sep 20 23:20:31.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=services-4684 execpodfc92c -- /bin/sh -x -c nc -zv -t -w 2 10.101.157.79 80'
Sep 20 23:20:31.578: INFO: stderr: "+ nc -zv -t -w 2 10.101.157.79 80\nConnection to 10.101.157.79 80 port [tcp/http] succeeded!\n"
Sep 20 23:20:31.578: INFO: stdout: ""
Sep 20 23:20:31.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=services-4684 execpodfc92c -- /bin/sh -x -c nc -zv -t -w 2 192.168.5.11 31842'
Sep 20 23:20:31.737: INFO: stderr: "+ nc -zv -t -w 2 192.168.5.11 31842\nConnection to 192.168.5.11 31842 port [tcp/31842] succeeded!\n"
Sep 20 23:20:31.737: INFO: stdout: ""
Sep 20 23:20:31.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=services-4684 execpodfc92c -- /bin/sh -x -c nc -zv -t -w 2 192.168.5.101 31842'
Sep 20 23:20:31.891: INFO: stderr: "+ nc -zv -t -w 2 192.168.5.101 31842\nConnection to 192.168.5.101 31842 port [tcp/31842] succeeded!\n"
Sep 20 23:20:31.891: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:20:31.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4684" for this suite.
Sep 20 23:20:37.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:20:37.956: INFO: namespace services-4684 deletion completed in 6.062015399s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:12.863 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:20:37.956: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 23:20:38.054: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8231b056-b6ad-4627-b488-f58c3d2bf5a0" in namespace "downward-api-9704" to be "success or failure"
Sep 20 23:20:38.072: INFO: Pod "downwardapi-volume-8231b056-b6ad-4627-b488-f58c3d2bf5a0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.479182ms
Sep 20 23:20:40.085: INFO: Pod "downwardapi-volume-8231b056-b6ad-4627-b488-f58c3d2bf5a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030900173s
STEP: Saw pod success
Sep 20 23:20:40.085: INFO: Pod "downwardapi-volume-8231b056-b6ad-4627-b488-f58c3d2bf5a0" satisfied condition "success or failure"
Sep 20 23:20:40.090: INFO: Trying to get logs from node worker-2 pod downwardapi-volume-8231b056-b6ad-4627-b488-f58c3d2bf5a0 container client-container: <nil>
STEP: delete the pod
Sep 20 23:20:40.128: INFO: Waiting for pod downwardapi-volume-8231b056-b6ad-4627-b488-f58c3d2bf5a0 to disappear
Sep 20 23:20:40.131: INFO: Pod downwardapi-volume-8231b056-b6ad-4627-b488-f58c3d2bf5a0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:20:40.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9704" for this suite.
Sep 20 23:20:46.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:20:46.231: INFO: namespace downward-api-9704 deletion completed in 6.096225486s

• [SLOW TEST:8.275 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:20:46.231: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 23:20:46.268: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7bd13246-6ba1-461a-a65c-18d484316af0" in namespace "projected-1965" to be "success or failure"
Sep 20 23:20:46.270: INFO: Pod "downwardapi-volume-7bd13246-6ba1-461a-a65c-18d484316af0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.195039ms
Sep 20 23:20:48.277: INFO: Pod "downwardapi-volume-7bd13246-6ba1-461a-a65c-18d484316af0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009218257s
STEP: Saw pod success
Sep 20 23:20:48.277: INFO: Pod "downwardapi-volume-7bd13246-6ba1-461a-a65c-18d484316af0" satisfied condition "success or failure"
Sep 20 23:20:48.284: INFO: Trying to get logs from node worker-2 pod downwardapi-volume-7bd13246-6ba1-461a-a65c-18d484316af0 container client-container: <nil>
STEP: delete the pod
Sep 20 23:20:48.323: INFO: Waiting for pod downwardapi-volume-7bd13246-6ba1-461a-a65c-18d484316af0 to disappear
Sep 20 23:20:48.327: INFO: Pod downwardapi-volume-7bd13246-6ba1-461a-a65c-18d484316af0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:20:48.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1965" for this suite.
Sep 20 23:20:54.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:20:54.446: INFO: namespace projected-1965 deletion completed in 6.115312379s

• [SLOW TEST:8.215 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:20:54.447: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 20 23:20:54.495: INFO: Waiting up to 5m0s for pod "pod-a310ed84-cf8b-4219-8429-69667516c9e4" in namespace "emptydir-1269" to be "success or failure"
Sep 20 23:20:54.499: INFO: Pod "pod-a310ed84-cf8b-4219-8429-69667516c9e4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.720395ms
Sep 20 23:20:56.505: INFO: Pod "pod-a310ed84-cf8b-4219-8429-69667516c9e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009622887s
Sep 20 23:20:58.527: INFO: Pod "pod-a310ed84-cf8b-4219-8429-69667516c9e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031957752s
STEP: Saw pod success
Sep 20 23:20:58.527: INFO: Pod "pod-a310ed84-cf8b-4219-8429-69667516c9e4" satisfied condition "success or failure"
Sep 20 23:20:58.529: INFO: Trying to get logs from node worker-2 pod pod-a310ed84-cf8b-4219-8429-69667516c9e4 container test-container: <nil>
STEP: delete the pod
Sep 20 23:20:58.544: INFO: Waiting for pod pod-a310ed84-cf8b-4219-8429-69667516c9e4 to disappear
Sep 20 23:20:58.547: INFO: Pod pod-a310ed84-cf8b-4219-8429-69667516c9e4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:20:58.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1269" for this suite.
Sep 20 23:21:04.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:21:04.643: INFO: namespace emptydir-1269 deletion completed in 6.092553713s

• [SLOW TEST:10.197 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:21:04.644: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep 20 23:21:04.680: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6237 /api/v1/namespaces/watch-6237/configmaps/e2e-watch-test-resource-version 0685b678-a8fb-43af-96ce-c78fa870dc94 23365 0 2019-09-20 23:21:04 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 20 23:21:04.680: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6237 /api/v1/namespaces/watch-6237/configmaps/e2e-watch-test-resource-version 0685b678-a8fb-43af-96ce-c78fa870dc94 23366 0 2019-09-20 23:21:04 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:21:04.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6237" for this suite.
Sep 20 23:21:10.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:21:10.770: INFO: namespace watch-6237 deletion completed in 6.088020039s

• [SLOW TEST:6.126 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:21:10.770: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 23:21:36.810: INFO: Container started at 2019-09-20 23:21:12 +0000 UTC, pod became ready at 2019-09-20 23:21:35 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:21:36.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9229" for this suite.
Sep 20 23:21:48.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:21:48.945: INFO: namespace container-probe-9229 deletion completed in 12.128479119s

• [SLOW TEST:38.175 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:21:48.946: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:22:17.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1630" for this suite.
Sep 20 23:22:23.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:22:23.553: INFO: namespace container-runtime-1630 deletion completed in 6.141016703s

• [SLOW TEST:34.608 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:22:23.554: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-2657
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 20 23:22:23.575: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 20 23:22:45.764: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.38.0.1:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2657 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 23:22:45.764: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 23:22:45.868: INFO: Found all expected endpoints: [netserver-0]
Sep 20 23:22:45.870: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.32.0.4:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2657 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 23:22:45.870: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 23:22:45.949: INFO: Found all expected endpoints: [netserver-1]
Sep 20 23:22:45.951: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.40.0.3:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2657 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 23:22:45.951: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 23:22:46.037: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:22:46.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2657" for this suite.
Sep 20 23:22:58.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:22:58.211: INFO: namespace pod-network-test-2657 deletion completed in 12.171837462s

• [SLOW TEST:34.657 seconds]
[sig-network] Networking
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:22:58.214: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 20 23:23:02.311: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 20 23:23:02.317: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 20 23:23:04.318: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 20 23:23:04.320: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 20 23:23:06.318: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 20 23:23:06.324: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 20 23:23:08.318: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 20 23:23:08.325: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 20 23:23:10.318: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 20 23:23:10.324: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 20 23:23:12.318: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 20 23:23:12.325: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 20 23:23:14.318: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 20 23:23:14.325: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:23:14.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1492" for this suite.
Sep 20 23:23:26.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:23:26.464: INFO: namespace container-lifecycle-hook-1492 deletion completed in 12.092486779s

• [SLOW TEST:28.251 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:23:26.464: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 23:23:26.494: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4431f56f-9613-4133-ac77-6f8b8d7d3153" in namespace "downward-api-5611" to be "success or failure"
Sep 20 23:23:26.496: INFO: Pod "downwardapi-volume-4431f56f-9613-4133-ac77-6f8b8d7d3153": Phase="Pending", Reason="", readiness=false. Elapsed: 2.332052ms
Sep 20 23:23:28.506: INFO: Pod "downwardapi-volume-4431f56f-9613-4133-ac77-6f8b8d7d3153": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012300391s
Sep 20 23:23:30.512: INFO: Pod "downwardapi-volume-4431f56f-9613-4133-ac77-6f8b8d7d3153": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01742279s
STEP: Saw pod success
Sep 20 23:23:30.512: INFO: Pod "downwardapi-volume-4431f56f-9613-4133-ac77-6f8b8d7d3153" satisfied condition "success or failure"
Sep 20 23:23:30.516: INFO: Trying to get logs from node worker-2 pod downwardapi-volume-4431f56f-9613-4133-ac77-6f8b8d7d3153 container client-container: <nil>
STEP: delete the pod
Sep 20 23:23:30.547: INFO: Waiting for pod downwardapi-volume-4431f56f-9613-4133-ac77-6f8b8d7d3153 to disappear
Sep 20 23:23:30.552: INFO: Pod downwardapi-volume-4431f56f-9613-4133-ac77-6f8b8d7d3153 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:23:30.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5611" for this suite.
Sep 20 23:23:36.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:23:36.640: INFO: namespace downward-api-5611 deletion completed in 6.084842193s

• [SLOW TEST:10.176 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:23:36.640: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 20 23:23:36.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-1606'
Sep 20 23:23:36.734: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 20 23:23:36.734: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Sep 20 23:23:36.742: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-s688w]
Sep 20 23:23:36.742: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-s688w" in namespace "kubectl-1606" to be "running and ready"
Sep 20 23:23:36.746: INFO: Pod "e2e-test-httpd-rc-s688w": Phase="Pending", Reason="", readiness=false. Elapsed: 3.615065ms
Sep 20 23:23:38.758: INFO: Pod "e2e-test-httpd-rc-s688w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015893994s
Sep 20 23:23:40.766: INFO: Pod "e2e-test-httpd-rc-s688w": Phase="Running", Reason="", readiness=true. Elapsed: 4.023571001s
Sep 20 23:23:40.766: INFO: Pod "e2e-test-httpd-rc-s688w" satisfied condition "running and ready"
Sep 20 23:23:40.766: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-s688w]
Sep 20 23:23:40.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 logs rc/e2e-test-httpd-rc --namespace=kubectl-1606'
Sep 20 23:23:40.864: INFO: stderr: ""
Sep 20 23:23:40.864: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.40.0.3. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.40.0.3. Set the 'ServerName' directive globally to suppress this message\n[Fri Sep 20 23:23:38.297995 2019] [mpm_event:notice] [pid 1:tid 139909934426984] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Fri Sep 20 23:23:38.298027 2019] [core:notice] [pid 1:tid 139909934426984] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Sep 20 23:23:40.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 delete rc e2e-test-httpd-rc --namespace=kubectl-1606'
Sep 20 23:23:40.925: INFO: stderr: ""
Sep 20 23:23:40.925: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:23:40.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1606" for this suite.
Sep 20 23:24:08.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:24:09.016: INFO: namespace kubectl-1606 deletion completed in 28.086884875s

• [SLOW TEST:32.376 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:24:09.017: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-49c8ef7a-6e96-4d55-bcd8-16482c6a4087
STEP: Creating a pod to test consume configMaps
Sep 20 23:24:09.044: INFO: Waiting up to 5m0s for pod "pod-configmaps-aac1516d-87f7-4711-83d4-adf5d111657f" in namespace "configmap-5179" to be "success or failure"
Sep 20 23:24:09.050: INFO: Pod "pod-configmaps-aac1516d-87f7-4711-83d4-adf5d111657f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.448211ms
Sep 20 23:24:11.052: INFO: Pod "pod-configmaps-aac1516d-87f7-4711-83d4-adf5d111657f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008273511s
STEP: Saw pod success
Sep 20 23:24:11.052: INFO: Pod "pod-configmaps-aac1516d-87f7-4711-83d4-adf5d111657f" satisfied condition "success or failure"
Sep 20 23:24:11.054: INFO: Trying to get logs from node worker-2 pod pod-configmaps-aac1516d-87f7-4711-83d4-adf5d111657f container configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 23:24:11.070: INFO: Waiting for pod pod-configmaps-aac1516d-87f7-4711-83d4-adf5d111657f to disappear
Sep 20 23:24:11.071: INFO: Pod pod-configmaps-aac1516d-87f7-4711-83d4-adf5d111657f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:24:11.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5179" for this suite.
Sep 20 23:24:17.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:24:17.159: INFO: namespace configmap-5179 deletion completed in 6.084614282s

• [SLOW TEST:8.142 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:24:17.159: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 23:24:17.186: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:24:19.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2584" for this suite.
Sep 20 23:25:05.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:25:05.340: INFO: namespace pods-2584 deletion completed in 46.096492588s

• [SLOW TEST:48.181 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:25:05.340: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Sep 20 23:25:05.373: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-1658" to be "success or failure"
Sep 20 23:25:05.375: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.401575ms
Sep 20 23:25:07.378: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004847226s
Sep 20 23:25:09.384: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010523149s
STEP: Saw pod success
Sep 20 23:25:09.384: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Sep 20 23:25:09.389: INFO: Trying to get logs from node worker-2 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Sep 20 23:25:09.437: INFO: Waiting for pod pod-host-path-test to disappear
Sep 20 23:25:09.440: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:25:09.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-1658" for this suite.
Sep 20 23:25:15.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:25:15.510: INFO: namespace hostpath-1658 deletion completed in 6.067738566s

• [SLOW TEST:10.170 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:25:15.511: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 23:25:15.882: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 23:25:17.898: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618715, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618715, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618715, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704618715, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 23:25:20.933: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:25:20.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9952" for this suite.
Sep 20 23:25:26.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:25:27.028: INFO: namespace webhook-9952 deletion completed in 6.05568406s
STEP: Destroying namespace "webhook-9952-markers" for this suite.
Sep 20 23:25:33.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:25:33.126: INFO: namespace webhook-9952-markers deletion completed in 6.09801842s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.623 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:25:33.136: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Sep 20 23:25:37.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec pod-sharedvolume-056b573e-9d24-41fa-adc9-75efdff075e0 -c busybox-main-container --namespace=emptydir-2384 -- cat /usr/share/volumeshare/shareddata.txt'
Sep 20 23:25:37.354: INFO: stderr: ""
Sep 20 23:25:37.354: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:25:37.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2384" for this suite.
Sep 20 23:25:43.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:25:43.459: INFO: namespace emptydir-2384 deletion completed in 6.103293402s

• [SLOW TEST:10.324 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:25:43.462: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 23:25:43.907: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 23:25:46.947: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Sep 20 23:25:48.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 attach --namespace=webhook-3856 to-be-attached-pod -i -c=container1'
Sep 20 23:25:49.066: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:25:49.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3856" for this suite.
Sep 20 23:26:01.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:26:01.136: INFO: namespace webhook-3856 deletion completed in 12.061020446s
STEP: Destroying namespace "webhook-3856-markers" for this suite.
Sep 20 23:26:07.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:26:07.220: INFO: namespace webhook-3856-markers deletion completed in 6.084715693s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:23.766 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:26:07.228: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 20 23:26:07.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-813'
Sep 20 23:26:07.313: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 20 23:26:07.313: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Sep 20 23:26:07.319: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Sep 20 23:26:07.321: INFO: scanned /root for discovery docs: <nil>
Sep 20 23:26:07.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-813'
Sep 20 23:26:23.188: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep 20 23:26:23.188: INFO: stdout: "Created e2e-test-httpd-rc-071bb6eddea2bed8e428573c3c1e5a8b\nScaling up e2e-test-httpd-rc-071bb6eddea2bed8e428573c3c1e5a8b from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-071bb6eddea2bed8e428573c3c1e5a8b up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-071bb6eddea2bed8e428573c3c1e5a8b to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Sep 20 23:26:23.188: INFO: stdout: "Created e2e-test-httpd-rc-071bb6eddea2bed8e428573c3c1e5a8b\nScaling up e2e-test-httpd-rc-071bb6eddea2bed8e428573c3c1e5a8b from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-071bb6eddea2bed8e428573c3c1e5a8b up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-071bb6eddea2bed8e428573c3c1e5a8b to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Sep 20 23:26:23.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-813'
Sep 20 23:26:23.247: INFO: stderr: ""
Sep 20 23:26:23.247: INFO: stdout: "e2e-test-httpd-rc-071bb6eddea2bed8e428573c3c1e5a8b-frdf9 "
Sep 20 23:26:23.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods e2e-test-httpd-rc-071bb6eddea2bed8e428573c3c1e5a8b-frdf9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-813'
Sep 20 23:26:23.298: INFO: stderr: ""
Sep 20 23:26:23.298: INFO: stdout: "true"
Sep 20 23:26:23.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pods e2e-test-httpd-rc-071bb6eddea2bed8e428573c3c1e5a8b-frdf9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-813'
Sep 20 23:26:23.348: INFO: stderr: ""
Sep 20 23:26:23.348: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Sep 20 23:26:23.348: INFO: e2e-test-httpd-rc-071bb6eddea2bed8e428573c3c1e5a8b-frdf9 is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Sep 20 23:26:23.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 delete rc e2e-test-httpd-rc --namespace=kubectl-813'
Sep 20 23:26:23.411: INFO: stderr: ""
Sep 20 23:26:23.411: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:26:23.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-813" for this suite.
Sep 20 23:26:29.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:26:29.478: INFO: namespace kubectl-813 deletion completed in 6.064124113s

• [SLOW TEST:22.250 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:26:29.479: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 20 23:26:29.503: INFO: Waiting up to 5m0s for pod "pod-daf633ba-f628-4f6b-ac92-9ac650852c59" in namespace "emptydir-6076" to be "success or failure"
Sep 20 23:26:29.517: INFO: Pod "pod-daf633ba-f628-4f6b-ac92-9ac650852c59": Phase="Pending", Reason="", readiness=false. Elapsed: 13.529551ms
Sep 20 23:26:31.525: INFO: Pod "pod-daf633ba-f628-4f6b-ac92-9ac650852c59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021794478s
Sep 20 23:26:33.531: INFO: Pod "pod-daf633ba-f628-4f6b-ac92-9ac650852c59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02782487s
STEP: Saw pod success
Sep 20 23:26:33.531: INFO: Pod "pod-daf633ba-f628-4f6b-ac92-9ac650852c59" satisfied condition "success or failure"
Sep 20 23:26:33.537: INFO: Trying to get logs from node worker-2 pod pod-daf633ba-f628-4f6b-ac92-9ac650852c59 container test-container: <nil>
STEP: delete the pod
Sep 20 23:26:33.585: INFO: Waiting for pod pod-daf633ba-f628-4f6b-ac92-9ac650852c59 to disappear
Sep 20 23:26:33.588: INFO: Pod pod-daf633ba-f628-4f6b-ac92-9ac650852c59 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:26:33.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6076" for this suite.
Sep 20 23:26:39.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:26:39.665: INFO: namespace emptydir-6076 deletion completed in 6.074197884s

• [SLOW TEST:10.186 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:26:39.667: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep 20 23:26:39.694: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-900 /api/v1/namespaces/watch-900/configmaps/e2e-watch-test-configmap-a bb9ac96c-09d9-4123-9e58-db514f4991ab 24511 0 2019-09-20 23:26:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 20 23:26:39.694: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-900 /api/v1/namespaces/watch-900/configmaps/e2e-watch-test-configmap-a bb9ac96c-09d9-4123-9e58-db514f4991ab 24511 0 2019-09-20 23:26:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep 20 23:26:49.701: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-900 /api/v1/namespaces/watch-900/configmaps/e2e-watch-test-configmap-a bb9ac96c-09d9-4123-9e58-db514f4991ab 24526 0 2019-09-20 23:26:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep 20 23:26:49.701: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-900 /api/v1/namespaces/watch-900/configmaps/e2e-watch-test-configmap-a bb9ac96c-09d9-4123-9e58-db514f4991ab 24526 0 2019-09-20 23:26:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep 20 23:26:59.711: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-900 /api/v1/namespaces/watch-900/configmaps/e2e-watch-test-configmap-a bb9ac96c-09d9-4123-9e58-db514f4991ab 24542 0 2019-09-20 23:26:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 20 23:26:59.711: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-900 /api/v1/namespaces/watch-900/configmaps/e2e-watch-test-configmap-a bb9ac96c-09d9-4123-9e58-db514f4991ab 24542 0 2019-09-20 23:26:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep 20 23:27:09.727: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-900 /api/v1/namespaces/watch-900/configmaps/e2e-watch-test-configmap-a bb9ac96c-09d9-4123-9e58-db514f4991ab 24557 0 2019-09-20 23:26:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 20 23:27:09.727: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-900 /api/v1/namespaces/watch-900/configmaps/e2e-watch-test-configmap-a bb9ac96c-09d9-4123-9e58-db514f4991ab 24557 0 2019-09-20 23:26:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep 20 23:27:19.738: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-900 /api/v1/namespaces/watch-900/configmaps/e2e-watch-test-configmap-b c7977306-82a5-406f-8ada-888488a1118e 24572 0 2019-09-20 23:27:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 20 23:27:19.739: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-900 /api/v1/namespaces/watch-900/configmaps/e2e-watch-test-configmap-b c7977306-82a5-406f-8ada-888488a1118e 24572 0 2019-09-20 23:27:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep 20 23:27:29.761: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-900 /api/v1/namespaces/watch-900/configmaps/e2e-watch-test-configmap-b c7977306-82a5-406f-8ada-888488a1118e 24589 0 2019-09-20 23:27:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 20 23:27:29.761: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-900 /api/v1/namespaces/watch-900/configmaps/e2e-watch-test-configmap-b c7977306-82a5-406f-8ada-888488a1118e 24589 0 2019-09-20 23:27:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:27:39.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-900" for this suite.
Sep 20 23:27:45.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:27:45.859: INFO: namespace watch-900 deletion completed in 6.09422756s

• [SLOW TEST:66.193 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:27:45.861: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:27:49.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4136" for this suite.
Sep 20 23:28:33.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:28:34.013: INFO: namespace kubelet-test-4136 deletion completed in 44.103117513s

• [SLOW TEST:48.152 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:28:34.015: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep 20 23:28:37.081: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:28:37.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6976" for this suite.
Sep 20 23:29:05.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:29:05.227: INFO: namespace replicaset-6976 deletion completed in 28.104013201s

• [SLOW TEST:31.212 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:29:05.228: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-6640
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6640 to expose endpoints map[]
Sep 20 23:29:05.265: INFO: successfully validated that service multi-endpoint-test in namespace services-6640 exposes endpoints map[] (6.949558ms elapsed)
STEP: Creating pod pod1 in namespace services-6640
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6640 to expose endpoints map[pod1:[100]]
Sep 20 23:29:08.312: INFO: successfully validated that service multi-endpoint-test in namespace services-6640 exposes endpoints map[pod1:[100]] (3.041877597s elapsed)
STEP: Creating pod pod2 in namespace services-6640
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6640 to expose endpoints map[pod1:[100] pod2:[101]]
Sep 20 23:29:10.384: INFO: successfully validated that service multi-endpoint-test in namespace services-6640 exposes endpoints map[pod1:[100] pod2:[101]] (2.04900415s elapsed)
STEP: Deleting pod pod1 in namespace services-6640
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6640 to expose endpoints map[pod2:[101]]
Sep 20 23:29:11.440: INFO: successfully validated that service multi-endpoint-test in namespace services-6640 exposes endpoints map[pod2:[101]] (1.046517701s elapsed)
STEP: Deleting pod pod2 in namespace services-6640
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6640 to expose endpoints map[]
Sep 20 23:29:11.453: INFO: successfully validated that service multi-endpoint-test in namespace services-6640 exposes endpoints map[] (3.985957ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:29:11.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6640" for this suite.
Sep 20 23:29:39.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:29:39.545: INFO: namespace services-6640 deletion completed in 28.073489539s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:34.317 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:29:39.545: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-3274
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 20 23:29:39.569: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 20 23:30:05.715: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.38.0.1 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3274 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 23:30:05.715: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 23:30:06.872: INFO: Found all expected endpoints: [netserver-0]
Sep 20 23:30:06.874: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.40.0.3 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3274 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 23:30:06.874: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 23:30:07.985: INFO: Found all expected endpoints: [netserver-1]
Sep 20 23:30:07.987: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.32.0.4 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3274 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 23:30:07.987: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
Sep 20 23:30:09.099: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:30:09.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3274" for this suite.
Sep 20 23:30:21.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:30:21.184: INFO: namespace pod-network-test-3274 deletion completed in 12.082357216s

• [SLOW TEST:41.639 seconds]
[sig-network] Networking
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:30:21.184: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:30:37.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5567" for this suite.
Sep 20 23:30:43.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:30:43.462: INFO: namespace resourcequota-5567 deletion completed in 6.067594388s

• [SLOW TEST:22.278 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:30:43.464: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 23:30:45.602: INFO: Waiting up to 5m0s for pod "client-envvars-c38c9409-3286-40f9-a05d-1cec8a6972f7" in namespace "pods-9915" to be "success or failure"
Sep 20 23:30:45.611: INFO: Pod "client-envvars-c38c9409-3286-40f9-a05d-1cec8a6972f7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.051685ms
Sep 20 23:30:47.617: INFO: Pod "client-envvars-c38c9409-3286-40f9-a05d-1cec8a6972f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014880227s
STEP: Saw pod success
Sep 20 23:30:47.617: INFO: Pod "client-envvars-c38c9409-3286-40f9-a05d-1cec8a6972f7" satisfied condition "success or failure"
Sep 20 23:30:47.618: INFO: Trying to get logs from node worker-2 pod client-envvars-c38c9409-3286-40f9-a05d-1cec8a6972f7 container env3cont: <nil>
STEP: delete the pod
Sep 20 23:30:47.638: INFO: Waiting for pod client-envvars-c38c9409-3286-40f9-a05d-1cec8a6972f7 to disappear
Sep 20 23:30:47.640: INFO: Pod client-envvars-c38c9409-3286-40f9-a05d-1cec8a6972f7 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:30:47.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9915" for this suite.
Sep 20 23:31:15.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:31:15.734: INFO: namespace pods-9915 deletion completed in 28.092526005s

• [SLOW TEST:32.270 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:31:15.734: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:31:26.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4375" for this suite.
Sep 20 23:31:32.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:31:32.898: INFO: namespace resourcequota-4375 deletion completed in 6.072478536s

• [SLOW TEST:17.163 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:31:32.898: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep 20 23:31:36.941: INFO: &Pod{ObjectMeta:{send-events-0adcc498-d67b-4df9-8aad-5c07b6a6f3a8  events-6914 /api/v1/namespaces/events-6914/pods/send-events-0adcc498-d67b-4df9-8aad-5c07b6a6f3a8 33619954-6d24-4c82-94ea-e2479ca52736 25293 0 2019-09-20 23:31:32 +0000 UTC <nil> <nil> map[name:foo time:919080230] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k8h4z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k8h4z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k8h4z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 23:31:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 23:31:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 23:31:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 23:31:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.102,PodIP:10.40.0.3,StartTime:2019-09-20 23:31:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 23:31:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:docker://6b680b8107f71854c5010128d3a5ba9a888d532b6b9ea79c4a65cc9d7a9c2a97,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.40.0.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Sep 20 23:31:38.946: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep 20 23:31:40.955: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:31:40.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6914" for this suite.
Sep 20 23:32:24.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:32:25.042: INFO: namespace events-6914 deletion completed in 44.055189372s

• [SLOW TEST:52.144 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:32:25.043: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Sep 20 23:32:25.064: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 20 23:32:25.069: INFO: Waiting for terminating namespaces to be deleted...
Sep 20 23:32:25.071: INFO: 
Logging pods the kubelet thinks is on node controlplane-1 before test
Sep 20 23:32:25.080: INFO: kube-apiserver-controlplane-1 from kube-system started at 2019-09-20 21:29:15 +0000 UTC (1 container statuses recorded)
Sep 20 23:32:25.080: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep 20 23:32:25.080: INFO: weave-net-n52f9 from kube-system started at 2019-09-20 21:41:30 +0000 UTC (2 container statuses recorded)
Sep 20 23:32:25.080: INFO: 	Container weave ready: true, restart count 0
Sep 20 23:32:25.080: INFO: 	Container weave-npc ready: true, restart count 0
Sep 20 23:32:25.080: INFO: etcd-controlplane-1 from kube-system started at 2019-09-20 21:29:15 +0000 UTC (1 container statuses recorded)
Sep 20 23:32:25.080: INFO: 	Container etcd ready: true, restart count 0
Sep 20 23:32:25.080: INFO: kube-proxy-62wrx from kube-system started at 2019-09-20 21:29:38 +0000 UTC (1 container statuses recorded)
Sep 20 23:32:25.080: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 23:32:25.080: INFO: kube-controller-manager-controlplane-1 from kube-system started at 2019-09-20 21:29:15 +0000 UTC (1 container statuses recorded)
Sep 20 23:32:25.080: INFO: 	Container kube-controller-manager ready: true, restart count 0
Sep 20 23:32:25.080: INFO: kube-scheduler-controlplane-1 from kube-system started at 2019-09-20 21:29:15 +0000 UTC (1 container statuses recorded)
Sep 20 23:32:25.080: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep 20 23:32:25.080: INFO: sonobuoy-systemd-logs-daemon-set-63bbc52511204dd7-g8gl8 from sonobuoy started at 2019-09-20 21:43:06 +0000 UTC (2 container statuses recorded)
Sep 20 23:32:25.080: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 20 23:32:25.080: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 20 23:32:25.080: INFO: 
Logging pods the kubelet thinks is on node worker-1 before test
Sep 20 23:32:25.089: INFO: weave-net-bnv9z from kube-system started at 2019-09-20 21:41:30 +0000 UTC (2 container statuses recorded)
Sep 20 23:32:25.089: INFO: 	Container weave ready: true, restart count 0
Sep 20 23:32:25.090: INFO: 	Container weave-npc ready: true, restart count 0
Sep 20 23:32:25.090: INFO: coredns-5644d7b6d9-pvh6f from kube-system started at 2019-09-20 21:42:42 +0000 UTC (1 container statuses recorded)
Sep 20 23:32:25.090: INFO: 	Container coredns ready: true, restart count 0
Sep 20 23:32:25.090: INFO: sonobuoy-e2e-job-558ae789799345e3 from sonobuoy started at 2019-09-20 21:43:07 +0000 UTC (2 container statuses recorded)
Sep 20 23:32:25.090: INFO: 	Container e2e ready: true, restart count 0
Sep 20 23:32:25.090: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 23:32:25.090: INFO: sonobuoy-systemd-logs-daemon-set-63bbc52511204dd7-7p8gp from sonobuoy started at 2019-09-20 21:43:07 +0000 UTC (2 container statuses recorded)
Sep 20 23:32:25.090: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 20 23:32:25.090: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 20 23:32:25.090: INFO: kube-proxy-dxbgr from kube-system started at 2019-09-20 21:29:43 +0000 UTC (1 container statuses recorded)
Sep 20 23:32:25.090: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 23:32:25.090: INFO: 
Logging pods the kubelet thinks is on node worker-2 before test
Sep 20 23:32:25.099: INFO: weave-net-cwtjw from kube-system started at 2019-09-20 21:41:30 +0000 UTC (2 container statuses recorded)
Sep 20 23:32:25.099: INFO: 	Container weave ready: true, restart count 0
Sep 20 23:32:25.099: INFO: 	Container weave-npc ready: true, restart count 0
Sep 20 23:32:25.099: INFO: coredns-5644d7b6d9-47g5v from kube-system started at 2019-09-20 21:42:42 +0000 UTC (1 container statuses recorded)
Sep 20 23:32:25.099: INFO: 	Container coredns ready: true, restart count 0
Sep 20 23:32:25.099: INFO: kube-proxy-dcssj from kube-system started at 2019-09-20 21:29:43 +0000 UTC (1 container statuses recorded)
Sep 20 23:32:25.099: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 23:32:25.099: INFO: sonobuoy-systemd-logs-daemon-set-63bbc52511204dd7-4bfzz from sonobuoy started at 2019-09-20 21:43:07 +0000 UTC (2 container statuses recorded)
Sep 20 23:32:25.099: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 20 23:32:25.099: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 20 23:32:25.099: INFO: sonobuoy from sonobuoy started at 2019-09-20 21:42:42 +0000 UTC (1 container statuses recorded)
Sep 20 23:32:25.099: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b72ebc2a-b305-49b9-998b-2e2416a6d91e 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-b72ebc2a-b305-49b9-998b-2e2416a6d91e off the node worker-2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b72ebc2a-b305-49b9-998b-2e2416a6d91e
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:32:29.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7388" for this suite.
Sep 20 23:32:47.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:32:47.222: INFO: namespace sched-pred-7388 deletion completed in 18.053312907s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
I0920 23:32:47.222380      16 request.go:706] Error in request: resource name may not be empty

• [SLOW TEST:22.179 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:32:47.222: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 23:32:47.251: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf742e20-1c7b-4fab-81b8-2576b278e9be" in namespace "downward-api-2713" to be "success or failure"
Sep 20 23:32:47.257: INFO: Pod "downwardapi-volume-cf742e20-1c7b-4fab-81b8-2576b278e9be": Phase="Pending", Reason="", readiness=false. Elapsed: 6.207639ms
Sep 20 23:32:49.262: INFO: Pod "downwardapi-volume-cf742e20-1c7b-4fab-81b8-2576b278e9be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011750939s
Sep 20 23:32:51.265: INFO: Pod "downwardapi-volume-cf742e20-1c7b-4fab-81b8-2576b278e9be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01483636s
STEP: Saw pod success
Sep 20 23:32:51.265: INFO: Pod "downwardapi-volume-cf742e20-1c7b-4fab-81b8-2576b278e9be" satisfied condition "success or failure"
Sep 20 23:32:51.268: INFO: Trying to get logs from node worker-2 pod downwardapi-volume-cf742e20-1c7b-4fab-81b8-2576b278e9be container client-container: <nil>
STEP: delete the pod
Sep 20 23:32:51.284: INFO: Waiting for pod downwardapi-volume-cf742e20-1c7b-4fab-81b8-2576b278e9be to disappear
Sep 20 23:32:51.286: INFO: Pod downwardapi-volume-cf742e20-1c7b-4fab-81b8-2576b278e9be no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:32:51.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2713" for this suite.
Sep 20 23:32:57.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:32:57.380: INFO: namespace downward-api-2713 deletion completed in 6.091771938s

• [SLOW TEST:10.158 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:32:57.380: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-5c80ec0f-2cf8-45b1-81ff-8fbeab19058f in namespace container-probe-7258
Sep 20 23:32:59.428: INFO: Started pod busybox-5c80ec0f-2cf8-45b1-81ff-8fbeab19058f in namespace container-probe-7258
STEP: checking the pod's current state and verifying that restartCount is present
Sep 20 23:32:59.432: INFO: Initial restart count of pod busybox-5c80ec0f-2cf8-45b1-81ff-8fbeab19058f is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:37:00.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7258" for this suite.
Sep 20 23:37:06.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:37:06.452: INFO: namespace container-probe-7258 deletion completed in 6.101989205s

• [SLOW TEST:249.072 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:37:06.454: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6137
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-6137
I0920 23:37:06.551826      16 runners.go:184] Created replication controller with name: externalname-service, namespace: services-6137, replica count: 2
I0920 23:37:06.551914      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 23:37:06.551932      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 23:37:09.602: INFO: Creating new exec pod
I0920 23:37:09.602834      16 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 23:37:11.618393      16 reflector.go:120] Starting reflector *v1.Endpoints (0s) from k8s.io/kubernetes/test/e2e/framework/service/jig.go:389
I0920 23:37:11.618410      16 reflector.go:158] Listing and watching *v1.Endpoints from k8s.io/kubernetes/test/e2e/framework/service/jig.go:389
Sep 20 23:37:12.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=services-6137 execpodw9vp2 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Sep 20 23:37:13.333: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 20 23:37:13.333: INFO: stdout: ""
Sep 20 23:37:13.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=services-6137 execpodw9vp2 -- /bin/sh -x -c nc -zv -t -w 2 10.108.238.166 80'
Sep 20 23:37:13.467: INFO: stderr: "+ nc -zv -t -w 2 10.108.238.166 80\nConnection to 10.108.238.166 80 port [tcp/http] succeeded!\n"
Sep 20 23:37:13.467: INFO: stdout: ""
Sep 20 23:37:13.467: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:37:13.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6137" for this suite.
Sep 20 23:37:19.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:37:19.568: INFO: namespace services-6137 deletion completed in 6.070480906s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:13.114 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:37:19.569: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 23:37:19.597: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep 20 23:37:24.605: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 20 23:37:24.605: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Sep 20 23:37:28.663: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6262 /apis/apps/v1/namespaces/deployment-6262/deployments/test-cleanup-deployment 1e0c0e8f-0c50-4ec2-9766-4d16b91a0b8b 26038 1 2019-09-20 23:37:24 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002feffe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-09-20 23:37:24 +0000 UTC,LastTransitionTime:2019-09-20 23:37:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-65db99849b" has successfully progressed.,LastUpdateTime:2019-09-20 23:37:27 +0000 UTC,LastTransitionTime:2019-09-20 23:37:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 20 23:37:28.669: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-6262 /apis/apps/v1/namespaces/deployment-6262/replicasets/test-cleanup-deployment-65db99849b 87333ce6-1113-425f-b154-ee4b8afe678b 26028 1 2019-09-20 23:37:24 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 1e0c0e8f-0c50-4ec2-9766-4d16b91a0b8b 0xc006cac407 0xc006cac408}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006cac468 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 20 23:37:28.676: INFO: Pod "test-cleanup-deployment-65db99849b-hm5wm" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-65db99849b-hm5wm test-cleanup-deployment-65db99849b- deployment-6262 /api/v1/namespaces/deployment-6262/pods/test-cleanup-deployment-65db99849b-hm5wm e9571f07-1281-4c9a-be6e-571fcf389c70 26027 0 2019-09-20 23:37:24 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-65db99849b 87333ce6-1113-425f-b154-ee4b8afe678b 0xc006cac807 0xc006cac808}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wctts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wctts,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wctts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 23:37:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 23:37:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 23:37:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 23:37:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.5.102,PodIP:10.40.0.3,StartTime:2019-09-20 23:37:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 23:37:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://319ee1c53f7812f41bd727d30e3180c78a7fc7e8c7a60270f8349ba151e40d30,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.40.0.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:37:28.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6262" for this suite.
Sep 20 23:37:34.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:37:34.739: INFO: namespace deployment-6262 deletion completed in 6.054956974s

• [SLOW TEST:15.170 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:37:34.739: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 23:37:34.770: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep 20 23:37:34.775: INFO: Number of nodes with available pods: 0
Sep 20 23:37:34.775: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep 20 23:37:34.786: INFO: Number of nodes with available pods: 0
Sep 20 23:37:34.786: INFO: Node controlplane-1 is running more than one daemon pod
Sep 20 23:37:35.790: INFO: Number of nodes with available pods: 0
Sep 20 23:37:35.790: INFO: Node controlplane-1 is running more than one daemon pod
Sep 20 23:37:36.793: INFO: Number of nodes with available pods: 1
Sep 20 23:37:36.793: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep 20 23:37:36.827: INFO: Number of nodes with available pods: 1
Sep 20 23:37:36.827: INFO: Number of running nodes: 0, number of available pods: 1
Sep 20 23:37:37.834: INFO: Number of nodes with available pods: 0
Sep 20 23:37:37.834: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep 20 23:37:37.856: INFO: Number of nodes with available pods: 0
Sep 20 23:37:37.856: INFO: Node controlplane-1 is running more than one daemon pod
Sep 20 23:37:38.858: INFO: Number of nodes with available pods: 0
Sep 20 23:37:38.858: INFO: Node controlplane-1 is running more than one daemon pod
Sep 20 23:37:39.879: INFO: Number of nodes with available pods: 0
Sep 20 23:37:39.879: INFO: Node controlplane-1 is running more than one daemon pod
Sep 20 23:37:40.862: INFO: Number of nodes with available pods: 0
Sep 20 23:37:40.862: INFO: Node controlplane-1 is running more than one daemon pod
Sep 20 23:37:41.858: INFO: Number of nodes with available pods: 0
Sep 20 23:37:41.858: INFO: Node controlplane-1 is running more than one daemon pod
Sep 20 23:37:42.865: INFO: Number of nodes with available pods: 0
Sep 20 23:37:42.865: INFO: Node controlplane-1 is running more than one daemon pod
Sep 20 23:37:43.864: INFO: Number of nodes with available pods: 0
Sep 20 23:37:43.864: INFO: Node controlplane-1 is running more than one daemon pod
Sep 20 23:37:44.858: INFO: Number of nodes with available pods: 0
Sep 20 23:37:44.858: INFO: Node controlplane-1 is running more than one daemon pod
Sep 20 23:37:45.858: INFO: Number of nodes with available pods: 0
Sep 20 23:37:45.859: INFO: Node controlplane-1 is running more than one daemon pod
Sep 20 23:37:46.862: INFO: Number of nodes with available pods: 0
Sep 20 23:37:46.862: INFO: Node controlplane-1 is running more than one daemon pod
Sep 20 23:37:47.864: INFO: Number of nodes with available pods: 1
Sep 20 23:37:47.864: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7272, will wait for the garbage collector to delete the pods
I0920 23:37:47.881371      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 23:37:47.881418      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 23:37:47.943: INFO: Deleting DaemonSet.extensions daemon-set took: 12.106682ms
Sep 20 23:37:48.255: INFO: Terminating DaemonSet.extensions daemon-set pods took: 311.714108ms
I0920 23:37:48.255372      16 controller_utils.go:810] Ignoring inactive pod daemonsets-7272/daemon-set-cf4mv in state Running, deletion time 2019-09-20 23:38:18 +0000 UTC
Sep 20 23:37:51.457: INFO: Number of nodes with available pods: 0
Sep 20 23:37:51.457: INFO: Number of running nodes: 0, number of available pods: 0
Sep 20 23:37:51.459: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7272/daemonsets","resourceVersion":"26153"},"items":null}

Sep 20 23:37:51.460: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7272/pods","resourceVersion":"26153"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:37:51.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7272" for this suite.
Sep 20 23:37:57.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:37:57.554: INFO: namespace daemonsets-7272 deletion completed in 6.080590293s

• [SLOW TEST:22.815 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:37:57.554: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:38:10.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5615" for this suite.
Sep 20 23:38:16.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:38:16.791: INFO: namespace resourcequota-5615 deletion completed in 6.069527604s

• [SLOW TEST:19.237 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:38:16.792: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:39:16.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8701" for this suite.
Sep 20 23:39:28.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:39:28.910: INFO: namespace container-probe-8701 deletion completed in 12.069369397s

• [SLOW TEST:72.119 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:39:28.913: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-cd704339-62d1-4c75-8095-dbc7d6665f61
STEP: Creating a pod to test consume secrets
Sep 20 23:39:28.943: INFO: Waiting up to 5m0s for pod "pod-secrets-8a9db5d8-1ee6-4962-8f4f-42cc7b53aba5" in namespace "secrets-7175" to be "success or failure"
Sep 20 23:39:28.950: INFO: Pod "pod-secrets-8a9db5d8-1ee6-4962-8f4f-42cc7b53aba5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.659257ms
Sep 20 23:39:30.953: INFO: Pod "pod-secrets-8a9db5d8-1ee6-4962-8f4f-42cc7b53aba5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009430471s
Sep 20 23:39:32.960: INFO: Pod "pod-secrets-8a9db5d8-1ee6-4962-8f4f-42cc7b53aba5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016825339s
STEP: Saw pod success
Sep 20 23:39:32.960: INFO: Pod "pod-secrets-8a9db5d8-1ee6-4962-8f4f-42cc7b53aba5" satisfied condition "success or failure"
Sep 20 23:39:32.966: INFO: Trying to get logs from node worker-2 pod pod-secrets-8a9db5d8-1ee6-4962-8f4f-42cc7b53aba5 container secret-volume-test: <nil>
STEP: delete the pod
Sep 20 23:39:33.021: INFO: Waiting for pod pod-secrets-8a9db5d8-1ee6-4962-8f4f-42cc7b53aba5 to disappear
Sep 20 23:39:33.023: INFO: Pod pod-secrets-8a9db5d8-1ee6-4962-8f4f-42cc7b53aba5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:39:33.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7175" for this suite.
Sep 20 23:39:39.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:39:39.105: INFO: namespace secrets-7175 deletion completed in 6.079535774s

• [SLOW TEST:10.192 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:39:39.105: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-636193ac-6c81-49b7-8570-e4695a76e34d
STEP: Creating a pod to test consume secrets
Sep 20 23:39:39.136: INFO: Waiting up to 5m0s for pod "pod-secrets-63a6ef42-38b1-4d9f-a5bc-e3224067004c" in namespace "secrets-1014" to be "success or failure"
Sep 20 23:39:39.140: INFO: Pod "pod-secrets-63a6ef42-38b1-4d9f-a5bc-e3224067004c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.639218ms
Sep 20 23:39:41.145: INFO: Pod "pod-secrets-63a6ef42-38b1-4d9f-a5bc-e3224067004c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009060104s
Sep 20 23:39:43.151: INFO: Pod "pod-secrets-63a6ef42-38b1-4d9f-a5bc-e3224067004c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014591632s
STEP: Saw pod success
Sep 20 23:39:43.151: INFO: Pod "pod-secrets-63a6ef42-38b1-4d9f-a5bc-e3224067004c" satisfied condition "success or failure"
Sep 20 23:39:43.156: INFO: Trying to get logs from node worker-2 pod pod-secrets-63a6ef42-38b1-4d9f-a5bc-e3224067004c container secret-env-test: <nil>
STEP: delete the pod
Sep 20 23:39:43.195: INFO: Waiting for pod pod-secrets-63a6ef42-38b1-4d9f-a5bc-e3224067004c to disappear
Sep 20 23:39:43.197: INFO: Pod pod-secrets-63a6ef42-38b1-4d9f-a5bc-e3224067004c no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:39:43.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1014" for this suite.
Sep 20 23:39:49.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:39:49.287: INFO: namespace secrets-1014 deletion completed in 6.086911009s

• [SLOW TEST:10.181 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:39:49.287: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:39:49.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1014" for this suite.
Sep 20 23:39:55.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:39:55.387: INFO: namespace resourcequota-1014 deletion completed in 6.058979116s

• [SLOW TEST:6.101 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:39:55.389: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-69460035-a18e-4cb7-a1f7-0dfef056950a in namespace container-probe-255
Sep 20 23:39:57.425: INFO: Started pod test-webserver-69460035-a18e-4cb7-a1f7-0dfef056950a in namespace container-probe-255
STEP: checking the pod's current state and verifying that restartCount is present
Sep 20 23:39:57.427: INFO: Initial restart count of pod test-webserver-69460035-a18e-4cb7-a1f7-0dfef056950a is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:43:58.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-255" for this suite.
Sep 20 23:44:04.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:44:04.167: INFO: namespace container-probe-255 deletion completed in 6.05331271s

• [SLOW TEST:248.779 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:44:04.168: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 23:44:04.188: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:44:04.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-561" for this suite.
Sep 20 23:44:10.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:44:10.296: INFO: namespace custom-resource-definition-561 deletion completed in 6.070263204s

• [SLOW TEST:6.129 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:44:10.298: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Sep 20 23:44:10.333: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:44:14.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1896" for this suite.
Sep 20 23:44:26.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:44:26.770: INFO: namespace init-container-1896 deletion completed in 12.059567329s

• [SLOW TEST:16.472 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:44:26.770: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 20 23:44:26.804: INFO: Waiting up to 5m0s for pod "pod-04ddf5fd-6828-4d0a-a977-8ac6b73501ad" in namespace "emptydir-6511" to be "success or failure"
Sep 20 23:44:26.808: INFO: Pod "pod-04ddf5fd-6828-4d0a-a977-8ac6b73501ad": Phase="Pending", Reason="", readiness=false. Elapsed: 3.471779ms
Sep 20 23:44:28.813: INFO: Pod "pod-04ddf5fd-6828-4d0a-a977-8ac6b73501ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008768565s
Sep 20 23:44:30.816: INFO: Pod "pod-04ddf5fd-6828-4d0a-a977-8ac6b73501ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011140899s
STEP: Saw pod success
Sep 20 23:44:30.816: INFO: Pod "pod-04ddf5fd-6828-4d0a-a977-8ac6b73501ad" satisfied condition "success or failure"
Sep 20 23:44:30.818: INFO: Trying to get logs from node worker-2 pod pod-04ddf5fd-6828-4d0a-a977-8ac6b73501ad container test-container: <nil>
STEP: delete the pod
Sep 20 23:44:30.844: INFO: Waiting for pod pod-04ddf5fd-6828-4d0a-a977-8ac6b73501ad to disappear
Sep 20 23:44:30.846: INFO: Pod pod-04ddf5fd-6828-4d0a-a977-8ac6b73501ad no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:44:30.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6511" for this suite.
Sep 20 23:44:36.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:44:36.918: INFO: namespace emptydir-6511 deletion completed in 6.069886494s

• [SLOW TEST:10.149 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:44:36.920: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 23:44:37.900: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 23:44:39.917: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704619877, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704619877, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704619877, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704619877, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 23:44:42.940: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 23:44:42.946: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7956-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:44:43.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4051" for this suite.
Sep 20 23:44:49.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:44:49.711: INFO: namespace webhook-4051 deletion completed in 6.084642774s
STEP: Destroying namespace "webhook-4051-markers" for this suite.
Sep 20 23:44:55.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:44:55.767: INFO: namespace webhook-4051-markers deletion completed in 6.055878789s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.856 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:44:55.776: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-155/configmap-test-1caca266-492d-47e6-85c1-f2ca2f22c8a0
STEP: Creating a pod to test consume configMaps
Sep 20 23:44:55.815: INFO: Waiting up to 5m0s for pod "pod-configmaps-75c6e8f4-80b4-424c-af8a-e5f312b991f4" in namespace "configmap-155" to be "success or failure"
Sep 20 23:44:55.817: INFO: Pod "pod-configmaps-75c6e8f4-80b4-424c-af8a-e5f312b991f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.850839ms
Sep 20 23:44:57.824: INFO: Pod "pod-configmaps-75c6e8f4-80b4-424c-af8a-e5f312b991f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008575106s
STEP: Saw pod success
Sep 20 23:44:57.824: INFO: Pod "pod-configmaps-75c6e8f4-80b4-424c-af8a-e5f312b991f4" satisfied condition "success or failure"
Sep 20 23:44:57.829: INFO: Trying to get logs from node worker-2 pod pod-configmaps-75c6e8f4-80b4-424c-af8a-e5f312b991f4 container env-test: <nil>
STEP: delete the pod
Sep 20 23:44:57.874: INFO: Waiting for pod pod-configmaps-75c6e8f4-80b4-424c-af8a-e5f312b991f4 to disappear
Sep 20 23:44:57.876: INFO: Pod pod-configmaps-75c6e8f4-80b4-424c-af8a-e5f312b991f4 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:44:57.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-155" for this suite.
Sep 20 23:45:03.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:45:03.931: INFO: namespace configmap-155 deletion completed in 6.047929666s

• [SLOW TEST:8.156 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:45:03.932: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 20 23:45:03.965: INFO: Waiting up to 5m0s for pod "pod-ae013189-9812-4a0e-abfc-4edbed58712f" in namespace "emptydir-6396" to be "success or failure"
Sep 20 23:45:03.969: INFO: Pod "pod-ae013189-9812-4a0e-abfc-4edbed58712f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.883437ms
Sep 20 23:45:05.971: INFO: Pod "pod-ae013189-9812-4a0e-abfc-4edbed58712f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005925684s
Sep 20 23:45:07.976: INFO: Pod "pod-ae013189-9812-4a0e-abfc-4edbed58712f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010788424s
STEP: Saw pod success
Sep 20 23:45:07.976: INFO: Pod "pod-ae013189-9812-4a0e-abfc-4edbed58712f" satisfied condition "success or failure"
Sep 20 23:45:07.981: INFO: Trying to get logs from node worker-2 pod pod-ae013189-9812-4a0e-abfc-4edbed58712f container test-container: <nil>
STEP: delete the pod
Sep 20 23:45:08.009: INFO: Waiting for pod pod-ae013189-9812-4a0e-abfc-4edbed58712f to disappear
Sep 20 23:45:08.012: INFO: Pod pod-ae013189-9812-4a0e-abfc-4edbed58712f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:45:08.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6396" for this suite.
Sep 20 23:45:14.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:45:14.120: INFO: namespace emptydir-6396 deletion completed in 6.103971687s

• [SLOW TEST:10.188 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:45:14.120: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-kz2w
STEP: Creating a pod to test atomic-volume-subpath
Sep 20 23:45:14.154: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-kz2w" in namespace "subpath-4784" to be "success or failure"
Sep 20 23:45:14.161: INFO: Pod "pod-subpath-test-configmap-kz2w": Phase="Pending", Reason="", readiness=false. Elapsed: 6.592645ms
Sep 20 23:45:16.165: INFO: Pod "pod-subpath-test-configmap-kz2w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010916468s
Sep 20 23:45:18.168: INFO: Pod "pod-subpath-test-configmap-kz2w": Phase="Running", Reason="", readiness=true. Elapsed: 4.013430777s
Sep 20 23:45:20.176: INFO: Pod "pod-subpath-test-configmap-kz2w": Phase="Running", Reason="", readiness=true. Elapsed: 6.021227937s
Sep 20 23:45:22.184: INFO: Pod "pod-subpath-test-configmap-kz2w": Phase="Running", Reason="", readiness=true. Elapsed: 8.029984939s
Sep 20 23:45:24.190: INFO: Pod "pod-subpath-test-configmap-kz2w": Phase="Running", Reason="", readiness=true. Elapsed: 10.036166815s
Sep 20 23:45:26.192: INFO: Pod "pod-subpath-test-configmap-kz2w": Phase="Running", Reason="", readiness=true. Elapsed: 12.038143246s
Sep 20 23:45:28.195: INFO: Pod "pod-subpath-test-configmap-kz2w": Phase="Running", Reason="", readiness=true. Elapsed: 14.040894537s
Sep 20 23:45:30.200: INFO: Pod "pod-subpath-test-configmap-kz2w": Phase="Running", Reason="", readiness=true. Elapsed: 16.045875478s
Sep 20 23:45:32.205: INFO: Pod "pod-subpath-test-configmap-kz2w": Phase="Running", Reason="", readiness=true. Elapsed: 18.050954719s
Sep 20 23:45:34.211: INFO: Pod "pod-subpath-test-configmap-kz2w": Phase="Running", Reason="", readiness=true. Elapsed: 20.057035092s
Sep 20 23:45:36.229: INFO: Pod "pod-subpath-test-configmap-kz2w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.074603092s
STEP: Saw pod success
Sep 20 23:45:36.229: INFO: Pod "pod-subpath-test-configmap-kz2w" satisfied condition "success or failure"
Sep 20 23:45:36.234: INFO: Trying to get logs from node worker-2 pod pod-subpath-test-configmap-kz2w container test-container-subpath-configmap-kz2w: <nil>
STEP: delete the pod
Sep 20 23:45:36.267: INFO: Waiting for pod pod-subpath-test-configmap-kz2w to disappear
Sep 20 23:45:36.269: INFO: Pod pod-subpath-test-configmap-kz2w no longer exists
STEP: Deleting pod pod-subpath-test-configmap-kz2w
Sep 20 23:45:36.269: INFO: Deleting pod "pod-subpath-test-configmap-kz2w" in namespace "subpath-4784"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:45:36.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4784" for this suite.
Sep 20 23:45:42.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:45:42.372: INFO: namespace subpath-4784 deletion completed in 6.098901488s

• [SLOW TEST:28.252 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:45:42.372: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 23:45:42.806: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 23:45:44.816: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704619942, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704619942, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704619942, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704619942, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 23:45:46.818: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704619942, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704619942, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704619942, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704619942, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 23:45:49.845: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Sep 20 23:45:49.857: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:45:49.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8409" for this suite.
Sep 20 23:45:55.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:45:55.940: INFO: namespace webhook-8409 deletion completed in 6.070124315s
STEP: Destroying namespace "webhook-8409-markers" for this suite.
Sep 20 23:46:01.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:46:02.048: INFO: namespace webhook-8409-markers deletion completed in 6.107821671s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:19.685 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:46:02.057: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 20 23:46:02.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-3476'
Sep 20 23:46:02.142: INFO: stderr: ""
Sep 20 23:46:02.142: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
I0920 23:46:02.142264      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 23:46:02.142288      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
STEP: verifying the pod e2e-test-httpd-pod was created
Sep 20 23:46:07.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 get pod e2e-test-httpd-pod --namespace=kubectl-3476 -o json'
Sep 20 23:46:07.281: INFO: stderr: ""
Sep 20 23:46:07.281: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-09-20T23:46:02Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3476\",\n        \"resourceVersion\": \"27390\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-3476/pods/e2e-test-httpd-pod\",\n        \"uid\": \"eb18d1bc-f1fd-4f2b-ac14-cda17e929b95\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-wcbbb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker-2\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-wcbbb\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-wcbbb\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-20T23:46:02Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-20T23:46:04Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-20T23:46:04Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-20T23:46:02Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://de72f948eccb226a687f3d004f4c6c19b88ca92b7c106465f0d80bf6802cc46a\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-09-20T23:46:03Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.5.102\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.40.0.3\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.40.0.3\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-09-20T23:46:02Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep 20 23:46:07.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 replace -f - --namespace=kubectl-3476'
Sep 20 23:46:07.425: INFO: stderr: ""
Sep 20 23:46:07.425: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Sep 20 23:46:07.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 delete pods e2e-test-httpd-pod --namespace=kubectl-3476'
Sep 20 23:46:09.683: INFO: stderr: ""
Sep 20 23:46:09.683: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:46:09.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3476" for this suite.
Sep 20 23:46:15.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:46:15.740: INFO: namespace kubectl-3476 deletion completed in 6.052697562s

• [SLOW TEST:13.683 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:46:15.740: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 23:46:15.769: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5603b382-02f6-4114-b266-9e20723320b7" in namespace "projected-3182" to be "success or failure"
Sep 20 23:46:15.771: INFO: Pod "downwardapi-volume-5603b382-02f6-4114-b266-9e20723320b7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.982134ms
Sep 20 23:46:17.781: INFO: Pod "downwardapi-volume-5603b382-02f6-4114-b266-9e20723320b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011532765s
Sep 20 23:46:19.787: INFO: Pod "downwardapi-volume-5603b382-02f6-4114-b266-9e20723320b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017329214s
STEP: Saw pod success
Sep 20 23:46:19.787: INFO: Pod "downwardapi-volume-5603b382-02f6-4114-b266-9e20723320b7" satisfied condition "success or failure"
Sep 20 23:46:19.792: INFO: Trying to get logs from node worker-2 pod downwardapi-volume-5603b382-02f6-4114-b266-9e20723320b7 container client-container: <nil>
STEP: delete the pod
Sep 20 23:46:19.831: INFO: Waiting for pod downwardapi-volume-5603b382-02f6-4114-b266-9e20723320b7 to disappear
Sep 20 23:46:19.837: INFO: Pod downwardapi-volume-5603b382-02f6-4114-b266-9e20723320b7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:46:19.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3182" for this suite.
Sep 20 23:46:25.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:46:25.934: INFO: namespace projected-3182 deletion completed in 6.091787683s

• [SLOW TEST:10.194 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:46:25.936: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-7bd5dcb5-e747-450f-bcbf-39522212be9d
STEP: Creating a pod to test consume configMaps
Sep 20 23:46:26.012: INFO: Waiting up to 5m0s for pod "pod-configmaps-c2956dea-a049-4338-8a0d-247f40b63384" in namespace "configmap-6838" to be "success or failure"
Sep 20 23:46:26.014: INFO: Pod "pod-configmaps-c2956dea-a049-4338-8a0d-247f40b63384": Phase="Pending", Reason="", readiness=false. Elapsed: 2.506964ms
Sep 20 23:46:28.019: INFO: Pod "pod-configmaps-c2956dea-a049-4338-8a0d-247f40b63384": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007464648s
STEP: Saw pod success
Sep 20 23:46:28.019: INFO: Pod "pod-configmaps-c2956dea-a049-4338-8a0d-247f40b63384" satisfied condition "success or failure"
Sep 20 23:46:28.022: INFO: Trying to get logs from node worker-2 pod pod-configmaps-c2956dea-a049-4338-8a0d-247f40b63384 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 23:46:28.041: INFO: Waiting for pod pod-configmaps-c2956dea-a049-4338-8a0d-247f40b63384 to disappear
Sep 20 23:46:28.042: INFO: Pod pod-configmaps-c2956dea-a049-4338-8a0d-247f40b63384 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:46:28.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6838" for this suite.
Sep 20 23:46:34.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:46:34.105: INFO: namespace configmap-6838 deletion completed in 6.059979428s

• [SLOW TEST:8.169 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:46:34.105: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Sep 20 23:46:34.127: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:46:38.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2051" for this suite.
Sep 20 23:46:44.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:46:44.229: INFO: namespace init-container-2051 deletion completed in 6.068744651s

• [SLOW TEST:10.124 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:46:44.229: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2070
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2070
STEP: creating replication controller externalsvc in namespace services-2070
I0920 23:46:44.280300      16 runners.go:184] Created replication controller with name: externalsvc, namespace: services-2070, replica count: 2
I0920 23:46:44.281537      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 23:46:44.281566      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 23:46:47.372495      16 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Sep 20 23:46:47.407: INFO: Creating new exec pod
Sep 20 23:46:49.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-786974626 exec --namespace=services-2070 execpodfl2jv -- /bin/sh -x -c nslookup clusterip-service'
Sep 20 23:46:49.594: INFO: stderr: "+ nslookup clusterip-service\n"
Sep 20 23:46:49.594: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-2070.svc.cluster.local\tcanonical name = externalsvc.services-2070.svc.cluster.local.\nName:\texternalsvc.services-2070.svc.cluster.local\nAddress: 10.101.63.100\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2070, will wait for the garbage collector to delete the pods
I0920 23:46:49.595900      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 23:46:49.595919      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 23:46:49.657: INFO: Deleting ReplicationController externalsvc took: 3.081668ms
Sep 20 23:46:49.958: INFO: Terminating ReplicationController externalsvc pods took: 300.266557ms
I0920 23:46:49.958103      16 controller_utils.go:810] Ignoring inactive pod services-2070/externalsvc-q8sr4 in state Running, deletion time 2019-09-20 23:46:50 +0000 UTC
I0920 23:46:49.958142      16 controller_utils.go:810] Ignoring inactive pod services-2070/externalsvc-w8xxm in state Running, deletion time 2019-09-20 23:46:50 +0000 UTC
Sep 20 23:46:53.889: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:46:53.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2070" for this suite.
Sep 20 23:46:59.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:46:59.958: INFO: namespace services-2070 deletion completed in 6.051334234s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:15.729 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:46:59.958: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 20 23:46:59.989: INFO: Waiting up to 5m0s for pod "pod-29d8c888-8d80-4962-b674-a1a4574efdc2" in namespace "emptydir-6376" to be "success or failure"
Sep 20 23:46:59.991: INFO: Pod "pod-29d8c888-8d80-4962-b674-a1a4574efdc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.199502ms
Sep 20 23:47:01.995: INFO: Pod "pod-29d8c888-8d80-4962-b674-a1a4574efdc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005853905s
STEP: Saw pod success
Sep 20 23:47:01.995: INFO: Pod "pod-29d8c888-8d80-4962-b674-a1a4574efdc2" satisfied condition "success or failure"
Sep 20 23:47:02.000: INFO: Trying to get logs from node worker-2 pod pod-29d8c888-8d80-4962-b674-a1a4574efdc2 container test-container: <nil>
STEP: delete the pod
Sep 20 23:47:02.022: INFO: Waiting for pod pod-29d8c888-8d80-4962-b674-a1a4574efdc2 to disappear
Sep 20 23:47:02.027: INFO: Pod pod-29d8c888-8d80-4962-b674-a1a4574efdc2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:47:02.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6376" for this suite.
Sep 20 23:47:08.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:47:08.159: INFO: namespace emptydir-6376 deletion completed in 6.126488926s

• [SLOW TEST:8.201 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:47:08.160: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 23:47:08.195: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a4b80a6c-a88a-4ff6-a556-587a21dba0ca" in namespace "downward-api-7596" to be "success or failure"
Sep 20 23:47:08.200: INFO: Pod "downwardapi-volume-a4b80a6c-a88a-4ff6-a556-587a21dba0ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.984276ms
Sep 20 23:47:10.204: INFO: Pod "downwardapi-volume-a4b80a6c-a88a-4ff6-a556-587a21dba0ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008312537s
STEP: Saw pod success
Sep 20 23:47:10.204: INFO: Pod "downwardapi-volume-a4b80a6c-a88a-4ff6-a556-587a21dba0ca" satisfied condition "success or failure"
Sep 20 23:47:10.206: INFO: Trying to get logs from node worker-2 pod downwardapi-volume-a4b80a6c-a88a-4ff6-a556-587a21dba0ca container client-container: <nil>
STEP: delete the pod
Sep 20 23:47:10.221: INFO: Waiting for pod downwardapi-volume-a4b80a6c-a88a-4ff6-a556-587a21dba0ca to disappear
Sep 20 23:47:10.222: INFO: Pod downwardapi-volume-a4b80a6c-a88a-4ff6-a556-587a21dba0ca no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:47:10.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7596" for this suite.
Sep 20 23:47:16.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:47:16.312: INFO: namespace downward-api-7596 deletion completed in 6.08770702s

• [SLOW TEST:8.153 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 23:47:16.313: INFO: >>> kubeConfig: /tmp/kubeconfig-786974626
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 20 23:47:16.355: INFO: Number of nodes with available pods: 0
Sep 20 23:47:16.355: INFO: Node controlplane-1 is running more than one daemon pod
Sep 20 23:47:17.361: INFO: Number of nodes with available pods: 0
Sep 20 23:47:17.361: INFO: Node controlplane-1 is running more than one daemon pod
Sep 20 23:47:18.361: INFO: Number of nodes with available pods: 2
Sep 20 23:47:18.361: INFO: Node worker-2 is running more than one daemon pod
Sep 20 23:47:19.360: INFO: Number of nodes with available pods: 3
Sep 20 23:47:19.360: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep 20 23:47:19.372: INFO: Number of nodes with available pods: 2
Sep 20 23:47:19.372: INFO: Node worker-2 is running more than one daemon pod
Sep 20 23:47:20.386: INFO: Number of nodes with available pods: 2
Sep 20 23:47:20.386: INFO: Node worker-2 is running more than one daemon pod
Sep 20 23:47:21.377: INFO: Number of nodes with available pods: 2
Sep 20 23:47:21.377: INFO: Node worker-2 is running more than one daemon pod
Sep 20 23:47:22.395: INFO: Number of nodes with available pods: 2
Sep 20 23:47:22.396: INFO: Node worker-2 is running more than one daemon pod
Sep 20 23:47:23.387: INFO: Number of nodes with available pods: 2
Sep 20 23:47:23.388: INFO: Node worker-2 is running more than one daemon pod
Sep 20 23:47:24.377: INFO: Number of nodes with available pods: 2
Sep 20 23:47:24.377: INFO: Node worker-2 is running more than one daemon pod
Sep 20 23:47:25.387: INFO: Number of nodes with available pods: 2
Sep 20 23:47:25.387: INFO: Node worker-2 is running more than one daemon pod
Sep 20 23:47:26.378: INFO: Number of nodes with available pods: 3
Sep 20 23:47:26.378: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4627, will wait for the garbage collector to delete the pods
I0920 23:47:26.382437      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 23:47:26.382453      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 23:47:26.442: INFO: Deleting DaemonSet.extensions daemon-set took: 3.772364ms
I0920 23:47:26.743147      16 controller_utils.go:810] Ignoring inactive pod daemonsets-4627/daemon-set-vpn4w in state Running, deletion time 2019-09-20 23:47:56 +0000 UTC
I0920 23:47:26.743224      16 controller_utils.go:810] Ignoring inactive pod daemonsets-4627/daemon-set-qtpn9 in state Running, deletion time 2019-09-20 23:47:56 +0000 UTC
I0920 23:47:26.743250      16 controller_utils.go:810] Ignoring inactive pod daemonsets-4627/daemon-set-wf4w6 in state Running, deletion time 2019-09-20 23:47:56 +0000 UTC
Sep 20 23:47:26.743: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.447675ms
Sep 20 23:47:35.445: INFO: Number of nodes with available pods: 0
Sep 20 23:47:35.445: INFO: Number of running nodes: 0, number of available pods: 0
Sep 20 23:47:35.446: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4627/daemonsets","resourceVersion":"27849"},"items":null}

Sep 20 23:47:35.447: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4627/pods","resourceVersion":"27849"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 23:47:35.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4627" for this suite.
Sep 20 23:47:41.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 23:47:41.508: INFO: namespace daemonsets-4627 deletion completed in 6.053839768s

• [SLOW TEST:25.196 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSep 20 23:47:41.509: INFO: Running AfterSuite actions on all nodes
Sep 20 23:47:41.509: INFO: Running AfterSuite actions on node 1
Sep 20 23:47:41.509: INFO: Skipping dumping logs from cluster

Ran 274 of 4897 Specs in 7253.742 seconds
SUCCESS! -- 274 Passed | 0 Failed | 0 Pending | 4623 Skipped
PASS

Ginkgo ran 1 suite in 2h0m54.75372619s
Test Suite Passed
