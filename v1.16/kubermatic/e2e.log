I0920 00:35:07.744528      16 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-342770709
I0920 00:35:07.744669      16 e2e.go:92] Starting e2e run "88a73f13-b6b0-4120-abe3-f39ba7ab759e" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1568939705 - Will randomize all specs
Will run 276 of 4897 specs

Sep 20 00:35:07.797: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 00:35:07.800: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep 20 00:35:07.830: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep 20 00:35:07.894: INFO: The status of Pod canal-pwz75 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:07.894: INFO: The status of Pod coredns-57f944bd9f-vlw9n is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:07.894: INFO: The status of Pod kubernetes-dashboard-7d5fb85f7f-t28jk is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:07.894: INFO: The status of Pod node-exporter-rtkgz is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:07.894: INFO: 20 / 24 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep 20 00:35:07.894: INFO: expected 4 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Sep 20 00:35:07.894: INFO: POD                                    NODE                           PHASE    GRACE  CONDITIONS
Sep 20 00:35:07.894: INFO: canal-pwz75                            worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [calico-node kube-flannel]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [calico-node kube-flannel]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:33:51 +0000 UTC  }]
Sep 20 00:35:07.894: INFO: coredns-57f944bd9f-vlw9n               worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  }]
Sep 20 00:35:07.894: INFO: kubernetes-dashboard-7d5fb85f7f-t28jk  worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [kubernetes-dashboard]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [kubernetes-dashboard]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  }]
Sep 20 00:35:07.894: INFO: node-exporter-rtkgz                    worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:33:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [node-exporter kube-rbac-proxy]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [node-exporter kube-rbac-proxy]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:33:51 +0000 UTC  }]
Sep 20 00:35:07.894: INFO: 
Sep 20 00:35:09.935: INFO: The status of Pod canal-pwz75 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:09.935: INFO: The status of Pod coredns-57f944bd9f-vlw9n is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:09.935: INFO: The status of Pod kubernetes-dashboard-7d5fb85f7f-t28jk is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:09.935: INFO: The status of Pod node-exporter-rtkgz is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:09.935: INFO: 20 / 24 pods in namespace 'kube-system' are running and ready (2 seconds elapsed)
Sep 20 00:35:09.935: INFO: expected 4 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Sep 20 00:35:09.935: INFO: POD                                    NODE                           PHASE    GRACE  CONDITIONS
Sep 20 00:35:09.935: INFO: canal-pwz75                            worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [calico-node kube-flannel]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [calico-node kube-flannel]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:33:51 +0000 UTC  }]
Sep 20 00:35:09.935: INFO: coredns-57f944bd9f-vlw9n               worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  }]
Sep 20 00:35:09.935: INFO: kubernetes-dashboard-7d5fb85f7f-t28jk  worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [kubernetes-dashboard]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [kubernetes-dashboard]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  }]
Sep 20 00:35:09.935: INFO: node-exporter-rtkgz                    worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:33:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [node-exporter kube-rbac-proxy]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [node-exporter kube-rbac-proxy]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:33:51 +0000 UTC  }]
Sep 20 00:35:09.935: INFO: 
Sep 20 00:35:11.935: INFO: The status of Pod canal-pwz75 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:11.935: INFO: The status of Pod coredns-57f944bd9f-vlw9n is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:11.935: INFO: The status of Pod kubernetes-dashboard-7d5fb85f7f-t28jk is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:11.935: INFO: The status of Pod node-exporter-rtkgz is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:11.935: INFO: 20 / 24 pods in namespace 'kube-system' are running and ready (4 seconds elapsed)
Sep 20 00:35:11.935: INFO: expected 4 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Sep 20 00:35:11.935: INFO: POD                                    NODE                           PHASE    GRACE  CONDITIONS
Sep 20 00:35:11.935: INFO: canal-pwz75                            worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [calico-node kube-flannel]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [calico-node kube-flannel]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:33:51 +0000 UTC  }]
Sep 20 00:35:11.935: INFO: coredns-57f944bd9f-vlw9n               worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  }]
Sep 20 00:35:11.935: INFO: kubernetes-dashboard-7d5fb85f7f-t28jk  worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [kubernetes-dashboard]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [kubernetes-dashboard]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  }]
Sep 20 00:35:11.935: INFO: node-exporter-rtkgz                    worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:33:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [node-exporter kube-rbac-proxy]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [node-exporter kube-rbac-proxy]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:33:51 +0000 UTC  }]
Sep 20 00:35:11.935: INFO: 
Sep 20 00:35:13.938: INFO: The status of Pod canal-pwz75 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:13.938: INFO: The status of Pod coredns-57f944bd9f-vlw9n is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:13.938: INFO: The status of Pod kubernetes-dashboard-7d5fb85f7f-t28jk is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:13.938: INFO: The status of Pod node-exporter-rtkgz is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:13.938: INFO: 20 / 24 pods in namespace 'kube-system' are running and ready (6 seconds elapsed)
Sep 20 00:35:13.938: INFO: expected 4 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Sep 20 00:35:13.938: INFO: POD                                    NODE                           PHASE    GRACE  CONDITIONS
Sep 20 00:35:13.938: INFO: canal-pwz75                            worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [calico-node kube-flannel]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [calico-node kube-flannel]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:33:51 +0000 UTC  }]
Sep 20 00:35:13.939: INFO: coredns-57f944bd9f-vlw9n               worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  }]
Sep 20 00:35:13.939: INFO: kubernetes-dashboard-7d5fb85f7f-t28jk  worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [kubernetes-dashboard]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [kubernetes-dashboard]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  }]
Sep 20 00:35:13.939: INFO: node-exporter-rtkgz                    worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:33:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [node-exporter kube-rbac-proxy]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [node-exporter kube-rbac-proxy]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:33:51 +0000 UTC  }]
Sep 20 00:35:13.939: INFO: 
Sep 20 00:35:15.935: INFO: The status of Pod canal-pwz75 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:15.935: INFO: The status of Pod coredns-57f944bd9f-vlw9n is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:15.935: INFO: The status of Pod kubernetes-dashboard-7d5fb85f7f-t28jk is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:15.935: INFO: The status of Pod node-exporter-rtkgz is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:15.936: INFO: 20 / 24 pods in namespace 'kube-system' are running and ready (8 seconds elapsed)
Sep 20 00:35:15.936: INFO: expected 4 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Sep 20 00:35:15.936: INFO: POD                                    NODE                           PHASE    GRACE  CONDITIONS
Sep 20 00:35:15.936: INFO: canal-pwz75                            worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [calico-node kube-flannel]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [calico-node kube-flannel]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:33:51 +0000 UTC  }]
Sep 20 00:35:15.936: INFO: coredns-57f944bd9f-vlw9n               worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  }]
Sep 20 00:35:15.936: INFO: kubernetes-dashboard-7d5fb85f7f-t28jk  worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [kubernetes-dashboard]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [kubernetes-dashboard]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  }]
Sep 20 00:35:15.936: INFO: node-exporter-rtkgz                    worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:33:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [node-exporter kube-rbac-proxy]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [node-exporter kube-rbac-proxy]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:33:51 +0000 UTC  }]
Sep 20 00:35:15.936: INFO: 
Sep 20 00:35:17.936: INFO: The status of Pod canal-pwz75 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:17.936: INFO: The status of Pod coredns-57f944bd9f-vlw9n is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:17.937: INFO: The status of Pod kubernetes-dashboard-7d5fb85f7f-t28jk is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:17.937: INFO: 21 / 24 pods in namespace 'kube-system' are running and ready (10 seconds elapsed)
Sep 20 00:35:17.937: INFO: expected 4 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Sep 20 00:35:17.937: INFO: POD                                    NODE                           PHASE    GRACE  CONDITIONS
Sep 20 00:35:17.937: INFO: canal-pwz75                            worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [calico-node kube-flannel]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [calico-node kube-flannel]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:33:51 +0000 UTC  }]
Sep 20 00:35:17.937: INFO: coredns-57f944bd9f-vlw9n               worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  }]
Sep 20 00:35:17.937: INFO: kubernetes-dashboard-7d5fb85f7f-t28jk  worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [kubernetes-dashboard]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [kubernetes-dashboard]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  }]
Sep 20 00:35:17.937: INFO: 
Sep 20 00:35:19.933: INFO: The status of Pod canal-pwz75 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:19.933: INFO: The status of Pod coredns-57f944bd9f-vlw9n is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:19.933: INFO: The status of Pod kubernetes-dashboard-7d5fb85f7f-t28jk is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:19.933: INFO: 21 / 24 pods in namespace 'kube-system' are running and ready (12 seconds elapsed)
Sep 20 00:35:19.933: INFO: expected 4 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Sep 20 00:35:19.933: INFO: POD                                    NODE                           PHASE    GRACE  CONDITIONS
Sep 20 00:35:19.933: INFO: canal-pwz75                            worker-wqshf-7859ffd555-kqpmz  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:33:51 +0000 UTC  }]
Sep 20 00:35:19.934: INFO: coredns-57f944bd9f-vlw9n               worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  }]
Sep 20 00:35:19.934: INFO: kubernetes-dashboard-7d5fb85f7f-t28jk  worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [kubernetes-dashboard]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [kubernetes-dashboard]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  }]
Sep 20 00:35:19.934: INFO: 
Sep 20 00:35:21.931: INFO: The status of Pod canal-pwz75 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:21.931: INFO: The status of Pod coredns-57f944bd9f-vlw9n is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:21.931: INFO: The status of Pod kubernetes-dashboard-7d5fb85f7f-t28jk is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:21.931: INFO: 21 / 24 pods in namespace 'kube-system' are running and ready (14 seconds elapsed)
Sep 20 00:35:21.931: INFO: expected 4 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Sep 20 00:35:21.931: INFO: POD                                    NODE                           PHASE    GRACE  CONDITIONS
Sep 20 00:35:21.931: INFO: canal-pwz75                            worker-wqshf-7859ffd555-kqpmz  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:33:51 +0000 UTC  }]
Sep 20 00:35:21.931: INFO: coredns-57f944bd9f-vlw9n               worker-wqshf-7859ffd555-kqpmz  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  }]
Sep 20 00:35:21.931: INFO: kubernetes-dashboard-7d5fb85f7f-t28jk  worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [kubernetes-dashboard]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [kubernetes-dashboard]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  }]
Sep 20 00:35:21.931: INFO: 
Sep 20 00:35:23.933: INFO: The status of Pod canal-pwz75 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:23.933: INFO: The status of Pod coredns-57f944bd9f-vlw9n is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:23.933: INFO: The status of Pod kubernetes-dashboard-7d5fb85f7f-t28jk is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:23.933: INFO: 21 / 24 pods in namespace 'kube-system' are running and ready (16 seconds elapsed)
Sep 20 00:35:23.933: INFO: expected 4 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Sep 20 00:35:23.933: INFO: POD                                    NODE                           PHASE    GRACE  CONDITIONS
Sep 20 00:35:23.933: INFO: canal-pwz75                            worker-wqshf-7859ffd555-kqpmz  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:33:51 +0000 UTC  }]
Sep 20 00:35:23.934: INFO: coredns-57f944bd9f-vlw9n               worker-wqshf-7859ffd555-kqpmz  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  }]
Sep 20 00:35:23.934: INFO: kubernetes-dashboard-7d5fb85f7f-t28jk  worker-wqshf-7859ffd555-kqpmz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [kubernetes-dashboard]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [kubernetes-dashboard]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  }]
Sep 20 00:35:23.934: INFO: 
Sep 20 00:35:25.932: INFO: The status of Pod coredns-57f944bd9f-vlw9n is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:25.932: INFO: 23 / 24 pods in namespace 'kube-system' are running and ready (18 seconds elapsed)
Sep 20 00:35:25.932: INFO: expected 4 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Sep 20 00:35:25.933: INFO: POD                       NODE                           PHASE    GRACE  CONDITIONS
Sep 20 00:35:25.933: INFO: coredns-57f944bd9f-vlw9n  worker-wqshf-7859ffd555-kqpmz  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  }]
Sep 20 00:35:25.933: INFO: 
Sep 20 00:35:27.932: INFO: The status of Pod coredns-57f944bd9f-vlw9n is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:27.932: INFO: 23 / 24 pods in namespace 'kube-system' are running and ready (20 seconds elapsed)
Sep 20 00:35:27.932: INFO: expected 4 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Sep 20 00:35:27.932: INFO: POD                       NODE                           PHASE    GRACE  CONDITIONS
Sep 20 00:35:27.933: INFO: coredns-57f944bd9f-vlw9n  worker-wqshf-7859ffd555-kqpmz  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  }]
Sep 20 00:35:27.933: INFO: 
Sep 20 00:35:29.931: INFO: The status of Pod coredns-57f944bd9f-vlw9n is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 00:35:29.931: INFO: 23 / 24 pods in namespace 'kube-system' are running and ready (22 seconds elapsed)
Sep 20 00:35:29.931: INFO: expected 4 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Sep 20 00:35:29.932: INFO: POD                       NODE                           PHASE    GRACE  CONDITIONS
Sep 20 00:35:29.932: INFO: coredns-57f944bd9f-vlw9n  worker-wqshf-7859ffd555-kqpmz  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:31:31 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 00:34:23 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 20:34:21 +0000 UTC  }]
Sep 20 00:35:29.932: INFO: 
Sep 20 00:35:31.937: INFO: 24 / 24 pods in namespace 'kube-system' are running and ready (24 seconds elapsed)
Sep 20 00:35:31.937: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Sep 20 00:35:31.937: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep 20 00:35:31.959: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Sep 20 00:35:31.959: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Sep 20 00:35:31.959: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Sep 20 00:35:31.959: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Sep 20 00:35:31.960: INFO: e2e test version: v1.16.0
Sep 20 00:35:31.963: INFO: kube-apiserver version: v1.16.0
Sep 20 00:35:31.963: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 00:35:31.976: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:35:31.983: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename webhook
Sep 20 00:35:32.038: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Sep 20 00:35:32.054: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3986
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 00:35:32.762: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 20 00:35:34.788: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704536532, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704536532, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704536532, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704536532, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 00:35:36.795: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704536532, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704536532, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704536532, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704536532, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 00:35:38.798: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704536532, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704536532, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704536532, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704536532, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 00:35:41.815: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:35:52.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3986" for this suite.
Sep 20 00:35:59.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:35:59.341: INFO: namespace webhook-3986 deletion completed in 6.348004096s
STEP: Destroying namespace "webhook-3986-markers" for this suite.
Sep 20 00:36:05.369: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:36:05.604: INFO: namespace webhook-3986-markers deletion completed in 6.263037919s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:33.654 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:36:05.638: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-2441
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-2441, will wait for the garbage collector to delete the pods
I0920 00:36:11.837940      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 00:36:11.838148      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 00:36:11.904: INFO: Deleting Job.batch foo took: 15.438416ms
I0920 00:36:12.304787      16 controller_utils.go:810] Ignoring inactive pod job-2441/foo-29pm8 in state Running, deletion time 2019-09-20 00:36:42 +0000 UTC
I0920 00:36:12.305051      16 controller_utils.go:810] Ignoring inactive pod job-2441/foo-nh4cc in state Running, deletion time 2019-09-20 00:36:42 +0000 UTC
Sep 20 00:36:12.305: INFO: Terminating Job.batch foo pods took: 400.894401ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:36:46.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2441" for this suite.
Sep 20 00:36:52.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:36:52.667: INFO: namespace job-2441 deletion completed in 6.248430537s

• [SLOW TEST:47.030 seconds]
[sig-apps] Job
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:36:52.675: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-8312
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 00:36:52.870: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-2e38c94f-78a4-4504-9284-66c43650efab" in namespace "security-context-test-8312" to be "success or failure"
Sep 20 00:36:52.878: INFO: Pod "busybox-readonly-false-2e38c94f-78a4-4504-9284-66c43650efab": Phase="Pending", Reason="", readiness=false. Elapsed: 7.852117ms
Sep 20 00:36:54.884: INFO: Pod "busybox-readonly-false-2e38c94f-78a4-4504-9284-66c43650efab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014451456s
Sep 20 00:36:56.893: INFO: Pod "busybox-readonly-false-2e38c94f-78a4-4504-9284-66c43650efab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022808002s
Sep 20 00:36:58.900: INFO: Pod "busybox-readonly-false-2e38c94f-78a4-4504-9284-66c43650efab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030226649s
Sep 20 00:36:58.900: INFO: Pod "busybox-readonly-false-2e38c94f-78a4-4504-9284-66c43650efab" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:36:58.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8312" for this suite.
Sep 20 00:37:04.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:37:05.179: INFO: namespace security-context-test-8312 deletion completed in 6.271131001s

• [SLOW TEST:12.505 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:37:05.191: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5444
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-fc085353-6fde-478c-aebd-0beeca9815ea
STEP: Creating a pod to test consume configMaps
Sep 20 00:37:05.381: INFO: Waiting up to 5m0s for pod "pod-configmaps-f587f93c-4f9f-4ed1-8722-306b2a0be442" in namespace "configmap-5444" to be "success or failure"
Sep 20 00:37:05.391: INFO: Pod "pod-configmaps-f587f93c-4f9f-4ed1-8722-306b2a0be442": Phase="Pending", Reason="", readiness=false. Elapsed: 9.968439ms
Sep 20 00:37:07.399: INFO: Pod "pod-configmaps-f587f93c-4f9f-4ed1-8722-306b2a0be442": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018233804s
Sep 20 00:37:09.407: INFO: Pod "pod-configmaps-f587f93c-4f9f-4ed1-8722-306b2a0be442": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025518891s
STEP: Saw pod success
Sep 20 00:37:09.407: INFO: Pod "pod-configmaps-f587f93c-4f9f-4ed1-8722-306b2a0be442" satisfied condition "success or failure"
Sep 20 00:37:09.413: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod pod-configmaps-f587f93c-4f9f-4ed1-8722-306b2a0be442 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 00:37:09.490: INFO: Waiting for pod pod-configmaps-f587f93c-4f9f-4ed1-8722-306b2a0be442 to disappear
Sep 20 00:37:09.496: INFO: Pod pod-configmaps-f587f93c-4f9f-4ed1-8722-306b2a0be442 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:37:09.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5444" for this suite.
Sep 20 00:37:15.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:37:15.793: INFO: namespace configmap-5444 deletion completed in 6.290137503s

• [SLOW TEST:10.603 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:37:15.795: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7310
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 00:37:16.000: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7eb17217-924d-4e2b-949c-d3e08a22ed40" in namespace "downward-api-7310" to be "success or failure"
Sep 20 00:37:16.010: INFO: Pod "downwardapi-volume-7eb17217-924d-4e2b-949c-d3e08a22ed40": Phase="Pending", Reason="", readiness=false. Elapsed: 9.915238ms
Sep 20 00:37:18.018: INFO: Pod "downwardapi-volume-7eb17217-924d-4e2b-949c-d3e08a22ed40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018464176s
Sep 20 00:37:20.025: INFO: Pod "downwardapi-volume-7eb17217-924d-4e2b-949c-d3e08a22ed40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025429128s
STEP: Saw pod success
Sep 20 00:37:20.025: INFO: Pod "downwardapi-volume-7eb17217-924d-4e2b-949c-d3e08a22ed40" satisfied condition "success or failure"
Sep 20 00:37:20.031: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod downwardapi-volume-7eb17217-924d-4e2b-949c-d3e08a22ed40 container client-container: <nil>
STEP: delete the pod
Sep 20 00:37:20.071: INFO: Waiting for pod downwardapi-volume-7eb17217-924d-4e2b-949c-d3e08a22ed40 to disappear
Sep 20 00:37:20.077: INFO: Pod downwardapi-volume-7eb17217-924d-4e2b-949c-d3e08a22ed40 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:37:20.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7310" for this suite.
Sep 20 00:37:26.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:37:26.334: INFO: namespace downward-api-7310 deletion completed in 6.248096593s

• [SLOW TEST:10.539 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:37:26.334: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5867
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Sep 20 00:37:26.522: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:37:34.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5867" for this suite.
Sep 20 00:37:40.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:37:40.555: INFO: namespace init-container-5867 deletion completed in 6.279454846s

• [SLOW TEST:14.221 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:37:40.557: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-61
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 00:37:41.196: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 00:37:43.216: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704536661, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704536661, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704536661, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704536661, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 00:37:45.223: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704536661, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704536661, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704536661, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704536661, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 00:37:48.240: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:37:48.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-61" for this suite.
Sep 20 00:37:54.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:37:55.022: INFO: namespace webhook-61 deletion completed in 6.27472628s
STEP: Destroying namespace "webhook-61-markers" for this suite.
Sep 20 00:38:01.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:38:01.288: INFO: namespace webhook-61-markers deletion completed in 6.265539479s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:20.765 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:38:01.338: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9138
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:38:05.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9138" for this suite.
Sep 20 00:38:11.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:38:11.838: INFO: namespace kubelet-test-9138 deletion completed in 6.27438736s

• [SLOW TEST:10.501 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:38:11.840: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7912
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 00:38:12.029: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5143cb27-4c22-4abd-a6a9-77c53d7ec915" in namespace "downward-api-7912" to be "success or failure"
Sep 20 00:38:12.036: INFO: Pod "downwardapi-volume-5143cb27-4c22-4abd-a6a9-77c53d7ec915": Phase="Pending", Reason="", readiness=false. Elapsed: 6.718424ms
Sep 20 00:38:14.044: INFO: Pod "downwardapi-volume-5143cb27-4c22-4abd-a6a9-77c53d7ec915": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014323111s
Sep 20 00:38:16.050: INFO: Pod "downwardapi-volume-5143cb27-4c22-4abd-a6a9-77c53d7ec915": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021042813s
STEP: Saw pod success
Sep 20 00:38:16.050: INFO: Pod "downwardapi-volume-5143cb27-4c22-4abd-a6a9-77c53d7ec915" satisfied condition "success or failure"
Sep 20 00:38:16.056: INFO: Trying to get logs from node worker-wqshf-7859ffd555-lmjf4 pod downwardapi-volume-5143cb27-4c22-4abd-a6a9-77c53d7ec915 container client-container: <nil>
STEP: delete the pod
Sep 20 00:38:16.133: INFO: Waiting for pod downwardapi-volume-5143cb27-4c22-4abd-a6a9-77c53d7ec915 to disappear
Sep 20 00:38:16.139: INFO: Pod downwardapi-volume-5143cb27-4c22-4abd-a6a9-77c53d7ec915 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:38:16.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7912" for this suite.
Sep 20 00:38:22.171: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:38:22.394: INFO: namespace downward-api-7912 deletion completed in 6.24763492s

• [SLOW TEST:10.555 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:38:22.402: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4872
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 00:38:22.640: INFO: (0) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 54.626586ms)
Sep 20 00:38:22.687: INFO: (1) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 46.46183ms)
Sep 20 00:38:22.700: INFO: (2) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 13.234666ms)
Sep 20 00:38:22.714: INFO: (3) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 13.129618ms)
Sep 20 00:38:22.724: INFO: (4) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.641683ms)
Sep 20 00:38:22.740: INFO: (5) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 15.353442ms)
Sep 20 00:38:22.757: INFO: (6) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 16.161232ms)
Sep 20 00:38:22.767: INFO: (7) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.603763ms)
Sep 20 00:38:22.787: INFO: (8) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 19.877425ms)
Sep 20 00:38:22.800: INFO: (9) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.271039ms)
Sep 20 00:38:22.813: INFO: (10) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 13.677919ms)
Sep 20 00:38:22.828: INFO: (11) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 13.775041ms)
Sep 20 00:38:22.846: INFO: (12) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 17.565095ms)
Sep 20 00:38:22.857: INFO: (13) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.446729ms)
Sep 20 00:38:22.869: INFO: (14) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.595942ms)
Sep 20 00:38:22.881: INFO: (15) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.265444ms)
Sep 20 00:38:22.893: INFO: (16) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.085511ms)
Sep 20 00:38:22.906: INFO: (17) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.390979ms)
Sep 20 00:38:22.920: INFO: (18) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 14.469916ms)
Sep 20 00:38:22.932: INFO: (19) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.468298ms)
[AfterEach] version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:38:22.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4872" for this suite.
Sep 20 00:38:28.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:38:29.221: INFO: namespace proxy-4872 deletion completed in 6.279540965s

• [SLOW TEST:6.819 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:38:29.222: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4986
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 00:38:29.403: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4dd4135a-124f-4871-82c4-c034234a62cf" in namespace "projected-4986" to be "success or failure"
Sep 20 00:38:29.413: INFO: Pod "downwardapi-volume-4dd4135a-124f-4871-82c4-c034234a62cf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.688335ms
Sep 20 00:38:31.422: INFO: Pod "downwardapi-volume-4dd4135a-124f-4871-82c4-c034234a62cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018721904s
Sep 20 00:38:33.433: INFO: Pod "downwardapi-volume-4dd4135a-124f-4871-82c4-c034234a62cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029485178s
STEP: Saw pod success
Sep 20 00:38:33.433: INFO: Pod "downwardapi-volume-4dd4135a-124f-4871-82c4-c034234a62cf" satisfied condition "success or failure"
Sep 20 00:38:33.440: INFO: Trying to get logs from node worker-wqshf-7859ffd555-lmjf4 pod downwardapi-volume-4dd4135a-124f-4871-82c4-c034234a62cf container client-container: <nil>
STEP: delete the pod
Sep 20 00:38:33.487: INFO: Waiting for pod downwardapi-volume-4dd4135a-124f-4871-82c4-c034234a62cf to disappear
Sep 20 00:38:33.492: INFO: Pod downwardapi-volume-4dd4135a-124f-4871-82c4-c034234a62cf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:38:33.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4986" for this suite.
Sep 20 00:38:39.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:38:39.874: INFO: namespace projected-4986 deletion completed in 6.373106747s

• [SLOW TEST:10.653 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:38:39.883: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-228
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-228
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-228
STEP: Deleting pre-stop pod
Sep 20 00:38:53.229: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:38:53.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-228" for this suite.
Sep 20 00:39:37.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:39:37.526: INFO: namespace prestop-228 deletion completed in 44.274723781s

• [SLOW TEST:57.644 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:39:37.527: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2512
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:39:53.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2512" for this suite.
Sep 20 00:39:59.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:40:00.110: INFO: namespace resourcequota-2512 deletion completed in 6.327411058s

• [SLOW TEST:22.584 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:40:00.121: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1215
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 00:40:00.294: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 20 00:40:03.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 --namespace=crd-publish-openapi-1215 create -f -'
Sep 20 00:40:04.025: INFO: stderr: ""
Sep 20 00:40:04.025: INFO: stdout: "e2e-test-crd-publish-openapi-124-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 20 00:40:04.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 --namespace=crd-publish-openapi-1215 delete e2e-test-crd-publish-openapi-124-crds test-cr'
Sep 20 00:40:04.144: INFO: stderr: ""
Sep 20 00:40:04.144: INFO: stdout: "e2e-test-crd-publish-openapi-124-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Sep 20 00:40:04.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 --namespace=crd-publish-openapi-1215 apply -f -'
Sep 20 00:40:04.357: INFO: stderr: ""
Sep 20 00:40:04.357: INFO: stdout: "e2e-test-crd-publish-openapi-124-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 20 00:40:04.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 --namespace=crd-publish-openapi-1215 delete e2e-test-crd-publish-openapi-124-crds test-cr'
Sep 20 00:40:04.482: INFO: stderr: ""
Sep 20 00:40:04.482: INFO: stdout: "e2e-test-crd-publish-openapi-124-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep 20 00:40:04.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 explain e2e-test-crd-publish-openapi-124-crds'
Sep 20 00:40:04.671: INFO: stderr: ""
Sep 20 00:40:04.671: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-124-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:40:08.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1215" for this suite.
Sep 20 00:40:14.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:40:14.424: INFO: namespace crd-publish-openapi-1215 deletion completed in 6.262819118s

• [SLOW TEST:14.304 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:40:14.431: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9009
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 00:40:14.604: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:40:15.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9009" for this suite.
Sep 20 00:40:21.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:40:21.886: INFO: namespace custom-resource-definition-9009 deletion completed in 6.223891309s

• [SLOW TEST:7.456 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:40:21.886: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5027
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 00:40:22.076: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d647e637-ffcb-4120-ad51-db8b7c876d32" in namespace "downward-api-5027" to be "success or failure"
Sep 20 00:40:22.081: INFO: Pod "downwardapi-volume-d647e637-ffcb-4120-ad51-db8b7c876d32": Phase="Pending", Reason="", readiness=false. Elapsed: 4.810926ms
Sep 20 00:40:24.088: INFO: Pod "downwardapi-volume-d647e637-ffcb-4120-ad51-db8b7c876d32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011920289s
Sep 20 00:40:26.095: INFO: Pod "downwardapi-volume-d647e637-ffcb-4120-ad51-db8b7c876d32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018760214s
STEP: Saw pod success
Sep 20 00:40:26.095: INFO: Pod "downwardapi-volume-d647e637-ffcb-4120-ad51-db8b7c876d32" satisfied condition "success or failure"
Sep 20 00:40:26.100: INFO: Trying to get logs from node worker-wqshf-7859ffd555-lmjf4 pod downwardapi-volume-d647e637-ffcb-4120-ad51-db8b7c876d32 container client-container: <nil>
STEP: delete the pod
Sep 20 00:40:26.152: INFO: Waiting for pod downwardapi-volume-d647e637-ffcb-4120-ad51-db8b7c876d32 to disappear
Sep 20 00:40:26.156: INFO: Pod downwardapi-volume-d647e637-ffcb-4120-ad51-db8b7c876d32 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:40:26.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5027" for this suite.
Sep 20 00:40:32.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:40:32.384: INFO: namespace downward-api-5027 deletion completed in 6.221257751s

• [SLOW TEST:10.498 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:40:32.390: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-21
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-05ccc980-324d-435b-817f-dbf2538c37dc
STEP: Creating a pod to test consume configMaps
Sep 20 00:40:32.594: INFO: Waiting up to 5m0s for pod "pod-configmaps-e792caa3-fc52-4d77-8335-a5f89ee19a4b" in namespace "configmap-21" to be "success or failure"
Sep 20 00:40:32.600: INFO: Pod "pod-configmaps-e792caa3-fc52-4d77-8335-a5f89ee19a4b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.29123ms
Sep 20 00:40:34.606: INFO: Pod "pod-configmaps-e792caa3-fc52-4d77-8335-a5f89ee19a4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012000909s
Sep 20 00:40:36.613: INFO: Pod "pod-configmaps-e792caa3-fc52-4d77-8335-a5f89ee19a4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019187137s
STEP: Saw pod success
Sep 20 00:40:36.613: INFO: Pod "pod-configmaps-e792caa3-fc52-4d77-8335-a5f89ee19a4b" satisfied condition "success or failure"
Sep 20 00:40:36.617: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod pod-configmaps-e792caa3-fc52-4d77-8335-a5f89ee19a4b container configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 00:40:36.719: INFO: Waiting for pod pod-configmaps-e792caa3-fc52-4d77-8335-a5f89ee19a4b to disappear
Sep 20 00:40:36.729: INFO: Pod pod-configmaps-e792caa3-fc52-4d77-8335-a5f89ee19a4b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:40:36.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-21" for this suite.
Sep 20 00:40:42.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:40:42.951: INFO: namespace configmap-21 deletion completed in 6.215107919s

• [SLOW TEST:10.562 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:40:42.953: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9006
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 20 00:40:43.132: INFO: Waiting up to 5m0s for pod "pod-70ebf100-2e12-4425-aa49-cfdaf46e6de4" in namespace "emptydir-9006" to be "success or failure"
Sep 20 00:40:43.137: INFO: Pod "pod-70ebf100-2e12-4425-aa49-cfdaf46e6de4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.024889ms
Sep 20 00:40:45.144: INFO: Pod "pod-70ebf100-2e12-4425-aa49-cfdaf46e6de4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012393563s
Sep 20 00:40:47.153: INFO: Pod "pod-70ebf100-2e12-4425-aa49-cfdaf46e6de4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020903002s
STEP: Saw pod success
Sep 20 00:40:47.153: INFO: Pod "pod-70ebf100-2e12-4425-aa49-cfdaf46e6de4" satisfied condition "success or failure"
Sep 20 00:40:47.158: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod pod-70ebf100-2e12-4425-aa49-cfdaf46e6de4 container test-container: <nil>
STEP: delete the pod
Sep 20 00:40:47.198: INFO: Waiting for pod pod-70ebf100-2e12-4425-aa49-cfdaf46e6de4 to disappear
Sep 20 00:40:47.204: INFO: Pod pod-70ebf100-2e12-4425-aa49-cfdaf46e6de4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:40:47.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9006" for this suite.
Sep 20 00:40:53.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:40:53.457: INFO: namespace emptydir-9006 deletion completed in 6.245816144s

• [SLOW TEST:10.505 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:40:53.459: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8236
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-08c430a2-2fc6-495a-8e51-ab612e468d3d
STEP: Creating secret with name s-test-opt-upd-1778ad1c-46cc-4bb2-9087-5bf7a914011a
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-08c430a2-2fc6-495a-8e51-ab612e468d3d
STEP: Updating secret s-test-opt-upd-1778ad1c-46cc-4bb2-9087-5bf7a914011a
STEP: Creating secret with name s-test-opt-create-e7a31f22-67c4-4e90-ab29-129de90c4ec4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:41:02.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8236" for this suite.
Sep 20 00:41:30.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:41:30.393: INFO: namespace secrets-8236 deletion completed in 28.207899697s

• [SLOW TEST:36.934 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:41:30.394: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6486
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Sep 20 00:41:30.560: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:41:47.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6486" for this suite.
Sep 20 00:41:53.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:41:54.122: INFO: namespace crd-publish-openapi-6486 deletion completed in 6.241094616s

• [SLOW TEST:23.729 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:41:54.128: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3577
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-3aedbee2-7c51-48a1-8ec9-222a7a257bf9
STEP: Creating configMap with name cm-test-opt-upd-0fa752ae-ccfa-4015-9ecf-b69e83d2cfc6
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-3aedbee2-7c51-48a1-8ec9-222a7a257bf9
STEP: Updating configmap cm-test-opt-upd-0fa752ae-ccfa-4015-9ecf-b69e83d2cfc6
STEP: Creating configMap with name cm-test-opt-create-f0c9f114-3fd9-4b40-863c-ed32f09538f5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:43:01.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3577" for this suite.
Sep 20 00:43:17.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:43:18.277: INFO: namespace configmap-3577 deletion completed in 16.320108436s

• [SLOW TEST:84.149 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:43:18.280: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7199
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 20 00:43:18.463: INFO: Waiting up to 5m0s for pod "pod-13f36c9b-8efb-4fdc-b038-26b42a207841" in namespace "emptydir-7199" to be "success or failure"
Sep 20 00:43:18.470: INFO: Pod "pod-13f36c9b-8efb-4fdc-b038-26b42a207841": Phase="Pending", Reason="", readiness=false. Elapsed: 7.136457ms
Sep 20 00:43:20.479: INFO: Pod "pod-13f36c9b-8efb-4fdc-b038-26b42a207841": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015817543s
Sep 20 00:43:22.485: INFO: Pod "pod-13f36c9b-8efb-4fdc-b038-26b42a207841": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022305675s
STEP: Saw pod success
Sep 20 00:43:22.485: INFO: Pod "pod-13f36c9b-8efb-4fdc-b038-26b42a207841" satisfied condition "success or failure"
Sep 20 00:43:22.490: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod pod-13f36c9b-8efb-4fdc-b038-26b42a207841 container test-container: <nil>
STEP: delete the pod
Sep 20 00:43:22.658: INFO: Waiting for pod pod-13f36c9b-8efb-4fdc-b038-26b42a207841 to disappear
Sep 20 00:43:22.665: INFO: Pod pod-13f36c9b-8efb-4fdc-b038-26b42a207841 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:43:22.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7199" for this suite.
Sep 20 00:43:28.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:43:28.903: INFO: namespace emptydir-7199 deletion completed in 6.230806115s

• [SLOW TEST:10.623 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:43:28.904: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8683
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 00:43:29.501: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 00:43:31.531: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704537009, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704537009, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704537009, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704537009, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 00:43:33.538: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704537009, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704537009, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704537009, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704537009, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 00:43:35.538: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704537009, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704537009, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704537009, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704537009, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 00:43:38.558: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 00:43:38.564: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4992-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:43:40.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8683" for this suite.
Sep 20 00:43:46.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:43:46.345: INFO: namespace webhook-8683 deletion completed in 6.28438474s
STEP: Destroying namespace "webhook-8683-markers" for this suite.
Sep 20 00:43:52.369: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:43:52.553: INFO: namespace webhook-8683-markers deletion completed in 6.207489715s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:23.677 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:43:52.584: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-7155
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
Sep 20 00:43:52.751: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 20 00:44:52.795: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 00:44:52.802: INFO: Starting informer...
STEP: Starting pod...
I0920 00:44:52.803039      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/e2e/scheduling/taints.go:146
I0920 00:44:52.803093      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/e2e/scheduling/taints.go:146
Sep 20 00:44:53.026: INFO: Pod is running on worker-wqshf-7859ffd555-r6tnq. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Sep 20 00:44:53.051: INFO: Pod wasn't evicted. Proceeding
Sep 20 00:44:53.051: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Sep 20 00:46:08.079: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:46:08.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-7155" for this suite.
Sep 20 00:46:36.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:46:36.325: INFO: namespace taint-single-pod-7155 deletion completed in 28.238638411s

• [SLOW TEST:163.741 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:46:36.325: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-31
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 20 00:46:44.612: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 20 00:46:44.619: INFO: Pod pod-with-poststart-http-hook still exists
Sep 20 00:46:46.619: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 20 00:46:46.626: INFO: Pod pod-with-poststart-http-hook still exists
Sep 20 00:46:48.619: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 20 00:46:48.627: INFO: Pod pod-with-poststart-http-hook still exists
Sep 20 00:46:50.619: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 20 00:46:50.629: INFO: Pod pod-with-poststart-http-hook still exists
Sep 20 00:46:52.619: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 20 00:46:52.625: INFO: Pod pod-with-poststart-http-hook still exists
Sep 20 00:46:54.619: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 20 00:46:54.626: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:46:54.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-31" for this suite.
Sep 20 00:47:22.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:47:22.842: INFO: namespace container-lifecycle-hook-31 deletion completed in 28.207110136s

• [SLOW TEST:46.517 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:47:22.842: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5280
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5280
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5280
STEP: creating replication controller externalsvc in namespace services-5280
I0920 00:47:23.044715      16 runners.go:184] Created replication controller with name: externalsvc, namespace: services-5280, replica count: 2
I0920 00:47:23.044882      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 00:47:23.044919      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 00:47:26.095302      16 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Sep 20 00:47:26.119: INFO: Creating new exec pod
Sep 20 00:47:30.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=services-5280 execpod4zn4p -- /bin/sh -x -c nslookup clusterip-service'
Sep 20 00:47:30.881: INFO: stderr: "+ nslookup clusterip-service\n"
Sep 20 00:47:30.881: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nclusterip-service.services-5280.svc.cluster.local\tcanonical name = externalsvc.services-5280.svc.cluster.local.\nName:\texternalsvc.services-5280.svc.cluster.local\nAddress: 10.240.22.128\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5280, will wait for the garbage collector to delete the pods
I0920 00:47:30.888452      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 00:47:30.888524      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 00:47:30.953: INFO: Deleting ReplicationController externalsvc took: 14.955227ms
Sep 20 00:47:31.353: INFO: Terminating ReplicationController externalsvc pods took: 400.32586ms
I0920 00:47:31.353858      16 controller_utils.go:810] Ignoring inactive pod services-5280/externalsvc-h8mnm in state Running, deletion time 2019-09-20 00:47:32 +0000 UTC
I0920 00:47:31.353900      16 controller_utils.go:810] Ignoring inactive pod services-5280/externalsvc-49jps in state Running, deletion time 2019-09-20 00:47:32 +0000 UTC
Sep 20 00:47:35.879: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:47:35.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5280" for this suite.
Sep 20 00:47:41.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:47:42.134: INFO: namespace services-5280 deletion completed in 6.227665531s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:19.293 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:47:42.140: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3135
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep 20 00:47:42.329: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3135 /api/v1/namespaces/watch-3135/configmaps/e2e-watch-test-configmap-a 60cb5ae6-fe48-4c3f-a423-cfe889276a68 62301 0 2019-09-20 00:47:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 20 00:47:42.329: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3135 /api/v1/namespaces/watch-3135/configmaps/e2e-watch-test-configmap-a 60cb5ae6-fe48-4c3f-a423-cfe889276a68 62301 0 2019-09-20 00:47:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep 20 00:47:52.347: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3135 /api/v1/namespaces/watch-3135/configmaps/e2e-watch-test-configmap-a 60cb5ae6-fe48-4c3f-a423-cfe889276a68 62329 0 2019-09-20 00:47:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep 20 00:47:52.347: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3135 /api/v1/namespaces/watch-3135/configmaps/e2e-watch-test-configmap-a 60cb5ae6-fe48-4c3f-a423-cfe889276a68 62329 0 2019-09-20 00:47:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep 20 00:48:02.365: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3135 /api/v1/namespaces/watch-3135/configmaps/e2e-watch-test-configmap-a 60cb5ae6-fe48-4c3f-a423-cfe889276a68 62356 0 2019-09-20 00:47:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 20 00:48:02.365: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3135 /api/v1/namespaces/watch-3135/configmaps/e2e-watch-test-configmap-a 60cb5ae6-fe48-4c3f-a423-cfe889276a68 62356 0 2019-09-20 00:47:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep 20 00:48:12.378: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3135 /api/v1/namespaces/watch-3135/configmaps/e2e-watch-test-configmap-a 60cb5ae6-fe48-4c3f-a423-cfe889276a68 62384 0 2019-09-20 00:47:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 20 00:48:12.378: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3135 /api/v1/namespaces/watch-3135/configmaps/e2e-watch-test-configmap-a 60cb5ae6-fe48-4c3f-a423-cfe889276a68 62384 0 2019-09-20 00:47:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep 20 00:48:22.389: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3135 /api/v1/namespaces/watch-3135/configmaps/e2e-watch-test-configmap-b 39153e4e-73df-4fec-bebe-6ac0eb8febdd 62413 0 2019-09-20 00:48:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 20 00:48:22.389: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3135 /api/v1/namespaces/watch-3135/configmaps/e2e-watch-test-configmap-b 39153e4e-73df-4fec-bebe-6ac0eb8febdd 62413 0 2019-09-20 00:48:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep 20 00:48:32.405: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3135 /api/v1/namespaces/watch-3135/configmaps/e2e-watch-test-configmap-b 39153e4e-73df-4fec-bebe-6ac0eb8febdd 62443 0 2019-09-20 00:48:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 20 00:48:32.405: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3135 /api/v1/namespaces/watch-3135/configmaps/e2e-watch-test-configmap-b 39153e4e-73df-4fec-bebe-6ac0eb8febdd 62443 0 2019-09-20 00:48:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:48:42.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3135" for this suite.
Sep 20 00:48:48.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:48:48.634: INFO: namespace watch-3135 deletion completed in 6.217797116s

• [SLOW TEST:66.495 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:48:48.636: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3281
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Sep 20 00:48:48.828: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-342770709 proxy --unix-socket=/tmp/kubectl-proxy-unix992023756/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:48:48.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3281" for this suite.
Sep 20 00:48:55.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:48:55.199: INFO: namespace kubectl-3281 deletion completed in 6.299775111s

• [SLOW TEST:6.564 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:48:55.206: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5957
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep 20 00:48:55.416: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5957 /api/v1/namespaces/watch-5957/configmaps/e2e-watch-test-resource-version 969c63ea-9b1e-4206-94c2-2eb945fd71d8 62533 0 2019-09-20 00:48:55 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 20 00:48:55.416: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5957 /api/v1/namespaces/watch-5957/configmaps/e2e-watch-test-resource-version 969c63ea-9b1e-4206-94c2-2eb945fd71d8 62534 0 2019-09-20 00:48:55 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:48:55.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5957" for this suite.
Sep 20 00:49:01.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:49:01.652: INFO: namespace watch-5957 deletion completed in 6.229101542s

• [SLOW TEST:6.450 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:49:01.671: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3678
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-66373201-6a2b-488d-ba20-ed5837ba1224
STEP: Creating a pod to test consume configMaps
Sep 20 00:49:01.860: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7830a7df-5709-413d-b5f6-54141eb88fc7" in namespace "projected-3678" to be "success or failure"
Sep 20 00:49:01.874: INFO: Pod "pod-projected-configmaps-7830a7df-5709-413d-b5f6-54141eb88fc7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.787459ms
Sep 20 00:49:03.883: INFO: Pod "pod-projected-configmaps-7830a7df-5709-413d-b5f6-54141eb88fc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022738058s
Sep 20 00:49:05.900: INFO: Pod "pod-projected-configmaps-7830a7df-5709-413d-b5f6-54141eb88fc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039858446s
STEP: Saw pod success
Sep 20 00:49:05.900: INFO: Pod "pod-projected-configmaps-7830a7df-5709-413d-b5f6-54141eb88fc7" satisfied condition "success or failure"
Sep 20 00:49:05.908: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod pod-projected-configmaps-7830a7df-5709-413d-b5f6-54141eb88fc7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 00:49:06.039: INFO: Waiting for pod pod-projected-configmaps-7830a7df-5709-413d-b5f6-54141eb88fc7 to disappear
Sep 20 00:49:06.046: INFO: Pod pod-projected-configmaps-7830a7df-5709-413d-b5f6-54141eb88fc7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:49:06.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3678" for this suite.
Sep 20 00:49:12.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:49:12.277: INFO: namespace projected-3678 deletion completed in 6.222478429s

• [SLOW TEST:10.607 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:49:12.278: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3671
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3671.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3671.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3671.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3671.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3671.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3671.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3671.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3671.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3671.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3671.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3671.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3671.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3671.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 163.18.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.18.163_udp@PTR;check="$$(dig +tcp +noall +answer +search 163.18.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.18.163_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3671.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3671.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3671.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3671.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3671.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3671.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3671.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3671.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3671.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3671.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3671.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3671.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3671.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 163.18.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.18.163_udp@PTR;check="$$(dig +tcp +noall +answer +search 163.18.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.18.163_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 20 00:49:26.622: INFO: Unable to read wheezy_udp@dns-test-service.dns-3671.svc.cluster.local from pod dns-3671/dns-test-c4808526-ccf2-4954-9da2-2aee3d5f8842: the server could not find the requested resource (get pods dns-test-c4808526-ccf2-4954-9da2-2aee3d5f8842)
Sep 20 00:49:26.666: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3671.svc.cluster.local from pod dns-3671/dns-test-c4808526-ccf2-4954-9da2-2aee3d5f8842: the server could not find the requested resource (get pods dns-test-c4808526-ccf2-4954-9da2-2aee3d5f8842)
Sep 20 00:49:26.677: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3671.svc.cluster.local from pod dns-3671/dns-test-c4808526-ccf2-4954-9da2-2aee3d5f8842: the server could not find the requested resource (get pods dns-test-c4808526-ccf2-4954-9da2-2aee3d5f8842)
Sep 20 00:49:26.689: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3671.svc.cluster.local from pod dns-3671/dns-test-c4808526-ccf2-4954-9da2-2aee3d5f8842: the server could not find the requested resource (get pods dns-test-c4808526-ccf2-4954-9da2-2aee3d5f8842)
Sep 20 00:49:27.250: INFO: Unable to read jessie_udp@dns-test-service.dns-3671.svc.cluster.local from pod dns-3671/dns-test-c4808526-ccf2-4954-9da2-2aee3d5f8842: the server could not find the requested resource (get pods dns-test-c4808526-ccf2-4954-9da2-2aee3d5f8842)
Sep 20 00:49:27.260: INFO: Unable to read jessie_tcp@dns-test-service.dns-3671.svc.cluster.local from pod dns-3671/dns-test-c4808526-ccf2-4954-9da2-2aee3d5f8842: the server could not find the requested resource (get pods dns-test-c4808526-ccf2-4954-9da2-2aee3d5f8842)
Sep 20 00:49:27.270: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3671.svc.cluster.local from pod dns-3671/dns-test-c4808526-ccf2-4954-9da2-2aee3d5f8842: the server could not find the requested resource (get pods dns-test-c4808526-ccf2-4954-9da2-2aee3d5f8842)
Sep 20 00:49:27.281: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3671.svc.cluster.local from pod dns-3671/dns-test-c4808526-ccf2-4954-9da2-2aee3d5f8842: the server could not find the requested resource (get pods dns-test-c4808526-ccf2-4954-9da2-2aee3d5f8842)
Sep 20 00:49:27.782: INFO: Lookups using dns-3671/dns-test-c4808526-ccf2-4954-9da2-2aee3d5f8842 failed for: [wheezy_udp@dns-test-service.dns-3671.svc.cluster.local wheezy_tcp@dns-test-service.dns-3671.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3671.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3671.svc.cluster.local jessie_udp@dns-test-service.dns-3671.svc.cluster.local jessie_tcp@dns-test-service.dns-3671.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3671.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3671.svc.cluster.local]

Sep 20 00:49:34.462: INFO: DNS probes using dns-3671/dns-test-c4808526-ccf2-4954-9da2-2aee3d5f8842 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:49:34.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3671" for this suite.
Sep 20 00:49:40.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:49:40.761: INFO: namespace dns-3671 deletion completed in 6.217763685s

• [SLOW TEST:28.483 seconds]
[sig-network] DNS
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:49:40.762: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3894
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Sep 20 00:49:40.928: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 20 00:49:40.952: INFO: Waiting for terminating namespaces to be deleted...
Sep 20 00:49:40.957: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-kqpmz before test
Sep 20 00:49:41.065: INFO: kube-proxy-xmcqx from kube-system started at 2019-09-19 20:33:51 +0000 UTC (1 container statuses recorded)
Sep 20 00:49:41.065: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 00:49:41.065: INFO: node-local-dns-7tzn5 from kube-system started at 2019-09-19 20:34:11 +0000 UTC (1 container statuses recorded)
Sep 20 00:49:41.065: INFO: 	Container node-cache ready: true, restart count 0
Sep 20 00:49:41.065: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-2ks25 from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 00:49:41.065: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 00:49:41.065: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 00:49:41.065: INFO: coredns-57f944bd9f-vlw9n from kube-system started at 2019-09-19 20:34:21 +0000 UTC (1 container statuses recorded)
Sep 20 00:49:41.065: INFO: 	Container coredns ready: true, restart count 0
Sep 20 00:49:41.065: INFO: kubernetes-dashboard-7d5fb85f7f-t28jk from kube-system started at 2019-09-19 20:34:21 +0000 UTC (1 container statuses recorded)
Sep 20 00:49:41.065: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Sep 20 00:49:41.065: INFO: canal-pwz75 from kube-system started at 2019-09-19 20:33:51 +0000 UTC (2 container statuses recorded)
Sep 20 00:49:41.065: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 00:49:41.065: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 00:49:41.065: INFO: node-exporter-rtkgz from kube-system started at 2019-09-19 20:33:51 +0000 UTC (2 container statuses recorded)
Sep 20 00:49:41.065: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 00:49:41.065: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 00:49:41.065: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-ldlfc before test
Sep 20 00:49:41.122: INFO: kube-proxy-zvhzd from kube-system started at 2019-09-19 20:33:57 +0000 UTC (1 container statuses recorded)
Sep 20 00:49:41.122: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 00:49:41.122: INFO: node-exporter-47p98 from kube-system started at 2019-09-19 20:33:57 +0000 UTC (2 container statuses recorded)
Sep 20 00:49:41.122: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 00:49:41.122: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 00:49:41.122: INFO: canal-hf9f7 from kube-system started at 2019-09-19 20:33:57 +0000 UTC (2 container statuses recorded)
Sep 20 00:49:41.122: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 00:49:41.122: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 00:49:41.122: INFO: node-local-dns-rj6pt from kube-system started at 2019-09-19 21:38:47 +0000 UTC (1 container statuses recorded)
Sep 20 00:49:41.122: INFO: 	Container node-cache ready: true, restart count 0
Sep 20 00:49:41.122: INFO: sonobuoy-e2e-job-8f5b7ab9f6954bcb from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 00:49:41.122: INFO: 	Container e2e ready: true, restart count 0
Sep 20 00:49:41.122: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 00:49:41.122: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-djczp from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 00:49:41.122: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 00:49:41.122: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 00:49:41.122: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-lmjf4 before test
Sep 20 00:49:41.202: INFO: canal-s8rlz from kube-system started at 2019-09-19 20:34:06 +0000 UTC (2 container statuses recorded)
Sep 20 00:49:41.202: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 00:49:41.202: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 00:49:41.202: INFO: kube-proxy-mhstl from kube-system started at 2019-09-19 20:34:06 +0000 UTC (1 container statuses recorded)
Sep 20 00:49:41.203: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 00:49:41.203: INFO: node-exporter-65hp9 from kube-system started at 2019-09-19 20:34:06 +0000 UTC (2 container statuses recorded)
Sep 20 00:49:41.203: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 00:49:41.203: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 00:49:41.203: INFO: node-local-dns-bx7jl from kube-system started at 2019-09-19 20:34:26 +0000 UTC (1 container statuses recorded)
Sep 20 00:49:41.204: INFO: 	Container node-cache ready: true, restart count 0
Sep 20 00:49:41.204: INFO: coredns-57f944bd9f-rp6nc from kube-system started at 2019-09-19 21:35:33 +0000 UTC (1 container statuses recorded)
Sep 20 00:49:41.204: INFO: 	Container coredns ready: true, restart count 0
Sep 20 00:49:41.204: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-n9zxp from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 00:49:41.204: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 00:49:41.204: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 00:49:41.204: INFO: openvpn-client-84ccd8596d-s64vh from kube-system started at 2019-09-20 00:44:53 +0000 UTC (2 container statuses recorded)
Sep 20 00:49:41.204: INFO: 	Container dnat-controller ready: true, restart count 0
Sep 20 00:49:41.205: INFO: 	Container openvpn-client ready: true, restart count 0
Sep 20 00:49:41.205: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-mxxqz before test
Sep 20 00:49:41.280: INFO: sonobuoy from sonobuoy started at 2019-09-19 20:40:21 +0000 UTC (1 container statuses recorded)
Sep 20 00:49:41.281: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 20 00:49:41.281: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-7brnz from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 00:49:41.281: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 00:49:41.281: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 00:49:41.281: INFO: canal-kdmx8 from kube-system started at 2019-09-19 20:34:07 +0000 UTC (2 container statuses recorded)
Sep 20 00:49:41.281: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 00:49:41.281: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 00:49:41.281: INFO: kube-proxy-dtrjq from kube-system started at 2019-09-19 20:34:07 +0000 UTC (1 container statuses recorded)
Sep 20 00:49:41.281: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 00:49:41.281: INFO: node-exporter-xq8tf from kube-system started at 2019-09-19 20:34:07 +0000 UTC (2 container statuses recorded)
Sep 20 00:49:41.281: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 00:49:41.281: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 00:49:41.282: INFO: node-local-dns-bxghq from kube-system started at 2019-09-19 20:34:27 +0000 UTC (1 container statuses recorded)
Sep 20 00:49:41.282: INFO: 	Container node-cache ready: true, restart count 0
Sep 20 00:49:41.282: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-r6tnq before test
Sep 20 00:49:41.358: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-bph2c from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 00:49:41.358: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 00:49:41.358: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 00:49:41.358: INFO: node-exporter-ld8fb from kube-system started at 2019-09-19 20:33:57 +0000 UTC (2 container statuses recorded)
Sep 20 00:49:41.358: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 00:49:41.358: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 00:49:41.359: INFO: canal-8lwhv from kube-system started at 2019-09-19 20:33:57 +0000 UTC (2 container statuses recorded)
Sep 20 00:49:41.359: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 00:49:41.360: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 00:49:41.360: INFO: kube-proxy-p4g4f from kube-system started at 2019-09-19 20:33:57 +0000 UTC (1 container statuses recorded)
Sep 20 00:49:41.360: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 00:49:41.360: INFO: node-local-dns-2krxh from kube-system started at 2019-09-20 00:44:53 +0000 UTC (1 container statuses recorded)
Sep 20 00:49:41.360: INFO: 	Container node-cache ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-d6d0c01e-5456-4f97-906d-54802171d984 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-d6d0c01e-5456-4f97-906d-54802171d984 off the node worker-wqshf-7859ffd555-kqpmz
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d6d0c01e-5456-4f97-906d-54802171d984
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:49:57.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3894" for this suite.
Sep 20 00:50:17.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:50:17.745: INFO: namespace sched-pred-3894 deletion completed in 20.209890519s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:36.983 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:50:17.745: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
I0920 00:50:17.745498      16 request.go:706] Error in request: resource name may not be empty
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-5241
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1709
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6032
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:50:24.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5241" for this suite.
Sep 20 00:50:30.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:50:30.520: INFO: namespace namespaces-5241 deletion completed in 6.218610914s
STEP: Destroying namespace "nsdeletetest-1709" for this suite.
Sep 20 00:50:30.525: INFO: Namespace nsdeletetest-1709 was already deleted
STEP: Destroying namespace "nsdeletetest-6032" for this suite.
Sep 20 00:50:36.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:50:36.793: INFO: namespace nsdeletetest-6032 deletion completed in 6.267026546s

• [SLOW TEST:19.048 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:50:36.802: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2538
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Sep 20 00:50:41.016: INFO: Pod pod-hostip-43b09ac2-7c43-49d6-a3d2-d3d4efc110e1 has hostIP: 104.248.240.176
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:50:41.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2538" for this suite.
Sep 20 00:51:09.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:51:09.278: INFO: namespace pods-2538 deletion completed in 28.25390529s

• [SLOW TEST:32.477 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:51:09.279: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8325
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 00:51:09.501: INFO: Create a RollingUpdate DaemonSet
Sep 20 00:51:09.508: INFO: Check that daemon pods launch on every node of the cluster
Sep 20 00:51:09.523: INFO: Number of nodes with available pods: 0
Sep 20 00:51:09.523: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 00:51:10.538: INFO: Number of nodes with available pods: 0
Sep 20 00:51:10.538: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 00:51:11.541: INFO: Number of nodes with available pods: 0
Sep 20 00:51:11.541: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 00:51:12.540: INFO: Number of nodes with available pods: 0
Sep 20 00:51:12.541: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 00:51:13.541: INFO: Number of nodes with available pods: 0
Sep 20 00:51:13.541: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 00:51:14.535: INFO: Number of nodes with available pods: 0
Sep 20 00:51:14.535: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 00:51:15.535: INFO: Number of nodes with available pods: 0
Sep 20 00:51:15.536: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 00:51:16.536: INFO: Number of nodes with available pods: 0
Sep 20 00:51:16.536: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 00:51:17.536: INFO: Number of nodes with available pods: 3
Sep 20 00:51:17.536: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 00:51:18.538: INFO: Number of nodes with available pods: 5
Sep 20 00:51:18.538: INFO: Number of running nodes: 5, number of available pods: 5
Sep 20 00:51:18.538: INFO: Update the DaemonSet to trigger a rollout
Sep 20 00:51:18.551: INFO: Updating DaemonSet daemon-set
Sep 20 00:51:27.578: INFO: Roll back the DaemonSet before rollout is complete
Sep 20 00:51:27.591: INFO: Updating DaemonSet daemon-set
Sep 20 00:51:27.591: INFO: Make sure DaemonSet rollback is complete
Sep 20 00:51:27.596: INFO: Wrong image for pod: daemon-set-6d7vf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 20 00:51:27.597: INFO: Pod daemon-set-6d7vf is not available
Sep 20 00:51:28.611: INFO: Wrong image for pod: daemon-set-6d7vf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 20 00:51:28.611: INFO: Pod daemon-set-6d7vf is not available
Sep 20 00:51:29.616: INFO: Pod daemon-set-rbsz2 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8325, will wait for the garbage collector to delete the pods
I0920 00:51:29.638097      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 00:51:29.638150      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 00:51:29.699: INFO: Deleting DaemonSet.extensions daemon-set took: 11.605647ms
I0920 00:51:30.111871      16 controller_utils.go:810] Ignoring inactive pod daemonsets-8325/daemon-set-rbsz2 in state Pending, deletion time 2019-09-20 00:52:00 +0000 UTC
I0920 00:51:30.111982      16 controller_utils.go:810] Ignoring inactive pod daemonsets-8325/daemon-set-8vcm9 in state Running, deletion time 2019-09-20 00:52:00 +0000 UTC
I0920 00:51:30.112014      16 controller_utils.go:810] Ignoring inactive pod daemonsets-8325/daemon-set-k669d in state Running, deletion time 2019-09-20 00:52:00 +0000 UTC
I0920 00:51:30.112063      16 controller_utils.go:810] Ignoring inactive pod daemonsets-8325/daemon-set-bj4lf in state Running, deletion time 2019-09-20 00:52:00 +0000 UTC
I0920 00:51:30.112126      16 controller_utils.go:810] Ignoring inactive pod daemonsets-8325/daemon-set-lmfg6 in state Running, deletion time 2019-09-20 00:52:00 +0000 UTC
Sep 20 00:51:30.112: INFO: Terminating DaemonSet.extensions daemon-set pods took: 412.604885ms
Sep 20 00:52:17.917: INFO: Number of nodes with available pods: 0
Sep 20 00:52:17.917: INFO: Number of running nodes: 0, number of available pods: 0
Sep 20 00:52:17.927: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8325/daemonsets","resourceVersion":"63473"},"items":null}

Sep 20 00:52:17.933: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8325/pods","resourceVersion":"63473"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:52:17.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8325" for this suite.
Sep 20 00:52:23.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:52:24.213: INFO: namespace daemonsets-8325 deletion completed in 6.239295149s

• [SLOW TEST:74.934 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:52:24.218: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4954
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:52:40.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4954" for this suite.
Sep 20 00:52:46.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:52:46.818: INFO: namespace resourcequota-4954 deletion completed in 6.255127704s

• [SLOW TEST:22.600 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:52:46.821: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-378
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Sep 20 00:52:47.046: INFO: namespace kubectl-378
Sep 20 00:52:47.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 create -f - --namespace=kubectl-378'
Sep 20 00:52:47.618: INFO: stderr: ""
Sep 20 00:52:47.618: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep 20 00:52:48.625: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 00:52:48.625: INFO: Found 0 / 1
Sep 20 00:52:49.624: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 00:52:49.624: INFO: Found 0 / 1
Sep 20 00:52:50.624: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 00:52:50.624: INFO: Found 0 / 1
Sep 20 00:52:51.625: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 00:52:51.626: INFO: Found 0 / 1
Sep 20 00:52:52.625: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 00:52:52.625: INFO: Found 0 / 1
Sep 20 00:52:53.627: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 00:52:53.627: INFO: Found 0 / 1
Sep 20 00:52:54.625: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 00:52:54.625: INFO: Found 1 / 1
Sep 20 00:52:54.625: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 20 00:52:54.630: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 00:52:54.631: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 20 00:52:54.631: INFO: wait on redis-master startup in kubectl-378 
Sep 20 00:52:54.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 logs redis-master-b8lwh redis-master --namespace=kubectl-378'
Sep 20 00:52:54.749: INFO: stderr: ""
Sep 20 00:52:54.749: INFO: stdout: "1:C 20 Sep 2019 00:52:53.705 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 20 Sep 2019 00:52:53.706 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 20 Sep 2019 00:52:53.706 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 20 Sep 2019 00:52:53.708 * Running mode=standalone, port=6379.\n1:M 20 Sep 2019 00:52:53.708 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 20 Sep 2019 00:52:53.708 # Server initialized\n1:M 20 Sep 2019 00:52:53.708 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 20 Sep 2019 00:52:53.708 * Ready to accept connections\n"
STEP: exposing RC
Sep 20 00:52:54.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-378'
Sep 20 00:52:54.870: INFO: stderr: ""
Sep 20 00:52:54.870: INFO: stdout: "service/rm2 exposed\n"
Sep 20 00:52:54.874: INFO: Service rm2 in namespace kubectl-378 found.
STEP: exposing service
Sep 20 00:52:56.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-378'
Sep 20 00:52:57.026: INFO: stderr: ""
Sep 20 00:52:57.026: INFO: stdout: "service/rm3 exposed\n"
Sep 20 00:52:57.031: INFO: Service rm3 in namespace kubectl-378 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:52:59.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-378" for this suite.
Sep 20 00:53:11.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:53:11.271: INFO: namespace kubectl-378 deletion completed in 12.216392283s

• [SLOW TEST:24.451 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:53:11.273: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-376
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Sep 20 00:53:15.983: INFO: Successfully updated pod "adopt-release-g72gv"
STEP: Checking that the Job readopts the Pod
Sep 20 00:53:15.983: INFO: Waiting up to 15m0s for pod "adopt-release-g72gv" in namespace "job-376" to be "adopted"
Sep 20 00:53:15.989: INFO: Pod "adopt-release-g72gv": Phase="Running", Reason="", readiness=true. Elapsed: 5.975124ms
Sep 20 00:53:17.996: INFO: Pod "adopt-release-g72gv": Phase="Running", Reason="", readiness=true. Elapsed: 2.012987574s
Sep 20 00:53:17.996: INFO: Pod "adopt-release-g72gv" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Sep 20 00:53:18.512: INFO: Successfully updated pod "adopt-release-g72gv"
STEP: Checking that the Job releases the Pod
Sep 20 00:53:18.512: INFO: Waiting up to 15m0s for pod "adopt-release-g72gv" in namespace "job-376" to be "released"
Sep 20 00:53:18.518: INFO: Pod "adopt-release-g72gv": Phase="Running", Reason="", readiness=true. Elapsed: 5.927226ms
Sep 20 00:53:20.524: INFO: Pod "adopt-release-g72gv": Phase="Running", Reason="", readiness=true. Elapsed: 2.011543349s
Sep 20 00:53:20.524: INFO: Pod "adopt-release-g72gv" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:53:20.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-376" for this suite.
Sep 20 00:54:04.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:54:04.806: INFO: namespace job-376 deletion completed in 44.275213328s

• [SLOW TEST:53.534 seconds]
[sig-apps] Job
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:54:04.816: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-249
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:54:16.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-249" for this suite.
Sep 20 00:54:22.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:54:22.296: INFO: namespace resourcequota-249 deletion completed in 6.206297213s

• [SLOW TEST:17.481 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:54:22.298: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6135
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 00:54:22.911: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 00:54:24.929: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704537662, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704537662, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704537663, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704537662, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 00:54:27.951: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Sep 20 00:54:32.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 attach --namespace=webhook-6135 to-be-attached-pod -i -c=container1'
Sep 20 00:54:32.351: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:54:32.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6135" for this suite.
Sep 20 00:54:44.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:54:44.639: INFO: namespace webhook-6135 deletion completed in 12.265803482s
STEP: Destroying namespace "webhook-6135-markers" for this suite.
Sep 20 00:54:50.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
I0920 00:54:50.795458      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 10 items received
Sep 20 00:54:50.864: INFO: namespace webhook-6135-markers deletion completed in 6.223831775s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:28.591 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:54:50.890: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1963
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Sep 20 00:54:51.062: INFO: PodSpec: initContainers in spec.initContainers
Sep 20 00:55:39.743: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-14c6de61-88b8-437e-b682-c2c98a510410", GenerateName:"", Namespace:"init-container-1963", SelfLink:"/api/v1/namespaces/init-container-1963/pods/pod-init-14c6de61-88b8-437e-b682-c2c98a510410", UID:"a7f51c81-27ab-4509-9071-e78cece7d3ee", ResourceVersion:"64420", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63704537691, loc:(*time.Location)(0x84be2c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"61964671"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.25.3.12/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-sfrgn", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0025c8000), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-sfrgn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-sfrgn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-sfrgn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003fe2648), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"worker-wqshf-7859ffd555-lmjf4", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002a48ea0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003fe26c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003fe26e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003fe26e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003fe26ec), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704537691, loc:(*time.Location)(0x84be2c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704537691, loc:(*time.Location)(0x84be2c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704537691, loc:(*time.Location)(0x84be2c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704537691, loc:(*time.Location)(0x84be2c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"104.248.240.206", PodIP:"172.25.3.12", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.25.3.12"}}, StartTime:(*v1.Time)(0xc000f226c0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0021b5f80)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001d82000)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://29b906dd2aa4c3339b7e3db47a097f0d4c787275194449277f8ebff235629d95", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000f22700), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000f226e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc003fe276f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:55:39.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1963" for this suite.
Sep 20 00:56:07.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:56:07.998: INFO: namespace init-container-1963 deletion completed in 28.246566725s

• [SLOW TEST:77.108 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:56:07.999: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1961
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 20 00:56:14.725: INFO: Successfully updated pod "pod-update-49be1712-1b4c-44b7-94d1-dfd30180cfc1"
STEP: verifying the updated pod is in kubernetes
Sep 20 00:56:14.735: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:56:14.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1961" for this suite.
Sep 20 00:56:26.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:56:26.969: INFO: namespace pods-1961 deletion completed in 12.225567225s

• [SLOW TEST:18.969 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:56:26.971: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8151
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-77c82932-eb30-4217-b568-b88e6030583e
STEP: Creating a pod to test consume configMaps
Sep 20 00:56:27.180: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7d0635c1-d4b8-4662-ad4a-eb75a66217a9" in namespace "projected-8151" to be "success or failure"
Sep 20 00:56:27.191: INFO: Pod "pod-projected-configmaps-7d0635c1-d4b8-4662-ad4a-eb75a66217a9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.505953ms
Sep 20 00:56:29.197: INFO: Pod "pod-projected-configmaps-7d0635c1-d4b8-4662-ad4a-eb75a66217a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01697257s
Sep 20 00:56:31.205: INFO: Pod "pod-projected-configmaps-7d0635c1-d4b8-4662-ad4a-eb75a66217a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024245114s
STEP: Saw pod success
Sep 20 00:56:31.205: INFO: Pod "pod-projected-configmaps-7d0635c1-d4b8-4662-ad4a-eb75a66217a9" satisfied condition "success or failure"
Sep 20 00:56:31.210: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod pod-projected-configmaps-7d0635c1-d4b8-4662-ad4a-eb75a66217a9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 00:56:31.283: INFO: Waiting for pod pod-projected-configmaps-7d0635c1-d4b8-4662-ad4a-eb75a66217a9 to disappear
Sep 20 00:56:31.289: INFO: Pod pod-projected-configmaps-7d0635c1-d4b8-4662-ad4a-eb75a66217a9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:56:31.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8151" for this suite.
Sep 20 00:56:37.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:56:37.574: INFO: namespace projected-8151 deletion completed in 6.276446129s

• [SLOW TEST:10.604 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:56:37.579: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3694
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-8666
STEP: Creating secret with name secret-test-e8133909-a78c-4652-85c6-24203399c7ae
STEP: Creating a pod to test consume secrets
Sep 20 00:56:37.939: INFO: Waiting up to 5m0s for pod "pod-secrets-6fe9a5a4-28ce-49a4-a6ec-b3140ba76ae2" in namespace "secrets-3694" to be "success or failure"
Sep 20 00:56:37.948: INFO: Pod "pod-secrets-6fe9a5a4-28ce-49a4-a6ec-b3140ba76ae2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.879209ms
Sep 20 00:56:39.958: INFO: Pod "pod-secrets-6fe9a5a4-28ce-49a4-a6ec-b3140ba76ae2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01839353s
Sep 20 00:56:41.964: INFO: Pod "pod-secrets-6fe9a5a4-28ce-49a4-a6ec-b3140ba76ae2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024152311s
STEP: Saw pod success
Sep 20 00:56:41.964: INFO: Pod "pod-secrets-6fe9a5a4-28ce-49a4-a6ec-b3140ba76ae2" satisfied condition "success or failure"
Sep 20 00:56:41.974: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod pod-secrets-6fe9a5a4-28ce-49a4-a6ec-b3140ba76ae2 container secret-volume-test: <nil>
STEP: delete the pod
Sep 20 00:56:42.055: INFO: Waiting for pod pod-secrets-6fe9a5a4-28ce-49a4-a6ec-b3140ba76ae2 to disappear
Sep 20 00:56:42.060: INFO: Pod pod-secrets-6fe9a5a4-28ce-49a4-a6ec-b3140ba76ae2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:56:42.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3694" for this suite.
Sep 20 00:56:48.105: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:56:48.308: INFO: namespace secrets-3694 deletion completed in 6.239960482s
STEP: Destroying namespace "secret-namespace-8666" for this suite.
Sep 20 00:56:54.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:56:54.577: INFO: namespace secret-namespace-8666 deletion completed in 6.269127625s

• [SLOW TEST:16.999 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:56:54.578: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2990
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 00:56:54.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 create -f - --namespace=kubectl-2990'
Sep 20 00:56:54.930: INFO: stderr: ""
Sep 20 00:56:54.930: INFO: stdout: "replicationcontroller/redis-master created\n"
Sep 20 00:56:54.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 create -f - --namespace=kubectl-2990'
Sep 20 00:56:55.104: INFO: stderr: ""
Sep 20 00:56:55.104: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep 20 00:56:56.111: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 00:56:56.111: INFO: Found 0 / 1
Sep 20 00:56:57.110: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 00:56:57.110: INFO: Found 0 / 1
Sep 20 00:56:58.111: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 00:56:58.111: INFO: Found 1 / 1
Sep 20 00:56:58.111: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 20 00:56:58.118: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 00:56:58.118: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 20 00:56:58.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 describe pod redis-master-mhrpq --namespace=kubectl-2990'
Sep 20 00:56:58.241: INFO: stderr: ""
Sep 20 00:56:58.241: INFO: stdout: "Name:         redis-master-mhrpq\nNamespace:    kubectl-2990\nPriority:     0\nNode:         worker-wqshf-7859ffd555-kqpmz/104.248.240.176\nStart Time:   Fri, 20 Sep 2019 00:56:55 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 172.25.0.23/32\nStatus:       Running\nIP:           172.25.0.23\nIPs:\n  IP:           172.25.0.23\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://037681c4606e818f2315525a5f6a88c7a4db009a9a3c0f5fc6d649b5ad60b143\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 20 Sep 2019 00:56:57 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-59wwn (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-59wwn:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-59wwn\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                                    Message\n  ----    ------     ----       ----                                    -------\n  Normal  Scheduled  <unknown>  default-scheduler                       Successfully assigned kubectl-2990/redis-master-mhrpq to worker-wqshf-7859ffd555-kqpmz\n  Normal  Pulled     2s         kubelet, worker-wqshf-7859ffd555-kqpmz  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    2s         kubelet, worker-wqshf-7859ffd555-kqpmz  Created container redis-master\n  Normal  Started    1s         kubelet, worker-wqshf-7859ffd555-kqpmz  Started container redis-master\n"
Sep 20 00:56:58.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 describe rc redis-master --namespace=kubectl-2990'
Sep 20 00:56:58.361: INFO: stderr: ""
Sep 20 00:56:58.361: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-2990\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-mhrpq\n"
Sep 20 00:56:58.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 describe service redis-master --namespace=kubectl-2990'
Sep 20 00:56:58.477: INFO: stderr: ""
Sep 20 00:56:58.477: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-2990\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.240.28.32\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.25.0.23:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep 20 00:56:58.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 describe node worker-wqshf-7859ffd555-kqpmz'
Sep 20 00:56:58.654: INFO: stderr: ""
Sep 20 00:56:58.654: INFO: stdout: "Name:               worker-wqshf-7859ffd555-kqpmz\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=worker-wqshf-7859ffd555-kqpmz\n                    kubernetes.io/os=linux\n                    machine-controller/owned-by=b8ea94ff-9680-4513-bc8d-6399740cbf54\n                    system/cluster=dkh5sbgdg6\n                    system/project=zdcr98vvt7\nAnnotations:        cluster.k8s.io/machine: kube-system/worker-wqshf-7859ffd555-kqpmz\n                    flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"12:b1:f0:8a:3e:29\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 104.248.240.176\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.25.0.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 19 Sep 2019 20:33:51 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Fri, 20 Sep 2019 00:56:55 +0000   Fri, 20 Sep 2019 00:34:23 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 20 Sep 2019 00:56:55 +0000   Fri, 20 Sep 2019 00:34:23 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 20 Sep 2019 00:56:55 +0000   Fri, 20 Sep 2019 00:34:23 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 20 Sep 2019 00:56:55 +0000   Fri, 20 Sep 2019 00:34:33 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  104.248.240.176\n  Hostname:    worker-wqshf-7859ffd555-kqpmz\nCapacity:\n cpu:                2\n ephemeral-storage:  60795672Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             2041164Ki\n pods:               110\nAllocatable:\n cpu:                1800m\n ephemeral-storage:  53881807575\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             1733964Ki\n pods:               110\nSystem Info:\n Machine ID:                 17a258b4a45e4bdbb11b5fee1d6162e2\n System UUID:                17A258B4-A45E-4BDB-B11B-5FEE1D6162E2\n Boot ID:                    2399fdf3-fe68-4f60-8af6-93ba5673a68a\n Kernel Version:             4.15.0-58-generic\n OS Image:                   Ubuntu 18.04.3 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.2\n Kubelet Version:            v1.16.0\n Kube-Proxy Version:         v1.16.0\nPodCIDR:                     172.25.0.0/24\nPodCIDRs:                    172.25.0.0/24\nNon-terminated Pods:         (8 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                canal-pwz75                                                250m (13%)    0 (0%)      0 (0%)           0 (0%)         4h23m\n  kube-system                coredns-57f944bd9f-vlw9n                                   100m (5%)     0 (0%)      70Mi (4%)        170Mi (10%)    4h25m\n  kube-system                kube-proxy-xmcqx                                           75m (4%)      250m (13%)  50Mi (2%)        250Mi (14%)    4h23m\n  kube-system                kubernetes-dashboard-7d5fb85f7f-t28jk                      75m (4%)      75m (4%)    50Mi (2%)        50Mi (2%)      4h25m\n  kube-system                node-exporter-rtkgz                                        20m (1%)      45m (2%)    48Mi (2%)        96Mi (5%)      4h23m\n  kube-system                node-local-dns-7tzn5                                       25m (1%)      0 (0%)      5Mi (0%)         30Mi (1%)      4h22m\n  kubectl-2990               redis-master-mhrpq                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-2ks25    0 (0%)        0 (0%)      0 (0%)           0 (0%)         22m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                545m (30%)   370m (20%)\n  memory             223Mi (13%)  596Mi (35%)\n  ephemeral-storage  0 (0%)       0 (0%)\nEvents:\n  Type     Reason                   Age                  From                                       Message\n  ----     ------                   ----                 ----                                       -------\n  Normal   NodeHasSufficientMemory  37m (x2 over 4h23m)  kubelet, worker-wqshf-7859ffd555-kqpmz     Node worker-wqshf-7859ffd555-kqpmz status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    37m (x2 over 4h23m)  kubelet, worker-wqshf-7859ffd555-kqpmz     Node worker-wqshf-7859ffd555-kqpmz status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     37m (x2 over 4h23m)  kubelet, worker-wqshf-7859ffd555-kqpmz     Node worker-wqshf-7859ffd555-kqpmz status is now: NodeHasSufficientPID\n  Normal   NodeReady                37m (x2 over 4h22m)  kubelet, worker-wqshf-7859ffd555-kqpmz     Node worker-wqshf-7859ffd555-kqpmz status is now: NodeReady\n  Normal   NodeHasNoDiskPressure    22m (x2 over 22m)    kubelet, worker-wqshf-7859ffd555-kqpmz     Node worker-wqshf-7859ffd555-kqpmz status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientMemory  22m (x2 over 22m)    kubelet, worker-wqshf-7859ffd555-kqpmz     Node worker-wqshf-7859ffd555-kqpmz status is now: NodeHasSufficientMemory\n  Normal   Starting                 22m                  kubelet, worker-wqshf-7859ffd555-kqpmz     Starting kubelet.\n  Normal   NodeHasSufficientPID     22m (x2 over 22m)    kubelet, worker-wqshf-7859ffd555-kqpmz     Node worker-wqshf-7859ffd555-kqpmz status is now: NodeHasSufficientPID\n  Warning  Rebooted                 22m                  kubelet, worker-wqshf-7859ffd555-kqpmz     Node worker-wqshf-7859ffd555-kqpmz has been rebooted, boot id: 2399fdf3-fe68-4f60-8af6-93ba5673a68a\n  Normal   NodeNotReady             22m                  kubelet, worker-wqshf-7859ffd555-kqpmz     Node worker-wqshf-7859ffd555-kqpmz status is now: NodeNotReady\n  Normal   NodeAllocatableEnforced  22m                  kubelet, worker-wqshf-7859ffd555-kqpmz     Updated Node Allocatable limit across pods\n  Normal   NodeReady                22m                  kubelet, worker-wqshf-7859ffd555-kqpmz     Node worker-wqshf-7859ffd555-kqpmz status is now: NodeReady\n  Normal   Starting                 22m                  kube-proxy, worker-wqshf-7859ffd555-kqpmz  Starting kube-proxy.\n"
Sep 20 00:56:58.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 describe namespace kubectl-2990'
Sep 20 00:56:58.749: INFO: stderr: ""
Sep 20 00:56:58.749: INFO: stdout: "Name:         kubectl-2990\nLabels:       e2e-framework=kubectl\n              e2e-run=88a73f13-b6b0-4120-abe3-f39ba7ab759e\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:56:58.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2990" for this suite.
Sep 20 00:57:26.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:57:27.013: INFO: namespace kubectl-2990 deletion completed in 28.257726551s

• [SLOW TEST:32.435 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:57:27.015: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4133
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep 20 00:57:27.211: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4133 /api/v1/namespaces/watch-4133/configmaps/e2e-watch-test-watch-closed 9da28b49-2e3c-424c-9b7c-8a954e12d0bc 64883 0 2019-09-20 00:57:27 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 20 00:57:27.212: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4133 /api/v1/namespaces/watch-4133/configmaps/e2e-watch-test-watch-closed 9da28b49-2e3c-424c-9b7c-8a954e12d0bc 64884 0 2019-09-20 00:57:27 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep 20 00:57:27.247: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4133 /api/v1/namespaces/watch-4133/configmaps/e2e-watch-test-watch-closed 9da28b49-2e3c-424c-9b7c-8a954e12d0bc 64885 0 2019-09-20 00:57:27 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 20 00:57:27.248: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4133 /api/v1/namespaces/watch-4133/configmaps/e2e-watch-test-watch-closed 9da28b49-2e3c-424c-9b7c-8a954e12d0bc 64886 0 2019-09-20 00:57:27 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:57:27.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4133" for this suite.
Sep 20 00:57:33.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:57:33.510: INFO: namespace watch-4133 deletion completed in 6.252194384s

• [SLOW TEST:6.495 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:57:33.512: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6934
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-7e8ba923-23b2-4a61-8d63-3252a39aa01b
STEP: Creating a pod to test consume secrets
Sep 20 00:57:33.707: INFO: Waiting up to 5m0s for pod "pod-secrets-b4ea897e-c1ce-482c-aa57-79d142918d0d" in namespace "secrets-6934" to be "success or failure"
Sep 20 00:57:33.713: INFO: Pod "pod-secrets-b4ea897e-c1ce-482c-aa57-79d142918d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.468972ms
Sep 20 00:57:35.719: INFO: Pod "pod-secrets-b4ea897e-c1ce-482c-aa57-79d142918d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012031919s
Sep 20 00:57:37.727: INFO: Pod "pod-secrets-b4ea897e-c1ce-482c-aa57-79d142918d0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019604664s
STEP: Saw pod success
Sep 20 00:57:37.727: INFO: Pod "pod-secrets-b4ea897e-c1ce-482c-aa57-79d142918d0d" satisfied condition "success or failure"
Sep 20 00:57:37.732: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod pod-secrets-b4ea897e-c1ce-482c-aa57-79d142918d0d container secret-volume-test: <nil>
STEP: delete the pod
Sep 20 00:57:37.771: INFO: Waiting for pod pod-secrets-b4ea897e-c1ce-482c-aa57-79d142918d0d to disappear
Sep 20 00:57:37.775: INFO: Pod pod-secrets-b4ea897e-c1ce-482c-aa57-79d142918d0d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:57:37.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6934" for this suite.
Sep 20 00:57:43.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:57:44.014: INFO: namespace secrets-6934 deletion completed in 6.231139109s

• [SLOW TEST:10.503 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:57:44.017: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6648
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 20 00:57:44.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-6648'
Sep 20 00:57:44.316: INFO: stderr: ""
Sep 20 00:57:44.316: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Sep 20 00:57:44.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 delete pods e2e-test-httpd-pod --namespace=kubectl-6648'
Sep 20 00:57:53.566: INFO: stderr: ""
Sep 20 00:57:53.566: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:57:53.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6648" for this suite.
Sep 20 00:57:59.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:57:59.774: INFO: namespace kubectl-6648 deletion completed in 6.200849335s

• [SLOW TEST:15.758 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:57:59.776: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9262
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 00:57:59.975: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6ed7c701-0c93-41e7-b83e-50f61bfcbbd1" in namespace "downward-api-9262" to be "success or failure"
Sep 20 00:57:59.980: INFO: Pod "downwardapi-volume-6ed7c701-0c93-41e7-b83e-50f61bfcbbd1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.610021ms
Sep 20 00:58:01.985: INFO: Pod "downwardapi-volume-6ed7c701-0c93-41e7-b83e-50f61bfcbbd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010082445s
Sep 20 00:58:03.992: INFO: Pod "downwardapi-volume-6ed7c701-0c93-41e7-b83e-50f61bfcbbd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017101506s
STEP: Saw pod success
Sep 20 00:58:03.992: INFO: Pod "downwardapi-volume-6ed7c701-0c93-41e7-b83e-50f61bfcbbd1" satisfied condition "success or failure"
Sep 20 00:58:03.997: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod downwardapi-volume-6ed7c701-0c93-41e7-b83e-50f61bfcbbd1 container client-container: <nil>
STEP: delete the pod
Sep 20 00:58:04.071: INFO: Waiting for pod downwardapi-volume-6ed7c701-0c93-41e7-b83e-50f61bfcbbd1 to disappear
Sep 20 00:58:04.076: INFO: Pod downwardapi-volume-6ed7c701-0c93-41e7-b83e-50f61bfcbbd1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:58:04.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9262" for this suite.
Sep 20 00:58:10.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:58:10.290: INFO: namespace downward-api-9262 deletion completed in 6.205792929s

• [SLOW TEST:10.515 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:58:10.298: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-630
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
Sep 20 00:58:10.588: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 20 00:59:10.627: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 00:59:10.632: INFO: Starting informer...
STEP: Starting pods...
I0920 00:59:10.632842      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/e2e/scheduling/taints.go:146
I0920 00:59:10.632867      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/e2e/scheduling/taints.go:146
Sep 20 00:59:10.858: INFO: Pod1 is running on worker-wqshf-7859ffd555-kqpmz. Tainting Node
Sep 20 00:59:15.088: INFO: Pod2 is running on worker-wqshf-7859ffd555-kqpmz. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Sep 20 00:59:33.149: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Sep 20 00:59:43.779: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:59:43.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-630" for this suite.
Sep 20 00:59:51.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 00:59:52.043: INFO: namespace taint-multiple-pods-630 deletion completed in 8.223286365s

• [SLOW TEST:101.747 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 00:59:52.044: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9774
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 00:59:52.915: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 00:59:54.932: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704537992, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704537992, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704537992, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704537992, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 00:59:57.954: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 00:59:58.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9774" for this suite.
Sep 20 01:00:04.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:00:04.459: INFO: namespace webhook-9774 deletion completed in 6.21765662s
STEP: Destroying namespace "webhook-9774-markers" for this suite.
Sep 20 01:00:10.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:00:10.662: INFO: namespace webhook-9774-markers deletion completed in 6.201244779s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.648 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:00:10.698: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3390
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 01:00:10.883: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-d0088cc1-0fb1-4ad7-abed-3ec1f21c4e5b" in namespace "security-context-test-3390" to be "success or failure"
Sep 20 01:00:10.892: INFO: Pod "alpine-nnp-false-d0088cc1-0fb1-4ad7-abed-3ec1f21c4e5b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016288ms
Sep 20 01:00:12.898: INFO: Pod "alpine-nnp-false-d0088cc1-0fb1-4ad7-abed-3ec1f21c4e5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014033365s
Sep 20 01:00:14.905: INFO: Pod "alpine-nnp-false-d0088cc1-0fb1-4ad7-abed-3ec1f21c4e5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021318144s
Sep 20 01:00:14.905: INFO: Pod "alpine-nnp-false-d0088cc1-0fb1-4ad7-abed-3ec1f21c4e5b" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:00:14.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3390" for this suite.
Sep 20 01:00:20.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:00:21.203: INFO: namespace security-context-test-3390 deletion completed in 6.238951367s

• [SLOW TEST:10.506 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:00:21.211: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-269
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-0d744165-0b51-4e54-bab7-4b3a53b1daca
STEP: Creating a pod to test consume configMaps
Sep 20 01:00:21.418: INFO: Waiting up to 5m0s for pod "pod-configmaps-b8223041-b3d5-48e2-a3ef-d78ad17d81d8" in namespace "configmap-269" to be "success or failure"
Sep 20 01:00:21.426: INFO: Pod "pod-configmaps-b8223041-b3d5-48e2-a3ef-d78ad17d81d8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.029687ms
Sep 20 01:00:23.434: INFO: Pod "pod-configmaps-b8223041-b3d5-48e2-a3ef-d78ad17d81d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016027169s
Sep 20 01:00:25.441: INFO: Pod "pod-configmaps-b8223041-b3d5-48e2-a3ef-d78ad17d81d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022407239s
STEP: Saw pod success
Sep 20 01:00:25.441: INFO: Pod "pod-configmaps-b8223041-b3d5-48e2-a3ef-d78ad17d81d8" satisfied condition "success or failure"
Sep 20 01:00:25.445: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod pod-configmaps-b8223041-b3d5-48e2-a3ef-d78ad17d81d8 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 01:00:25.522: INFO: Waiting for pod pod-configmaps-b8223041-b3d5-48e2-a3ef-d78ad17d81d8 to disappear
Sep 20 01:00:25.530: INFO: Pod pod-configmaps-b8223041-b3d5-48e2-a3ef-d78ad17d81d8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:00:25.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-269" for this suite.
Sep 20 01:00:31.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:00:31.790: INFO: namespace configmap-269 deletion completed in 6.252279489s

• [SLOW TEST:10.580 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:00:31.791: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename resourcequota
I0920 01:00:31.804292      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1095
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:00:43.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1095" for this suite.
Sep 20 01:00:49.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:00:49.313: INFO: namespace resourcequota-1095 deletion completed in 6.24982094s

• [SLOW TEST:17.522 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:00:49.323: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7995
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Sep 20 01:00:50.040: INFO: created pod pod-service-account-defaultsa
Sep 20 01:00:50.040: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep 20 01:00:50.049: INFO: created pod pod-service-account-mountsa
Sep 20 01:00:50.049: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep 20 01:00:50.057: INFO: created pod pod-service-account-nomountsa
Sep 20 01:00:50.057: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep 20 01:00:50.067: INFO: created pod pod-service-account-defaultsa-mountspec
Sep 20 01:00:50.067: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep 20 01:00:50.079: INFO: created pod pod-service-account-mountsa-mountspec
Sep 20 01:00:50.079: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep 20 01:00:50.089: INFO: created pod pod-service-account-nomountsa-mountspec
Sep 20 01:00:50.089: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep 20 01:00:50.097: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep 20 01:00:50.097: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep 20 01:00:50.107: INFO: created pod pod-service-account-mountsa-nomountspec
Sep 20 01:00:50.107: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep 20 01:00:50.118: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep 20 01:00:50.118: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:00:50.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7995" for this suite.
Sep 20 01:00:56.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:00:56.347: INFO: namespace svcaccounts-7995 deletion completed in 6.222502497s

• [SLOW TEST:7.025 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:00:56.355: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8945
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-8w7r
STEP: Creating a pod to test atomic-volume-subpath
Sep 20 01:00:56.563: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-8w7r" in namespace "subpath-8945" to be "success or failure"
Sep 20 01:00:56.571: INFO: Pod "pod-subpath-test-projected-8w7r": Phase="Pending", Reason="", readiness=false. Elapsed: 8.153676ms
Sep 20 01:00:58.577: INFO: Pod "pod-subpath-test-projected-8w7r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013920517s
Sep 20 01:01:00.584: INFO: Pod "pod-subpath-test-projected-8w7r": Phase="Running", Reason="", readiness=true. Elapsed: 4.021186667s
Sep 20 01:01:02.590: INFO: Pod "pod-subpath-test-projected-8w7r": Phase="Running", Reason="", readiness=true. Elapsed: 6.027374858s
Sep 20 01:01:04.598: INFO: Pod "pod-subpath-test-projected-8w7r": Phase="Running", Reason="", readiness=true. Elapsed: 8.035375795s
Sep 20 01:01:06.605: INFO: Pod "pod-subpath-test-projected-8w7r": Phase="Running", Reason="", readiness=true. Elapsed: 10.042442s
Sep 20 01:01:08.612: INFO: Pod "pod-subpath-test-projected-8w7r": Phase="Running", Reason="", readiness=true. Elapsed: 12.049059459s
Sep 20 01:01:10.619: INFO: Pod "pod-subpath-test-projected-8w7r": Phase="Running", Reason="", readiness=true. Elapsed: 14.056532008s
Sep 20 01:01:12.626: INFO: Pod "pod-subpath-test-projected-8w7r": Phase="Running", Reason="", readiness=true. Elapsed: 16.063372658s
Sep 20 01:01:14.634: INFO: Pod "pod-subpath-test-projected-8w7r": Phase="Running", Reason="", readiness=true. Elapsed: 18.071087324s
Sep 20 01:01:16.645: INFO: Pod "pod-subpath-test-projected-8w7r": Phase="Running", Reason="", readiness=true. Elapsed: 20.081912228s
Sep 20 01:01:18.652: INFO: Pod "pod-subpath-test-projected-8w7r": Phase="Running", Reason="", readiness=true. Elapsed: 22.088975578s
Sep 20 01:01:20.660: INFO: Pod "pod-subpath-test-projected-8w7r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.097628033s
STEP: Saw pod success
Sep 20 01:01:20.661: INFO: Pod "pod-subpath-test-projected-8w7r" satisfied condition "success or failure"
Sep 20 01:01:20.668: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod pod-subpath-test-projected-8w7r container test-container-subpath-projected-8w7r: <nil>
STEP: delete the pod
Sep 20 01:01:20.747: INFO: Waiting for pod pod-subpath-test-projected-8w7r to disappear
Sep 20 01:01:20.756: INFO: Pod pod-subpath-test-projected-8w7r no longer exists
STEP: Deleting pod pod-subpath-test-projected-8w7r
Sep 20 01:01:20.756: INFO: Deleting pod "pod-subpath-test-projected-8w7r" in namespace "subpath-8945"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:01:20.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8945" for this suite.
Sep 20 01:01:26.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:01:26.982: INFO: namespace subpath-8945 deletion completed in 6.212776713s

• [SLOW TEST:30.628 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:01:26.983: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3170
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3170.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3170.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3170.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3170.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3170.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3170.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 20 01:01:43.856: INFO: DNS probes using dns-3170/dns-test-0f29ca67-7751-4890-89a4-611619c909fe succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:01:43.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3170" for this suite.
Sep 20 01:01:49.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:01:50.176: INFO: namespace dns-3170 deletion completed in 6.271483546s

• [SLOW TEST:23.193 seconds]
[sig-network] DNS
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:01:50.182: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-509
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-5d465f15-6219-4466-b108-216dbaeae7fe
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-5d465f15-6219-4466-b108-216dbaeae7fe
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:02:57.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-509" for this suite.
Sep 20 01:03:09.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:03:09.866: INFO: namespace configmap-509 deletion completed in 12.204512546s

• [SLOW TEST:79.684 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:03:09.868: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5287
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Sep 20 01:03:14.610: INFO: Successfully updated pod "labelsupdate69656617-aa0d-450e-92d3-4a77ac257bf8"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:03:16.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5287" for this suite.
Sep 20 01:03:28.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:03:28.873: INFO: namespace downward-api-5287 deletion completed in 12.212877268s

• [SLOW TEST:19.005 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:03:28.874: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-498
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 20 01:03:29.064: INFO: Waiting up to 5m0s for pod "pod-14f85160-d8d8-4dc8-97bb-a5ebb425eea2" in namespace "emptydir-498" to be "success or failure"
Sep 20 01:03:29.071: INFO: Pod "pod-14f85160-d8d8-4dc8-97bb-a5ebb425eea2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.423496ms
Sep 20 01:03:31.077: INFO: Pod "pod-14f85160-d8d8-4dc8-97bb-a5ebb425eea2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012725079s
Sep 20 01:03:33.086: INFO: Pod "pod-14f85160-d8d8-4dc8-97bb-a5ebb425eea2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021540241s
STEP: Saw pod success
Sep 20 01:03:33.086: INFO: Pod "pod-14f85160-d8d8-4dc8-97bb-a5ebb425eea2" satisfied condition "success or failure"
Sep 20 01:03:33.091: INFO: Trying to get logs from node worker-wqshf-7859ffd555-lmjf4 pod pod-14f85160-d8d8-4dc8-97bb-a5ebb425eea2 container test-container: <nil>
STEP: delete the pod
Sep 20 01:03:33.128: INFO: Waiting for pod pod-14f85160-d8d8-4dc8-97bb-a5ebb425eea2 to disappear
Sep 20 01:03:33.133: INFO: Pod pod-14f85160-d8d8-4dc8-97bb-a5ebb425eea2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:03:33.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-498" for this suite.
Sep 20 01:03:39.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:03:39.387: INFO: namespace emptydir-498 deletion completed in 6.248524418s

• [SLOW TEST:10.513 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:03:39.390: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-628
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 01:03:39.585: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ef2d6cf8-c299-4e26-a511-ccd8bdf8d765" in namespace "projected-628" to be "success or failure"
Sep 20 01:03:39.591: INFO: Pod "downwardapi-volume-ef2d6cf8-c299-4e26-a511-ccd8bdf8d765": Phase="Pending", Reason="", readiness=false. Elapsed: 5.85329ms
Sep 20 01:03:41.598: INFO: Pod "downwardapi-volume-ef2d6cf8-c299-4e26-a511-ccd8bdf8d765": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012927644s
Sep 20 01:03:43.604: INFO: Pod "downwardapi-volume-ef2d6cf8-c299-4e26-a511-ccd8bdf8d765": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018876903s
STEP: Saw pod success
Sep 20 01:03:43.605: INFO: Pod "downwardapi-volume-ef2d6cf8-c299-4e26-a511-ccd8bdf8d765" satisfied condition "success or failure"
Sep 20 01:03:43.612: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod downwardapi-volume-ef2d6cf8-c299-4e26-a511-ccd8bdf8d765 container client-container: <nil>
STEP: delete the pod
Sep 20 01:03:43.662: INFO: Waiting for pod downwardapi-volume-ef2d6cf8-c299-4e26-a511-ccd8bdf8d765 to disappear
Sep 20 01:03:43.671: INFO: Pod downwardapi-volume-ef2d6cf8-c299-4e26-a511-ccd8bdf8d765 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:03:43.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-628" for this suite.
Sep 20 01:03:49.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:03:49.894: INFO: namespace projected-628 deletion completed in 6.214632035s

• [SLOW TEST:10.505 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:03:49.900: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-645
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-624cb200-d99d-42ee-9070-96ae8923f0e8
STEP: Creating a pod to test consume secrets
Sep 20 01:03:50.086: INFO: Waiting up to 5m0s for pod "pod-secrets-31509be2-e03c-4317-b8e8-0f026de5044f" in namespace "secrets-645" to be "success or failure"
Sep 20 01:03:50.092: INFO: Pod "pod-secrets-31509be2-e03c-4317-b8e8-0f026de5044f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.905672ms
Sep 20 01:03:52.097: INFO: Pod "pod-secrets-31509be2-e03c-4317-b8e8-0f026de5044f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011462821s
Sep 20 01:03:54.104: INFO: Pod "pod-secrets-31509be2-e03c-4317-b8e8-0f026de5044f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01790994s
STEP: Saw pod success
Sep 20 01:03:54.104: INFO: Pod "pod-secrets-31509be2-e03c-4317-b8e8-0f026de5044f" satisfied condition "success or failure"
Sep 20 01:03:54.109: INFO: Trying to get logs from node worker-wqshf-7859ffd555-lmjf4 pod pod-secrets-31509be2-e03c-4317-b8e8-0f026de5044f container secret-env-test: <nil>
STEP: delete the pod
Sep 20 01:03:54.153: INFO: Waiting for pod pod-secrets-31509be2-e03c-4317-b8e8-0f026de5044f to disappear
Sep 20 01:03:54.158: INFO: Pod pod-secrets-31509be2-e03c-4317-b8e8-0f026de5044f no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:03:54.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-645" for this suite.
Sep 20 01:04:00.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:04:00.379: INFO: namespace secrets-645 deletion completed in 6.212174361s

• [SLOW TEST:10.480 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:04:00.380: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-690
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 01:04:01.294: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 20 01:04:03.313: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538241, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538241, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538241, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538241, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 01:04:06.334: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:04:19.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-690" for this suite.
Sep 20 01:04:25.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:04:25.502: INFO: namespace webhook-690 deletion completed in 6.215191965s
STEP: Destroying namespace "webhook-690-markers" for this suite.
Sep 20 01:04:31.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:04:31.734: INFO: namespace webhook-690-markers deletion completed in 6.232167113s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:31.381 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:04:31.766: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2721
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 01:04:31.999: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep 20 01:04:32.023: INFO: Number of nodes with available pods: 0
Sep 20 01:04:32.024: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 01:04:33.035: INFO: Number of nodes with available pods: 0
Sep 20 01:04:33.035: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 01:04:34.038: INFO: Number of nodes with available pods: 0
Sep 20 01:04:34.038: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 01:04:35.038: INFO: Number of nodes with available pods: 5
Sep 20 01:04:35.038: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep 20 01:04:35.082: INFO: Wrong image for pod: daemon-set-2qqqm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:35.082: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:35.082: INFO: Wrong image for pod: daemon-set-lll4j. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:35.082: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:35.083: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:36.102: INFO: Wrong image for pod: daemon-set-2qqqm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:36.102: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:36.103: INFO: Wrong image for pod: daemon-set-lll4j. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:36.103: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:36.103: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:37.102: INFO: Wrong image for pod: daemon-set-2qqqm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:37.103: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:37.103: INFO: Wrong image for pod: daemon-set-lll4j. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:37.103: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:37.103: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:38.102: INFO: Wrong image for pod: daemon-set-2qqqm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:38.102: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:38.102: INFO: Wrong image for pod: daemon-set-lll4j. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:38.102: INFO: Pod daemon-set-lll4j is not available
Sep 20 01:04:38.102: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:38.103: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:39.104: INFO: Wrong image for pod: daemon-set-2qqqm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:39.104: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:39.104: INFO: Wrong image for pod: daemon-set-lll4j. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:39.105: INFO: Pod daemon-set-lll4j is not available
Sep 20 01:04:39.105: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:39.105: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:40.107: INFO: Wrong image for pod: daemon-set-2qqqm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:40.107: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:40.108: INFO: Wrong image for pod: daemon-set-lll4j. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:40.108: INFO: Pod daemon-set-lll4j is not available
Sep 20 01:04:40.108: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:40.108: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:41.101: INFO: Wrong image for pod: daemon-set-2qqqm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:41.101: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:41.101: INFO: Wrong image for pod: daemon-set-lll4j. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:41.102: INFO: Pod daemon-set-lll4j is not available
Sep 20 01:04:41.102: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:41.102: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:42.104: INFO: Wrong image for pod: daemon-set-2qqqm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:42.105: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:42.105: INFO: Wrong image for pod: daemon-set-lll4j. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:42.105: INFO: Pod daemon-set-lll4j is not available
Sep 20 01:04:42.105: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:42.106: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:43.103: INFO: Wrong image for pod: daemon-set-2qqqm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:43.103: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:43.103: INFO: Wrong image for pod: daemon-set-lll4j. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:43.103: INFO: Pod daemon-set-lll4j is not available
Sep 20 01:04:43.103: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:43.103: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:44.105: INFO: Wrong image for pod: daemon-set-2qqqm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:44.105: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:44.105: INFO: Pod daemon-set-lq2pd is not available
Sep 20 01:04:44.105: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:44.105: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:45.103: INFO: Wrong image for pod: daemon-set-2qqqm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:45.104: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:45.104: INFO: Pod daemon-set-lq2pd is not available
Sep 20 01:04:45.104: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:45.104: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:46.104: INFO: Wrong image for pod: daemon-set-2qqqm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:46.104: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:46.104: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:46.104: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:47.105: INFO: Wrong image for pod: daemon-set-2qqqm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:47.106: INFO: Pod daemon-set-2qqqm is not available
Sep 20 01:04:47.106: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:47.106: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:47.106: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:48.101: INFO: Wrong image for pod: daemon-set-2qqqm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:48.101: INFO: Pod daemon-set-2qqqm is not available
Sep 20 01:04:48.101: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:48.101: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:48.101: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:49.103: INFO: Wrong image for pod: daemon-set-2qqqm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:49.103: INFO: Pod daemon-set-2qqqm is not available
Sep 20 01:04:49.103: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:49.103: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:49.103: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:50.102: INFO: Wrong image for pod: daemon-set-2qqqm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:50.102: INFO: Pod daemon-set-2qqqm is not available
Sep 20 01:04:50.102: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:50.102: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:50.102: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:51.101: INFO: Wrong image for pod: daemon-set-2qqqm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:51.102: INFO: Pod daemon-set-2qqqm is not available
Sep 20 01:04:51.102: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:51.102: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:51.102: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:52.103: INFO: Wrong image for pod: daemon-set-2qqqm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:52.103: INFO: Pod daemon-set-2qqqm is not available
Sep 20 01:04:52.103: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:52.103: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:52.103: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:53.102: INFO: Wrong image for pod: daemon-set-2qqqm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:53.102: INFO: Pod daemon-set-2qqqm is not available
Sep 20 01:04:53.102: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:53.102: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:53.102: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:54.102: INFO: Wrong image for pod: daemon-set-2qqqm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:54.102: INFO: Pod daemon-set-2qqqm is not available
Sep 20 01:04:54.102: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:54.102: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:54.102: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:55.102: INFO: Wrong image for pod: daemon-set-2qqqm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:55.102: INFO: Pod daemon-set-2qqqm is not available
Sep 20 01:04:55.102: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:55.102: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:55.102: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:56.102: INFO: Pod daemon-set-2wl25 is not available
Sep 20 01:04:56.102: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:56.102: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:56.102: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:57.103: INFO: Pod daemon-set-2wl25 is not available
Sep 20 01:04:57.103: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:57.103: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:57.103: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:58.102: INFO: Pod daemon-set-2wl25 is not available
Sep 20 01:04:58.102: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:58.102: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:58.102: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:59.102: INFO: Pod daemon-set-2wl25 is not available
Sep 20 01:04:59.102: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:59.102: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:04:59.102: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:00.103: INFO: Pod daemon-set-2wl25 is not available
Sep 20 01:05:00.103: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:00.103: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:00.103: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:01.103: INFO: Pod daemon-set-2wl25 is not available
Sep 20 01:05:01.103: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:01.103: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:01.103: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:02.102: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:02.102: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:02.102: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:03.105: INFO: Wrong image for pod: daemon-set-glb2p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:03.105: INFO: Pod daemon-set-glb2p is not available
Sep 20 01:05:03.105: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:03.105: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:04.101: INFO: Pod daemon-set-c7pnx is not available
Sep 20 01:05:04.101: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:04.101: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:05.102: INFO: Pod daemon-set-c7pnx is not available
Sep 20 01:05:05.102: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:05.102: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:06.102: INFO: Pod daemon-set-c7pnx is not available
Sep 20 01:05:06.102: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:06.102: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:07.100: INFO: Pod daemon-set-c7pnx is not available
Sep 20 01:05:07.100: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:07.100: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:08.101: INFO: Pod daemon-set-c7pnx is not available
Sep 20 01:05:08.101: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:08.101: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:09.102: INFO: Pod daemon-set-c7pnx is not available
Sep 20 01:05:09.102: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:09.103: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:10.101: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:10.101: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:11.102: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:11.102: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:12.103: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:12.103: INFO: Pod daemon-set-n4cx5 is not available
Sep 20 01:05:12.103: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:13.103: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:13.103: INFO: Pod daemon-set-n4cx5 is not available
Sep 20 01:05:13.103: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:14.101: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:14.101: INFO: Pod daemon-set-n4cx5 is not available
Sep 20 01:05:14.102: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:15.102: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:15.103: INFO: Pod daemon-set-n4cx5 is not available
Sep 20 01:05:15.103: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:16.102: INFO: Wrong image for pod: daemon-set-n4cx5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:16.102: INFO: Pod daemon-set-n4cx5 is not available
Sep 20 01:05:16.102: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:17.102: INFO: Pod daemon-set-nmm9s is not available
Sep 20 01:05:17.102: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:18.100: INFO: Pod daemon-set-nmm9s is not available
Sep 20 01:05:18.100: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:19.102: INFO: Pod daemon-set-nmm9s is not available
Sep 20 01:05:19.103: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:20.102: INFO: Pod daemon-set-nmm9s is not available
Sep 20 01:05:20.102: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:21.101: INFO: Pod daemon-set-nmm9s is not available
Sep 20 01:05:21.101: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:22.101: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:23.103: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:24.102: INFO: Wrong image for pod: daemon-set-pxpnl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Sep 20 01:05:24.103: INFO: Pod daemon-set-pxpnl is not available
Sep 20 01:05:25.101: INFO: Pod daemon-set-dglbn is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Sep 20 01:05:25.121: INFO: Number of nodes with available pods: 4
Sep 20 01:05:25.121: INFO: Node worker-wqshf-7859ffd555-r6tnq is running more than one daemon pod
Sep 20 01:05:26.134: INFO: Number of nodes with available pods: 4
Sep 20 01:05:26.134: INFO: Node worker-wqshf-7859ffd555-r6tnq is running more than one daemon pod
Sep 20 01:05:27.137: INFO: Number of nodes with available pods: 4
Sep 20 01:05:27.137: INFO: Node worker-wqshf-7859ffd555-r6tnq is running more than one daemon pod
Sep 20 01:05:28.133: INFO: Number of nodes with available pods: 4
Sep 20 01:05:28.133: INFO: Node worker-wqshf-7859ffd555-r6tnq is running more than one daemon pod
Sep 20 01:05:29.135: INFO: Number of nodes with available pods: 4
Sep 20 01:05:29.135: INFO: Node worker-wqshf-7859ffd555-r6tnq is running more than one daemon pod
Sep 20 01:05:30.133: INFO: Number of nodes with available pods: 4
Sep 20 01:05:30.133: INFO: Node worker-wqshf-7859ffd555-r6tnq is running more than one daemon pod
Sep 20 01:05:31.135: INFO: Number of nodes with available pods: 5
Sep 20 01:05:31.136: INFO: Number of running nodes: 5, number of available pods: 5
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2721, will wait for the garbage collector to delete the pods
I0920 01:05:31.168892      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 01:05:31.169164      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 01:05:31.230: INFO: Deleting DaemonSet.extensions daemon-set took: 11.533593ms
I0920 01:05:31.634399      16 controller_utils.go:810] Ignoring inactive pod daemonsets-2721/daemon-set-nmm9s in state Running, deletion time 2019-09-20 01:06:01 +0000 UTC
I0920 01:05:31.634568      16 controller_utils.go:810] Ignoring inactive pod daemonsets-2721/daemon-set-dglbn in state Running, deletion time 2019-09-20 01:06:01 +0000 UTC
I0920 01:05:31.634655      16 controller_utils.go:810] Ignoring inactive pod daemonsets-2721/daemon-set-c7pnx in state Running, deletion time 2019-09-20 01:06:01 +0000 UTC
I0920 01:05:31.634704      16 controller_utils.go:810] Ignoring inactive pod daemonsets-2721/daemon-set-2wl25 in state Running, deletion time 2019-09-20 01:06:01 +0000 UTC
I0920 01:05:31.634775      16 controller_utils.go:810] Ignoring inactive pod daemonsets-2721/daemon-set-lq2pd in state Running, deletion time 2019-09-20 01:06:01 +0000 UTC
Sep 20 01:05:31.634: INFO: Terminating DaemonSet.extensions daemon-set pods took: 404.389476ms
Sep 20 01:05:43.640: INFO: Number of nodes with available pods: 0
Sep 20 01:05:43.640: INFO: Number of running nodes: 0, number of available pods: 0
Sep 20 01:05:43.648: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2721/daemonsets","resourceVersion":"67225"},"items":null}

Sep 20 01:05:43.656: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2721/pods","resourceVersion":"67225"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:05:43.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2721" for this suite.
Sep 20 01:05:49.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:05:49.940: INFO: namespace daemonsets-2721 deletion completed in 6.232906276s

• [SLOW TEST:78.174 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:05:49.944: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1927
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Sep 20 01:05:50.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 api-versions'
Sep 20 01:05:50.228: INFO: stderr: ""
Sep 20 01:05:50.228: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:05:50.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1927" for this suite.
Sep 20 01:05:56.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:05:56.469: INFO: namespace kubectl-1927 deletion completed in 6.234023087s

• [SLOW TEST:6.526 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:05:56.471: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5426
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-5426
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 20 01:05:56.643: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
I0920 01:06:06.652686      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 19 items received
STEP: Creating test pods
Sep 20 01:06:20.872: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.39:8080/dial?request=hostName&protocol=udp&host=172.25.1.29&port=8081&tries=1'] Namespace:pod-network-test-5426 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 01:06:20.872: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 01:06:21.518: INFO: Waiting for endpoints: map[]
Sep 20 01:06:21.525: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.39:8080/dial?request=hostName&protocol=udp&host=172.25.2.7&port=8081&tries=1'] Namespace:pod-network-test-5426 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 01:06:21.525: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 01:06:22.161: INFO: Waiting for endpoints: map[]
Sep 20 01:06:22.168: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.39:8080/dial?request=hostName&protocol=udp&host=172.25.3.23&port=8081&tries=1'] Namespace:pod-network-test-5426 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 01:06:22.168: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 01:06:22.830: INFO: Waiting for endpoints: map[]
Sep 20 01:06:22.836: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.39:8080/dial?request=hostName&protocol=udp&host=172.25.4.10&port=8081&tries=1'] Namespace:pod-network-test-5426 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 01:06:22.836: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 01:06:23.514: INFO: Waiting for endpoints: map[]
Sep 20 01:06:23.520: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.39:8080/dial?request=hostName&protocol=udp&host=172.25.0.38&port=8081&tries=1'] Namespace:pod-network-test-5426 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 01:06:23.520: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 01:06:24.125: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:06:24.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5426" for this suite.
Sep 20 01:06:36.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:06:36.356: INFO: namespace pod-network-test-5426 deletion completed in 12.220646636s

• [SLOW TEST:39.886 seconds]
[sig-network] Networking
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:06:36.368: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-814
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Sep 20 01:06:36.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-814 -- logs-generator --log-lines-total 100 --run-duration 20s'
Sep 20 01:06:36.937: INFO: stderr: ""
Sep 20 01:06:36.937: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Sep 20 01:06:36.937: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Sep 20 01:06:36.937: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-814" to be "running and ready, or succeeded"
Sep 20 01:06:36.946: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 8.345721ms
Sep 20 01:06:38.952: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014555496s
Sep 20 01:06:40.959: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.021341112s
Sep 20 01:06:40.959: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Sep 20 01:06:40.959: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Sep 20 01:06:40.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 logs logs-generator logs-generator --namespace=kubectl-814'
Sep 20 01:06:41.092: INFO: stderr: ""
Sep 20 01:06:41.092: INFO: stdout: "I0920 01:06:38.656467       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/ckhd 432\nI0920 01:06:38.856897       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/58m 406\nI0920 01:06:39.056655       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/jbcc 205\nI0920 01:06:39.256671       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/sdzq 549\nI0920 01:06:39.456700       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/5lk 333\nI0920 01:06:39.656791       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/8bd 389\nI0920 01:06:39.856814       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/5zzc 585\nI0920 01:06:40.056803       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/gttz 331\nI0920 01:06:40.256795       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/pksl 582\nI0920 01:06:40.456770       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/94z 580\nI0920 01:06:40.656788       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/qfrb 340\nI0920 01:06:40.856691       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/8kkq 375\nI0920 01:06:41.056700       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/nvw 343\n"
STEP: limiting log lines
Sep 20 01:06:41.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 logs logs-generator logs-generator --namespace=kubectl-814 --tail=1'
Sep 20 01:06:41.208: INFO: stderr: ""
Sep 20 01:06:41.208: INFO: stdout: "I0920 01:06:41.056700       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/nvw 343\n"
STEP: limiting log bytes
Sep 20 01:06:41.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 logs logs-generator logs-generator --namespace=kubectl-814 --limit-bytes=1'
Sep 20 01:06:41.320: INFO: stderr: ""
Sep 20 01:06:41.320: INFO: stdout: "I"
STEP: exposing timestamps
Sep 20 01:06:41.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 logs logs-generator logs-generator --namespace=kubectl-814 --tail=1 --timestamps'
Sep 20 01:06:41.435: INFO: stderr: ""
Sep 20 01:06:41.435: INFO: stdout: "2019-09-20T01:06:41.257185696Z I0920 01:06:41.256795       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/p29 463\n"
STEP: restricting to a time range
Sep 20 01:06:43.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 logs logs-generator logs-generator --namespace=kubectl-814 --since=1s'
Sep 20 01:06:44.146: INFO: stderr: ""
Sep 20 01:06:44.146: INFO: stdout: "I0920 01:06:43.256816       1 logs_generator.go:76] 23 GET /api/v1/namespaces/ns/pods/jwd 424\nI0920 01:06:43.456805       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/jt59 229\nI0920 01:06:43.656872       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/9tj 522\nI0920 01:06:43.857062       1 logs_generator.go:76] 26 POST /api/v1/namespaces/ns/pods/jk2b 294\nI0920 01:06:44.057117       1 logs_generator.go:76] 27 POST /api/v1/namespaces/kube-system/pods/96dc 318\n"
Sep 20 01:06:44.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 logs logs-generator logs-generator --namespace=kubectl-814 --since=24h'
Sep 20 01:06:44.277: INFO: stderr: ""
Sep 20 01:06:44.277: INFO: stdout: "I0920 01:06:38.656467       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/ckhd 432\nI0920 01:06:38.856897       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/58m 406\nI0920 01:06:39.056655       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/jbcc 205\nI0920 01:06:39.256671       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/sdzq 549\nI0920 01:06:39.456700       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/5lk 333\nI0920 01:06:39.656791       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/8bd 389\nI0920 01:06:39.856814       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/5zzc 585\nI0920 01:06:40.056803       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/gttz 331\nI0920 01:06:40.256795       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/pksl 582\nI0920 01:06:40.456770       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/94z 580\nI0920 01:06:40.656788       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/qfrb 340\nI0920 01:06:40.856691       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/8kkq 375\nI0920 01:06:41.056700       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/nvw 343\nI0920 01:06:41.256795       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/p29 463\nI0920 01:06:41.456819       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/8vlh 333\nI0920 01:06:41.656939       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/6vt 440\nI0920 01:06:41.856630       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/qcfb 464\nI0920 01:06:42.056724       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/6dg 404\nI0920 01:06:42.256684       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/jtd 325\nI0920 01:06:42.456931       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/d8k 280\nI0920 01:06:42.657010       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/ff7 478\nI0920 01:06:42.856866       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/lf2 497\nI0920 01:06:43.056812       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/b8th 488\nI0920 01:06:43.256816       1 logs_generator.go:76] 23 GET /api/v1/namespaces/ns/pods/jwd 424\nI0920 01:06:43.456805       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/jt59 229\nI0920 01:06:43.656872       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/9tj 522\nI0920 01:06:43.857062       1 logs_generator.go:76] 26 POST /api/v1/namespaces/ns/pods/jk2b 294\nI0920 01:06:44.057117       1 logs_generator.go:76] 27 POST /api/v1/namespaces/kube-system/pods/96dc 318\nI0920 01:06:44.257058       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/ns/pods/bbct 335\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Sep 20 01:06:44.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 delete pod logs-generator --namespace=kubectl-814'
Sep 20 01:06:46.273: INFO: stderr: ""
Sep 20 01:06:46.273: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:06:46.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-814" for this suite.
Sep 20 01:06:52.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:06:52.507: INFO: namespace kubectl-814 deletion completed in 6.224929784s

• [SLOW TEST:16.140 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:06:52.509: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-7372
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Sep 20 01:06:52.682: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-7372" to be "success or failure"
Sep 20 01:06:52.689: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.78984ms
Sep 20 01:06:54.695: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012912416s
Sep 20 01:06:56.701: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019218398s
STEP: Saw pod success
Sep 20 01:06:56.702: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Sep 20 01:06:56.707: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Sep 20 01:06:56.790: INFO: Waiting for pod pod-host-path-test to disappear
Sep 20 01:06:56.796: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:06:56.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-7372" for this suite.
Sep 20 01:07:02.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:07:03.015: INFO: namespace hostpath-7372 deletion completed in 6.21157916s

• [SLOW TEST:10.505 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:07:03.015: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8735
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 01:07:03.182: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:07:03.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8735" for this suite.
Sep 20 01:07:09.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:07:09.479: INFO: namespace custom-resource-definition-8735 deletion completed in 6.210388145s

• [SLOW TEST:6.465 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:07:09.480: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1352
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Sep 20 01:07:14.223: INFO: Successfully updated pod "annotationupdate7f841f82-5b30-4fe7-b92d-5ee55ca5ca8a"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:07:16.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1352" for this suite.
Sep 20 01:07:28.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:07:28.590: INFO: namespace projected-1352 deletion completed in 12.269681496s

• [SLOW TEST:19.111 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:07:28.594: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1920
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-9982cedb-38fe-4bed-aefa-68ef34fc557f
STEP: Creating a pod to test consume secrets
Sep 20 01:07:28.781: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-60a9ff37-9fc7-4533-b3b5-7d6e7adf83f2" in namespace "projected-1920" to be "success or failure"
Sep 20 01:07:28.790: INFO: Pod "pod-projected-secrets-60a9ff37-9fc7-4533-b3b5-7d6e7adf83f2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.602081ms
Sep 20 01:07:30.796: INFO: Pod "pod-projected-secrets-60a9ff37-9fc7-4533-b3b5-7d6e7adf83f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015244965s
Sep 20 01:07:32.808: INFO: Pod "pod-projected-secrets-60a9ff37-9fc7-4533-b3b5-7d6e7adf83f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026409243s
STEP: Saw pod success
Sep 20 01:07:32.808: INFO: Pod "pod-projected-secrets-60a9ff37-9fc7-4533-b3b5-7d6e7adf83f2" satisfied condition "success or failure"
Sep 20 01:07:32.817: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod pod-projected-secrets-60a9ff37-9fc7-4533-b3b5-7d6e7adf83f2 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 20 01:07:32.857: INFO: Waiting for pod pod-projected-secrets-60a9ff37-9fc7-4533-b3b5-7d6e7adf83f2 to disappear
Sep 20 01:07:32.863: INFO: Pod pod-projected-secrets-60a9ff37-9fc7-4533-b3b5-7d6e7adf83f2 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:07:32.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1920" for this suite.
Sep 20 01:07:38.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:07:39.146: INFO: namespace projected-1920 deletion completed in 6.276413829s

• [SLOW TEST:10.552 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:07:39.156: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2047
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 01:07:39.342: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep 20 01:07:44.350: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 20 01:07:44.350: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep 20 01:07:46.363: INFO: Creating deployment "test-rollover-deployment"
Sep 20 01:07:46.378: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep 20 01:07:48.392: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep 20 01:07:48.404: INFO: Ensure that both replica sets have 1 created replica
Sep 20 01:07:48.414: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep 20 01:07:48.427: INFO: Updating deployment test-rollover-deployment
Sep 20 01:07:48.427: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep 20 01:07:50.439: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep 20 01:07:50.453: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep 20 01:07:50.469: INFO: all replica sets need to contain the pod-template-hash label
Sep 20 01:07:50.469: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538466, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538466, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538468, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538466, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 01:07:52.482: INFO: all replica sets need to contain the pod-template-hash label
Sep 20 01:07:52.482: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538466, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538466, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538470, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538466, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 01:07:54.487: INFO: all replica sets need to contain the pod-template-hash label
Sep 20 01:07:54.488: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538466, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538466, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538470, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538466, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 01:07:56.482: INFO: all replica sets need to contain the pod-template-hash label
Sep 20 01:07:56.482: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538466, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538466, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538470, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538466, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 01:07:58.482: INFO: all replica sets need to contain the pod-template-hash label
Sep 20 01:07:58.482: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538466, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538466, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538470, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538466, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 01:08:00.483: INFO: all replica sets need to contain the pod-template-hash label
Sep 20 01:08:00.484: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538466, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538466, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538470, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538466, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 01:08:02.482: INFO: 
Sep 20 01:08:02.482: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Sep 20 01:08:02.501: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2047 /apis/apps/v1/namespaces/deployment-2047/deployments/test-rollover-deployment d2278163-3c54-4422-b584-eddc0ff2c56b 68036 2 2019-09-20 01:07:46 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005517f88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-09-20 01:07:46 +0000 UTC,LastTransitionTime:2019-09-20 01:07:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2019-09-20 01:08:00 +0000 UTC,LastTransitionTime:2019-09-20 01:07:46 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 20 01:08:02.507: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-2047 /apis/apps/v1/namespaces/deployment-2047/replicasets/test-rollover-deployment-7d7dc6548c 2374cce8-483d-402a-ade8-491d33d23fd4 68025 2 2019-09-20 01:07:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment d2278163-3c54-4422-b584-eddc0ff2c56b 0xc006226587 0xc006226588}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006226718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 20 01:08:02.508: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep 20 01:08:02.509: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2047 /apis/apps/v1/namespaces/deployment-2047/replicasets/test-rollover-controller f948c66c-c11e-4d51-a21c-545914e487c0 68034 2 2019-09-20 01:07:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment d2278163-3c54-4422-b584-eddc0ff2c56b 0xc006226387 0xc006226388}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006226518 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 20 01:08:02.509: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-2047 /apis/apps/v1/namespaces/deployment-2047/replicasets/test-rollover-deployment-f6c94f66c 6fb7586d-7b8e-4bba-91af-e91f24cde023 67978 2 2019-09-20 01:07:46 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment d2278163-3c54-4422-b584-eddc0ff2c56b 0xc006226780 0xc006226781}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0062267f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 20 01:08:02.517: INFO: Pod "test-rollover-deployment-7d7dc6548c-nnhqp" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-nnhqp test-rollover-deployment-7d7dc6548c- deployment-2047 /api/v1/namespaces/deployment-2047/pods/test-rollover-deployment-7d7dc6548c-nnhqp c8cd5082-c6d4-4ab3-8fcc-6944ed94751b 67994 0 2019-09-20 01:07:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[cni.projectcalico.org/podIP:172.25.3.26/32] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c 2374cce8-483d-402a-ade8-491d33d23fd4 0xc0062fa367 0xc0062fa368}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zjvmk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zjvmk,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zjvmk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-lmjf4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:07:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:07:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:07:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:07:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.206,PodIP:172.25.3.26,StartTime:2019-09-20 01:07:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 01:07:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://171015d85196c489043fc4bd9d644e10cfdaccd770799369108e263b8e7c5e1f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.3.26,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:08:02.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2047" for this suite.
Sep 20 01:08:08.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:08:08.760: INFO: namespace deployment-2047 deletion completed in 6.234010922s

• [SLOW TEST:29.605 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:08:08.765: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1683
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Sep 20 01:08:08.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 create -f - --namespace=kubectl-1683'
Sep 20 01:08:09.231: INFO: stderr: ""
Sep 20 01:08:09.231: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 20 01:08:09.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1683'
Sep 20 01:08:09.326: INFO: stderr: ""
Sep 20 01:08:09.326: INFO: stdout: "update-demo-nautilus-kdpld update-demo-nautilus-mmllk "
Sep 20 01:08:09.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-nautilus-kdpld -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1683'
Sep 20 01:08:09.408: INFO: stderr: ""
Sep 20 01:08:09.408: INFO: stdout: ""
Sep 20 01:08:09.408: INFO: update-demo-nautilus-kdpld is created but not running
Sep 20 01:08:14.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1683'
Sep 20 01:08:14.537: INFO: stderr: ""
Sep 20 01:08:14.537: INFO: stdout: "update-demo-nautilus-kdpld update-demo-nautilus-mmllk "
Sep 20 01:08:14.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-nautilus-kdpld -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1683'
Sep 20 01:08:14.622: INFO: stderr: ""
Sep 20 01:08:14.622: INFO: stdout: "true"
Sep 20 01:08:14.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-nautilus-kdpld -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1683'
Sep 20 01:08:14.710: INFO: stderr: ""
Sep 20 01:08:14.710: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 20 01:08:14.710: INFO: validating pod update-demo-nautilus-kdpld
Sep 20 01:08:14.808: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 01:08:14.808: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 01:08:14.808: INFO: update-demo-nautilus-kdpld is verified up and running
Sep 20 01:08:14.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-nautilus-mmllk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1683'
Sep 20 01:08:14.914: INFO: stderr: ""
Sep 20 01:08:14.914: INFO: stdout: "true"
Sep 20 01:08:14.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-nautilus-mmllk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1683'
Sep 20 01:08:14.999: INFO: stderr: ""
Sep 20 01:08:14.999: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 20 01:08:14.999: INFO: validating pod update-demo-nautilus-mmllk
Sep 20 01:08:15.094: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 01:08:15.095: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 01:08:15.095: INFO: update-demo-nautilus-mmllk is verified up and running
STEP: using delete to clean up resources
Sep 20 01:08:15.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 delete --grace-period=0 --force -f - --namespace=kubectl-1683'
Sep 20 01:08:15.204: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 01:08:15.204: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 20 01:08:15.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1683'
Sep 20 01:08:15.301: INFO: stderr: "No resources found in kubectl-1683 namespace.\n"
Sep 20 01:08:15.301: INFO: stdout: ""
Sep 20 01:08:15.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods -l name=update-demo --namespace=kubectl-1683 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 20 01:08:15.388: INFO: stderr: ""
Sep 20 01:08:15.388: INFO: stdout: "update-demo-nautilus-kdpld\nupdate-demo-nautilus-mmllk\n"
Sep 20 01:08:15.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1683'
Sep 20 01:08:15.993: INFO: stderr: "No resources found in kubectl-1683 namespace.\n"
Sep 20 01:08:15.994: INFO: stdout: ""
Sep 20 01:08:15.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods -l name=update-demo --namespace=kubectl-1683 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 20 01:08:16.090: INFO: stderr: ""
Sep 20 01:08:16.090: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:08:16.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1683" for this suite.
Sep 20 01:08:28.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:08:28.300: INFO: namespace kubectl-1683 deletion completed in 12.201636429s

• [SLOW TEST:19.536 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:08:28.310: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2655
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 01:08:28.492: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0bf98cbc-92dd-4829-8e40-c567d8a5f069" in namespace "projected-2655" to be "success or failure"
Sep 20 01:08:28.505: INFO: Pod "downwardapi-volume-0bf98cbc-92dd-4829-8e40-c567d8a5f069": Phase="Pending", Reason="", readiness=false. Elapsed: 12.184934ms
Sep 20 01:08:30.511: INFO: Pod "downwardapi-volume-0bf98cbc-92dd-4829-8e40-c567d8a5f069": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018533585s
Sep 20 01:08:32.518: INFO: Pod "downwardapi-volume-0bf98cbc-92dd-4829-8e40-c567d8a5f069": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025014056s
STEP: Saw pod success
Sep 20 01:08:32.518: INFO: Pod "downwardapi-volume-0bf98cbc-92dd-4829-8e40-c567d8a5f069" satisfied condition "success or failure"
Sep 20 01:08:32.523: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod downwardapi-volume-0bf98cbc-92dd-4829-8e40-c567d8a5f069 container client-container: <nil>
STEP: delete the pod
Sep 20 01:08:32.570: INFO: Waiting for pod downwardapi-volume-0bf98cbc-92dd-4829-8e40-c567d8a5f069 to disappear
Sep 20 01:08:32.575: INFO: Pod downwardapi-volume-0bf98cbc-92dd-4829-8e40-c567d8a5f069 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:08:32.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2655" for this suite.
Sep 20 01:08:38.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:08:38.799: INFO: namespace projected-2655 deletion completed in 6.217902671s

• [SLOW TEST:10.490 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:08:38.812: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7780
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Sep 20 01:08:39.048: INFO: Waiting up to 5m0s for pod "var-expansion-f21eb783-6b7b-4dbf-a523-9a6c068794d2" in namespace "var-expansion-7780" to be "success or failure"
Sep 20 01:08:39.054: INFO: Pod "var-expansion-f21eb783-6b7b-4dbf-a523-9a6c068794d2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.345405ms
Sep 20 01:08:41.060: INFO: Pod "var-expansion-f21eb783-6b7b-4dbf-a523-9a6c068794d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011582485s
Sep 20 01:08:43.065: INFO: Pod "var-expansion-f21eb783-6b7b-4dbf-a523-9a6c068794d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017499164s
STEP: Saw pod success
Sep 20 01:08:43.066: INFO: Pod "var-expansion-f21eb783-6b7b-4dbf-a523-9a6c068794d2" satisfied condition "success or failure"
Sep 20 01:08:43.071: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod var-expansion-f21eb783-6b7b-4dbf-a523-9a6c068794d2 container dapi-container: <nil>
STEP: delete the pod
Sep 20 01:08:43.107: INFO: Waiting for pod var-expansion-f21eb783-6b7b-4dbf-a523-9a6c068794d2 to disappear
Sep 20 01:08:43.111: INFO: Pod var-expansion-f21eb783-6b7b-4dbf-a523-9a6c068794d2 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:08:43.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7780" for this suite.
Sep 20 01:08:49.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:08:49.343: INFO: namespace var-expansion-7780 deletion completed in 6.224833431s

• [SLOW TEST:10.532 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:08:49.351: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2889
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 01:08:49.573: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep 20 01:08:49.590: INFO: Number of nodes with available pods: 0
Sep 20 01:08:49.590: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep 20 01:08:49.623: INFO: Number of nodes with available pods: 0
Sep 20 01:08:49.623: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 01:08:50.629: INFO: Number of nodes with available pods: 0
Sep 20 01:08:50.629: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 01:08:51.629: INFO: Number of nodes with available pods: 0
Sep 20 01:08:51.629: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 01:08:52.629: INFO: Number of nodes with available pods: 1
Sep 20 01:08:52.629: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep 20 01:08:52.655: INFO: Number of nodes with available pods: 1
Sep 20 01:08:52.655: INFO: Number of running nodes: 0, number of available pods: 1
Sep 20 01:08:53.662: INFO: Number of nodes with available pods: 0
Sep 20 01:08:53.662: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep 20 01:08:53.675: INFO: Number of nodes with available pods: 0
Sep 20 01:08:53.676: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 01:08:54.683: INFO: Number of nodes with available pods: 0
Sep 20 01:08:54.683: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 01:08:55.682: INFO: Number of nodes with available pods: 0
Sep 20 01:08:55.682: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 01:08:56.682: INFO: Number of nodes with available pods: 0
Sep 20 01:08:56.682: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 01:08:57.683: INFO: Number of nodes with available pods: 0
Sep 20 01:08:57.683: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 01:08:58.682: INFO: Number of nodes with available pods: 1
Sep 20 01:08:58.682: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2889, will wait for the garbage collector to delete the pods
I0920 01:08:58.699116      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 01:08:58.699741      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 01:08:58.763: INFO: Deleting DaemonSet.extensions daemon-set took: 13.72179ms
Sep 20 01:08:59.163: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.655221ms
I0920 01:08:59.163553      16 controller_utils.go:810] Ignoring inactive pod daemonsets-2889/daemon-set-qvp8d in state Running, deletion time 2019-09-20 01:09:29 +0000 UTC
Sep 20 01:09:13.168: INFO: Number of nodes with available pods: 0
Sep 20 01:09:13.168: INFO: Number of running nodes: 0, number of available pods: 0
Sep 20 01:09:13.173: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2889/daemonsets","resourceVersion":"68440"},"items":null}

Sep 20 01:09:13.178: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2889/pods","resourceVersion":"68440"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:09:13.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2889" for this suite.
Sep 20 01:09:19.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:09:19.504: INFO: namespace daemonsets-2889 deletion completed in 6.269435089s

• [SLOW TEST:30.153 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:09:19.509: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9342
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Sep 20 01:09:19.698: INFO: Waiting up to 5m0s for pod "var-expansion-8c398350-ded7-4a91-8719-b15f7e84a0e5" in namespace "var-expansion-9342" to be "success or failure"
Sep 20 01:09:19.703: INFO: Pod "var-expansion-8c398350-ded7-4a91-8719-b15f7e84a0e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.868165ms
Sep 20 01:09:21.709: INFO: Pod "var-expansion-8c398350-ded7-4a91-8719-b15f7e84a0e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011123094s
Sep 20 01:09:23.715: INFO: Pod "var-expansion-8c398350-ded7-4a91-8719-b15f7e84a0e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017473632s
STEP: Saw pod success
Sep 20 01:09:23.716: INFO: Pod "var-expansion-8c398350-ded7-4a91-8719-b15f7e84a0e5" satisfied condition "success or failure"
Sep 20 01:09:23.720: INFO: Trying to get logs from node worker-wqshf-7859ffd555-lmjf4 pod var-expansion-8c398350-ded7-4a91-8719-b15f7e84a0e5 container dapi-container: <nil>
STEP: delete the pod
Sep 20 01:09:23.790: INFO: Waiting for pod var-expansion-8c398350-ded7-4a91-8719-b15f7e84a0e5 to disappear
Sep 20 01:09:23.795: INFO: Pod var-expansion-8c398350-ded7-4a91-8719-b15f7e84a0e5 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:09:23.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9342" for this suite.
Sep 20 01:09:29.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:09:30.039: INFO: namespace var-expansion-9342 deletion completed in 6.235835592s

• [SLOW TEST:10.531 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:09:30.043: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-807
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Sep 20 01:09:30.214: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 20 01:09:30.235: INFO: Waiting for terminating namespaces to be deleted...
Sep 20 01:09:30.240: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-kqpmz before test
Sep 20 01:09:30.268: INFO: canal-pwz75 from kube-system started at 2019-09-19 20:33:51 +0000 UTC (2 container statuses recorded)
Sep 20 01:09:30.268: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 01:09:30.269: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 01:09:30.269: INFO: node-exporter-rtkgz from kube-system started at 2019-09-19 20:33:51 +0000 UTC (2 container statuses recorded)
Sep 20 01:09:30.269: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 01:09:30.270: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 01:09:30.270: INFO: kube-proxy-xmcqx from kube-system started at 2019-09-19 20:33:51 +0000 UTC (1 container statuses recorded)
Sep 20 01:09:30.270: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 01:09:30.271: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-2ks25 from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 01:09:30.271: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 01:09:30.271: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 01:09:30.272: INFO: node-local-dns-tppj5 from kube-system started at 2019-09-20 00:59:43 +0000 UTC (1 container statuses recorded)
Sep 20 01:09:30.272: INFO: 	Container node-cache ready: true, restart count 0
Sep 20 01:09:30.272: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-ldlfc before test
Sep 20 01:09:30.327: INFO: kube-proxy-zvhzd from kube-system started at 2019-09-19 20:33:57 +0000 UTC (1 container statuses recorded)
Sep 20 01:09:30.327: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 01:09:30.328: INFO: node-exporter-47p98 from kube-system started at 2019-09-19 20:33:57 +0000 UTC (2 container statuses recorded)
Sep 20 01:09:30.328: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 01:09:30.328: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 01:09:30.329: INFO: canal-hf9f7 from kube-system started at 2019-09-19 20:33:57 +0000 UTC (2 container statuses recorded)
Sep 20 01:09:30.329: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 01:09:30.330: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 01:09:30.330: INFO: node-local-dns-rj6pt from kube-system started at 2019-09-19 21:38:47 +0000 UTC (1 container statuses recorded)
Sep 20 01:09:30.330: INFO: 	Container node-cache ready: true, restart count 0
Sep 20 01:09:30.331: INFO: sonobuoy-e2e-job-8f5b7ab9f6954bcb from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 01:09:30.331: INFO: 	Container e2e ready: true, restart count 0
Sep 20 01:09:30.331: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 01:09:30.332: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-djczp from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 01:09:30.332: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 01:09:30.332: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 01:09:30.333: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-lmjf4 before test
Sep 20 01:09:30.415: INFO: kube-proxy-mhstl from kube-system started at 2019-09-19 20:34:06 +0000 UTC (1 container statuses recorded)
Sep 20 01:09:30.415: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 01:09:30.415: INFO: node-local-dns-bx7jl from kube-system started at 2019-09-19 20:34:26 +0000 UTC (1 container statuses recorded)
Sep 20 01:09:30.415: INFO: 	Container node-cache ready: true, restart count 0
Sep 20 01:09:30.415: INFO: coredns-57f944bd9f-rp6nc from kube-system started at 2019-09-19 21:35:33 +0000 UTC (1 container statuses recorded)
Sep 20 01:09:30.415: INFO: 	Container coredns ready: true, restart count 0
Sep 20 01:09:30.415: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-n9zxp from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 01:09:30.415: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 01:09:30.415: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 01:09:30.415: INFO: kubernetes-dashboard-7d5fb85f7f-jqgq6 from kube-system started at 2019-09-20 00:59:15 +0000 UTC (1 container statuses recorded)
Sep 20 01:09:30.415: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Sep 20 01:09:30.416: INFO: canal-s8rlz from kube-system started at 2019-09-19 20:34:06 +0000 UTC (2 container statuses recorded)
Sep 20 01:09:30.416: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 01:09:30.416: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 01:09:30.416: INFO: node-exporter-65hp9 from kube-system started at 2019-09-19 20:34:06 +0000 UTC (2 container statuses recorded)
Sep 20 01:09:30.416: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 01:09:30.416: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 01:09:30.416: INFO: openvpn-client-84ccd8596d-s64vh from kube-system started at 2019-09-20 00:44:53 +0000 UTC (2 container statuses recorded)
Sep 20 01:09:30.416: INFO: 	Container dnat-controller ready: true, restart count 0
Sep 20 01:09:30.416: INFO: 	Container openvpn-client ready: true, restart count 0
Sep 20 01:09:30.416: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-mxxqz before test
Sep 20 01:09:30.503: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-7brnz from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 01:09:30.503: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 01:09:30.504: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 01:09:30.504: INFO: canal-kdmx8 from kube-system started at 2019-09-19 20:34:07 +0000 UTC (2 container statuses recorded)
Sep 20 01:09:30.505: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 01:09:30.505: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 01:09:30.505: INFO: kube-proxy-dtrjq from kube-system started at 2019-09-19 20:34:07 +0000 UTC (1 container statuses recorded)
Sep 20 01:09:30.505: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 01:09:30.506: INFO: node-exporter-xq8tf from kube-system started at 2019-09-19 20:34:07 +0000 UTC (2 container statuses recorded)
Sep 20 01:09:30.506: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 01:09:30.507: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 01:09:30.507: INFO: node-local-dns-bxghq from kube-system started at 2019-09-19 20:34:27 +0000 UTC (1 container statuses recorded)
Sep 20 01:09:30.507: INFO: 	Container node-cache ready: true, restart count 0
Sep 20 01:09:30.508: INFO: sonobuoy from sonobuoy started at 2019-09-19 20:40:21 +0000 UTC (1 container statuses recorded)
Sep 20 01:09:30.508: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 20 01:09:30.508: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-r6tnq before test
Sep 20 01:09:30.589: INFO: coredns-57f944bd9f-6qlrr from kube-system started at 2019-09-20 00:59:15 +0000 UTC (1 container statuses recorded)
Sep 20 01:09:30.589: INFO: 	Container coredns ready: true, restart count 0
Sep 20 01:09:30.589: INFO: node-exporter-ld8fb from kube-system started at 2019-09-19 20:33:57 +0000 UTC (2 container statuses recorded)
Sep 20 01:09:30.589: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 01:09:30.589: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 01:09:30.589: INFO: canal-8lwhv from kube-system started at 2019-09-19 20:33:57 +0000 UTC (2 container statuses recorded)
Sep 20 01:09:30.589: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 01:09:30.589: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 01:09:30.589: INFO: kube-proxy-p4g4f from kube-system started at 2019-09-19 20:33:57 +0000 UTC (1 container statuses recorded)
Sep 20 01:09:30.589: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 01:09:30.589: INFO: node-local-dns-2krxh from kube-system started at 2019-09-20 00:44:53 +0000 UTC (1 container statuses recorded)
Sep 20 01:09:30.589: INFO: 	Container node-cache ready: true, restart count 0
Sep 20 01:09:30.589: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-bph2c from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 01:09:30.589: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 01:09:30.589: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node worker-wqshf-7859ffd555-kqpmz
STEP: verifying the node has the label node worker-wqshf-7859ffd555-ldlfc
STEP: verifying the node has the label node worker-wqshf-7859ffd555-lmjf4
STEP: verifying the node has the label node worker-wqshf-7859ffd555-mxxqz
STEP: verifying the node has the label node worker-wqshf-7859ffd555-r6tnq
Sep 20 01:09:30.713: INFO: Pod canal-8lwhv requesting resource cpu=250m on Node worker-wqshf-7859ffd555-r6tnq
Sep 20 01:09:30.713: INFO: Pod canal-hf9f7 requesting resource cpu=250m on Node worker-wqshf-7859ffd555-ldlfc
Sep 20 01:09:30.713: INFO: Pod canal-kdmx8 requesting resource cpu=250m on Node worker-wqshf-7859ffd555-mxxqz
Sep 20 01:09:30.713: INFO: Pod canal-pwz75 requesting resource cpu=250m on Node worker-wqshf-7859ffd555-kqpmz
Sep 20 01:09:30.713: INFO: Pod canal-s8rlz requesting resource cpu=250m on Node worker-wqshf-7859ffd555-lmjf4
Sep 20 01:09:30.713: INFO: Pod coredns-57f944bd9f-6qlrr requesting resource cpu=100m on Node worker-wqshf-7859ffd555-r6tnq
Sep 20 01:09:30.713: INFO: Pod coredns-57f944bd9f-rp6nc requesting resource cpu=100m on Node worker-wqshf-7859ffd555-lmjf4
Sep 20 01:09:30.713: INFO: Pod kube-proxy-dtrjq requesting resource cpu=75m on Node worker-wqshf-7859ffd555-mxxqz
Sep 20 01:09:30.713: INFO: Pod kube-proxy-mhstl requesting resource cpu=75m on Node worker-wqshf-7859ffd555-lmjf4
Sep 20 01:09:30.713: INFO: Pod kube-proxy-p4g4f requesting resource cpu=75m on Node worker-wqshf-7859ffd555-r6tnq
Sep 20 01:09:30.713: INFO: Pod kube-proxy-xmcqx requesting resource cpu=75m on Node worker-wqshf-7859ffd555-kqpmz
Sep 20 01:09:30.713: INFO: Pod kube-proxy-zvhzd requesting resource cpu=75m on Node worker-wqshf-7859ffd555-ldlfc
Sep 20 01:09:30.713: INFO: Pod kubernetes-dashboard-7d5fb85f7f-jqgq6 requesting resource cpu=75m on Node worker-wqshf-7859ffd555-lmjf4
Sep 20 01:09:30.713: INFO: Pod node-exporter-47p98 requesting resource cpu=20m on Node worker-wqshf-7859ffd555-ldlfc
Sep 20 01:09:30.713: INFO: Pod node-exporter-65hp9 requesting resource cpu=20m on Node worker-wqshf-7859ffd555-lmjf4
Sep 20 01:09:30.713: INFO: Pod node-exporter-ld8fb requesting resource cpu=20m on Node worker-wqshf-7859ffd555-r6tnq
Sep 20 01:09:30.713: INFO: Pod node-exporter-rtkgz requesting resource cpu=20m on Node worker-wqshf-7859ffd555-kqpmz
Sep 20 01:09:30.713: INFO: Pod node-exporter-xq8tf requesting resource cpu=20m on Node worker-wqshf-7859ffd555-mxxqz
Sep 20 01:09:30.713: INFO: Pod node-local-dns-2krxh requesting resource cpu=25m on Node worker-wqshf-7859ffd555-r6tnq
Sep 20 01:09:30.713: INFO: Pod node-local-dns-bx7jl requesting resource cpu=25m on Node worker-wqshf-7859ffd555-lmjf4
Sep 20 01:09:30.713: INFO: Pod node-local-dns-bxghq requesting resource cpu=25m on Node worker-wqshf-7859ffd555-mxxqz
Sep 20 01:09:30.713: INFO: Pod node-local-dns-rj6pt requesting resource cpu=25m on Node worker-wqshf-7859ffd555-ldlfc
Sep 20 01:09:30.713: INFO: Pod node-local-dns-tppj5 requesting resource cpu=25m on Node worker-wqshf-7859ffd555-kqpmz
Sep 20 01:09:30.713: INFO: Pod openvpn-client-84ccd8596d-s64vh requesting resource cpu=30m on Node worker-wqshf-7859ffd555-lmjf4
Sep 20 01:09:30.713: INFO: Pod sonobuoy requesting resource cpu=0m on Node worker-wqshf-7859ffd555-mxxqz
Sep 20 01:09:30.713: INFO: Pod sonobuoy-e2e-job-8f5b7ab9f6954bcb requesting resource cpu=0m on Node worker-wqshf-7859ffd555-ldlfc
Sep 20 01:09:30.713: INFO: Pod sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-2ks25 requesting resource cpu=0m on Node worker-wqshf-7859ffd555-kqpmz
Sep 20 01:09:30.713: INFO: Pod sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-7brnz requesting resource cpu=0m on Node worker-wqshf-7859ffd555-mxxqz
Sep 20 01:09:30.713: INFO: Pod sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-bph2c requesting resource cpu=0m on Node worker-wqshf-7859ffd555-r6tnq
Sep 20 01:09:30.713: INFO: Pod sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-djczp requesting resource cpu=0m on Node worker-wqshf-7859ffd555-ldlfc
Sep 20 01:09:30.713: INFO: Pod sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-n9zxp requesting resource cpu=0m on Node worker-wqshf-7859ffd555-lmjf4
STEP: Starting Pods to consume most of the cluster CPU.
Sep 20 01:09:30.713: INFO: Creating a pod which consumes cpu=1001m on Node worker-wqshf-7859ffd555-kqpmz
Sep 20 01:09:30.724: INFO: Creating a pod which consumes cpu=1001m on Node worker-wqshf-7859ffd555-ldlfc
Sep 20 01:09:30.737: INFO: Creating a pod which consumes cpu=857m on Node worker-wqshf-7859ffd555-lmjf4
Sep 20 01:09:30.744: INFO: Creating a pod which consumes cpu=1001m on Node worker-wqshf-7859ffd555-mxxqz
Sep 20 01:09:30.759: INFO: Creating a pod which consumes cpu=931m on Node worker-wqshf-7859ffd555-r6tnq
STEP: Creating another pod that requires unavailable amount of CPU.
I0920 01:09:34.806100      16 reflector.go:120] Starting reflector *v1.Event (0s) from k8s.io/kubernetes/test/e2e/common/events.go:136
I0920 01:09:34.806642      16 reflector.go:158] Listing and watching *v1.Event from k8s.io/kubernetes/test/e2e/common/events.go:136
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0a3bc2ea-4b5d-4e7d-ae3b-d9ab95fb28ed.15c6003a587e2829], Reason = [Scheduled], Message = [Successfully assigned sched-pred-807/filler-pod-0a3bc2ea-4b5d-4e7d-ae3b-d9ab95fb28ed to worker-wqshf-7859ffd555-kqpmz]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0a3bc2ea-4b5d-4e7d-ae3b-d9ab95fb28ed.15c6003aa940eb11], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0a3bc2ea-4b5d-4e7d-ae3b-d9ab95fb28ed.15c6003ac669c1f2], Reason = [Created], Message = [Created container filler-pod-0a3bc2ea-4b5d-4e7d-ae3b-d9ab95fb28ed]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0a3bc2ea-4b5d-4e7d-ae3b-d9ab95fb28ed.15c6003add65f24e], Reason = [Started], Message = [Started container filler-pod-0a3bc2ea-4b5d-4e7d-ae3b-d9ab95fb28ed]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3a0d23e1-5c75-40c8-9c3d-e97861bacf5d.15c6003a59c67afb], Reason = [Scheduled], Message = [Successfully assigned sched-pred-807/filler-pod-3a0d23e1-5c75-40c8-9c3d-e97861bacf5d to worker-wqshf-7859ffd555-lmjf4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3a0d23e1-5c75-40c8-9c3d-e97861bacf5d.15c6003aa446b5e2], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3a0d23e1-5c75-40c8-9c3d-e97861bacf5d.15c6003aa97593bd], Reason = [Created], Message = [Created container filler-pod-3a0d23e1-5c75-40c8-9c3d-e97861bacf5d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3a0d23e1-5c75-40c8-9c3d-e97861bacf5d.15c6003ab9e8300d], Reason = [Started], Message = [Started container filler-pod-3a0d23e1-5c75-40c8-9c3d-e97861bacf5d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-42c7d15c-52b1-4a60-9925-d6a193e59fb1.15c6003a5af725bb], Reason = [Scheduled], Message = [Successfully assigned sched-pred-807/filler-pod-42c7d15c-52b1-4a60-9925-d6a193e59fb1 to worker-wqshf-7859ffd555-r6tnq]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-42c7d15c-52b1-4a60-9925-d6a193e59fb1.15c6003aae75b8f1], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-42c7d15c-52b1-4a60-9925-d6a193e59fb1.15c6003ab3a3f4cd], Reason = [Created], Message = [Created container filler-pod-42c7d15c-52b1-4a60-9925-d6a193e59fb1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-42c7d15c-52b1-4a60-9925-d6a193e59fb1.15c6003acaaff969], Reason = [Started], Message = [Started container filler-pod-42c7d15c-52b1-4a60-9925-d6a193e59fb1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b6ab9fb9-c069-4775-aa9b-9a27ef323072.15c6003a59783af9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-807/filler-pod-b6ab9fb9-c069-4775-aa9b-9a27ef323072 to worker-wqshf-7859ffd555-ldlfc]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b6ab9fb9-c069-4775-aa9b-9a27ef323072.15c6003aa58a0c5b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b6ab9fb9-c069-4775-aa9b-9a27ef323072.15c6003aabf63cb3], Reason = [Created], Message = [Created container filler-pod-b6ab9fb9-c069-4775-aa9b-9a27ef323072]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b6ab9fb9-c069-4775-aa9b-9a27ef323072.15c6003abf25d502], Reason = [Started], Message = [Started container filler-pod-b6ab9fb9-c069-4775-aa9b-9a27ef323072]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7200ef5-4863-46ea-8f19-b7beb688a53c.15c6003a5a42eb4a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-807/filler-pod-c7200ef5-4863-46ea-8f19-b7beb688a53c to worker-wqshf-7859ffd555-mxxqz]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7200ef5-4863-46ea-8f19-b7beb688a53c.15c6003aa75b495d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7200ef5-4863-46ea-8f19-b7beb688a53c.15c6003aae4ef3ad], Reason = [Created], Message = [Created container filler-pod-c7200ef5-4863-46ea-8f19-b7beb688a53c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7200ef5-4863-46ea-8f19-b7beb688a53c.15c6003ac03b6572], Reason = [Started], Message = [Started container filler-pod-c7200ef5-4863-46ea-8f19-b7beb688a53c]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15c6003b4d9fc938], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15c6003b4ebd42d2], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu.]
STEP: removing the label node off the node worker-wqshf-7859ffd555-lmjf4
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker-wqshf-7859ffd555-mxxqz
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker-wqshf-7859ffd555-r6tnq
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker-wqshf-7859ffd555-kqpmz
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker-wqshf-7859ffd555-ldlfc
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:09:35.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-807" for this suite.
I0920 01:09:37.827834      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
Sep 20 01:09:42.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:09:42.285: INFO: namespace sched-pred-807 deletion completed in 6.280343332s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
I0920 01:09:42.285740      16 request.go:706] Error in request: resource name may not be empty

• [SLOW TEST:12.243 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:09:42.289: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2483
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 01:09:42.479: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ff82aca1-b1ad-4e46-84f4-ea099eb226e9" in namespace "projected-2483" to be "success or failure"
Sep 20 01:09:42.484: INFO: Pod "downwardapi-volume-ff82aca1-b1ad-4e46-84f4-ea099eb226e9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.083531ms
Sep 20 01:09:44.491: INFO: Pod "downwardapi-volume-ff82aca1-b1ad-4e46-84f4-ea099eb226e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011794115s
Sep 20 01:09:46.512: INFO: Pod "downwardapi-volume-ff82aca1-b1ad-4e46-84f4-ea099eb226e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032535005s
STEP: Saw pod success
Sep 20 01:09:46.512: INFO: Pod "downwardapi-volume-ff82aca1-b1ad-4e46-84f4-ea099eb226e9" satisfied condition "success or failure"
Sep 20 01:09:46.535: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod downwardapi-volume-ff82aca1-b1ad-4e46-84f4-ea099eb226e9 container client-container: <nil>
STEP: delete the pod
Sep 20 01:09:46.650: INFO: Waiting for pod downwardapi-volume-ff82aca1-b1ad-4e46-84f4-ea099eb226e9 to disappear
Sep 20 01:09:46.658: INFO: Pod downwardapi-volume-ff82aca1-b1ad-4e46-84f4-ea099eb226e9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:09:46.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2483" for this suite.
Sep 20 01:09:52.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:09:52.905: INFO: namespace projected-2483 deletion completed in 6.220676503s

• [SLOW TEST:10.616 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:09:52.905: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3564
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 20 01:09:53.080: INFO: Waiting up to 5m0s for pod "pod-a8dec0df-268f-48f7-ba05-c1f02924709e" in namespace "emptydir-3564" to be "success or failure"
Sep 20 01:09:53.087: INFO: Pod "pod-a8dec0df-268f-48f7-ba05-c1f02924709e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.411066ms
Sep 20 01:09:55.094: INFO: Pod "pod-a8dec0df-268f-48f7-ba05-c1f02924709e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013839178s
Sep 20 01:09:57.101: INFO: Pod "pod-a8dec0df-268f-48f7-ba05-c1f02924709e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02134003s
STEP: Saw pod success
Sep 20 01:09:57.101: INFO: Pod "pod-a8dec0df-268f-48f7-ba05-c1f02924709e" satisfied condition "success or failure"
Sep 20 01:09:57.107: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod pod-a8dec0df-268f-48f7-ba05-c1f02924709e container test-container: <nil>
STEP: delete the pod
Sep 20 01:09:57.211: INFO: Waiting for pod pod-a8dec0df-268f-48f7-ba05-c1f02924709e to disappear
Sep 20 01:09:57.218: INFO: Pod pod-a8dec0df-268f-48f7-ba05-c1f02924709e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:09:57.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3564" for this suite.
Sep 20 01:10:03.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:10:03.466: INFO: namespace emptydir-3564 deletion completed in 6.24066685s

• [SLOW TEST:10.562 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:10:03.468: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9655
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:10:29.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9655" for this suite.
Sep 20 01:10:36.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:10:36.261: INFO: namespace container-runtime-9655 deletion completed in 6.25749784s

• [SLOW TEST:32.794 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:10:36.262: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7807
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Sep 20 01:10:40.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec pod-sharedvolume-df37e351-ab68-421f-a939-75a4c50892dd -c busybox-main-container --namespace=emptydir-7807 -- cat /usr/share/volumeshare/shareddata.txt'
Sep 20 01:10:41.160: INFO: stderr: ""
Sep 20 01:10:41.160: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:10:41.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7807" for this suite.
Sep 20 01:10:47.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:10:47.378: INFO: namespace emptydir-7807 deletion completed in 6.209623159s

• [SLOW TEST:11.116 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:10:47.380: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-8412
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-rk6x2 in namespace proxy-8412
I0920 01:10:47.580129      16 runners.go:184] Created replication controller with name: proxy-service-rk6x2, namespace: proxy-8412, replica count: 1
I0920 01:10:47.580818      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 01:10:47.580866      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 01:10:48.633496      16 runners.go:184] proxy-service-rk6x2 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 01:10:49.633836      16 runners.go:184] proxy-service-rk6x2 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 01:10:50.634282      16 runners.go:184] proxy-service-rk6x2 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0920 01:10:51.637822      16 runners.go:184] proxy-service-rk6x2 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 20 01:10:51.646: INFO: setup took 4.093828843s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep 20 01:10:51.774: INFO: (0) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">test<... (200; 128.69772ms)
Sep 20 01:10:51.775: INFO: (0) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 128.621829ms)
Sep 20 01:10:51.817: INFO: (0) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 170.612419ms)
Sep 20 01:10:51.817: INFO: (0) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname2/proxy/: bar (200; 170.894869ms)
Sep 20 01:10:51.817: INFO: (0) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname1/proxy/: foo (200; 170.940985ms)
Sep 20 01:10:51.818: INFO: (0) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">... (200; 171.301057ms)
Sep 20 01:10:51.818: INFO: (0) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname2/proxy/: bar (200; 171.946836ms)
Sep 20 01:10:51.871: INFO: (0) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 225.33371ms)
Sep 20 01:10:51.872: INFO: (0) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/rewriteme">test</a> (200; 226.072547ms)
Sep 20 01:10:51.873: INFO: (0) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 226.914913ms)
Sep 20 01:10:51.873: INFO: (0) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname1/proxy/: foo (200; 226.505822ms)
Sep 20 01:10:51.885: INFO: (0) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname1/proxy/: tls baz (200; 238.587104ms)
Sep 20 01:10:51.885: INFO: (0) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:462/proxy/: tls qux (200; 239.101105ms)
Sep 20 01:10:51.885: INFO: (0) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname2/proxy/: tls qux (200; 238.772141ms)
Sep 20 01:10:51.886: INFO: (0) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/tlsrewritem... (200; 240.081361ms)
Sep 20 01:10:51.887: INFO: (0) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:460/proxy/: tls baz (200; 241.020102ms)
Sep 20 01:10:51.902: INFO: (1) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 13.561601ms)
Sep 20 01:10:51.908: INFO: (1) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">test<... (200; 19.376472ms)
Sep 20 01:10:51.908: INFO: (1) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:460/proxy/: tls baz (200; 20.70845ms)
Sep 20 01:10:51.908: INFO: (1) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 19.654382ms)
Sep 20 01:10:51.909: INFO: (1) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 20.134487ms)
Sep 20 01:10:51.910: INFO: (1) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/rewriteme">test</a> (200; 20.921412ms)
Sep 20 01:10:51.910: INFO: (1) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:462/proxy/: tls qux (200; 22.589987ms)
Sep 20 01:10:51.911: INFO: (1) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/tlsrewritem... (200; 21.997518ms)
Sep 20 01:10:51.910: INFO: (1) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 21.494377ms)
Sep 20 01:10:51.912: INFO: (1) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname1/proxy/: tls baz (200; 23.732341ms)
Sep 20 01:10:51.913: INFO: (1) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname1/proxy/: foo (200; 24.805284ms)
Sep 20 01:10:51.913: INFO: (1) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">... (200; 24.086063ms)
Sep 20 01:10:51.912: INFO: (1) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname2/proxy/: tls qux (200; 24.187742ms)
Sep 20 01:10:51.913: INFO: (1) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname2/proxy/: bar (200; 24.441685ms)
Sep 20 01:10:51.913: INFO: (1) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname2/proxy/: bar (200; 25.55765ms)
Sep 20 01:10:51.915: INFO: (1) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname1/proxy/: foo (200; 26.600464ms)
Sep 20 01:10:51.970: INFO: (2) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname2/proxy/: bar (200; 54.249182ms)
Sep 20 01:10:51.970: INFO: (2) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname2/proxy/: bar (200; 54.62356ms)
Sep 20 01:10:51.970: INFO: (2) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:460/proxy/: tls baz (200; 53.888412ms)
Sep 20 01:10:51.970: INFO: (2) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 53.536714ms)
Sep 20 01:10:51.971: INFO: (2) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 56.463779ms)
Sep 20 01:10:51.972: INFO: (2) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">... (200; 53.71222ms)
Sep 20 01:10:51.972: INFO: (2) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname2/proxy/: tls qux (200; 55.283392ms)
Sep 20 01:10:51.972: INFO: (2) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/rewriteme">test</a> (200; 53.217501ms)
Sep 20 01:10:51.974: INFO: (2) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">test<... (200; 56.744772ms)
Sep 20 01:10:51.975: INFO: (2) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/tlsrewritem... (200; 57.493908ms)
Sep 20 01:10:51.976: INFO: (2) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname1/proxy/: foo (200; 57.687223ms)
Sep 20 01:10:51.976: INFO: (2) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname1/proxy/: foo (200; 57.402205ms)
Sep 20 01:10:51.977: INFO: (2) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 58.43541ms)
Sep 20 01:10:51.977: INFO: (2) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:462/proxy/: tls qux (200; 58.500524ms)
Sep 20 01:10:51.977: INFO: (2) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname1/proxy/: tls baz (200; 59.161993ms)
Sep 20 01:10:51.978: INFO: (2) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 60.725526ms)
Sep 20 01:10:51.993: INFO: (3) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/tlsrewritem... (200; 14.41371ms)
Sep 20 01:10:51.998: INFO: (3) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 20.455139ms)
Sep 20 01:10:51.998: INFO: (3) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 19.791477ms)
Sep 20 01:10:51.998: INFO: (3) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname1/proxy/: tls baz (200; 20.104441ms)
Sep 20 01:10:51.998: INFO: (3) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">... (200; 20.322492ms)
Sep 20 01:10:51.998: INFO: (3) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 20.691679ms)
Sep 20 01:10:51.999: INFO: (3) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">test<... (200; 20.144907ms)
Sep 20 01:10:51.999: INFO: (3) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname2/proxy/: tls qux (200; 20.489382ms)
Sep 20 01:10:51.999: INFO: (3) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:462/proxy/: tls qux (200; 20.135927ms)
Sep 20 01:10:51.999: INFO: (3) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname1/proxy/: foo (200; 20.331346ms)
Sep 20 01:10:51.999: INFO: (3) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:460/proxy/: tls baz (200; 21.007443ms)
Sep 20 01:10:51.999: INFO: (3) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/rewriteme">test</a> (200; 20.138922ms)
Sep 20 01:10:52.037: INFO: (3) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 58.85729ms)
Sep 20 01:10:52.038: INFO: (3) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname2/proxy/: bar (200; 59.852682ms)
Sep 20 01:10:52.038: INFO: (3) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname1/proxy/: foo (200; 60.004832ms)
Sep 20 01:10:52.038: INFO: (3) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname2/proxy/: bar (200; 59.302283ms)
Sep 20 01:10:52.060: INFO: (4) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:462/proxy/: tls qux (200; 20.16082ms)
Sep 20 01:10:52.072: INFO: (4) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname2/proxy/: bar (200; 33.038813ms)
Sep 20 01:10:52.073: INFO: (4) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname2/proxy/: tls qux (200; 33.911183ms)
Sep 20 01:10:52.073: INFO: (4) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/rewriteme">test</a> (200; 34.915041ms)
Sep 20 01:10:52.074: INFO: (4) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname2/proxy/: bar (200; 35.257562ms)
Sep 20 01:10:52.074: INFO: (4) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname1/proxy/: tls baz (200; 34.770869ms)
Sep 20 01:10:52.074: INFO: (4) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">... (200; 34.532629ms)
Sep 20 01:10:52.074: INFO: (4) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">test<... (200; 34.736621ms)
Sep 20 01:10:52.074: INFO: (4) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:460/proxy/: tls baz (200; 35.762906ms)
Sep 20 01:10:52.074: INFO: (4) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 35.272211ms)
Sep 20 01:10:52.074: INFO: (4) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 35.284528ms)
Sep 20 01:10:52.074: INFO: (4) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 34.437276ms)
Sep 20 01:10:52.075: INFO: (4) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/tlsrewritem... (200; 34.93691ms)
Sep 20 01:10:52.076: INFO: (4) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 35.740704ms)
Sep 20 01:10:52.079: INFO: (4) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname1/proxy/: foo (200; 40.282267ms)
Sep 20 01:10:52.079: INFO: (4) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname1/proxy/: foo (200; 39.670206ms)
Sep 20 01:10:52.096: INFO: (5) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/tlsrewritem... (200; 14.755615ms)
Sep 20 01:10:52.096: INFO: (5) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 15.501644ms)
Sep 20 01:10:52.096: INFO: (5) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname2/proxy/: bar (200; 15.914722ms)
Sep 20 01:10:52.096: INFO: (5) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">... (200; 15.058206ms)
Sep 20 01:10:52.096: INFO: (5) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:462/proxy/: tls qux (200; 16.851548ms)
Sep 20 01:10:52.096: INFO: (5) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 15.797705ms)
Sep 20 01:10:52.097: INFO: (5) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">test<... (200; 15.582687ms)
Sep 20 01:10:52.097: INFO: (5) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:460/proxy/: tls baz (200; 16.69935ms)
Sep 20 01:10:52.097: INFO: (5) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/rewriteme">test</a> (200; 15.074865ms)
Sep 20 01:10:52.097: INFO: (5) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 15.205244ms)
Sep 20 01:10:52.099: INFO: (5) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname2/proxy/: tls qux (200; 18.505495ms)
Sep 20 01:10:52.099: INFO: (5) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname1/proxy/: tls baz (200; 18.143279ms)
Sep 20 01:10:52.102: INFO: (5) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname2/proxy/: bar (200; 22.063885ms)
Sep 20 01:10:52.103: INFO: (5) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname1/proxy/: foo (200; 21.810228ms)
Sep 20 01:10:52.103: INFO: (5) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname1/proxy/: foo (200; 22.452126ms)
Sep 20 01:10:52.103: INFO: (5) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 20.965187ms)
Sep 20 01:10:52.125: INFO: (6) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/rewriteme">test</a> (200; 20.358683ms)
Sep 20 01:10:52.125: INFO: (6) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">... (200; 20.598345ms)
Sep 20 01:10:52.125: INFO: (6) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 20.4028ms)
Sep 20 01:10:52.125: INFO: (6) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 21.366732ms)
Sep 20 01:10:52.125: INFO: (6) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname2/proxy/: tls qux (200; 21.646975ms)
Sep 20 01:10:52.125: INFO: (6) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname1/proxy/: tls baz (200; 21.277735ms)
Sep 20 01:10:52.126: INFO: (6) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 21.081032ms)
Sep 20 01:10:52.127: INFO: (6) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">test<... (200; 22.730752ms)
Sep 20 01:10:52.127: INFO: (6) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname1/proxy/: foo (200; 22.93753ms)
Sep 20 01:10:52.131: INFO: (6) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:460/proxy/: tls baz (200; 27.638374ms)
Sep 20 01:10:52.132: INFO: (6) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname1/proxy/: foo (200; 26.733498ms)
Sep 20 01:10:52.132: INFO: (6) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/tlsrewritem... (200; 27.307131ms)
Sep 20 01:10:52.132: INFO: (6) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:462/proxy/: tls qux (200; 27.0305ms)
Sep 20 01:10:52.132: INFO: (6) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname2/proxy/: bar (200; 26.981868ms)
Sep 20 01:10:52.132: INFO: (6) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname2/proxy/: bar (200; 26.849711ms)
Sep 20 01:10:52.134: INFO: (6) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 30.083886ms)
Sep 20 01:10:52.176: INFO: (7) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:460/proxy/: tls baz (200; 41.85915ms)
Sep 20 01:10:52.176: INFO: (7) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 41.037854ms)
Sep 20 01:10:52.176: INFO: (7) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">test<... (200; 41.38783ms)
Sep 20 01:10:52.176: INFO: (7) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 41.67911ms)
Sep 20 01:10:52.177: INFO: (7) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:462/proxy/: tls qux (200; 41.786219ms)
Sep 20 01:10:52.179: INFO: (7) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">... (200; 43.95377ms)
Sep 20 01:10:52.179: INFO: (7) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 43.517652ms)
Sep 20 01:10:52.179: INFO: (7) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname1/proxy/: tls baz (200; 44.302227ms)
Sep 20 01:10:52.179: INFO: (7) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/rewriteme">test</a> (200; 43.883991ms)
Sep 20 01:10:52.181: INFO: (7) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname2/proxy/: tls qux (200; 45.438728ms)
Sep 20 01:10:52.181: INFO: (7) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/tlsrewritem... (200; 45.339885ms)
Sep 20 01:10:52.182: INFO: (7) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname2/proxy/: bar (200; 46.449989ms)
Sep 20 01:10:52.192: INFO: (7) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname1/proxy/: foo (200; 56.975254ms)
Sep 20 01:10:52.193: INFO: (7) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname2/proxy/: bar (200; 57.665026ms)
Sep 20 01:10:52.194: INFO: (7) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname1/proxy/: foo (200; 58.367375ms)
Sep 20 01:10:52.212: INFO: (7) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 76.86461ms)
Sep 20 01:10:52.228: INFO: (8) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 15.490727ms)
Sep 20 01:10:52.229: INFO: (8) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">test<... (200; 14.109932ms)
Sep 20 01:10:52.229: INFO: (8) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname1/proxy/: foo (200; 16.325764ms)
Sep 20 01:10:52.229: INFO: (8) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 15.424023ms)
Sep 20 01:10:52.229: INFO: (8) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 14.678207ms)
Sep 20 01:10:52.229: INFO: (8) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:462/proxy/: tls qux (200; 14.567523ms)
Sep 20 01:10:52.229: INFO: (8) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/rewriteme">test</a> (200; 15.334243ms)
Sep 20 01:10:52.229: INFO: (8) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:460/proxy/: tls baz (200; 16.995527ms)
Sep 20 01:10:52.271: INFO: (8) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname2/proxy/: tls qux (200; 58.713226ms)
Sep 20 01:10:52.271: INFO: (8) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/tlsrewritem... (200; 56.89937ms)
Sep 20 01:10:52.272: INFO: (8) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname2/proxy/: bar (200; 57.127903ms)
Sep 20 01:10:52.272: INFO: (8) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">... (200; 57.230219ms)
Sep 20 01:10:52.295: INFO: (8) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname2/proxy/: bar (200; 82.62754ms)
Sep 20 01:10:52.295: INFO: (8) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 81.154225ms)
Sep 20 01:10:52.295: INFO: (8) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname1/proxy/: foo (200; 81.621087ms)
Sep 20 01:10:52.295: INFO: (8) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname1/proxy/: tls baz (200; 81.104655ms)
Sep 20 01:10:52.317: INFO: (9) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/tlsrewritem... (200; 20.445808ms)
Sep 20 01:10:52.317: INFO: (9) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname1/proxy/: foo (200; 20.390424ms)
Sep 20 01:10:52.317: INFO: (9) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 20.227095ms)
Sep 20 01:10:52.317: INFO: (9) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">test<... (200; 20.91565ms)
Sep 20 01:10:52.318: INFO: (9) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 21.426718ms)
Sep 20 01:10:52.318: INFO: (9) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">... (200; 20.649457ms)
Sep 20 01:10:52.318: INFO: (9) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:460/proxy/: tls baz (200; 21.128652ms)
Sep 20 01:10:52.320: INFO: (9) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 22.429689ms)
Sep 20 01:10:52.320: INFO: (9) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname2/proxy/: tls qux (200; 22.876819ms)
Sep 20 01:10:52.320: INFO: (9) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/rewriteme">test</a> (200; 23.664135ms)
Sep 20 01:10:52.320: INFO: (9) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:462/proxy/: tls qux (200; 22.402858ms)
Sep 20 01:10:52.320: INFO: (9) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname1/proxy/: tls baz (200; 22.561559ms)
Sep 20 01:10:52.322: INFO: (9) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname2/proxy/: bar (200; 25.970604ms)
Sep 20 01:10:52.323: INFO: (9) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname2/proxy/: bar (200; 26.731072ms)
Sep 20 01:10:52.323: INFO: (9) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 25.755549ms)
Sep 20 01:10:52.323: INFO: (9) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname1/proxy/: foo (200; 25.712285ms)
Sep 20 01:10:52.344: INFO: (10) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:460/proxy/: tls baz (200; 21.226382ms)
Sep 20 01:10:52.344: INFO: (10) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 21.049564ms)
Sep 20 01:10:52.344: INFO: (10) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:462/proxy/: tls qux (200; 21.213468ms)
Sep 20 01:10:52.345: INFO: (10) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/rewriteme">test</a> (200; 21.209077ms)
Sep 20 01:10:52.345: INFO: (10) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/tlsrewritem... (200; 20.957536ms)
Sep 20 01:10:52.345: INFO: (10) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname1/proxy/: foo (200; 21.48083ms)
Sep 20 01:10:52.345: INFO: (10) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 21.151574ms)
Sep 20 01:10:52.345: INFO: (10) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">... (200; 21.073894ms)
Sep 20 01:10:52.345: INFO: (10) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">test<... (200; 21.492978ms)
Sep 20 01:10:52.392: INFO: (10) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 68.922196ms)
Sep 20 01:10:52.433: INFO: (10) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname2/proxy/: tls qux (200; 109.686789ms)
Sep 20 01:10:52.433: INFO: (10) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname1/proxy/: tls baz (200; 109.93407ms)
Sep 20 01:10:52.433: INFO: (10) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname1/proxy/: foo (200; 109.746504ms)
Sep 20 01:10:52.434: INFO: (10) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 111.124597ms)
Sep 20 01:10:52.435: INFO: (10) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname2/proxy/: bar (200; 111.844974ms)
Sep 20 01:10:52.436: INFO: (10) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname2/proxy/: bar (200; 112.520449ms)
Sep 20 01:10:52.453: INFO: (11) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/tlsrewritem... (200; 17.063034ms)
Sep 20 01:10:52.454: INFO: (11) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 16.891951ms)
Sep 20 01:10:52.454: INFO: (11) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/rewriteme">test</a> (200; 17.43345ms)
Sep 20 01:10:52.454: INFO: (11) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 17.44148ms)
Sep 20 01:10:52.461: INFO: (11) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 22.441217ms)
Sep 20 01:10:52.461: INFO: (11) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:462/proxy/: tls qux (200; 23.502477ms)
Sep 20 01:10:52.461: INFO: (11) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname1/proxy/: foo (200; 22.547897ms)
Sep 20 01:10:52.462: INFO: (11) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">... (200; 21.339089ms)
Sep 20 01:10:52.461: INFO: (11) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">test<... (200; 21.206937ms)
Sep 20 01:10:52.463: INFO: (11) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:460/proxy/: tls baz (200; 24.995211ms)
Sep 20 01:10:52.463: INFO: (11) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname2/proxy/: bar (200; 25.412017ms)
Sep 20 01:10:52.463: INFO: (11) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname1/proxy/: foo (200; 25.417914ms)
Sep 20 01:10:52.464: INFO: (11) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname2/proxy/: tls qux (200; 25.115788ms)
Sep 20 01:10:52.465: INFO: (11) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname1/proxy/: tls baz (200; 25.476304ms)
Sep 20 01:10:52.467: INFO: (11) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 27.092432ms)
Sep 20 01:10:52.467: INFO: (11) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname2/proxy/: bar (200; 28.480679ms)
Sep 20 01:10:52.524: INFO: (12) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/tlsrewritem... (200; 56.44949ms)
Sep 20 01:10:52.524: INFO: (12) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 55.502846ms)
Sep 20 01:10:52.524: INFO: (12) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">test<... (200; 55.786795ms)
Sep 20 01:10:52.524: INFO: (12) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 55.425948ms)
Sep 20 01:10:52.524: INFO: (12) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname2/proxy/: bar (200; 56.676733ms)
Sep 20 01:10:52.524: INFO: (12) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:462/proxy/: tls qux (200; 57.37817ms)
Sep 20 01:10:52.528: INFO: (12) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/rewriteme">test</a> (200; 59.775649ms)
Sep 20 01:10:52.529: INFO: (12) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 59.812241ms)
Sep 20 01:10:52.531: INFO: (12) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 62.59677ms)
Sep 20 01:10:52.531: INFO: (12) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname2/proxy/: tls qux (200; 62.191544ms)
Sep 20 01:10:52.531: INFO: (12) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname1/proxy/: foo (200; 62.600373ms)
Sep 20 01:10:52.531: INFO: (12) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:460/proxy/: tls baz (200; 63.006353ms)
Sep 20 01:10:52.531: INFO: (12) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname1/proxy/: foo (200; 62.726472ms)
Sep 20 01:10:52.531: INFO: (12) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">... (200; 62.549271ms)
Sep 20 01:10:52.533: INFO: (12) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname1/proxy/: tls baz (200; 64.99845ms)
Sep 20 01:10:52.535: INFO: (12) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname2/proxy/: bar (200; 66.507865ms)
Sep 20 01:10:52.555: INFO: (13) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/tlsrewritem... (200; 20.310941ms)
Sep 20 01:10:52.556: INFO: (13) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 19.644115ms)
Sep 20 01:10:52.556: INFO: (13) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 19.466332ms)
Sep 20 01:10:52.558: INFO: (13) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname2/proxy/: tls qux (200; 22.803234ms)
Sep 20 01:10:52.558: INFO: (13) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">test<... (200; 21.724268ms)
Sep 20 01:10:52.559: INFO: (13) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">... (200; 21.722315ms)
Sep 20 01:10:52.564: INFO: (13) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/rewriteme">test</a> (200; 25.242913ms)
Sep 20 01:10:52.564: INFO: (13) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname2/proxy/: bar (200; 27.361872ms)
Sep 20 01:10:52.564: INFO: (13) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:460/proxy/: tls baz (200; 25.302918ms)
Sep 20 01:10:52.564: INFO: (13) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 25.922484ms)
Sep 20 01:10:52.564: INFO: (13) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname1/proxy/: foo (200; 26.351392ms)
Sep 20 01:10:52.564: INFO: (13) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:462/proxy/: tls qux (200; 26.866601ms)
Sep 20 01:10:52.564: INFO: (13) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname1/proxy/: tls baz (200; 26.442098ms)
Sep 20 01:10:52.565: INFO: (13) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname1/proxy/: foo (200; 28.137002ms)
Sep 20 01:10:52.599: INFO: (13) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 60.554108ms)
Sep 20 01:10:52.599: INFO: (13) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname2/proxy/: bar (200; 60.278762ms)
Sep 20 01:10:52.620: INFO: (14) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 19.460336ms)
Sep 20 01:10:52.620: INFO: (14) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 19.207135ms)
Sep 20 01:10:52.620: INFO: (14) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 19.323886ms)
Sep 20 01:10:52.621: INFO: (14) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:460/proxy/: tls baz (200; 19.960703ms)
Sep 20 01:10:52.621: INFO: (14) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">... (200; 19.576763ms)
Sep 20 01:10:52.621: INFO: (14) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/tlsrewritem... (200; 19.890501ms)
Sep 20 01:10:52.621: INFO: (14) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">test<... (200; 19.710469ms)
Sep 20 01:10:52.623: INFO: (14) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 23.052954ms)
Sep 20 01:10:52.626: INFO: (14) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:462/proxy/: tls qux (200; 25.014091ms)
Sep 20 01:10:52.626: INFO: (14) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname2/proxy/: tls qux (200; 25.881464ms)
Sep 20 01:10:52.626: INFO: (14) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/rewriteme">test</a> (200; 25.407937ms)
Sep 20 01:10:52.629: INFO: (14) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname1/proxy/: tls baz (200; 29.995724ms)
Sep 20 01:10:52.629: INFO: (14) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname2/proxy/: bar (200; 29.019229ms)
Sep 20 01:10:52.629: INFO: (14) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname2/proxy/: bar (200; 28.054592ms)
Sep 20 01:10:52.629: INFO: (14) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname1/proxy/: foo (200; 28.519262ms)
Sep 20 01:10:52.630: INFO: (14) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname1/proxy/: foo (200; 29.918489ms)
Sep 20 01:10:52.643: INFO: (15) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 11.288046ms)
Sep 20 01:10:52.644: INFO: (15) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:462/proxy/: tls qux (200; 13.72575ms)
Sep 20 01:10:52.645: INFO: (15) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname1/proxy/: foo (200; 13.523593ms)
Sep 20 01:10:52.645: INFO: (15) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">test<... (200; 12.980044ms)
Sep 20 01:10:52.645: INFO: (15) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">... (200; 13.209566ms)
Sep 20 01:10:52.648: INFO: (15) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname2/proxy/: tls qux (200; 16.642148ms)
Sep 20 01:10:52.649: INFO: (15) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname1/proxy/: tls baz (200; 17.750534ms)
Sep 20 01:10:52.649: INFO: (15) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 18.69808ms)
Sep 20 01:10:52.649: INFO: (15) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/tlsrewritem... (200; 19.068519ms)
Sep 20 01:10:52.649: INFO: (15) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 17.999013ms)
Sep 20 01:10:52.649: INFO: (15) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 18.676323ms)
Sep 20 01:10:52.649: INFO: (15) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:460/proxy/: tls baz (200; 18.431486ms)
Sep 20 01:10:52.650: INFO: (15) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/rewriteme">test</a> (200; 19.37114ms)
Sep 20 01:10:52.651: INFO: (15) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname1/proxy/: foo (200; 19.458682ms)
Sep 20 01:10:52.652: INFO: (15) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname2/proxy/: bar (200; 21.309973ms)
Sep 20 01:10:52.655: INFO: (15) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname2/proxy/: bar (200; 24.465992ms)
Sep 20 01:10:52.693: INFO: (16) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:462/proxy/: tls qux (200; 37.998584ms)
Sep 20 01:10:52.693: INFO: (16) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">... (200; 36.937889ms)
Sep 20 01:10:52.693: INFO: (16) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">test<... (200; 37.298799ms)
Sep 20 01:10:52.694: INFO: (16) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname2/proxy/: bar (200; 38.753223ms)
Sep 20 01:10:52.694: INFO: (16) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname1/proxy/: tls baz (200; 37.924223ms)
Sep 20 01:10:52.694: INFO: (16) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 37.913649ms)
Sep 20 01:10:52.694: INFO: (16) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/tlsrewritem... (200; 38.501575ms)
Sep 20 01:10:52.694: INFO: (16) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname2/proxy/: tls qux (200; 38.734372ms)
Sep 20 01:10:52.695: INFO: (16) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname1/proxy/: foo (200; 39.392247ms)
Sep 20 01:10:52.695: INFO: (16) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 38.611913ms)
Sep 20 01:10:52.695: INFO: (16) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/rewriteme">test</a> (200; 39.072131ms)
Sep 20 01:10:52.695: INFO: (16) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:460/proxy/: tls baz (200; 40.203734ms)
Sep 20 01:10:52.737: INFO: (16) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 81.279739ms)
Sep 20 01:10:52.737: INFO: (16) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname2/proxy/: bar (200; 81.180441ms)
Sep 20 01:10:52.737: INFO: (16) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 81.061724ms)
Sep 20 01:10:52.737: INFO: (16) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname1/proxy/: foo (200; 80.711731ms)
Sep 20 01:10:52.754: INFO: (17) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 15.518613ms)
Sep 20 01:10:52.754: INFO: (17) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 16.12875ms)
Sep 20 01:10:52.755: INFO: (17) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 16.234985ms)
Sep 20 01:10:52.755: INFO: (17) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/rewriteme">test</a> (200; 16.180561ms)
Sep 20 01:10:52.755: INFO: (17) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:460/proxy/: tls baz (200; 17.079835ms)
Sep 20 01:10:52.756: INFO: (17) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 17.605345ms)
Sep 20 01:10:52.756: INFO: (17) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">test<... (200; 17.096731ms)
Sep 20 01:10:52.756: INFO: (17) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:462/proxy/: tls qux (200; 18.171616ms)
Sep 20 01:10:52.756: INFO: (17) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/tlsrewritem... (200; 17.719945ms)
Sep 20 01:10:52.797: INFO: (17) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname1/proxy/: tls baz (200; 58.150182ms)
Sep 20 01:10:52.797: INFO: (17) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname2/proxy/: bar (200; 58.486333ms)
Sep 20 01:10:52.797: INFO: (17) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname2/proxy/: tls qux (200; 60.121627ms)
Sep 20 01:10:52.798: INFO: (17) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">... (200; 58.865662ms)
Sep 20 01:10:52.799: INFO: (17) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname1/proxy/: foo (200; 59.896586ms)
Sep 20 01:10:52.799: INFO: (17) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname1/proxy/: foo (200; 60.826057ms)
Sep 20 01:10:52.799: INFO: (17) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname2/proxy/: bar (200; 61.535119ms)
Sep 20 01:10:52.825: INFO: (18) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 23.64278ms)
Sep 20 01:10:52.825: INFO: (18) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">test<... (200; 24.029878ms)
Sep 20 01:10:52.826: INFO: (18) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname1/proxy/: foo (200; 25.260125ms)
Sep 20 01:10:52.826: INFO: (18) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/rewriteme">test</a> (200; 24.914638ms)
Sep 20 01:10:52.826: INFO: (18) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname2/proxy/: bar (200; 25.628765ms)
Sep 20 01:10:52.826: INFO: (18) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:462/proxy/: tls qux (200; 24.911674ms)
Sep 20 01:10:52.826: INFO: (18) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/tlsrewritem... (200; 26.34608ms)
Sep 20 01:10:52.827: INFO: (18) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 26.704981ms)
Sep 20 01:10:52.827: INFO: (18) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">... (200; 26.836103ms)
Sep 20 01:10:52.827: INFO: (18) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname1/proxy/: tls baz (200; 26.522532ms)
Sep 20 01:10:52.839: INFO: (18) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname2/proxy/: bar (200; 37.99545ms)
Sep 20 01:10:52.839: INFO: (18) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname2/proxy/: tls qux (200; 38.035945ms)
Sep 20 01:10:52.839: INFO: (18) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname1/proxy/: foo (200; 38.317264ms)
Sep 20 01:10:52.839: INFO: (18) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 38.550005ms)
Sep 20 01:10:52.839: INFO: (18) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:460/proxy/: tls baz (200; 38.711847ms)
Sep 20 01:10:52.839: INFO: (18) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 38.638937ms)
Sep 20 01:10:52.856: INFO: (19) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 15.541324ms)
Sep 20 01:10:52.857: INFO: (19) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 16.591308ms)
Sep 20 01:10:52.857: INFO: (19) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:162/proxy/: bar (200; 16.651599ms)
Sep 20 01:10:52.857: INFO: (19) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname2/proxy/: tls qux (200; 17.112417ms)
Sep 20 01:10:52.858: INFO: (19) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:160/proxy/: foo (200; 17.470294ms)
Sep 20 01:10:52.858: INFO: (19) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:443/proxy/tlsrewritem... (200; 18.030978ms)
Sep 20 01:10:52.858: INFO: (19) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh/proxy/rewriteme">test</a> (200; 17.843533ms)
Sep 20 01:10:52.858: INFO: (19) /api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">test<... (200; 17.082608ms)
Sep 20 01:10:52.862: INFO: (19) /api/v1/namespaces/proxy-8412/services/https:proxy-service-rk6x2:tlsportname1/proxy/: tls baz (200; 21.605928ms)
Sep 20 01:10:52.862: INFO: (19) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:462/proxy/: tls qux (200; 21.648666ms)
Sep 20 01:10:52.862: INFO: (19) /api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/: <a href="/api/v1/namespaces/proxy-8412/pods/http:proxy-service-rk6x2-ckhwh:1080/proxy/rewriteme">... (200; 20.861566ms)
Sep 20 01:10:52.862: INFO: (19) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname2/proxy/: bar (200; 21.926166ms)
Sep 20 01:10:52.863: INFO: (19) /api/v1/namespaces/proxy-8412/pods/https:proxy-service-rk6x2-ckhwh:460/proxy/: tls baz (200; 22.287315ms)
Sep 20 01:10:52.863: INFO: (19) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname1/proxy/: foo (200; 22.76373ms)
Sep 20 01:10:52.863: INFO: (19) /api/v1/namespaces/proxy-8412/services/proxy-service-rk6x2:portname1/proxy/: foo (200; 23.090481ms)
Sep 20 01:10:52.864: INFO: (19) /api/v1/namespaces/proxy-8412/services/http:proxy-service-rk6x2:portname2/proxy/: bar (200; 23.120237ms)
STEP: deleting ReplicationController proxy-service-rk6x2 in namespace proxy-8412, will wait for the garbage collector to delete the pods
I0920 01:10:52.873913      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 01:10:52.873978      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 01:10:52.936: INFO: Deleting ReplicationController proxy-service-rk6x2 took: 12.121625ms
Sep 20 01:10:53.336: INFO: Terminating ReplicationController proxy-service-rk6x2 pods took: 400.612338ms
I0920 01:10:53.336778      16 controller_utils.go:810] Ignoring inactive pod proxy-8412/proxy-service-rk6x2-ckhwh in state Running, deletion time 2019-09-20 01:10:54 +0000 UTC
[AfterEach] version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:10:56.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8412" for this suite.
Sep 20 01:11:02.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:11:02.329: INFO: namespace proxy-8412 deletion completed in 6.284237435s

• [SLOW TEST:14.949 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:11:02.330: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7317
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Sep 20 01:11:02.528: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 01:11:06.113: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:11:18.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7317" for this suite.
Sep 20 01:11:24.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:11:24.674: INFO: namespace crd-publish-openapi-7317 deletion completed in 6.231543079s

• [SLOW TEST:22.345 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:11:24.682: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1654
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Sep 20 01:11:34.998: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0920 01:11:34.998676      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:11:34.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1654" for this suite.
Sep 20 01:11:41.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:11:41.212: INFO: namespace gc-1654 deletion completed in 6.205527685s

• [SLOW TEST:16.532 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:11:41.220: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6739
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Sep 20 01:11:41.400: INFO: Waiting up to 5m0s for pod "pod-a718ed54-aa24-414e-b508-f8566c54cead" in namespace "emptydir-6739" to be "success or failure"
Sep 20 01:11:41.404: INFO: Pod "pod-a718ed54-aa24-414e-b508-f8566c54cead": Phase="Pending", Reason="", readiness=false. Elapsed: 4.196494ms
Sep 20 01:11:43.412: INFO: Pod "pod-a718ed54-aa24-414e-b508-f8566c54cead": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012474857s
Sep 20 01:11:45.420: INFO: Pod "pod-a718ed54-aa24-414e-b508-f8566c54cead": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020165317s
STEP: Saw pod success
Sep 20 01:11:45.420: INFO: Pod "pod-a718ed54-aa24-414e-b508-f8566c54cead" satisfied condition "success or failure"
Sep 20 01:11:45.426: INFO: Trying to get logs from node worker-wqshf-7859ffd555-lmjf4 pod pod-a718ed54-aa24-414e-b508-f8566c54cead container test-container: <nil>
STEP: delete the pod
Sep 20 01:11:45.468: INFO: Waiting for pod pod-a718ed54-aa24-414e-b508-f8566c54cead to disappear
Sep 20 01:11:45.473: INFO: Pod pod-a718ed54-aa24-414e-b508-f8566c54cead no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:11:45.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6739" for this suite.
Sep 20 01:11:51.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:11:51.705: INFO: namespace emptydir-6739 deletion completed in 6.227365748s

• [SLOW TEST:10.486 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:11:51.709: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-780
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-aa8e1b63-74d5-4f6b-83b7-502f4b563fe5
STEP: Creating configMap with name cm-test-opt-upd-f22c6c43-9ade-4e41-8fa7-5b72bad04952
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-aa8e1b63-74d5-4f6b-83b7-502f4b563fe5
STEP: Updating configmap cm-test-opt-upd-f22c6c43-9ade-4e41-8fa7-5b72bad04952
STEP: Creating configMap with name cm-test-opt-create-7c71c53a-2678-4dae-886f-8dbf6dfd16a9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:12:00.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-780" for this suite.
Sep 20 01:12:14.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:12:14.736: INFO: namespace projected-780 deletion completed in 14.213086767s

• [SLOW TEST:23.027 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:12:14.737: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9882
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-f398c821-1528-44de-875e-894c2b67078d
STEP: Creating a pod to test consume configMaps
Sep 20 01:12:14.951: INFO: Waiting up to 5m0s for pod "pod-configmaps-b8e5a0ea-5081-456f-8657-35ff40b6f2fb" in namespace "configmap-9882" to be "success or failure"
Sep 20 01:12:14.959: INFO: Pod "pod-configmaps-b8e5a0ea-5081-456f-8657-35ff40b6f2fb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017899ms
Sep 20 01:12:16.966: INFO: Pod "pod-configmaps-b8e5a0ea-5081-456f-8657-35ff40b6f2fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015063648s
Sep 20 01:12:18.975: INFO: Pod "pod-configmaps-b8e5a0ea-5081-456f-8657-35ff40b6f2fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024257352s
STEP: Saw pod success
Sep 20 01:12:18.975: INFO: Pod "pod-configmaps-b8e5a0ea-5081-456f-8657-35ff40b6f2fb" satisfied condition "success or failure"
Sep 20 01:12:18.982: INFO: Trying to get logs from node worker-wqshf-7859ffd555-lmjf4 pod pod-configmaps-b8e5a0ea-5081-456f-8657-35ff40b6f2fb container configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 01:12:19.048: INFO: Waiting for pod pod-configmaps-b8e5a0ea-5081-456f-8657-35ff40b6f2fb to disappear
Sep 20 01:12:19.052: INFO: Pod pod-configmaps-b8e5a0ea-5081-456f-8657-35ff40b6f2fb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:12:19.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9882" for this suite.
I0920 01:12:23.667550      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
Sep 20 01:12:25.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:12:25.277: INFO: namespace configmap-9882 deletion completed in 6.217058067s

• [SLOW TEST:10.541 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:12:25.278: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5001
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0920 01:12:55.999559      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 20 01:12:55.999: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:12:55.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5001" for this suite.
Sep 20 01:13:02.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:13:02.240: INFO: namespace gc-5001 deletion completed in 6.22939301s

• [SLOW TEST:36.963 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:13:02.243: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7245
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-91efb640-2515-4d0f-a7ea-0322c7dc9455
STEP: Creating a pod to test consume secrets
Sep 20 01:13:02.443: INFO: Waiting up to 5m0s for pod "pod-secrets-f98ffd8f-7459-4f09-81bf-9dccc9d5876f" in namespace "secrets-7245" to be "success or failure"
Sep 20 01:13:02.450: INFO: Pod "pod-secrets-f98ffd8f-7459-4f09-81bf-9dccc9d5876f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.456981ms
Sep 20 01:13:04.458: INFO: Pod "pod-secrets-f98ffd8f-7459-4f09-81bf-9dccc9d5876f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015089554s
Sep 20 01:13:06.466: INFO: Pod "pod-secrets-f98ffd8f-7459-4f09-81bf-9dccc9d5876f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022477708s
STEP: Saw pod success
Sep 20 01:13:06.466: INFO: Pod "pod-secrets-f98ffd8f-7459-4f09-81bf-9dccc9d5876f" satisfied condition "success or failure"
Sep 20 01:13:06.472: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod pod-secrets-f98ffd8f-7459-4f09-81bf-9dccc9d5876f container secret-volume-test: <nil>
STEP: delete the pod
Sep 20 01:13:06.552: INFO: Waiting for pod pod-secrets-f98ffd8f-7459-4f09-81bf-9dccc9d5876f to disappear
Sep 20 01:13:06.559: INFO: Pod pod-secrets-f98ffd8f-7459-4f09-81bf-9dccc9d5876f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:13:06.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7245" for this suite.
Sep 20 01:13:12.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:13:12.793: INFO: namespace secrets-7245 deletion completed in 6.225282122s

• [SLOW TEST:10.550 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:13:12.793: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2168
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-3b5f080a-ef5e-4cb8-baf5-0180c3984c4c in namespace container-probe-2168
Sep 20 01:13:16.992: INFO: Started pod liveness-3b5f080a-ef5e-4cb8-baf5-0180c3984c4c in namespace container-probe-2168
STEP: checking the pod's current state and verifying that restartCount is present
Sep 20 01:13:16.998: INFO: Initial restart count of pod liveness-3b5f080a-ef5e-4cb8-baf5-0180c3984c4c is 0
Sep 20 01:13:31.059: INFO: Restart count of pod container-probe-2168/liveness-3b5f080a-ef5e-4cb8-baf5-0180c3984c4c is now 1 (14.061012922s elapsed)
Sep 20 01:13:53.136: INFO: Restart count of pod container-probe-2168/liveness-3b5f080a-ef5e-4cb8-baf5-0180c3984c4c is now 2 (36.13816361s elapsed)
Sep 20 01:14:11.194: INFO: Restart count of pod container-probe-2168/liveness-3b5f080a-ef5e-4cb8-baf5-0180c3984c4c is now 3 (54.196020094s elapsed)
Sep 20 01:14:31.262: INFO: Restart count of pod container-probe-2168/liveness-3b5f080a-ef5e-4cb8-baf5-0180c3984c4c is now 4 (1m14.264445368s elapsed)
I0920 01:15:09.838646      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
Sep 20 01:15:33.489: INFO: Restart count of pod container-probe-2168/liveness-3b5f080a-ef5e-4cb8-baf5-0180c3984c4c is now 5 (2m16.491103914s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:15:33.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2168" for this suite.
Sep 20 01:15:39.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:15:39.738: INFO: namespace container-probe-2168 deletion completed in 6.218355035s

• [SLOW TEST:146.945 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:15:39.738: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1502
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1502
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-1502
I0920 01:15:39.962909      16 runners.go:184] Created replication controller with name: externalname-service, namespace: services-1502, replica count: 2
I0920 01:15:39.963468      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 01:15:39.963525      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 01:15:43.014537      16 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 20 01:15:43.015: INFO: Creating new exec pod
I0920 01:15:47.052183      16 reflector.go:120] Starting reflector *v1.Endpoints (0s) from k8s.io/kubernetes/test/e2e/framework/service/jig.go:389
I0920 01:15:47.052241      16 reflector.go:158] Listing and watching *v1.Endpoints from k8s.io/kubernetes/test/e2e/framework/service/jig.go:389
Sep 20 01:15:48.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=services-1502 execpodbgvm4 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Sep 20 01:15:48.716: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 20 01:15:48.716: INFO: stdout: ""
Sep 20 01:15:48.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=services-1502 execpodbgvm4 -- /bin/sh -x -c nc -zv -t -w 2 10.240.29.252 80'
Sep 20 01:15:49.394: INFO: stderr: "+ nc -zv -t -w 2 10.240.29.252 80\nConnection to 10.240.29.252 80 port [tcp/http] succeeded!\n"
Sep 20 01:15:49.394: INFO: stdout: ""
Sep 20 01:15:49.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=services-1502 execpodbgvm4 -- /bin/sh -x -c nc -zv -t -w 2 104.248.240.176 32342'
Sep 20 01:15:50.114: INFO: stderr: "+ nc -zv -t -w 2 104.248.240.176 32342\nConnection to 104.248.240.176 32342 port [tcp/32342] succeeded!\n"
Sep 20 01:15:50.114: INFO: stdout: ""
Sep 20 01:15:50.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=services-1502 execpodbgvm4 -- /bin/sh -x -c nc -zv -t -w 2 104.248.240.179 32342'
Sep 20 01:15:50.759: INFO: stderr: "+ nc -zv -t -w 2 104.248.240.179 32342\nConnection to 104.248.240.179 32342 port [tcp/32342] succeeded!\n"
Sep 20 01:15:50.759: INFO: stdout: ""
Sep 20 01:15:50.759: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:15:50.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1502" for this suite.
Sep 20 01:15:56.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:15:57.011: INFO: namespace services-1502 deletion completed in 6.214571346s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:17.272 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:15:57.012: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-907
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-96dddbdb-09b6-4e20-8d4b-043c9437766b
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:15:57.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-907" for this suite.
Sep 20 01:16:03.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:16:03.406: INFO: namespace configmap-907 deletion completed in 6.209519091s

• [SLOW TEST:6.394 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:16:03.406: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6025
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 01:16:03.584: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep 20 01:16:03.600: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 20 01:16:08.607: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 20 01:16:08.608: INFO: Creating deployment "test-rolling-update-deployment"
Sep 20 01:16:08.617: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep 20 01:16:08.629: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep 20 01:16:10.642: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep 20 01:16:10.647: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538968, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538968, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538968, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704538968, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 01:16:12.653: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Sep 20 01:16:12.674: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6025 /apis/apps/v1/namespaces/deployment-6025/deployments/test-rolling-update-deployment 00b2c751-ab0a-485a-a3a3-773dd694a7ee 70613 1 2019-09-20 01:16:08 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004f4d198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-09-20 01:16:08 +0000 UTC,LastTransitionTime:2019-09-20 01:16:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2019-09-20 01:16:10 +0000 UTC,LastTransitionTime:2019-09-20 01:16:08 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 20 01:16:12.681: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-6025 /apis/apps/v1/namespaces/deployment-6025/replicasets/test-rolling-update-deployment-55d946486 537ba356-b464-495a-a6ae-b4553069a217 70604 1 2019-09-20 01:16:08 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 00b2c751-ab0a-485a-a3a3-773dd694a7ee 0xc00174c790 0xc00174c791}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00174c7f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 20 01:16:12.681: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep 20 01:16:12.682: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6025 /apis/apps/v1/namespaces/deployment-6025/replicasets/test-rolling-update-controller b3c55240-784d-42c1-bb85-432eb15274f4 70612 2 2019-09-20 01:16:03 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 00b2c751-ab0a-485a-a3a3-773dd694a7ee 0xc00174c6c7 0xc00174c6c8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00174c728 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 20 01:16:12.689: INFO: Pod "test-rolling-update-deployment-55d946486-5nl4f" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-5nl4f test-rolling-update-deployment-55d946486- deployment-6025 /api/v1/namespaces/deployment-6025/pods/test-rolling-update-deployment-55d946486-5nl4f 74c89228-f4cf-47b0-ae37-b5566612b388 70603 0 2019-09-20 01:16:08 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[cni.projectcalico.org/podIP:172.25.3.38/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 537ba356-b464-495a-a6ae-b4553069a217 0xc00174cc70 0xc00174cc71}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6qzxf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6qzxf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6qzxf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-lmjf4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:16:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:16:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:16:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:16:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.206,PodIP:172.25.3.38,StartTime:2019-09-20 01:16:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 01:16:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://b73fc44feb02c57c555f049eabf751c8b6e5bb12dd746ab60a2838a19591e2bd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.3.38,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:16:12.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6025" for this suite.
Sep 20 01:16:18.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:16:18.909: INFO: namespace deployment-6025 deletion completed in 6.212834282s

• [SLOW TEST:15.503 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:16:18.909: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9975
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 20 01:16:19.083: INFO: Waiting up to 5m0s for pod "pod-8cc1609c-f625-4f7c-97d9-59c4a3b95fe0" in namespace "emptydir-9975" to be "success or failure"
Sep 20 01:16:19.091: INFO: Pod "pod-8cc1609c-f625-4f7c-97d9-59c4a3b95fe0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.256436ms
Sep 20 01:16:21.098: INFO: Pod "pod-8cc1609c-f625-4f7c-97d9-59c4a3b95fe0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014361897s
Sep 20 01:16:23.105: INFO: Pod "pod-8cc1609c-f625-4f7c-97d9-59c4a3b95fe0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021104241s
STEP: Saw pod success
Sep 20 01:16:23.105: INFO: Pod "pod-8cc1609c-f625-4f7c-97d9-59c4a3b95fe0" satisfied condition "success or failure"
Sep 20 01:16:23.109: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod pod-8cc1609c-f625-4f7c-97d9-59c4a3b95fe0 container test-container: <nil>
STEP: delete the pod
Sep 20 01:16:23.186: INFO: Waiting for pod pod-8cc1609c-f625-4f7c-97d9-59c4a3b95fe0 to disappear
Sep 20 01:16:23.190: INFO: Pod pod-8cc1609c-f625-4f7c-97d9-59c4a3b95fe0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:16:23.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9975" for this suite.
Sep 20 01:16:29.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:16:29.447: INFO: namespace emptydir-9975 deletion completed in 6.251070466s

• [SLOW TEST:10.537 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:16:29.447: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6859
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 01:16:29.627: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Sep 20 01:16:30.681: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:16:30.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6859" for this suite.
Sep 20 01:16:36.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:16:36.940: INFO: namespace replication-controller-6859 deletion completed in 6.245360249s

• [SLOW TEST:7.493 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:16:36.940: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7281
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 20 01:16:37.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-7281'
Sep 20 01:16:37.565: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 20 01:16:37.566: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Sep 20 01:16:37.584: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-926xp]
Sep 20 01:16:37.585: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-926xp" in namespace "kubectl-7281" to be "running and ready"
Sep 20 01:16:37.592: INFO: Pod "e2e-test-httpd-rc-926xp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.82547ms
Sep 20 01:16:39.602: INFO: Pod "e2e-test-httpd-rc-926xp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01668873s
Sep 20 01:16:41.608: INFO: Pod "e2e-test-httpd-rc-926xp": Phase="Running", Reason="", readiness=true. Elapsed: 4.022835041s
Sep 20 01:16:41.608: INFO: Pod "e2e-test-httpd-rc-926xp" satisfied condition "running and ready"
Sep 20 01:16:41.608: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-926xp]
Sep 20 01:16:41.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 logs rc/e2e-test-httpd-rc --namespace=kubectl-7281'
Sep 20 01:16:41.779: INFO: stderr: ""
Sep 20 01:16:41.779: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.25.1.45. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.25.1.45. Set the 'ServerName' directive globally to suppress this message\n[Fri Sep 20 01:16:39.473910 2019] [mpm_event:notice] [pid 1:tid 140413541469032] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Fri Sep 20 01:16:39.473983 2019] [core:notice] [pid 1:tid 140413541469032] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Sep 20 01:16:41.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 delete rc e2e-test-httpd-rc --namespace=kubectl-7281'
Sep 20 01:16:41.884: INFO: stderr: ""
Sep 20 01:16:41.884: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:16:41.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7281" for this suite.
Sep 20 01:16:59.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:17:00.142: INFO: namespace kubectl-7281 deletion completed in 18.245118626s

• [SLOW TEST:23.202 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:17:00.144: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4359
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Sep 20 01:17:00.327: INFO: Waiting up to 5m0s for pod "downward-api-33887de3-5417-4f5a-8744-47b6408edb4d" in namespace "downward-api-4359" to be "success or failure"
Sep 20 01:17:00.332: INFO: Pod "downward-api-33887de3-5417-4f5a-8744-47b6408edb4d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.466959ms
Sep 20 01:17:02.338: INFO: Pod "downward-api-33887de3-5417-4f5a-8744-47b6408edb4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011358247s
Sep 20 01:17:04.345: INFO: Pod "downward-api-33887de3-5417-4f5a-8744-47b6408edb4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017965734s
STEP: Saw pod success
Sep 20 01:17:04.345: INFO: Pod "downward-api-33887de3-5417-4f5a-8744-47b6408edb4d" satisfied condition "success or failure"
Sep 20 01:17:04.351: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod downward-api-33887de3-5417-4f5a-8744-47b6408edb4d container dapi-container: <nil>
STEP: delete the pod
Sep 20 01:17:04.396: INFO: Waiting for pod downward-api-33887de3-5417-4f5a-8744-47b6408edb4d to disappear
Sep 20 01:17:04.402: INFO: Pod downward-api-33887de3-5417-4f5a-8744-47b6408edb4d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:17:04.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4359" for this suite.
Sep 20 01:17:10.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:17:10.643: INFO: namespace downward-api-4359 deletion completed in 6.230973591s

• [SLOW TEST:10.499 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:17:10.655: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6259
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 01:17:11.425: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 01:17:13.445: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704539031, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704539031, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704539031, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704539031, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 01:17:16.478: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:17:16.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6259" for this suite.
Sep 20 01:17:22.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:17:23.095: INFO: namespace webhook-6259 deletion completed in 6.233827666s
STEP: Destroying namespace "webhook-6259-markers" for this suite.
Sep 20 01:17:29.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:17:29.323: INFO: namespace webhook-6259-markers deletion completed in 6.227412967s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.696 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:17:29.360: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1638
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-1638
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-1638
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1638
Sep 20 01:17:29.557: INFO: Found 0 stateful pods, waiting for 1
Sep 20 01:17:39.565: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep 20 01:17:39.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 01:17:40.309: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 01:17:40.309: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 01:17:40.309: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 01:17:40.315: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 20 01:17:50.323: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 20 01:17:50.323: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 01:17:50.347: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 20 01:17:50.347: INFO: ss-0  worker-wqshf-7859ffd555-r6tnq  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:29 +0000 UTC  }]
Sep 20 01:17:50.347: INFO: 
Sep 20 01:17:50.347: INFO: StatefulSet ss has not reached scale 3, at 1
Sep 20 01:17:51.353: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994700382s
Sep 20 01:17:52.361: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987952569s
Sep 20 01:17:53.368: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.980673522s
Sep 20 01:17:54.379: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.972820832s
Sep 20 01:17:55.387: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.962087598s
Sep 20 01:17:56.410: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.953859881s
Sep 20 01:17:57.418: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.930845014s
Sep 20 01:17:58.424: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.923289319s
Sep 20 01:17:59.432: INFO: Verifying statefulset ss doesn't scale past 3 for another 916.916703ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1638
Sep 20 01:18:00.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:18:01.166: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 20 01:18:01.166: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 01:18:01.166: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 20 01:18:01.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:18:01.846: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 20 01:18:01.846: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 01:18:01.846: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 20 01:18:01.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:18:02.542: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 20 01:18:02.542: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 01:18:02.542: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 20 01:18:02.549: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 01:18:02.549: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 01:18:02.549: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep 20 01:18:02.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 01:18:03.261: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 01:18:03.261: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 01:18:03.261: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 01:18:03.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 01:18:03.944: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 01:18:03.944: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 01:18:03.944: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 01:18:03.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 01:18:04.670: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 01:18:04.670: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 01:18:04.670: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 01:18:04.670: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 01:18:04.677: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Sep 20 01:18:14.690: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 20 01:18:14.690: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 20 01:18:14.690: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 20 01:18:14.707: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 20 01:18:14.707: INFO: ss-0  worker-wqshf-7859ffd555-r6tnq  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:29 +0000 UTC  }]
Sep 20 01:18:14.708: INFO: ss-1  worker-wqshf-7859ffd555-lmjf4  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  }]
Sep 20 01:18:14.708: INFO: ss-2  worker-wqshf-7859ffd555-kqpmz  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  }]
Sep 20 01:18:14.708: INFO: 
Sep 20 01:18:14.708: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 20 01:18:15.715: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 20 01:18:15.715: INFO: ss-0  worker-wqshf-7859ffd555-r6tnq  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:29 +0000 UTC  }]
Sep 20 01:18:15.715: INFO: ss-1  worker-wqshf-7859ffd555-lmjf4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  }]
Sep 20 01:18:15.715: INFO: ss-2  worker-wqshf-7859ffd555-kqpmz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  }]
Sep 20 01:18:15.715: INFO: 
Sep 20 01:18:15.715: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 20 01:18:16.724: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 20 01:18:16.724: INFO: ss-0  worker-wqshf-7859ffd555-r6tnq  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:29 +0000 UTC  }]
Sep 20 01:18:16.724: INFO: ss-1  worker-wqshf-7859ffd555-lmjf4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  }]
Sep 20 01:18:16.724: INFO: ss-2  worker-wqshf-7859ffd555-kqpmz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  }]
Sep 20 01:18:16.724: INFO: 
Sep 20 01:18:16.724: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 20 01:18:17.731: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 20 01:18:17.732: INFO: ss-0  worker-wqshf-7859ffd555-r6tnq  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:29 +0000 UTC  }]
Sep 20 01:18:17.732: INFO: ss-1  worker-wqshf-7859ffd555-lmjf4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  }]
Sep 20 01:18:17.732: INFO: ss-2  worker-wqshf-7859ffd555-kqpmz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  }]
Sep 20 01:18:17.732: INFO: 
Sep 20 01:18:17.732: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 20 01:18:18.739: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 20 01:18:18.740: INFO: ss-0  worker-wqshf-7859ffd555-r6tnq  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:29 +0000 UTC  }]
Sep 20 01:18:18.740: INFO: ss-1  worker-wqshf-7859ffd555-lmjf4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  }]
Sep 20 01:18:18.741: INFO: ss-2  worker-wqshf-7859ffd555-kqpmz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  }]
Sep 20 01:18:18.741: INFO: 
Sep 20 01:18:18.742: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 20 01:18:19.749: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 20 01:18:19.749: INFO: ss-0  worker-wqshf-7859ffd555-r6tnq  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:29 +0000 UTC  }]
Sep 20 01:18:19.750: INFO: ss-1  worker-wqshf-7859ffd555-lmjf4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  }]
Sep 20 01:18:19.750: INFO: ss-2  worker-wqshf-7859ffd555-kqpmz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  }]
Sep 20 01:18:19.750: INFO: 
Sep 20 01:18:19.750: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 20 01:18:20.757: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 20 01:18:20.757: INFO: ss-0  worker-wqshf-7859ffd555-r6tnq  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:29 +0000 UTC  }]
Sep 20 01:18:20.757: INFO: ss-1  worker-wqshf-7859ffd555-lmjf4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  }]
Sep 20 01:18:20.758: INFO: ss-2  worker-wqshf-7859ffd555-kqpmz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  }]
Sep 20 01:18:20.758: INFO: 
Sep 20 01:18:20.758: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 20 01:18:21.765: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 20 01:18:21.765: INFO: ss-0  worker-wqshf-7859ffd555-r6tnq  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:29 +0000 UTC  }]
Sep 20 01:18:21.765: INFO: ss-1  worker-wqshf-7859ffd555-lmjf4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  }]
Sep 20 01:18:21.765: INFO: ss-2  worker-wqshf-7859ffd555-kqpmz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  }]
Sep 20 01:18:21.765: INFO: 
Sep 20 01:18:21.765: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 20 01:18:22.772: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 20 01:18:22.772: INFO: ss-0  worker-wqshf-7859ffd555-r6tnq  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:29 +0000 UTC  }]
Sep 20 01:18:22.773: INFO: ss-1  worker-wqshf-7859ffd555-lmjf4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  }]
Sep 20 01:18:22.773: INFO: ss-2  worker-wqshf-7859ffd555-kqpmz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  }]
Sep 20 01:18:22.773: INFO: 
Sep 20 01:18:22.773: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 20 01:18:23.780: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 20 01:18:23.780: INFO: ss-1  worker-wqshf-7859ffd555-lmjf4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:18:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-20 01:17:50 +0000 UTC  }]
Sep 20 01:18:23.780: INFO: 
Sep 20 01:18:23.780: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1638
Sep 20 01:18:24.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:18:25.061: INFO: rc: 1
Sep 20 01:18:25.061: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  error: unable to upgrade connection: container not found ("webserver")
 [] <nil> 0xc001b17bc0 exit status 1 <nil> <nil> true [0xc0017c74e8 0xc0017c7518 0xc0017c7538] [0xc0017c74e8 0xc0017c7518 0xc0017c7538] [0xc0017c74f8 0xc0017c7528] [0x10ef310 0x10ef310] 0xc002149620 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Sep 20 01:18:35.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:18:35.181: INFO: rc: 1
Sep 20 01:18:35.181: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0037f5110 exit status 1 <nil> <nil> true [0xc0056d4360 0xc0056d4378 0xc0056d4390] [0xc0056d4360 0xc0056d4378 0xc0056d4390] [0xc0056d4370 0xc0056d4388] [0x10ef310 0x10ef310] 0xc002a66540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:18:45.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:18:45.293: INFO: rc: 1
Sep 20 01:18:45.293: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0037f5470 exit status 1 <nil> <nil> true [0xc0056d4398 0xc0056d43b0 0xc0056d43c8] [0xc0056d4398 0xc0056d43b0 0xc0056d43c8] [0xc0056d43a8 0xc0056d43c0] [0x10ef310 0x10ef310] 0xc002a668a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:18:55.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:18:55.413: INFO: rc: 1
Sep 20 01:18:55.413: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0037f5800 exit status 1 <nil> <nil> true [0xc0056d43d0 0xc0056d43e8 0xc0056d4400] [0xc0056d43d0 0xc0056d43e8 0xc0056d4400] [0xc0056d43e0 0xc0056d43f8] [0x10ef310 0x10ef310] 0xc002a66c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:19:05.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:19:05.507: INFO: rc: 1
Sep 20 01:19:05.507: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0037f5b90 exit status 1 <nil> <nil> true [0xc0056d4408 0xc0056d4420 0xc0056d4438] [0xc0056d4408 0xc0056d4420 0xc0056d4438] [0xc0056d4418 0xc0056d4430] [0x10ef310 0x10ef310] 0xc002a66fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:19:15.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:19:15.598: INFO: rc: 1
Sep 20 01:19:15.599: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001320210 exit status 1 <nil> <nil> true [0xc0017c7558 0xc0017c7588 0xc0017c75b8] [0xc0017c7558 0xc0017c7588 0xc0017c75b8] [0xc0017c7578 0xc0017c75a8] [0x10ef310 0x10ef310] 0xc002a80120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:19:25.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:19:25.690: INFO: rc: 1
Sep 20 01:19:25.690: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001b17e60 exit status 1 <nil> <nil> true [0xc003034030 0xc003034088 0xc0030340c8] [0xc003034030 0xc003034088 0xc0030340c8] [0xc003034058 0xc0030340b8] [0x10ef310 0x10ef310] 0xc002148ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:19:35.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:19:35.791: INFO: rc: 1
Sep 20 01:19:35.792: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001bb2300 exit status 1 <nil> <nil> true [0xc0000cc7b0 0xc0000cd8f0 0xc0000cdc78] [0xc0000cc7b0 0xc0000cd8f0 0xc0000cdc78] [0xc0000cd648 0xc0000cdb60] [0x10ef310 0x10ef310] 0xc0028461e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:19:45.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:19:45.895: INFO: rc: 1
Sep 20 01:19:45.895: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001bb2660 exit status 1 <nil> <nil> true [0xc0000cdd48 0xc0000cde60 0xc0024180d8] [0xc0000cdd48 0xc0000cde60 0xc0024180d8] [0xc0000cde50 0xc002418058] [0x10ef310 0x10ef310] 0xc002846540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:19:55.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:19:56.006: INFO: rc: 1
Sep 20 01:19:56.006: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0018fa210 exit status 1 <nil> <nil> true [0xc0030340e8 0xc003034160 0xc003034190] [0xc0030340e8 0xc003034160 0xc003034190] [0xc003034140 0xc003034180] [0x10ef310 0x10ef310] 0xc002149ec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:20:06.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:20:06.105: INFO: rc: 1
Sep 20 01:20:06.105: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0018fa5a0 exit status 1 <nil> <nil> true [0xc0030341d0 0xc003034278 0xc0030342b8] [0xc0030341d0 0xc003034278 0xc0030342b8] [0xc003034210 0xc003034298] [0x10ef310 0x10ef310] 0xc002e32600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:20:16.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:20:16.205: INFO: rc: 1
Sep 20 01:20:16.205: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0018fa930 exit status 1 <nil> <nil> true [0xc0030342c8 0xc003034318 0xc003034360] [0xc0030342c8 0xc003034318 0xc003034360] [0xc003034300 0xc003034348] [0x10ef310 0x10ef310] 0xc002e32cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:20:26.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:20:26.324: INFO: rc: 1
Sep 20 01:20:26.324: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0018fac90 exit status 1 <nil> <nil> true [0xc003034388 0xc003034410 0xc003034478] [0xc003034388 0xc003034410 0xc003034478] [0xc0030343f0 0xc003034458] [0x10ef310 0x10ef310] 0xc002e33380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:20:36.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:20:36.442: INFO: rc: 1
Sep 20 01:20:36.442: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0018fb020 exit status 1 <nil> <nil> true [0xc003034490 0xc0030344e0 0xc003034530] [0xc003034490 0xc0030344e0 0xc003034530] [0xc0030344c8 0xc003034510] [0x10ef310 0x10ef310] 0xc002e33860 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:20:46.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:20:46.546: INFO: rc: 1
Sep 20 01:20:46.546: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0018fb380 exit status 1 <nil> <nil> true [0xc003034550 0xc003034598 0xc0030345d8] [0xc003034550 0xc003034598 0xc0030345d8] [0xc003034588 0xc0030345b8] [0x10ef310 0x10ef310] 0xc002e33bc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:20:56.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:20:56.641: INFO: rc: 1
Sep 20 01:20:56.641: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001bb29c0 exit status 1 <nil> <nil> true [0xc002418110 0xc0024181d8 0xc002418360] [0xc002418110 0xc0024181d8 0xc002418360] [0xc002418190 0xc002418320] [0x10ef310 0x10ef310] 0xc0028468a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:21:06.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:21:06.755: INFO: rc: 1
Sep 20 01:21:06.755: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001bb2d50 exit status 1 <nil> <nil> true [0xc002418488 0xc002418848 0xc0024189a8] [0xc002418488 0xc002418848 0xc0024189a8] [0xc002418688 0xc002418970] [0x10ef310 0x10ef310] 0xc002846c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:21:16.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:21:16.861: INFO: rc: 1
Sep 20 01:21:16.862: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001bb30b0 exit status 1 <nil> <nil> true [0xc002418a58 0xc002418b88 0xc002418eb8] [0xc002418a58 0xc002418b88 0xc002418eb8] [0xc002418aa8 0xc002418d78] [0x10ef310 0x10ef310] 0xc002847020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:21:26.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:21:26.956: INFO: rc: 1
Sep 20 01:21:26.956: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001b17e30 exit status 1 <nil> <nil> true [0xc0000cd3b8 0xc0000cdaa0 0xc0000cdd48] [0xc0000cd3b8 0xc0000cdaa0 0xc0000cdd48] [0xc0000cd8f0 0xc0000cdc78] [0x10ef310 0x10ef310] 0xc002148ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:21:36.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:21:37.051: INFO: rc: 1
Sep 20 01:21:37.052: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001bb2210 exit status 1 <nil> <nil> true [0xc0000cddd8 0xc0000cdf60 0xc002418110] [0xc0000cddd8 0xc0000cdf60 0xc002418110] [0xc0000cde60 0xc0024180d8] [0x10ef310 0x10ef310] 0xc002149ec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:21:47.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:21:47.152: INFO: rc: 1
Sep 20 01:21:47.152: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001bb25a0 exit status 1 <nil> <nil> true [0xc002418128 0xc002418258 0xc002418488] [0xc002418128 0xc002418258 0xc002418488] [0xc0024181d8 0xc002418360] [0x10ef310 0x10ef310] 0xc002846240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:21:57.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:21:57.248: INFO: rc: 1
Sep 20 01:21:57.248: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0018fa330 exit status 1 <nil> <nil> true [0xc003034008 0xc003034058 0xc0030340b8] [0xc003034008 0xc003034058 0xc0030340b8] [0xc003034040 0xc0030340a8] [0x10ef310 0x10ef310] 0xc002e32540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:22:07.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:22:07.368: INFO: rc: 1
Sep 20 01:22:07.368: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0018fa6c0 exit status 1 <nil> <nil> true [0xc0030340c8 0xc003034140 0xc003034180] [0xc0030340c8 0xc003034140 0xc003034180] [0xc003034120 0xc003034170] [0x10ef310 0x10ef310] 0xc002e32c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:22:17.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:22:17.470: INFO: rc: 1
Sep 20 01:22:17.470: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001bb2930 exit status 1 <nil> <nil> true [0xc0024185b8 0xc0024188a8 0xc002418a58] [0xc0024185b8 0xc0024188a8 0xc002418a58] [0xc002418848 0xc0024189a8] [0x10ef310 0x10ef310] 0xc0028465a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
I0920 01:22:21.668330      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
Sep 20 01:22:27.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:22:27.566: INFO: rc: 1
Sep 20 01:22:27.567: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001bb2e10 exit status 1 <nil> <nil> true [0xc002418ee8 0xc002418fb8 0xc0024190c8] [0xc002418ee8 0xc002418fb8 0xc0024190c8] [0xc002418f50 0xc002419028] [0x10ef310 0x10ef310] 0xc002846ae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:22:37.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:22:37.665: INFO: rc: 1
Sep 20 01:22:37.665: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0018faa80 exit status 1 <nil> <nil> true [0xc003034190 0xc003034210 0xc003034298] [0xc003034190 0xc003034210 0xc003034298] [0xc0030341e0 0xc003034288] [0x10ef310 0x10ef310] 0xc002e332c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:22:47.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:22:47.768: INFO: rc: 1
Sep 20 01:22:47.768: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001bb31a0 exit status 1 <nil> <nil> true [0xc002419128 0xc002419290 0xc002419438] [0xc002419128 0xc002419290 0xc002419438] [0xc0024191f8 0xc0024193f0] [0x10ef310 0x10ef310] 0xc002846f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:22:57.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:22:57.885: INFO: rc: 1
Sep 20 01:22:57.885: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001bb3500 exit status 1 <nil> <nil> true [0xc002419480 0xc002419508 0xc002419630] [0xc002419480 0xc002419508 0xc002419630] [0xc0024194e8 0xc0024195b8] [0x10ef310 0x10ef310] 0xc002847380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:23:07.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:23:07.986: INFO: rc: 1
Sep 20 01:23:07.986: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0018fae10 exit status 1 <nil> <nil> true [0xc0030342b8 0xc003034300 0xc003034348] [0xc0030342b8 0xc003034300 0xc003034348] [0xc0030342d8 0xc003034328] [0x10ef310 0x10ef310] 0xc002e33800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:23:17.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:23:18.091: INFO: rc: 1
Sep 20 01:23:18.092: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0018fb1a0 exit status 1 <nil> <nil> true [0xc003034360 0xc0030343f0 0xc003034458] [0xc003034360 0xc0030343f0 0xc003034458] [0xc0030343d0 0xc003034430] [0x10ef310 0x10ef310] 0xc002e33b60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 20 01:23:28.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-1638 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:23:28.196: INFO: rc: 1
Sep 20 01:23:28.196: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Sep 20 01:23:28.196: INFO: Scaling statefulset ss to 0
Sep 20 01:23:28.212: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Sep 20 01:23:28.216: INFO: Deleting all statefulset in ns statefulset-1638
Sep 20 01:23:28.221: INFO: Scaling statefulset ss to 0
Sep 20 01:23:28.236: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 01:23:28.246: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:23:28.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1638" for this suite.
Sep 20 01:23:34.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:23:34.505: INFO: namespace statefulset-1638 deletion completed in 6.22700351s

• [SLOW TEST:365.146 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:23:34.510: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-6980
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Sep 20 01:23:40.778: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6980 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 01:23:40.778: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 01:23:41.439: INFO: Exec stderr: ""
Sep 20 01:23:41.439: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6980 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 01:23:41.439: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 01:23:42.191: INFO: Exec stderr: ""
Sep 20 01:23:42.191: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6980 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 01:23:42.192: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 01:23:42.912: INFO: Exec stderr: ""
Sep 20 01:23:42.912: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6980 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 01:23:42.912: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 01:23:43.621: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Sep 20 01:23:43.622: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6980 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 01:23:43.622: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 01:23:44.306: INFO: Exec stderr: ""
Sep 20 01:23:44.306: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6980 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 01:23:44.306: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 01:23:45.017: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Sep 20 01:23:45.017: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6980 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 01:23:45.017: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 01:23:45.669: INFO: Exec stderr: ""
Sep 20 01:23:45.669: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6980 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 01:23:45.669: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 01:23:46.468: INFO: Exec stderr: ""
Sep 20 01:23:46.468: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6980 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 01:23:46.468: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 01:23:47.188: INFO: Exec stderr: ""
Sep 20 01:23:47.188: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6980 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 01:23:47.188: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 01:23:47.873: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:23:47.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6980" for this suite.
Sep 20 01:24:33.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:24:34.183: INFO: namespace e2e-kubelet-etc-hosts-6980 deletion completed in 46.30015566s

• [SLOW TEST:59.673 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:24:34.186: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7490
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:24:38.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7490" for this suite.
Sep 20 01:24:46.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:24:46.682: INFO: namespace containers-7490 deletion completed in 8.263754629s

• [SLOW TEST:12.496 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:24:46.684: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9700
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Sep 20 01:24:46.883: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: rename a version
STEP: check the new version name is served
I0920 01:24:55.832682      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:25:04.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9700" for this suite.
Sep 20 01:25:10.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:25:10.857: INFO: namespace crd-publish-openapi-9700 deletion completed in 6.218388223s

• [SLOW TEST:24.173 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:25:10.858: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-160
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Sep 20 01:25:15.065: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-342770709 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Sep 20 01:25:20.205: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:25:20.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-160" for this suite.
Sep 20 01:25:26.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:25:26.446: INFO: namespace pods-160 deletion completed in 6.227520819s

• [SLOW TEST:15.589 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:25:26.454: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8469
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8469.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8469.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8469.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8469.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8469.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8469.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 20 01:25:31.376: INFO: DNS probes using dns-8469/dns-test-19a97f77-e240-4d41-9a7a-43e29c9b38bb succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:25:31.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8469" for this suite.
Sep 20 01:25:37.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:25:37.675: INFO: namespace dns-8469 deletion completed in 6.263145257s

• [SLOW TEST:11.222 seconds]
[sig-network] DNS
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:25:37.684: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7494
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 01:25:38.469: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 01:25:40.490: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704539538, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704539538, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704539538, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704539538, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 01:25:43.515: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:25:44.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7494" for this suite.
Sep 20 01:25:50.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:25:50.332: INFO: namespace webhook-7494 deletion completed in 6.255880481s
STEP: Destroying namespace "webhook-7494-markers" for this suite.
Sep 20 01:25:56.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:25:56.560: INFO: namespace webhook-7494-markers deletion completed in 6.227658792s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.904 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:25:56.589: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3117
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 01:25:56.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 version'
Sep 20 01:25:56.866: INFO: stderr: ""
Sep 20 01:25:56.866: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.0\", GitCommit:\"2bd9643cee5b3b3a5ecbd3af49d09018f0773c77\", GitTreeState:\"clean\", BuildDate:\"2019-09-18T14:36:53Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.0\", GitCommit:\"2bd9643cee5b3b3a5ecbd3af49d09018f0773c77\", GitTreeState:\"clean\", BuildDate:\"2019-09-18T14:27:17Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:25:56.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3117" for this suite.
Sep 20 01:26:02.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:26:03.083: INFO: namespace kubectl-3117 deletion completed in 6.211084897s

• [SLOW TEST:6.494 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:26:03.084: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4462
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 01:26:03.252: INFO: Creating ReplicaSet my-hostname-basic-1bc39273-e5ea-4272-b8ec-1b1e7be72147
Sep 20 01:26:03.268: INFO: Pod name my-hostname-basic-1bc39273-e5ea-4272-b8ec-1b1e7be72147: Found 0 pods out of 1
Sep 20 01:26:08.275: INFO: Pod name my-hostname-basic-1bc39273-e5ea-4272-b8ec-1b1e7be72147: Found 1 pods out of 1
Sep 20 01:26:08.275: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-1bc39273-e5ea-4272-b8ec-1b1e7be72147" is running
Sep 20 01:26:08.281: INFO: Pod "my-hostname-basic-1bc39273-e5ea-4272-b8ec-1b1e7be72147-xqxjq" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-20 01:26:03 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-20 01:26:05 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-20 01:26:05 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-20 01:26:03 +0000 UTC Reason: Message:}])
Sep 20 01:26:08.281: INFO: Trying to dial the pod
Sep 20 01:26:13.394: INFO: Controller my-hostname-basic-1bc39273-e5ea-4272-b8ec-1b1e7be72147: Got expected result from replica 1 [my-hostname-basic-1bc39273-e5ea-4272-b8ec-1b1e7be72147-xqxjq]: "my-hostname-basic-1bc39273-e5ea-4272-b8ec-1b1e7be72147-xqxjq", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:26:13.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4462" for this suite.
Sep 20 01:26:19.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:26:19.638: INFO: namespace replicaset-4462 deletion completed in 6.233324617s

• [SLOW TEST:16.554 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:26:19.639: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6698
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6698.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6698.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 20 01:26:24.551: INFO: DNS probes using dns-6698/dns-test-22041dd8-550f-4400-8149-7266bb659610 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:26:24.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6698" for this suite.
Sep 20 01:26:30.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:26:30.846: INFO: namespace dns-6698 deletion completed in 6.268946453s

• [SLOW TEST:11.208 seconds]
[sig-network] DNS
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:26:30.847: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1733
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 01:26:31.013: INFO: Creating deployment "test-recreate-deployment"
Sep 20 01:26:31.022: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep 20 01:26:31.039: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep 20 01:26:33.051: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep 20 01:26:33.056: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704539591, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704539591, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704539591, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704539591, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 01:26:35.065: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep 20 01:26:35.081: INFO: Updating deployment test-recreate-deployment
Sep 20 01:26:35.081: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Sep 20 01:26:35.174: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1733 /apis/apps/v1/namespaces/deployment-1733/deployments/test-recreate-deployment fbfe9cf2-4bf8-49e0-abe7-098c8f4bb469 73185 2 2019-09-20 01:26:31 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003fe25d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2019-09-20 01:26:35 +0000 UTC,LastTransitionTime:2019-09-20 01:26:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2019-09-20 01:26:35 +0000 UTC,LastTransitionTime:2019-09-20 01:26:31 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Sep 20 01:26:35.180: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-1733 /apis/apps/v1/namespaces/deployment-1733/replicasets/test-recreate-deployment-5f94c574ff 3defb712-68b1-4007-b063-5ebe111fcb05 73183 1 2019-09-20 01:26:35 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment fbfe9cf2-4bf8-49e0-abe7-098c8f4bb469 0xc003fe2a07 0xc003fe2a08}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003fe2a68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 20 01:26:35.180: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep 20 01:26:35.181: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-1733 /apis/apps/v1/namespaces/deployment-1733/replicasets/test-recreate-deployment-68fc85c7bb 7a8e7c9b-4d51-40ad-b106-3707ce1550cc 73174 2 2019-09-20 01:26:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment fbfe9cf2-4bf8-49e0-abe7-098c8f4bb469 0xc003fe2ad7 0xc003fe2ad8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003fe2b38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 20 01:26:35.188: INFO: Pod "test-recreate-deployment-5f94c574ff-bnhtf" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-bnhtf test-recreate-deployment-5f94c574ff- deployment-1733 /api/v1/namespaces/deployment-1733/pods/test-recreate-deployment-5f94c574ff-bnhtf da049712-ed50-4518-a549-6e08516719b6 73181 0 2019-09-20 01:26:35 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 3defb712-68b1-4007-b063-5ebe111fcb05 0xc003fe2fc7 0xc003fe2fc8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4g58r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4g58r,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4g58r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-r6tnq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:26:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:26:35.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1733" for this suite.
Sep 20 01:26:41.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:26:41.443: INFO: namespace deployment-1733 deletion completed in 6.242362233s

• [SLOW TEST:10.597 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:26:41.444: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3586
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 20 01:26:41.622: INFO: Waiting up to 5m0s for pod "pod-5fcbb028-2c04-4b78-a60e-0a8b1eb5b40f" in namespace "emptydir-3586" to be "success or failure"
Sep 20 01:26:41.629: INFO: Pod "pod-5fcbb028-2c04-4b78-a60e-0a8b1eb5b40f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.585738ms
Sep 20 01:26:43.636: INFO: Pod "pod-5fcbb028-2c04-4b78-a60e-0a8b1eb5b40f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013915904s
Sep 20 01:26:45.650: INFO: Pod "pod-5fcbb028-2c04-4b78-a60e-0a8b1eb5b40f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028363649s
STEP: Saw pod success
Sep 20 01:26:45.656: INFO: Pod "pod-5fcbb028-2c04-4b78-a60e-0a8b1eb5b40f" satisfied condition "success or failure"
Sep 20 01:26:45.665: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod pod-5fcbb028-2c04-4b78-a60e-0a8b1eb5b40f container test-container: <nil>
STEP: delete the pod
Sep 20 01:26:45.794: INFO: Waiting for pod pod-5fcbb028-2c04-4b78-a60e-0a8b1eb5b40f to disappear
Sep 20 01:26:45.799: INFO: Pod pod-5fcbb028-2c04-4b78-a60e-0a8b1eb5b40f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:26:45.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3586" for this suite.
Sep 20 01:26:51.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:26:52.045: INFO: namespace emptydir-3586 deletion completed in 6.237729457s

• [SLOW TEST:10.601 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:26:52.047: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7514
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:27:09.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7514" for this suite.
Sep 20 01:27:15.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:27:15.522: INFO: namespace resourcequota-7514 deletion completed in 6.213659833s

• [SLOW TEST:23.474 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:27:15.523: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9222
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 20 01:27:23.751: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 20 01:27:23.757: INFO: Pod pod-with-prestop-http-hook still exists
Sep 20 01:27:25.758: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 20 01:27:25.767: INFO: Pod pod-with-prestop-http-hook still exists
Sep 20 01:27:27.757: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 20 01:27:27.764: INFO: Pod pod-with-prestop-http-hook still exists
Sep 20 01:27:29.758: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 20 01:27:29.765: INFO: Pod pod-with-prestop-http-hook still exists
Sep 20 01:27:31.758: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 20 01:27:31.766: INFO: Pod pod-with-prestop-http-hook still exists
Sep 20 01:27:33.758: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 20 01:27:33.765: INFO: Pod pod-with-prestop-http-hook still exists
Sep 20 01:27:35.758: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 20 01:27:35.765: INFO: Pod pod-with-prestop-http-hook still exists
Sep 20 01:27:37.758: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 20 01:27:37.764: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:27:37.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9222" for this suite.
Sep 20 01:28:05.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:28:06.123: INFO: namespace container-lifecycle-hook-9222 deletion completed in 28.295260571s

• [SLOW TEST:50.600 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:28:06.133: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3807
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3807
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-3807
I0920 01:28:06.346675      16 runners.go:184] Created replication controller with name: externalname-service, namespace: services-3807, replica count: 2
I0920 01:28:06.347973      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 01:28:06.348554      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 01:28:09.398253      16 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 20 01:28:09.398: INFO: Creating new exec pod
I0920 01:28:13.426220      16 reflector.go:120] Starting reflector *v1.Endpoints (0s) from k8s.io/kubernetes/test/e2e/framework/service/jig.go:389
I0920 01:28:13.426284      16 reflector.go:158] Listing and watching *v1.Endpoints from k8s.io/kubernetes/test/e2e/framework/service/jig.go:389
Sep 20 01:28:14.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=services-3807 execpod4vhn6 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Sep 20 01:28:15.491: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 20 01:28:15.491: INFO: stdout: ""
Sep 20 01:28:15.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=services-3807 execpod4vhn6 -- /bin/sh -x -c nc -zv -t -w 2 10.240.16.61 80'
Sep 20 01:28:16.349: INFO: stderr: "+ nc -zv -t -w 2 10.240.16.61 80\nConnection to 10.240.16.61 80 port [tcp/http] succeeded!\n"
Sep 20 01:28:16.349: INFO: stdout: ""
Sep 20 01:28:16.349: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:28:16.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3807" for this suite.
Sep 20 01:28:22.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:28:22.617: INFO: namespace services-3807 deletion completed in 6.230019468s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:16.485 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:28:22.617: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8030
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 01:28:22.788: INFO: Creating deployment "webserver-deployment"
Sep 20 01:28:22.799: INFO: Waiting for observed generation 1
Sep 20 01:28:24.810: INFO: Waiting for all required pods to come up
Sep 20 01:28:24.822: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep 20 01:28:26.839: INFO: Waiting for deployment "webserver-deployment" to complete
Sep 20 01:28:26.848: INFO: Updating deployment "webserver-deployment" with a non-existent image
Sep 20 01:28:26.860: INFO: Updating deployment webserver-deployment
Sep 20 01:28:26.860: INFO: Waiting for observed generation 2
Sep 20 01:28:28.871: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep 20 01:28:28.876: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep 20 01:28:28.880: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 20 01:28:28.897: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep 20 01:28:28.897: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep 20 01:28:28.902: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 20 01:28:28.912: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Sep 20 01:28:28.912: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Sep 20 01:28:28.926: INFO: Updating deployment webserver-deployment
Sep 20 01:28:28.926: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Sep 20 01:28:28.939: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep 20 01:28:30.959: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Sep 20 01:28:30.969: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8030 /apis/apps/v1/namespaces/deployment-8030/deployments/webserver-deployment 820ff475-f540-4216-8956-1a8cda0390d4 73984 3 2019-09-20 01:28:22 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005434498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2019-09-20 01:28:28 +0000 UTC,LastTransitionTime:2019-09-20 01:28:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2019-09-20 01:28:29 +0000 UTC,LastTransitionTime:2019-09-20 01:28:22 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Sep 20 01:28:30.973: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-8030 /apis/apps/v1/namespaces/deployment-8030/replicasets/webserver-deployment-c7997dcc8 47aaf280-b5bd-45c0-b206-342498965db5 73965 3 2019-09-20 01:28:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 820ff475-f540-4216-8956-1a8cda0390d4 0xc006a26ed7 0xc006a26ed8}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006a26f48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 20 01:28:30.973: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Sep 20 01:28:30.974: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-8030 /apis/apps/v1/namespaces/deployment-8030/replicasets/webserver-deployment-595b5b9587 70ff8125-558b-4529-9443-04f12a9dc2e2 73980 3 2019-09-20 01:28:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 820ff475-f540-4216-8956-1a8cda0390d4 0xc006a26e17 0xc006a26e18}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006a26e78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Sep 20 01:28:30.985: INFO: Pod "webserver-deployment-595b5b9587-2ffnb" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2ffnb webserver-deployment-595b5b9587- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-595b5b9587-2ffnb a103a36e-1dee-4e02-b37b-8ed6f4de616c 73796 0 2019-09-20 01:28:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.4.15/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 70ff8125-558b-4529-9443-04f12a9dc2e2 0xc006a27447 0xc006a27448}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-mxxqz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.254.151,PodIP:172.25.4.15,StartTime:2019-09-20 01:28:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 01:28:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://04848b6cd017a92c1a0f417f67211241c889a3ef8e7ceab3b17b8d1176c313db,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.4.15,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.985: INFO: Pod "webserver-deployment-595b5b9587-4d7rh" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4d7rh webserver-deployment-595b5b9587- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-595b5b9587-4d7rh 64ba7390-a72c-4104-95e1-ca4b0815cf50 73807 0 2019-09-20 01:28:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.3.46/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 70ff8125-558b-4529-9443-04f12a9dc2e2 0xc006a275c0 0xc006a275c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-lmjf4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.206,PodIP:172.25.3.46,StartTime:2019-09-20 01:28:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 01:28:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://0a4a5dd203b6e9f31819527829b75aeec59096ee31d21e01073262b93f169d24,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.3.46,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.985: INFO: Pod "webserver-deployment-595b5b9587-7drp9" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-7drp9 webserver-deployment-595b5b9587- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-595b5b9587-7drp9 2f8ae4c2-5503-4bb7-a1c8-a84dec2ccaff 73803 0 2019-09-20 01:28:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.3.45/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 70ff8125-558b-4529-9443-04f12a9dc2e2 0xc006a27730 0xc006a27731}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-lmjf4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.206,PodIP:172.25.3.45,StartTime:2019-09-20 01:28:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 01:28:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://334578caa9945d41c0fa77a7fb1c9338e29bcfb5aa5d17c4a11067b9aa9f090c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.3.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.986: INFO: Pod "webserver-deployment-595b5b9587-8kgdl" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-8kgdl webserver-deployment-595b5b9587- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-595b5b9587-8kgdl ea4a2637-93a5-4855-be59-bf64e228b3ff 74045 0 2019-09-20 01:28:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.2.13/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 70ff8125-558b-4529-9443-04f12a9dc2e2 0xc006a278a0 0xc006a278a1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-ldlfc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.179,PodIP:,StartTime:2019-09-20 01:28:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.986: INFO: Pod "webserver-deployment-595b5b9587-bfkgn" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bfkgn webserver-deployment-595b5b9587- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-595b5b9587-bfkgn cd060f2e-d0cd-45a5-8b40-f23a18cf5f64 73996 0 2019-09-20 01:28:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 70ff8125-558b-4529-9443-04f12a9dc2e2 0xc006a279e7 0xc006a279e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-lmjf4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.206,PodIP:,StartTime:2019-09-20 01:28:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.986: INFO: Pod "webserver-deployment-595b5b9587-bzrnt" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bzrnt webserver-deployment-595b5b9587- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-595b5b9587-bzrnt a38f4106-8577-481b-bccb-7d9beea95ccc 73815 0 2019-09-20 01:28:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.0.62/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 70ff8125-558b-4529-9443-04f12a9dc2e2 0xc006a27b47 0xc006a27b48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-kqpmz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.176,PodIP:172.25.0.62,StartTime:2019-09-20 01:28:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 01:28:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://fdb493a99f4c25b00df059310da3eb18cadf7e76690d33cb6420839b380dd474,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.986: INFO: Pod "webserver-deployment-595b5b9587-fftdm" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fftdm webserver-deployment-595b5b9587- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-595b5b9587-fftdm 3590a548-ea49-4d81-8556-2f941993dbc4 73979 0 2019-09-20 01:28:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 70ff8125-558b-4529-9443-04f12a9dc2e2 0xc006a27cb0 0xc006a27cb1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-kqpmz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.176,PodIP:,StartTime:2019-09-20 01:28:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.987: INFO: Pod "webserver-deployment-595b5b9587-fgxdk" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fgxdk webserver-deployment-595b5b9587- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-595b5b9587-fgxdk 72bbe5ec-c1c7-4c39-8157-9bee9cbda8d6 73799 0 2019-09-20 01:28:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.4.14/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 70ff8125-558b-4529-9443-04f12a9dc2e2 0xc006a27e07 0xc006a27e08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-mxxqz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.254.151,PodIP:172.25.4.14,StartTime:2019-09-20 01:28:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 01:28:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://1ed8ead458a94645ecad184828ec02b20f87e5353c18ee4f139b44032d1fb55a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.4.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.987: INFO: Pod "webserver-deployment-595b5b9587-grcg7" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-grcg7 webserver-deployment-595b5b9587- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-595b5b9587-grcg7 0fe9002b-c793-45ef-b10b-e62d8b69d26c 74029 0 2019-09-20 01:28:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.3.48/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 70ff8125-558b-4529-9443-04f12a9dc2e2 0xc006a27f80 0xc006a27f81}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-lmjf4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.206,PodIP:,StartTime:2019-09-20 01:28:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.987: INFO: Pod "webserver-deployment-595b5b9587-jbcxs" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jbcxs webserver-deployment-595b5b9587- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-595b5b9587-jbcxs 83bc6779-00ed-4416-8a58-c8b5d48c332b 74027 0 2019-09-20 01:28:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.2.12/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 70ff8125-558b-4529-9443-04f12a9dc2e2 0xc00523e0d7 0xc00523e0d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-ldlfc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.179,PodIP:,StartTime:2019-09-20 01:28:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.988: INFO: Pod "webserver-deployment-595b5b9587-lftlb" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lftlb webserver-deployment-595b5b9587- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-595b5b9587-lftlb f81a6d56-921e-4808-9b9a-9eb0d0b0e536 74035 0 2019-09-20 01:28:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.1.59/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 70ff8125-558b-4529-9443-04f12a9dc2e2 0xc00523e237 0xc00523e238}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-r6tnq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.207,PodIP:,StartTime:2019-09-20 01:28:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.988: INFO: Pod "webserver-deployment-595b5b9587-lnmkk" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lnmkk webserver-deployment-595b5b9587- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-595b5b9587-lnmkk ac2c7893-812a-4fa6-8b11-52a25e76977a 73822 0 2019-09-20 01:28:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.1.56/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 70ff8125-558b-4529-9443-04f12a9dc2e2 0xc00523e397 0xc00523e398}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-r6tnq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.207,PodIP:172.25.1.56,StartTime:2019-09-20 01:28:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 01:28:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f394e4c67d0121fb4e041fca7d41da75972cf024547d0b8015e8e620c8af9d25,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.988: INFO: Pod "webserver-deployment-595b5b9587-nt6rv" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-nt6rv webserver-deployment-595b5b9587- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-595b5b9587-nt6rv 3cdb4a77-6fad-4212-91bb-c3b420678fe8 73789 0 2019-09-20 01:28:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.2.10/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 70ff8125-558b-4529-9443-04f12a9dc2e2 0xc00523e510 0xc00523e511}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-ldlfc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.179,PodIP:172.25.2.10,StartTime:2019-09-20 01:28:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 01:28:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://6dd479a15e003e51d539ab6c846e2d60f956f0349cb51fa20393ab77bc4cfb84,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.988: INFO: Pod "webserver-deployment-595b5b9587-r4bb5" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-r4bb5 webserver-deployment-595b5b9587- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-595b5b9587-r4bb5 700f2292-9489-4f74-a1f3-6087564b81c5 73998 0 2019-09-20 01:28:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 70ff8125-558b-4529-9443-04f12a9dc2e2 0xc00523e670 0xc00523e671}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-r6tnq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.207,PodIP:,StartTime:2019-09-20 01:28:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.989: INFO: Pod "webserver-deployment-595b5b9587-rxhch" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rxhch webserver-deployment-595b5b9587- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-595b5b9587-rxhch 7a727c5b-57c4-4f35-8b06-b8fa99454c6c 73948 0 2019-09-20 01:28:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 70ff8125-558b-4529-9443-04f12a9dc2e2 0xc00523e7b7 0xc00523e7b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-ldlfc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.179,PodIP:,StartTime:2019-09-20 01:28:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.989: INFO: Pod "webserver-deployment-595b5b9587-sbtn8" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-sbtn8 webserver-deployment-595b5b9587- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-595b5b9587-sbtn8 df8ea948-3063-4158-9c2c-326a2f5ac062 73812 0 2019-09-20 01:28:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.0.61/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 70ff8125-558b-4529-9443-04f12a9dc2e2 0xc00523e937 0xc00523e938}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-kqpmz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.176,PodIP:172.25.0.61,StartTime:2019-09-20 01:28:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 01:28:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f4b957dfad6e0221596d209d5955722ae917fb4644459b6f1cac41b8122075a3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.989: INFO: Pod "webserver-deployment-595b5b9587-tbbq2" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-tbbq2 webserver-deployment-595b5b9587- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-595b5b9587-tbbq2 df865402-0b17-42a1-8758-20f6097d8bd2 73946 0 2019-09-20 01:28:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 70ff8125-558b-4529-9443-04f12a9dc2e2 0xc00523eaa0 0xc00523eaa1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-r6tnq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.207,PodIP:,StartTime:2019-09-20 01:28:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.989: INFO: Pod "webserver-deployment-595b5b9587-wnbws" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-wnbws webserver-deployment-595b5b9587- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-595b5b9587-wnbws 41c85d7a-f927-4e0e-abd4-743841323ee5 74038 0 2019-09-20 01:28:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.4.18/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 70ff8125-558b-4529-9443-04f12a9dc2e2 0xc00523ebf7 0xc00523ebf8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-mxxqz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.254.151,PodIP:,StartTime:2019-09-20 01:28:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.989: INFO: Pod "webserver-deployment-595b5b9587-xxgw9" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xxgw9 webserver-deployment-595b5b9587- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-595b5b9587-xxgw9 d4160fa2-af9a-4460-9b45-db8d530b8308 74051 0 2019-09-20 01:28:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.4.19/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 70ff8125-558b-4529-9443-04f12a9dc2e2 0xc00523ed57 0xc00523ed58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-mxxqz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.254.151,PodIP:,StartTime:2019-09-20 01:28:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.989: INFO: Pod "webserver-deployment-595b5b9587-zx46c" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-zx46c webserver-deployment-595b5b9587- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-595b5b9587-zx46c 077d4b58-3c8e-4d34-8fc2-1fba6488c1ce 74052 0 2019-09-20 01:28:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.0.65/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 70ff8125-558b-4529-9443-04f12a9dc2e2 0xc00523eeb7 0xc00523eeb8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-kqpmz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.176,PodIP:,StartTime:2019-09-20 01:28:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.990: INFO: Pod "webserver-deployment-c7997dcc8-222z6" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-222z6 webserver-deployment-c7997dcc8- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-c7997dcc8-222z6 2aba8219-d55c-4bf7-9819-51eb01adeb2e 74040 0 2019-09-20 01:28:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.3.49/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47aaf280-b5bd-45c0-b206-342498965db5 0xc00523f017 0xc00523f018}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-lmjf4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.206,PodIP:,StartTime:2019-09-20 01:28:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.990: INFO: Pod "webserver-deployment-c7997dcc8-4fxjv" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-4fxjv webserver-deployment-c7997dcc8- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-c7997dcc8-4fxjv 87b2efd6-3e44-4752-a7ef-5de4052a94d0 73891 0 2019-09-20 01:28:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.4.16/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47aaf280-b5bd-45c0-b206-342498965db5 0xc00523f197 0xc00523f198}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-mxxqz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.254.151,PodIP:,StartTime:2019-09-20 01:28:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.990: INFO: Pod "webserver-deployment-c7997dcc8-75554" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-75554 webserver-deployment-c7997dcc8- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-c7997dcc8-75554 d880d5e5-02a7-4b32-abc9-c918202ff3a4 73887 0 2019-09-20 01:28:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.2.11/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47aaf280-b5bd-45c0-b206-342498965db5 0xc00523f317 0xc00523f318}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-ldlfc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.179,PodIP:,StartTime:2019-09-20 01:28:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.990: INFO: Pod "webserver-deployment-c7997dcc8-cqxgs" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-cqxgs webserver-deployment-c7997dcc8- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-c7997dcc8-cqxgs dbd11a22-c7df-4d52-afa1-9d1e06884022 74039 0 2019-09-20 01:28:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.0.64/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47aaf280-b5bd-45c0-b206-342498965db5 0xc00523f497 0xc00523f498}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-kqpmz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.176,PodIP:,StartTime:2019-09-20 01:28:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.991: INFO: Pod "webserver-deployment-c7997dcc8-fhkdk" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-fhkdk webserver-deployment-c7997dcc8- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-c7997dcc8-fhkdk c1e054fe-dca8-48be-a9e2-454dfb61015c 73995 0 2019-09-20 01:28:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47aaf280-b5bd-45c0-b206-342498965db5 0xc00523f607 0xc00523f608}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-kqpmz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.176,PodIP:,StartTime:2019-09-20 01:28:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.991: INFO: Pod "webserver-deployment-c7997dcc8-hlplh" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-hlplh webserver-deployment-c7997dcc8- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-c7997dcc8-hlplh 58c9ce11-8592-4928-aa1a-2a08f1301fc4 74047 0 2019-09-20 01:28:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.3.47/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47aaf280-b5bd-45c0-b206-342498965db5 0xc00523f787 0xc00523f788}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-lmjf4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.206,PodIP:172.25.3.47,StartTime:2019-09-20 01:28:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login',},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.3.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.991: INFO: Pod "webserver-deployment-c7997dcc8-jmhxn" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-jmhxn webserver-deployment-c7997dcc8- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-c7997dcc8-jmhxn eb59bdd3-2048-4fd0-85c4-25ef74c31f3d 73997 0 2019-09-20 01:28:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47aaf280-b5bd-45c0-b206-342498965db5 0xc00523f920 0xc00523f921}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-r6tnq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.207,PodIP:,StartTime:2019-09-20 01:28:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.991: INFO: Pod "webserver-deployment-c7997dcc8-l59lg" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-l59lg webserver-deployment-c7997dcc8- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-c7997dcc8-l59lg 4945002d-ec8d-4829-8d34-89f2c82c4a92 73981 0 2019-09-20 01:28:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47aaf280-b5bd-45c0-b206-342498965db5 0xc00523fa87 0xc00523fa88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-ldlfc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.179,PodIP:,StartTime:2019-09-20 01:28:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.991: INFO: Pod "webserver-deployment-c7997dcc8-ngqp6" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-ngqp6 webserver-deployment-c7997dcc8- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-c7997dcc8-ngqp6 64efa924-d28a-4d9d-8f41-54582eaf3aa1 74028 0 2019-09-20 01:28:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.4.17/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47aaf280-b5bd-45c0-b206-342498965db5 0xc00523fc17 0xc00523fc18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-mxxqz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.254.151,PodIP:,StartTime:2019-09-20 01:28:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.992: INFO: Pod "webserver-deployment-c7997dcc8-qpbpn" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qpbpn webserver-deployment-c7997dcc8- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-c7997dcc8-qpbpn 81a826ad-60a9-4a33-8eac-ac4c5ac9c21f 73894 0 2019-09-20 01:28:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.0.63/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47aaf280-b5bd-45c0-b206-342498965db5 0xc00523fda7 0xc00523fda8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-kqpmz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.176,PodIP:,StartTime:2019-09-20 01:28:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.992: INFO: Pod "webserver-deployment-c7997dcc8-xqfrf" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-xqfrf webserver-deployment-c7997dcc8- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-c7997dcc8-xqfrf ceb1025b-a321-421e-ae53-08d70b7d9b32 73991 0 2019-09-20 01:28:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47aaf280-b5bd-45c0-b206-342498965db5 0xc00523ff17 0xc00523ff18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-r6tnq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.207,PodIP:,StartTime:2019-09-20 01:28:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.992: INFO: Pod "webserver-deployment-c7997dcc8-z8s7d" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-z8s7d webserver-deployment-c7997dcc8- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-c7997dcc8-z8s7d f1a5d130-29e3-448c-923b-0840c72e4a8b 73893 0 2019-09-20 01:28:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.1.58/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47aaf280-b5bd-45c0-b206-342498965db5 0xc0036a8097 0xc0036a8098}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-r6tnq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.207,PodIP:,StartTime:2019-09-20 01:28:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 01:28:30.992: INFO: Pod "webserver-deployment-c7997dcc8-zpf5x" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-zpf5x webserver-deployment-c7997dcc8- deployment-8030 /api/v1/namespaces/deployment-8030/pods/webserver-deployment-c7997dcc8-zpf5x 2c9c3379-a0f5-4a49-b447-2b0fa31e5d17 74053 0 2019-09-20 01:28:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.4.20/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47aaf280-b5bd-45c0-b206-342498965db5 0xc0036a8217 0xc0036a8218}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-blr5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-blr5n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-blr5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-mxxqz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 01:28:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.254.151,PodIP:,StartTime:2019-09-20 01:28:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:28:30.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8030" for this suite.
Sep 20 01:28:39.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:28:39.242: INFO: namespace deployment-8030 deletion completed in 8.242846485s

• [SLOW TEST:16.625 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:28:39.243: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2697
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Sep 20 01:28:39.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 cluster-info'
Sep 20 01:28:39.587: INFO: stderr: ""
Sep 20 01:28:39.587: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.240.16.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.240.16.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:28:39.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2697" for this suite.
Sep 20 01:28:45.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:28:45.803: INFO: namespace kubectl-2697 deletion completed in 6.208958872s

• [SLOW TEST:6.561 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:28:45.808: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6390
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0920 01:28:46.559101      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 20 01:28:46.559: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:28:46.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6390" for this suite.
Sep 20 01:28:52.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:28:52.789: INFO: namespace gc-6390 deletion completed in 6.221043811s

• [SLOW TEST:6.982 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:28:52.792: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1960
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Sep 20 01:28:52.960: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:29:01.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1960" for this suite.
I0920 01:29:27.665239      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
Sep 20 01:29:29.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:29:29.391: INFO: namespace init-container-1960 deletion completed in 28.260306778s

• [SLOW TEST:36.600 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:29:29.392: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8528
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-8528/configmap-test-19bdbc3b-3791-4d39-9046-e29dda525a93
STEP: Creating a pod to test consume configMaps
Sep 20 01:29:29.577: INFO: Waiting up to 5m0s for pod "pod-configmaps-7df5fecb-f102-41c0-860c-a078b781d4a2" in namespace "configmap-8528" to be "success or failure"
Sep 20 01:29:29.584: INFO: Pod "pod-configmaps-7df5fecb-f102-41c0-860c-a078b781d4a2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.983712ms
Sep 20 01:29:31.589: INFO: Pod "pod-configmaps-7df5fecb-f102-41c0-860c-a078b781d4a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011727093s
Sep 20 01:29:33.595: INFO: Pod "pod-configmaps-7df5fecb-f102-41c0-860c-a078b781d4a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017914449s
STEP: Saw pod success
Sep 20 01:29:33.596: INFO: Pod "pod-configmaps-7df5fecb-f102-41c0-860c-a078b781d4a2" satisfied condition "success or failure"
Sep 20 01:29:33.600: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod pod-configmaps-7df5fecb-f102-41c0-860c-a078b781d4a2 container env-test: <nil>
STEP: delete the pod
Sep 20 01:29:33.687: INFO: Waiting for pod pod-configmaps-7df5fecb-f102-41c0-860c-a078b781d4a2 to disappear
Sep 20 01:29:33.693: INFO: Pod pod-configmaps-7df5fecb-f102-41c0-860c-a078b781d4a2 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:29:33.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8528" for this suite.
Sep 20 01:29:39.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:29:39.923: INFO: namespace configmap-8528 deletion completed in 6.222440789s

• [SLOW TEST:10.531 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:29:39.931: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5743
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Sep 20 01:29:40.100: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-342770709 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:29:40.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5743" for this suite.
Sep 20 01:29:46.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:29:46.464: INFO: namespace kubectl-5743 deletion completed in 6.276510695s

• [SLOW TEST:6.534 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:29:46.467: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5596
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 20 01:29:54.746: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 20 01:29:54.751: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 20 01:29:56.752: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 20 01:29:56.757: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 20 01:29:58.752: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 20 01:29:58.759: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 20 01:30:00.752: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 20 01:30:00.758: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 20 01:30:02.752: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 20 01:30:02.759: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 20 01:30:04.752: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 20 01:30:04.758: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 20 01:30:06.752: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 20 01:30:06.760: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:30:06.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5596" for this suite.
Sep 20 01:30:18.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:30:19.064: INFO: namespace container-lifecycle-hook-5596 deletion completed in 12.243112504s

• [SLOW TEST:32.598 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:30:19.067: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8442
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 20 01:30:22.284: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:30:22.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8442" for this suite.
Sep 20 01:30:28.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:30:28.590: INFO: namespace container-runtime-8442 deletion completed in 6.274270056s

• [SLOW TEST:9.524 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:30:28.599: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4863
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4863
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Sep 20 01:30:28.792: INFO: Found 0 stateful pods, waiting for 3
Sep 20 01:30:38.801: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 01:30:38.801: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 01:30:38.801: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 01:30:38.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-4863 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 01:30:39.610: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 01:30:39.610: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 01:30:39.610: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Sep 20 01:30:49.672: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep 20 01:30:59.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-4863 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:31:00.451: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 20 01:31:00.451: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 01:31:00.451: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 20 01:31:10.483: INFO: Waiting for StatefulSet statefulset-4863/ss2 to complete update
Sep 20 01:31:10.484: INFO: Waiting for Pod statefulset-4863/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 20 01:31:10.484: INFO: Waiting for Pod statefulset-4863/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 20 01:31:20.499: INFO: Waiting for StatefulSet statefulset-4863/ss2 to complete update
Sep 20 01:31:20.500: INFO: Waiting for Pod statefulset-4863/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 20 01:31:20.500: INFO: Waiting for Pod statefulset-4863/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 20 01:31:30.495: INFO: Waiting for StatefulSet statefulset-4863/ss2 to complete update
Sep 20 01:31:30.496: INFO: Waiting for Pod statefulset-4863/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 20 01:31:40.498: INFO: Waiting for StatefulSet statefulset-4863/ss2 to complete update
Sep 20 01:31:40.498: INFO: Waiting for Pod statefulset-4863/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 20 01:31:50.496: INFO: Waiting for StatefulSet statefulset-4863/ss2 to complete update
Sep 20 01:31:50.497: INFO: Waiting for Pod statefulset-4863/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 20 01:32:00.497: INFO: Waiting for StatefulSet statefulset-4863/ss2 to complete update
Sep 20 01:32:00.497: INFO: Waiting for Pod statefulset-4863/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 20 01:32:10.501: INFO: Waiting for StatefulSet statefulset-4863/ss2 to complete update
STEP: Rolling back to a previous revision
Sep 20 01:32:20.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-4863 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 01:32:21.291: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 01:32:21.291: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 01:32:21.291: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 01:32:31.342: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep 20 01:32:41.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-4863 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:32:42.124: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 20 01:32:42.125: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 01:32:42.125: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 20 01:32:52.169: INFO: Waiting for StatefulSet statefulset-4863/ss2 to complete update
Sep 20 01:32:52.169: INFO: Waiting for Pod statefulset-4863/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Sep 20 01:32:52.169: INFO: Waiting for Pod statefulset-4863/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Sep 20 01:32:52.169: INFO: Waiting for Pod statefulset-4863/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Sep 20 01:33:02.188: INFO: Waiting for StatefulSet statefulset-4863/ss2 to complete update
Sep 20 01:33:02.188: INFO: Waiting for Pod statefulset-4863/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Sep 20 01:33:02.189: INFO: Waiting for Pod statefulset-4863/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Sep 20 01:33:12.185: INFO: Deleting all statefulset in ns statefulset-4863
Sep 20 01:33:12.190: INFO: Scaling statefulset ss2 to 0
Sep 20 01:33:32.236: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 01:33:32.241: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:33:32.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4863" for this suite.
Sep 20 01:33:38.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:33:38.478: INFO: namespace statefulset-4863 deletion completed in 6.208556405s

• [SLOW TEST:189.879 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:33:38.480: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6159
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-9e90e6de-2b0c-41cd-b514-cbd6013f2150
STEP: Creating a pod to test consume configMaps
Sep 20 01:33:38.663: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b78ffe06-8f2e-4471-aadb-308370b994e6" in namespace "projected-6159" to be "success or failure"
Sep 20 01:33:38.669: INFO: Pod "pod-projected-configmaps-b78ffe06-8f2e-4471-aadb-308370b994e6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.55916ms
Sep 20 01:33:40.676: INFO: Pod "pod-projected-configmaps-b78ffe06-8f2e-4471-aadb-308370b994e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013136561s
Sep 20 01:33:42.684: INFO: Pod "pod-projected-configmaps-b78ffe06-8f2e-4471-aadb-308370b994e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020906294s
STEP: Saw pod success
Sep 20 01:33:42.684: INFO: Pod "pod-projected-configmaps-b78ffe06-8f2e-4471-aadb-308370b994e6" satisfied condition "success or failure"
Sep 20 01:33:42.690: INFO: Trying to get logs from node worker-wqshf-7859ffd555-lmjf4 pod pod-projected-configmaps-b78ffe06-8f2e-4471-aadb-308370b994e6 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 01:33:42.735: INFO: Waiting for pod pod-projected-configmaps-b78ffe06-8f2e-4471-aadb-308370b994e6 to disappear
Sep 20 01:33:42.740: INFO: Pod pod-projected-configmaps-b78ffe06-8f2e-4471-aadb-308370b994e6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:33:42.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6159" for this suite.
Sep 20 01:33:48.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:33:48.984: INFO: namespace projected-6159 deletion completed in 6.237937689s

• [SLOW TEST:10.504 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:33:48.986: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6112
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Sep 20 01:33:49.155: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 20 01:33:49.183: INFO: Waiting for terminating namespaces to be deleted...
Sep 20 01:33:49.188: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-kqpmz before test
Sep 20 01:33:49.297: INFO: kube-proxy-xmcqx from kube-system started at 2019-09-19 20:33:51 +0000 UTC (1 container statuses recorded)
Sep 20 01:33:49.297: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 01:33:49.297: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-2ks25 from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 01:33:49.297: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 01:33:49.297: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 01:33:49.297: INFO: canal-pwz75 from kube-system started at 2019-09-19 20:33:51 +0000 UTC (2 container statuses recorded)
Sep 20 01:33:49.297: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 01:33:49.297: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 01:33:49.297: INFO: node-exporter-rtkgz from kube-system started at 2019-09-19 20:33:51 +0000 UTC (2 container statuses recorded)
Sep 20 01:33:49.297: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 01:33:49.297: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 01:33:49.297: INFO: node-local-dns-tppj5 from kube-system started at 2019-09-20 00:59:43 +0000 UTC (1 container statuses recorded)
Sep 20 01:33:49.297: INFO: 	Container node-cache ready: true, restart count 0
Sep 20 01:33:49.297: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-ldlfc before test
Sep 20 01:33:49.357: INFO: node-local-dns-rj6pt from kube-system started at 2019-09-19 21:38:47 +0000 UTC (1 container statuses recorded)
Sep 20 01:33:49.357: INFO: 	Container node-cache ready: true, restart count 0
Sep 20 01:33:49.358: INFO: node-exporter-47p98 from kube-system started at 2019-09-19 20:33:57 +0000 UTC (2 container statuses recorded)
Sep 20 01:33:49.358: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 01:33:49.358: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 01:33:49.358: INFO: canal-hf9f7 from kube-system started at 2019-09-19 20:33:57 +0000 UTC (2 container statuses recorded)
Sep 20 01:33:49.358: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 01:33:49.358: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 01:33:49.358: INFO: sonobuoy-e2e-job-8f5b7ab9f6954bcb from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 01:33:49.358: INFO: 	Container e2e ready: true, restart count 0
Sep 20 01:33:49.358: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 01:33:49.358: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-djczp from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 01:33:49.358: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 01:33:49.358: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 01:33:49.358: INFO: kube-proxy-zvhzd from kube-system started at 2019-09-19 20:33:57 +0000 UTC (1 container statuses recorded)
Sep 20 01:33:49.358: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 01:33:49.358: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-lmjf4 before test
Sep 20 01:33:49.435: INFO: kube-proxy-mhstl from kube-system started at 2019-09-19 20:34:06 +0000 UTC (1 container statuses recorded)
Sep 20 01:33:49.435: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 01:33:49.435: INFO: node-local-dns-bx7jl from kube-system started at 2019-09-19 20:34:26 +0000 UTC (1 container statuses recorded)
Sep 20 01:33:49.435: INFO: 	Container node-cache ready: true, restart count 0
Sep 20 01:33:49.435: INFO: coredns-57f944bd9f-rp6nc from kube-system started at 2019-09-19 21:35:33 +0000 UTC (1 container statuses recorded)
Sep 20 01:33:49.435: INFO: 	Container coredns ready: true, restart count 0
Sep 20 01:33:49.435: INFO: kubernetes-dashboard-7d5fb85f7f-jqgq6 from kube-system started at 2019-09-20 00:59:15 +0000 UTC (1 container statuses recorded)
Sep 20 01:33:49.435: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Sep 20 01:33:49.435: INFO: canal-s8rlz from kube-system started at 2019-09-19 20:34:06 +0000 UTC (2 container statuses recorded)
Sep 20 01:33:49.435: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 01:33:49.435: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 01:33:49.435: INFO: node-exporter-65hp9 from kube-system started at 2019-09-19 20:34:06 +0000 UTC (2 container statuses recorded)
Sep 20 01:33:49.435: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 01:33:49.435: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 01:33:49.435: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-n9zxp from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 01:33:49.435: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 01:33:49.435: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 01:33:49.435: INFO: openvpn-client-84ccd8596d-s64vh from kube-system started at 2019-09-20 00:44:53 +0000 UTC (2 container statuses recorded)
Sep 20 01:33:49.436: INFO: 	Container dnat-controller ready: true, restart count 0
Sep 20 01:33:49.436: INFO: 	Container openvpn-client ready: true, restart count 0
Sep 20 01:33:49.436: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-mxxqz before test
Sep 20 01:33:49.520: INFO: canal-kdmx8 from kube-system started at 2019-09-19 20:34:07 +0000 UTC (2 container statuses recorded)
Sep 20 01:33:49.520: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 01:33:49.520: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 01:33:49.521: INFO: node-exporter-xq8tf from kube-system started at 2019-09-19 20:34:07 +0000 UTC (2 container statuses recorded)
Sep 20 01:33:49.521: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 01:33:49.521: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 01:33:49.521: INFO: node-local-dns-bxghq from kube-system started at 2019-09-19 20:34:27 +0000 UTC (1 container statuses recorded)
Sep 20 01:33:49.521: INFO: 	Container node-cache ready: true, restart count 0
Sep 20 01:33:49.522: INFO: sonobuoy from sonobuoy started at 2019-09-19 20:40:21 +0000 UTC (1 container statuses recorded)
Sep 20 01:33:49.522: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 20 01:33:49.523: INFO: kube-proxy-dtrjq from kube-system started at 2019-09-19 20:34:07 +0000 UTC (1 container statuses recorded)
Sep 20 01:33:49.523: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 01:33:49.523: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-7brnz from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 01:33:49.523: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 01:33:49.523: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 01:33:49.524: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-r6tnq before test
Sep 20 01:33:49.599: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-bph2c from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 01:33:49.599: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 01:33:49.599: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 01:33:49.599: INFO: node-exporter-ld8fb from kube-system started at 2019-09-19 20:33:57 +0000 UTC (2 container statuses recorded)
Sep 20 01:33:49.599: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 01:33:49.599: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 01:33:49.599: INFO: node-local-dns-2krxh from kube-system started at 2019-09-20 00:44:53 +0000 UTC (1 container statuses recorded)
Sep 20 01:33:49.599: INFO: 	Container node-cache ready: true, restart count 0
Sep 20 01:33:49.599: INFO: kube-proxy-p4g4f from kube-system started at 2019-09-19 20:33:57 +0000 UTC (1 container statuses recorded)
Sep 20 01:33:49.599: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 01:33:49.600: INFO: coredns-57f944bd9f-6qlrr from kube-system started at 2019-09-20 00:59:15 +0000 UTC (1 container statuses recorded)
Sep 20 01:33:49.600: INFO: 	Container coredns ready: true, restart count 0
Sep 20 01:33:49.600: INFO: canal-8lwhv from kube-system started at 2019-09-19 20:33:57 +0000 UTC (2 container statuses recorded)
Sep 20 01:33:49.600: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 01:33:49.600: INFO: 	Container kube-flannel ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
I0920 01:33:49.614296      16 reflector.go:120] Starting reflector *v1.Event (0s) from k8s.io/kubernetes/test/e2e/common/events.go:136
I0920 01:33:49.614389      16 reflector.go:158] Listing and watching *v1.Event from k8s.io/kubernetes/test/e2e/common/events.go:136
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15c6018e058697c8], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:33:50.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6112" for this suite.
Sep 20 01:33:56.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:33:56.872: INFO: namespace sched-pred-6112 deletion completed in 6.226626606s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:7.886 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:33:56.873: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
I0920 01:33:56.872273      16 request.go:706] Error in request: resource name may not be empty
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2799
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-ab71e4df-e781-45ef-b1fd-1826a1d1d5c3 in namespace container-probe-2799
Sep 20 01:34:01.077: INFO: Started pod liveness-ab71e4df-e781-45ef-b1fd-1826a1d1d5c3 in namespace container-probe-2799
STEP: checking the pod's current state and verifying that restartCount is present
Sep 20 01:34:01.083: INFO: Initial restart count of pod liveness-ab71e4df-e781-45ef-b1fd-1826a1d1d5c3 is 0
I0920 01:34:08.844069      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
Sep 20 01:34:17.155: INFO: Restart count of pod container-probe-2799/liveness-ab71e4df-e781-45ef-b1fd-1826a1d1d5c3 is now 1 (16.071927323s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:34:17.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2799" for this suite.
Sep 20 01:34:23.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:34:23.420: INFO: namespace container-probe-2799 deletion completed in 6.237840201s

• [SLOW TEST:26.548 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:34:23.437: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2626
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:34:23.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2626" for this suite.
Sep 20 01:34:35.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:34:35.830: INFO: namespace pods-2626 deletion completed in 12.201113204s

• [SLOW TEST:12.393 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:34:35.837: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3858
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
I0920 01:34:36.008426      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/client-go/tools/watch/informerwatcher.go:146
I0920 01:34:36.008476      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/client-go/tools/watch/informerwatcher.go:146
Sep 20 01:34:36.013: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Sep 20 01:34:45.204: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:34:45.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3858" for this suite.
Sep 20 01:34:51.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:34:51.499: INFO: namespace pods-3858 deletion completed in 6.279021284s

• [SLOW TEST:15.662 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:34:51.500: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4442
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0920 01:35:31.746303      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 20 01:35:31.746: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:35:31.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4442" for this suite.
Sep 20 01:35:37.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:35:37.959: INFO: namespace gc-4442 deletion completed in 6.205856749s

• [SLOW TEST:46.459 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:35:37.959: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2187
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Sep 20 01:35:38.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 create -f - --namespace=kubectl-2187'
Sep 20 01:35:38.381: INFO: stderr: ""
Sep 20 01:35:38.381: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 20 01:35:38.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2187'
Sep 20 01:35:38.484: INFO: stderr: ""
Sep 20 01:35:38.484: INFO: stdout: "update-demo-nautilus-kv5hb update-demo-nautilus-wqvjv "
Sep 20 01:35:38.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-nautilus-kv5hb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Sep 20 01:35:38.572: INFO: stderr: ""
Sep 20 01:35:38.572: INFO: stdout: ""
Sep 20 01:35:38.572: INFO: update-demo-nautilus-kv5hb is created but not running
Sep 20 01:35:43.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2187'
Sep 20 01:35:43.677: INFO: stderr: ""
Sep 20 01:35:43.677: INFO: stdout: "update-demo-nautilus-kv5hb update-demo-nautilus-wqvjv "
Sep 20 01:35:43.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-nautilus-kv5hb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Sep 20 01:35:43.755: INFO: stderr: ""
Sep 20 01:35:43.755: INFO: stdout: "true"
Sep 20 01:35:43.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-nautilus-kv5hb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Sep 20 01:35:43.842: INFO: stderr: ""
Sep 20 01:35:43.842: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 20 01:35:43.842: INFO: validating pod update-demo-nautilus-kv5hb
Sep 20 01:35:43.940: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 01:35:43.940: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 01:35:43.940: INFO: update-demo-nautilus-kv5hb is verified up and running
Sep 20 01:35:43.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-nautilus-wqvjv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Sep 20 01:35:44.036: INFO: stderr: ""
Sep 20 01:35:44.036: INFO: stdout: "true"
Sep 20 01:35:44.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-nautilus-wqvjv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Sep 20 01:35:44.123: INFO: stderr: ""
Sep 20 01:35:44.123: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 20 01:35:44.123: INFO: validating pod update-demo-nautilus-wqvjv
Sep 20 01:35:44.222: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 01:35:44.222: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 01:35:44.222: INFO: update-demo-nautilus-wqvjv is verified up and running
STEP: scaling down the replication controller
Sep 20 01:35:44.229: INFO: scanned /root for discovery docs: <nil>
Sep 20 01:35:44.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-2187'
Sep 20 01:35:45.373: INFO: stderr: ""
Sep 20 01:35:45.373: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 20 01:35:45.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2187'
Sep 20 01:35:45.484: INFO: stderr: ""
Sep 20 01:35:45.484: INFO: stdout: "update-demo-nautilus-kv5hb update-demo-nautilus-wqvjv "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep 20 01:35:50.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2187'
Sep 20 01:35:50.593: INFO: stderr: ""
Sep 20 01:35:50.593: INFO: stdout: "update-demo-nautilus-kv5hb "
Sep 20 01:35:50.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-nautilus-kv5hb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Sep 20 01:35:50.675: INFO: stderr: ""
Sep 20 01:35:50.675: INFO: stdout: "true"
Sep 20 01:35:50.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-nautilus-kv5hb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Sep 20 01:35:50.758: INFO: stderr: ""
Sep 20 01:35:50.758: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 20 01:35:50.758: INFO: validating pod update-demo-nautilus-kv5hb
Sep 20 01:35:50.771: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 01:35:50.771: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 01:35:50.772: INFO: update-demo-nautilus-kv5hb is verified up and running
STEP: scaling up the replication controller
Sep 20 01:35:50.774: INFO: scanned /root for discovery docs: <nil>
Sep 20 01:35:50.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-2187'
Sep 20 01:35:51.922: INFO: stderr: ""
Sep 20 01:35:51.922: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 20 01:35:51.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2187'
Sep 20 01:35:52.004: INFO: stderr: ""
Sep 20 01:35:52.004: INFO: stdout: "update-demo-nautilus-kv5hb update-demo-nautilus-thf2w "
Sep 20 01:35:52.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-nautilus-kv5hb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Sep 20 01:35:52.095: INFO: stderr: ""
Sep 20 01:35:52.095: INFO: stdout: "true"
Sep 20 01:35:52.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-nautilus-kv5hb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Sep 20 01:35:52.185: INFO: stderr: ""
Sep 20 01:35:52.185: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 20 01:35:52.185: INFO: validating pod update-demo-nautilus-kv5hb
Sep 20 01:35:52.199: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 01:35:52.199: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 01:35:52.199: INFO: update-demo-nautilus-kv5hb is verified up and running
Sep 20 01:35:52.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-nautilus-thf2w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Sep 20 01:35:52.289: INFO: stderr: ""
Sep 20 01:35:52.289: INFO: stdout: ""
Sep 20 01:35:52.289: INFO: update-demo-nautilus-thf2w is created but not running
I0920 01:35:53.683598      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
Sep 20 01:35:57.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2187'
Sep 20 01:35:57.390: INFO: stderr: ""
Sep 20 01:35:57.390: INFO: stdout: "update-demo-nautilus-kv5hb update-demo-nautilus-thf2w "
Sep 20 01:35:57.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-nautilus-kv5hb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Sep 20 01:35:57.484: INFO: stderr: ""
Sep 20 01:35:57.484: INFO: stdout: "true"
Sep 20 01:35:57.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-nautilus-kv5hb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Sep 20 01:35:57.567: INFO: stderr: ""
Sep 20 01:35:57.567: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 20 01:35:57.567: INFO: validating pod update-demo-nautilus-kv5hb
Sep 20 01:35:57.587: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 01:35:57.587: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 01:35:57.587: INFO: update-demo-nautilus-kv5hb is verified up and running
Sep 20 01:35:57.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-nautilus-thf2w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Sep 20 01:35:57.676: INFO: stderr: ""
Sep 20 01:35:57.676: INFO: stdout: "true"
Sep 20 01:35:57.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-nautilus-thf2w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2187'
Sep 20 01:35:57.781: INFO: stderr: ""
Sep 20 01:35:57.781: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 20 01:35:57.781: INFO: validating pod update-demo-nautilus-thf2w
Sep 20 01:35:57.877: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 01:35:57.878: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 01:35:57.878: INFO: update-demo-nautilus-thf2w is verified up and running
STEP: using delete to clean up resources
Sep 20 01:35:57.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 delete --grace-period=0 --force -f - --namespace=kubectl-2187'
Sep 20 01:35:57.996: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 01:35:57.996: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 20 01:35:57.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2187'
Sep 20 01:35:58.092: INFO: stderr: "No resources found in kubectl-2187 namespace.\n"
Sep 20 01:35:58.092: INFO: stdout: ""
Sep 20 01:35:58.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods -l name=update-demo --namespace=kubectl-2187 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 20 01:35:58.196: INFO: stderr: ""
Sep 20 01:35:58.196: INFO: stdout: "update-demo-nautilus-kv5hb\nupdate-demo-nautilus-thf2w\n"
Sep 20 01:35:58.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2187'
Sep 20 01:35:58.807: INFO: stderr: "No resources found in kubectl-2187 namespace.\n"
Sep 20 01:35:58.807: INFO: stdout: ""
Sep 20 01:35:58.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods -l name=update-demo --namespace=kubectl-2187 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 20 01:35:58.909: INFO: stderr: ""
Sep 20 01:35:58.909: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:35:58.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2187" for this suite.
Sep 20 01:36:04.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:36:05.143: INFO: namespace kubectl-2187 deletion completed in 6.227023265s

• [SLOW TEST:27.184 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:36:05.150: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3428
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 20 01:36:05.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-3428'
Sep 20 01:36:05.428: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 20 01:36:05.428: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Sep 20 01:36:05.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 delete jobs e2e-test-httpd-job --namespace=kubectl-3428'
Sep 20 01:36:05.523: INFO: stderr: ""
Sep 20 01:36:05.523: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:36:05.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3428" for this suite.
Sep 20 01:36:17.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:36:17.753: INFO: namespace kubectl-3428 deletion completed in 12.219108509s

• [SLOW TEST:12.604 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:36:17.758: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4953
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with configMap that has name projected-configmap-test-upd-58848000-22f7-4ede-9a78-a0d1409c67cd
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-58848000-22f7-4ede-9a78-a0d1409c67cd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:37:29.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4953" for this suite.
Sep 20 01:37:41.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:37:41.269: INFO: namespace projected-4953 deletion completed in 12.204892126s

• [SLOW TEST:83.512 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:37:41.269: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5168
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5168.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5168.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5168.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5168.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 20 01:37:47.678: INFO: DNS probes using dns-test-69676c4c-c1e6-4108-8f9e-bbe0e42dcb45 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5168.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5168.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5168.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5168.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 20 01:37:53.936: INFO: DNS probes using dns-test-0346ba7a-89df-4700-a225-720a862253b5 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5168.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5168.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5168.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5168.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 20 01:37:58.203: INFO: DNS probes using dns-test-04b940d2-fe6c-4e2f-9971-97c5b6a6779b succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:37:58.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5168" for this suite.
Sep 20 01:38:04.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:38:04.498: INFO: namespace dns-5168 deletion completed in 6.249682342s

• [SLOW TEST:23.230 seconds]
[sig-network] DNS
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:38:04.516: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8569
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:38:09.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8569" for this suite.
Sep 20 01:38:21.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:38:22.060: INFO: namespace replication-controller-8569 deletion completed in 12.308845364s

• [SLOW TEST:17.544 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:38:22.069: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3807
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-f737546e-b8d1-40fb-b542-bc533fa30019
Sep 20 01:38:22.277: INFO: Pod name my-hostname-basic-f737546e-b8d1-40fb-b542-bc533fa30019: Found 0 pods out of 1
Sep 20 01:38:27.285: INFO: Pod name my-hostname-basic-f737546e-b8d1-40fb-b542-bc533fa30019: Found 1 pods out of 1
Sep 20 01:38:27.285: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-f737546e-b8d1-40fb-b542-bc533fa30019" are running
Sep 20 01:38:27.290: INFO: Pod "my-hostname-basic-f737546e-b8d1-40fb-b542-bc533fa30019-64s7n" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-20 01:38:22 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-20 01:38:24 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-20 01:38:24 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-20 01:38:22 +0000 UTC Reason: Message:}])
Sep 20 01:38:27.290: INFO: Trying to dial the pod
Sep 20 01:38:32.410: INFO: Controller my-hostname-basic-f737546e-b8d1-40fb-b542-bc533fa30019: Got expected result from replica 1 [my-hostname-basic-f737546e-b8d1-40fb-b542-bc533fa30019-64s7n]: "my-hostname-basic-f737546e-b8d1-40fb-b542-bc533fa30019-64s7n", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:38:32.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3807" for this suite.
Sep 20 01:38:38.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:38:38.632: INFO: namespace replication-controller-3807 deletion completed in 6.214372334s

• [SLOW TEST:16.564 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:38:38.636: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3165
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-3165
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-3165
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3165
Sep 20 01:38:38.823: INFO: Found 0 stateful pods, waiting for 1
Sep 20 01:38:48.831: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep 20 01:38:48.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 01:38:49.853: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 01:38:49.853: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 01:38:49.853: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 01:38:49.861: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 20 01:38:59.869: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 20 01:38:59.869: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 01:38:59.889: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999242s
Sep 20 01:39:00.895: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995068209s
Sep 20 01:39:01.902: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988898299s
Sep 20 01:39:02.909: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.981875207s
Sep 20 01:39:03.916: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.974908809s
Sep 20 01:39:04.924: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.967284881s
Sep 20 01:39:05.931: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.959555663s
Sep 20 01:39:06.943: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.946431866s
Sep 20 01:39:07.951: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.940386827s
Sep 20 01:39:08.959: INFO: Verifying statefulset ss doesn't scale past 1 for another 932.535076ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3165
Sep 20 01:39:09.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:39:10.691: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 20 01:39:10.691: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 01:39:10.691: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 20 01:39:10.698: INFO: Found 2 stateful pods, waiting for 3
Sep 20 01:39:20.711: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 01:39:20.712: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 01:39:20.712: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep 20 01:39:20.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 01:39:21.455: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 01:39:21.455: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 01:39:21.455: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 01:39:21.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 01:39:22.262: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 01:39:22.262: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 01:39:22.262: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 01:39:22.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 01:39:22.961: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 01:39:22.961: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 01:39:22.961: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 01:39:22.961: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 01:39:22.966: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Sep 20 01:39:32.980: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 20 01:39:32.980: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 20 01:39:32.980: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 20 01:39:33.005: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999947s
Sep 20 01:39:34.013: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988692138s
Sep 20 01:39:35.020: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.981391943s
Sep 20 01:39:36.028: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.973758015s
Sep 20 01:39:37.036: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.966501188s
Sep 20 01:39:38.043: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.957862491s
Sep 20 01:39:39.050: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.950158452s
Sep 20 01:39:40.057: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.943625562s
Sep 20 01:39:41.066: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.934809786s
Sep 20 01:39:42.073: INFO: Verifying statefulset ss doesn't scale past 3 for another 928.450773ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3165
Sep 20 01:39:43.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:39:43.799: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 20 01:39:43.799: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 01:39:43.799: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 20 01:39:43.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:39:44.483: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 20 01:39:44.483: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 01:39:44.483: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 20 01:39:44.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:39:45.065: INFO: rc: 1
Sep 20 01:39:45.066: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  error: Internal error occurred: error executing command in container: container not running (1126bcb85086811f76cd82dbdf3caac4e1618be3bf0e9ea2e3cc423e9ebde340)
 [] <nil> 0xc003023a70 exit status 1 <nil> <nil> true [0xc005eb8b68 0xc005eb8b80 0xc005eb8b98] [0xc005eb8b68 0xc005eb8b80 0xc005eb8b98] [0xc005eb8b78 0xc005eb8b90] [0x10ef310 0x10ef310] 0xc002d22840 <nil>}:
Command stdout:

stderr:
error: Internal error occurred: error executing command in container: container not running (1126bcb85086811f76cd82dbdf3caac4e1618be3bf0e9ea2e3cc423e9ebde340)

error:
exit status 1
Sep 20 01:39:55.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:39:55.160: INFO: rc: 1
Sep 20 01:39:55.161: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0029c79b0 exit status 1 <nil> <nil> true [0xc001ffcee8 0xc001ffcf00 0xc001ffcf18] [0xc001ffcee8 0xc001ffcf00 0xc001ffcf18] [0xc001ffcef8 0xc001ffcf10] [0x10ef310 0x10ef310] 0xc001e8b500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:40:05.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:40:05.247: INFO: rc: 1
Sep 20 01:40:05.247: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003023da0 exit status 1 <nil> <nil> true [0xc005eb8ba0 0xc005eb8bb8 0xc005eb8bd0] [0xc005eb8ba0 0xc005eb8bb8 0xc005eb8bd0] [0xc005eb8bb0 0xc005eb8bc8] [0x10ef310 0x10ef310] 0xc002d22ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:40:15.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:40:15.347: INFO: rc: 1
Sep 20 01:40:15.347: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00295a150 exit status 1 <nil> <nil> true [0xc005eb8bd8 0xc005eb8bf0 0xc005eb8c08] [0xc005eb8bd8 0xc005eb8bf0 0xc005eb8c08] [0xc005eb8be8 0xc005eb8c00] [0x10ef310 0x10ef310] 0xc002d22f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
I0920 01:40:19.860509      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
Sep 20 01:40:25.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:40:25.452: INFO: rc: 1
Sep 20 01:40:25.452: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0029c7d10 exit status 1 <nil> <nil> true [0xc001ffcf20 0xc001ffcf38 0xc001ffcf50] [0xc001ffcf20 0xc001ffcf38 0xc001ffcf50] [0xc001ffcf30 0xc001ffcf48] [0x10ef310 0x10ef310] 0xc001e8ba40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:40:35.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:40:35.557: INFO: rc: 1
Sep 20 01:40:35.558: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003022300 exit status 1 <nil> <nil> true [0xc001ffc008 0xc001ffc020 0xc001ffc038] [0xc001ffc008 0xc001ffc020 0xc001ffc038] [0xc001ffc018 0xc001ffc030] [0x10ef310 0x10ef310] 0xc002c02300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:40:45.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:40:45.657: INFO: rc: 1
Sep 20 01:40:45.657: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0012843c0 exit status 1 <nil> <nil> true [0xc005eb8000 0xc005eb8018 0xc005eb8030] [0xc005eb8000 0xc005eb8018 0xc005eb8030] [0xc005eb8010 0xc005eb8028] [0x10ef310 0x10ef310] 0xc002e28960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:40:55.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:40:55.763: INFO: rc: 1
Sep 20 01:40:55.763: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001284780 exit status 1 <nil> <nil> true [0xc005eb8038 0xc005eb8050 0xc005eb8068] [0xc005eb8038 0xc005eb8050 0xc005eb8068] [0xc005eb8048 0xc005eb8060] [0x10ef310 0x10ef310] 0xc002e28f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:41:05.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:41:05.868: INFO: rc: 1
Sep 20 01:41:05.868: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003022690 exit status 1 <nil> <nil> true [0xc001ffc040 0xc001ffc058 0xc001ffc070] [0xc001ffc040 0xc001ffc058 0xc001ffc070] [0xc001ffc050 0xc001ffc068] [0x10ef310 0x10ef310] 0xc002c02720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:41:15.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:41:15.976: INFO: rc: 1
Sep 20 01:41:15.976: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0030229f0 exit status 1 <nil> <nil> true [0xc001ffc078 0xc001ffc090 0xc001ffc0a8] [0xc001ffc078 0xc001ffc090 0xc001ffc0a8] [0xc001ffc088 0xc001ffc0a0] [0x10ef310 0x10ef310] 0xc002c02fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:41:25.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:41:26.080: INFO: rc: 1
Sep 20 01:41:26.080: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001284ba0 exit status 1 <nil> <nil> true [0xc005eb8070 0xc005eb8088 0xc005eb80a0] [0xc005eb8070 0xc005eb8088 0xc005eb80a0] [0xc005eb8080 0xc005eb8098] [0x10ef310 0x10ef310] 0xc002e29500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:41:36.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:41:36.187: INFO: rc: 1
Sep 20 01:41:36.188: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001284fc0 exit status 1 <nil> <nil> true [0xc005eb80a8 0xc005eb80c0 0xc005eb80d8] [0xc005eb80a8 0xc005eb80c0 0xc005eb80d8] [0xc005eb80b8 0xc005eb80d0] [0x10ef310 0x10ef310] 0xc002e29b60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:41:46.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:41:46.292: INFO: rc: 1
Sep 20 01:41:46.292: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001285620 exit status 1 <nil> <nil> true [0xc005eb80e0 0xc005eb80f8 0xc005eb8110] [0xc005eb80e0 0xc005eb80f8 0xc005eb8110] [0xc005eb80f0 0xc005eb8108] [0x10ef310 0x10ef310] 0xc002f600c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:41:56.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:41:56.385: INFO: rc: 1
Sep 20 01:41:56.385: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003022d80 exit status 1 <nil> <nil> true [0xc001ffc0b0 0xc001ffc0c8 0xc001ffc0e0] [0xc001ffc0b0 0xc001ffc0c8 0xc001ffc0e0] [0xc001ffc0c0 0xc001ffc0d8] [0x10ef310 0x10ef310] 0xc002c035c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:42:06.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:42:06.488: INFO: rc: 1
Sep 20 01:42:06.488: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0030230e0 exit status 1 <nil> <nil> true [0xc001ffc0e8 0xc001ffc100 0xc001ffc118] [0xc001ffc0e8 0xc001ffc100 0xc001ffc118] [0xc001ffc0f8 0xc001ffc110] [0x10ef310 0x10ef310] 0xc002c03f20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:42:16.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:42:16.583: INFO: rc: 1
Sep 20 01:42:16.584: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0012859e0 exit status 1 <nil> <nil> true [0xc005eb8118 0xc005eb8130 0xc005eb8148] [0xc005eb8118 0xc005eb8130 0xc005eb8148] [0xc005eb8128 0xc005eb8140] [0x10ef310 0x10ef310] 0xc002f60420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:42:26.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:42:26.698: INFO: rc: 1
Sep 20 01:42:26.698: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001285a70 exit status 1 <nil> <nil> true [0xc001ffc128 0xc001ffc140 0xc001ffc158] [0xc001ffc128 0xc001ffc140 0xc001ffc158] [0xc001ffc138 0xc001ffc150] [0x10ef310 0x10ef310] 0xc002f60540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:42:36.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:42:36.802: INFO: rc: 1
Sep 20 01:42:36.802: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003022330 exit status 1 <nil> <nil> true [0xc005eb8000 0xc005eb8018 0xc005eb8030] [0xc005eb8000 0xc005eb8018 0xc005eb8030] [0xc005eb8010 0xc005eb8028] [0x10ef310 0x10ef310] 0xc002e28960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:42:46.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:42:46.917: INFO: rc: 1
Sep 20 01:42:46.917: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001284420 exit status 1 <nil> <nil> true [0xc001ffc000 0xc001ffc018 0xc001ffc030] [0xc001ffc000 0xc001ffc018 0xc001ffc030] [0xc001ffc010 0xc001ffc028] [0x10ef310 0x10ef310] 0xc002c02420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:42:56.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:42:57.023: INFO: rc: 1
Sep 20 01:42:57.024: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001284840 exit status 1 <nil> <nil> true [0xc001ffc038 0xc001ffc050 0xc001ffc068] [0xc001ffc038 0xc001ffc050 0xc001ffc068] [0xc001ffc048 0xc001ffc060] [0x10ef310 0x10ef310] 0xc002c02ae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
I0920 01:42:57.693843      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
Sep 20 01:43:07.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:43:07.114: INFO: rc: 1
Sep 20 01:43:07.114: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001284de0 exit status 1 <nil> <nil> true [0xc001ffc0a0 0xc001ffc0b8 0xc001ffc0d0] [0xc001ffc0a0 0xc001ffc0b8 0xc001ffc0d0] [0xc001ffc0b0 0xc001ffc0c8] [0x10ef310 0x10ef310] 0xc002c03560 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:43:17.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:43:17.206: INFO: rc: 1
Sep 20 01:43:17.207: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0012852c0 exit status 1 <nil> <nil> true [0xc001ffc0d8 0xc001ffc0f0 0xc001ffc108] [0xc001ffc0d8 0xc001ffc0f0 0xc001ffc108] [0xc001ffc0e8 0xc001ffc100] [0x10ef310 0x10ef310] 0xc002c03ec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:43:27.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:43:27.307: INFO: rc: 1
Sep 20 01:43:27.308: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0030226c0 exit status 1 <nil> <nil> true [0xc005eb8038 0xc005eb8050 0xc005eb8068] [0xc005eb8038 0xc005eb8050 0xc005eb8068] [0xc005eb8048 0xc005eb8060] [0x10ef310 0x10ef310] 0xc002e28f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:43:37.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:43:37.397: INFO: rc: 1
Sep 20 01:43:37.397: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0012858c0 exit status 1 <nil> <nil> true [0xc001ffc110 0xc001ffc168 0xc001ffc180] [0xc001ffc110 0xc001ffc168 0xc001ffc180] [0xc001ffc160 0xc001ffc178] [0x10ef310 0x10ef310] 0xc002f60180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:43:47.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:43:47.507: INFO: rc: 1
Sep 20 01:43:47.507: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003022a50 exit status 1 <nil> <nil> true [0xc005eb8070 0xc005eb8088 0xc005eb80a0] [0xc005eb8070 0xc005eb8088 0xc005eb80a0] [0xc005eb8080 0xc005eb8098] [0x10ef310 0x10ef310] 0xc002e29500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:43:57.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:43:57.596: INFO: rc: 1
Sep 20 01:43:57.596: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001285ef0 exit status 1 <nil> <nil> true [0xc001ffc188 0xc001ffc1a0 0xc001ffc1b8] [0xc001ffc188 0xc001ffc1a0 0xc001ffc1b8] [0xc001ffc198 0xc001ffc1b0] [0x10ef310 0x10ef310] 0xc002f604e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:44:07.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:44:07.700: INFO: rc: 1
Sep 20 01:44:07.700: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001a302a0 exit status 1 <nil> <nil> true [0xc001ffc1c0 0xc001ffc1d8 0xc001ffc1f0] [0xc001ffc1c0 0xc001ffc1d8 0xc001ffc1f0] [0xc001ffc1d0 0xc001ffc1e8] [0x10ef310 0x10ef310] 0xc002f60900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:44:17.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:44:17.800: INFO: rc: 1
Sep 20 01:44:17.800: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001a30600 exit status 1 <nil> <nil> true [0xc001ffc1f8 0xc001ffc210 0xc001ffc228] [0xc001ffc1f8 0xc001ffc210 0xc001ffc228] [0xc001ffc208 0xc001ffc220] [0x10ef310 0x10ef310] 0xc002f60cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:44:27.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:44:27.908: INFO: rc: 1
Sep 20 01:44:27.908: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001a30960 exit status 1 <nil> <nil> true [0xc001ffc238 0xc001ffc250 0xc001ffc268] [0xc001ffc238 0xc001ffc250 0xc001ffc268] [0xc001ffc248 0xc001ffc260] [0x10ef310 0x10ef310] 0xc002f61020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:44:37.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:44:38.010: INFO: rc: 1
Sep 20 01:44:38.010: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003022300 exit status 1 <nil> <nil> true [0xc005eb8000 0xc005eb8018 0xc005eb8030] [0xc005eb8000 0xc005eb8018 0xc005eb8030] [0xc005eb8010 0xc005eb8028] [0x10ef310 0x10ef310] 0xc002c02300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep 20 01:44:48.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=statefulset-3165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 01:44:48.103: INFO: rc: 1
Sep 20 01:44:48.103: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Sep 20 01:44:48.103: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Sep 20 01:44:48.122: INFO: Deleting all statefulset in ns statefulset-3165
Sep 20 01:44:48.129: INFO: Scaling statefulset ss to 0
Sep 20 01:44:48.150: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 01:44:48.159: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:44:48.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3165" for this suite.
Sep 20 01:44:54.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:44:54.423: INFO: namespace statefulset-3165 deletion completed in 6.214518131s

• [SLOW TEST:375.787 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:44:54.425: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7675
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Sep 20 01:44:54.606: INFO: Waiting up to 5m0s for pod "client-containers-57186d63-a259-4fe8-a534-ef3042984e3a" in namespace "containers-7675" to be "success or failure"
Sep 20 01:44:54.613: INFO: Pod "client-containers-57186d63-a259-4fe8-a534-ef3042984e3a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.308754ms
Sep 20 01:44:56.620: INFO: Pod "client-containers-57186d63-a259-4fe8-a534-ef3042984e3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013916875s
Sep 20 01:44:58.629: INFO: Pod "client-containers-57186d63-a259-4fe8-a534-ef3042984e3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023112338s
STEP: Saw pod success
Sep 20 01:44:58.629: INFO: Pod "client-containers-57186d63-a259-4fe8-a534-ef3042984e3a" satisfied condition "success or failure"
Sep 20 01:44:58.635: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod client-containers-57186d63-a259-4fe8-a534-ef3042984e3a container test-container: <nil>
STEP: delete the pod
Sep 20 01:44:58.709: INFO: Waiting for pod client-containers-57186d63-a259-4fe8-a534-ef3042984e3a to disappear
Sep 20 01:44:58.716: INFO: Pod client-containers-57186d63-a259-4fe8-a534-ef3042984e3a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:44:58.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7675" for this suite.
Sep 20 01:45:04.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:45:04.980: INFO: namespace containers-7675 deletion completed in 6.255679055s

• [SLOW TEST:10.555 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:45:04.981: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1085
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-b4613482-7b7f-42e8-a27e-14ae51257d30
STEP: Creating secret with name secret-projected-all-test-volume-76a009a5-d332-462d-a0ac-678bf46e1147
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep 20 01:45:05.184: INFO: Waiting up to 5m0s for pod "projected-volume-44aa1d0d-b81d-45d9-a45e-f8c058144bac" in namespace "projected-1085" to be "success or failure"
Sep 20 01:45:05.190: INFO: Pod "projected-volume-44aa1d0d-b81d-45d9-a45e-f8c058144bac": Phase="Pending", Reason="", readiness=false. Elapsed: 5.713722ms
Sep 20 01:45:07.199: INFO: Pod "projected-volume-44aa1d0d-b81d-45d9-a45e-f8c058144bac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014362877s
Sep 20 01:45:09.206: INFO: Pod "projected-volume-44aa1d0d-b81d-45d9-a45e-f8c058144bac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021499216s
STEP: Saw pod success
Sep 20 01:45:09.207: INFO: Pod "projected-volume-44aa1d0d-b81d-45d9-a45e-f8c058144bac" satisfied condition "success or failure"
Sep 20 01:45:09.212: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod projected-volume-44aa1d0d-b81d-45d9-a45e-f8c058144bac container projected-all-volume-test: <nil>
STEP: delete the pod
Sep 20 01:45:09.286: INFO: Waiting for pod projected-volume-44aa1d0d-b81d-45d9-a45e-f8c058144bac to disappear
Sep 20 01:45:09.291: INFO: Pod projected-volume-44aa1d0d-b81d-45d9-a45e-f8c058144bac no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:45:09.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1085" for this suite.
Sep 20 01:45:15.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:45:15.506: INFO: namespace projected-1085 deletion completed in 6.204653167s

• [SLOW TEST:10.526 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:45:15.507: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9840
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Sep 20 01:45:15.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 --namespace=kubectl-9840 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Sep 20 01:45:18.766: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Sep 20 01:45:18.766: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:45:20.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9840" for this suite.
Sep 20 01:45:26.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:45:27.008: INFO: namespace kubectl-9840 deletion completed in 6.213474207s

• [SLOW TEST:11.502 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:45:27.009: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1368
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-03ee8a42-24bf-4432-a099-dbea38e71f49
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:45:31.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1368" for this suite.
Sep 20 01:45:43.369: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:45:43.576: INFO: namespace configmap-1368 deletion completed in 12.233040489s

• [SLOW TEST:16.567 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:45:43.577: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7489
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 20 01:45:43.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-7489'
Sep 20 01:45:43.859: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 20 01:45:43.859: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Sep 20 01:45:45.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 delete deployment e2e-test-httpd-deployment --namespace=kubectl-7489'
Sep 20 01:45:45.990: INFO: stderr: ""
Sep 20 01:45:45.990: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:45:45.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7489" for this suite.
Sep 20 01:45:58.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:45:58.296: INFO: namespace kubectl-7489 deletion completed in 12.296874881s

• [SLOW TEST:14.720 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:45:58.302: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5007
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:45:58.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5007" for this suite.
Sep 20 01:46:04.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:46:04.764: INFO: namespace kubelet-test-5007 deletion completed in 6.232559678s

• [SLOW TEST:6.462 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:46:04.770: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9381
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 01:46:05.001: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e27f72ce-3fda-4867-83a4-ce5d19964440", Controller:(*bool)(0xc006599ea6), BlockOwnerDeletion:(*bool)(0xc006599ea7)}}
Sep 20 01:46:05.012: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"f21a7727-33ed-47bf-b12c-27a40c284123", Controller:(*bool)(0xc0047710ee), BlockOwnerDeletion:(*bool)(0xc0047710ef)}}
Sep 20 01:46:05.033: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"d8ebc795-89ce-448b-8d61-4fd5c68907f6", Controller:(*bool)(0xc005516066), BlockOwnerDeletion:(*bool)(0xc005516067)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:46:10.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9381" for this suite.
Sep 20 01:46:16.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:46:16.262: INFO: namespace gc-9381 deletion completed in 6.205637225s

• [SLOW TEST:11.493 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:46:16.265: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9307
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:46:32.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9307" for this suite.
Sep 20 01:46:38.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:46:38.915: INFO: namespace resourcequota-9307 deletion completed in 6.290687806s

• [SLOW TEST:22.651 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:46:38.917: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9047
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 20 01:46:39.119: INFO: Waiting up to 5m0s for pod "pod-8d2f98b9-473f-4d3c-846a-23aba894373a" in namespace "emptydir-9047" to be "success or failure"
Sep 20 01:46:39.125: INFO: Pod "pod-8d2f98b9-473f-4d3c-846a-23aba894373a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.636583ms
Sep 20 01:46:41.132: INFO: Pod "pod-8d2f98b9-473f-4d3c-846a-23aba894373a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012598699s
Sep 20 01:46:43.138: INFO: Pod "pod-8d2f98b9-473f-4d3c-846a-23aba894373a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018678363s
STEP: Saw pod success
Sep 20 01:46:43.138: INFO: Pod "pod-8d2f98b9-473f-4d3c-846a-23aba894373a" satisfied condition "success or failure"
Sep 20 01:46:43.144: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod pod-8d2f98b9-473f-4d3c-846a-23aba894373a container test-container: <nil>
STEP: delete the pod
Sep 20 01:46:43.220: INFO: Waiting for pod pod-8d2f98b9-473f-4d3c-846a-23aba894373a to disappear
Sep 20 01:46:43.227: INFO: Pod pod-8d2f98b9-473f-4d3c-846a-23aba894373a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:46:43.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9047" for this suite.
Sep 20 01:46:49.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:46:49.451: INFO: namespace emptydir-9047 deletion completed in 6.213574192s

• [SLOW TEST:10.534 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:46:49.452: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6851
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep 20 01:46:49.652: INFO: Waiting up to 5m0s for pod "pod-644b5f59-d40e-4949-bf4f-9107b1de02b7" in namespace "emptydir-6851" to be "success or failure"
Sep 20 01:46:49.658: INFO: Pod "pod-644b5f59-d40e-4949-bf4f-9107b1de02b7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.87478ms
Sep 20 01:46:51.672: INFO: Pod "pod-644b5f59-d40e-4949-bf4f-9107b1de02b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019087232s
Sep 20 01:46:53.679: INFO: Pod "pod-644b5f59-d40e-4949-bf4f-9107b1de02b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026174864s
STEP: Saw pod success
Sep 20 01:46:53.679: INFO: Pod "pod-644b5f59-d40e-4949-bf4f-9107b1de02b7" satisfied condition "success or failure"
Sep 20 01:46:53.684: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod pod-644b5f59-d40e-4949-bf4f-9107b1de02b7 container test-container: <nil>
STEP: delete the pod
Sep 20 01:46:53.737: INFO: Waiting for pod pod-644b5f59-d40e-4949-bf4f-9107b1de02b7 to disappear
Sep 20 01:46:53.742: INFO: Pod pod-644b5f59-d40e-4949-bf4f-9107b1de02b7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:46:53.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6851" for this suite.
Sep 20 01:46:59.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:46:59.960: INFO: namespace emptydir-6851 deletion completed in 6.211734427s

• [SLOW TEST:10.508 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:46:59.961: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-3017
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep 20 01:47:00.526: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Sep 20 01:47:02.548: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704540820, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704540820, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704540820, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704540820, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 01:47:05.574: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 01:47:05.580: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:47:07.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3017" for this suite.
Sep 20 01:47:13.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:47:13.249: INFO: namespace crd-webhook-3017 deletion completed in 6.222157282s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:13.319 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:47:13.282: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8338
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Sep 20 01:47:13.467: INFO: Waiting up to 5m0s for pod "client-containers-75b8517c-dd6e-4fb6-9320-3a46dbcf1e96" in namespace "containers-8338" to be "success or failure"
Sep 20 01:47:13.474: INFO: Pod "client-containers-75b8517c-dd6e-4fb6-9320-3a46dbcf1e96": Phase="Pending", Reason="", readiness=false. Elapsed: 6.91286ms
Sep 20 01:47:15.482: INFO: Pod "client-containers-75b8517c-dd6e-4fb6-9320-3a46dbcf1e96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014801544s
Sep 20 01:47:17.489: INFO: Pod "client-containers-75b8517c-dd6e-4fb6-9320-3a46dbcf1e96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021197097s
STEP: Saw pod success
Sep 20 01:47:17.489: INFO: Pod "client-containers-75b8517c-dd6e-4fb6-9320-3a46dbcf1e96" satisfied condition "success or failure"
Sep 20 01:47:17.494: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod client-containers-75b8517c-dd6e-4fb6-9320-3a46dbcf1e96 container test-container: <nil>
STEP: delete the pod
Sep 20 01:47:17.568: INFO: Waiting for pod client-containers-75b8517c-dd6e-4fb6-9320-3a46dbcf1e96 to disappear
Sep 20 01:47:17.573: INFO: Pod client-containers-75b8517c-dd6e-4fb6-9320-3a46dbcf1e96 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:47:17.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8338" for this suite.
Sep 20 01:47:23.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:47:23.830: INFO: namespace containers-8338 deletion completed in 6.249674633s

• [SLOW TEST:10.549 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:47:23.831: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5594
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-scdz
STEP: Creating a pod to test atomic-volume-subpath
Sep 20 01:47:24.037: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-scdz" in namespace "subpath-5594" to be "success or failure"
Sep 20 01:47:24.048: INFO: Pod "pod-subpath-test-configmap-scdz": Phase="Pending", Reason="", readiness=false. Elapsed: 10.765569ms
Sep 20 01:47:26.054: INFO: Pod "pod-subpath-test-configmap-scdz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016301098s
Sep 20 01:47:28.061: INFO: Pod "pod-subpath-test-configmap-scdz": Phase="Running", Reason="", readiness=true. Elapsed: 4.023844909s
Sep 20 01:47:30.068: INFO: Pod "pod-subpath-test-configmap-scdz": Phase="Running", Reason="", readiness=true. Elapsed: 6.030688767s
Sep 20 01:47:32.074: INFO: Pod "pod-subpath-test-configmap-scdz": Phase="Running", Reason="", readiness=true. Elapsed: 8.036914142s
Sep 20 01:47:34.080: INFO: Pod "pod-subpath-test-configmap-scdz": Phase="Running", Reason="", readiness=true. Elapsed: 10.042665895s
Sep 20 01:47:36.088: INFO: Pod "pod-subpath-test-configmap-scdz": Phase="Running", Reason="", readiness=true. Elapsed: 12.050233647s
Sep 20 01:47:38.093: INFO: Pod "pod-subpath-test-configmap-scdz": Phase="Running", Reason="", readiness=true. Elapsed: 14.055865038s
Sep 20 01:47:40.101: INFO: Pod "pod-subpath-test-configmap-scdz": Phase="Running", Reason="", readiness=true. Elapsed: 16.06320518s
Sep 20 01:47:42.108: INFO: Pod "pod-subpath-test-configmap-scdz": Phase="Running", Reason="", readiness=true. Elapsed: 18.070439984s
Sep 20 01:47:44.114: INFO: Pod "pod-subpath-test-configmap-scdz": Phase="Running", Reason="", readiness=true. Elapsed: 20.076200244s
Sep 20 01:47:46.119: INFO: Pod "pod-subpath-test-configmap-scdz": Phase="Running", Reason="", readiness=true. Elapsed: 22.081945817s
Sep 20 01:47:48.126: INFO: Pod "pod-subpath-test-configmap-scdz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.088936879s
STEP: Saw pod success
Sep 20 01:47:48.127: INFO: Pod "pod-subpath-test-configmap-scdz" satisfied condition "success or failure"
Sep 20 01:47:48.134: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod pod-subpath-test-configmap-scdz container test-container-subpath-configmap-scdz: <nil>
STEP: delete the pod
Sep 20 01:47:48.212: INFO: Waiting for pod pod-subpath-test-configmap-scdz to disappear
Sep 20 01:47:48.218: INFO: Pod pod-subpath-test-configmap-scdz no longer exists
STEP: Deleting pod pod-subpath-test-configmap-scdz
Sep 20 01:47:48.218: INFO: Deleting pod "pod-subpath-test-configmap-scdz" in namespace "subpath-5594"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:47:48.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5594" for this suite.
Sep 20 01:47:54.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:47:54.440: INFO: namespace subpath-5594 deletion completed in 6.21001617s

• [SLOW TEST:30.609 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:47:54.448: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-7751
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep 20 01:47:55.001: INFO: Pod name wrapped-volume-race-3ca70c67-d66e-4d0d-99ac-93053b3f5862: Found 0 pods out of 5
Sep 20 01:48:00.013: INFO: Pod name wrapped-volume-race-3ca70c67-d66e-4d0d-99ac-93053b3f5862: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3ca70c67-d66e-4d0d-99ac-93053b3f5862 in namespace emptydir-wrapper-7751, will wait for the garbage collector to delete the pods
I0920 01:48:12.072300      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 01:48:12.072360      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 01:48:12.135: INFO: Deleting ReplicationController wrapped-volume-race-3ca70c67-d66e-4d0d-99ac-93053b3f5862 took: 13.114564ms
I0920 01:48:12.536109      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-7751/wrapped-volume-race-3ca70c67-d66e-4d0d-99ac-93053b3f5862-jxhnt in state Running, deletion time 2019-09-20 01:48:42 +0000 UTC
I0920 01:48:12.536177      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-7751/wrapped-volume-race-3ca70c67-d66e-4d0d-99ac-93053b3f5862-9tmr4 in state Running, deletion time 2019-09-20 01:48:42 +0000 UTC
I0920 01:48:12.536193      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-7751/wrapped-volume-race-3ca70c67-d66e-4d0d-99ac-93053b3f5862-gdcfb in state Running, deletion time 2019-09-20 01:48:42 +0000 UTC
I0920 01:48:12.536206      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-7751/wrapped-volume-race-3ca70c67-d66e-4d0d-99ac-93053b3f5862-d764c in state Running, deletion time 2019-09-20 01:48:42 +0000 UTC
I0920 01:48:12.536217      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-7751/wrapped-volume-race-3ca70c67-d66e-4d0d-99ac-93053b3f5862-qtg4x in state Running, deletion time 2019-09-20 01:48:42 +0000 UTC
Sep 20 01:48:12.536: INFO: Terminating ReplicationController wrapped-volume-race-3ca70c67-d66e-4d0d-99ac-93053b3f5862 pods took: 400.718292ms
STEP: Creating RC which spawns configmap-volume pods
Sep 20 01:48:53.675: INFO: Pod name wrapped-volume-race-6738dacf-5a0a-4e0d-b7ae-c413abbe6e7a: Found 1 pods out of 5
Sep 20 01:48:58.684: INFO: Pod name wrapped-volume-race-6738dacf-5a0a-4e0d-b7ae-c413abbe6e7a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6738dacf-5a0a-4e0d-b7ae-c413abbe6e7a in namespace emptydir-wrapper-7751, will wait for the garbage collector to delete the pods
I0920 01:49:10.725815      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 01:49:10.726060      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 01:49:10.790: INFO: Deleting ReplicationController wrapped-volume-race-6738dacf-5a0a-4e0d-b7ae-c413abbe6e7a took: 14.655772ms
I0920 01:49:11.191343      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-7751/wrapped-volume-race-6738dacf-5a0a-4e0d-b7ae-c413abbe6e7a-xk5p2 in state Running, deletion time 2019-09-20 01:49:41 +0000 UTC
I0920 01:49:11.191394      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-7751/wrapped-volume-race-6738dacf-5a0a-4e0d-b7ae-c413abbe6e7a-zmqzj in state Running, deletion time 2019-09-20 01:49:41 +0000 UTC
I0920 01:49:11.191410      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-7751/wrapped-volume-race-6738dacf-5a0a-4e0d-b7ae-c413abbe6e7a-49w5z in state Running, deletion time 2019-09-20 01:49:41 +0000 UTC
I0920 01:49:11.191423      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-7751/wrapped-volume-race-6738dacf-5a0a-4e0d-b7ae-c413abbe6e7a-rkfnm in state Running, deletion time 2019-09-20 01:49:41 +0000 UTC
I0920 01:49:11.191437      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-7751/wrapped-volume-race-6738dacf-5a0a-4e0d-b7ae-c413abbe6e7a-h4j6f in state Running, deletion time 2019-09-20 01:49:41 +0000 UTC
Sep 20 01:49:11.191: INFO: Terminating ReplicationController wrapped-volume-race-6738dacf-5a0a-4e0d-b7ae-c413abbe6e7a pods took: 400.822844ms
STEP: Creating RC which spawns configmap-volume pods
Sep 20 01:49:54.223: INFO: Pod name wrapped-volume-race-5f1d2353-ab99-452a-815b-4433d80e1d50: Found 0 pods out of 5
Sep 20 01:49:59.232: INFO: Pod name wrapped-volume-race-5f1d2353-ab99-452a-815b-4433d80e1d50: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-5f1d2353-ab99-452a-815b-4433d80e1d50 in namespace emptydir-wrapper-7751, will wait for the garbage collector to delete the pods
I0920 01:50:11.272139      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 01:50:11.272385      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 01:50:11.335: INFO: Deleting ReplicationController wrapped-volume-race-5f1d2353-ab99-452a-815b-4433d80e1d50 took: 13.331103ms
I0920 01:50:11.737654      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-7751/wrapped-volume-race-5f1d2353-ab99-452a-815b-4433d80e1d50-fcgjr in state Running, deletion time 2019-09-20 01:50:41 +0000 UTC
I0920 01:50:11.737968      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-7751/wrapped-volume-race-5f1d2353-ab99-452a-815b-4433d80e1d50-xj5x8 in state Running, deletion time 2019-09-20 01:50:41 +0000 UTC
I0920 01:50:11.738200      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-7751/wrapped-volume-race-5f1d2353-ab99-452a-815b-4433d80e1d50-h9v4l in state Running, deletion time 2019-09-20 01:50:41 +0000 UTC
I0920 01:50:11.738405      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-7751/wrapped-volume-race-5f1d2353-ab99-452a-815b-4433d80e1d50-dpx69 in state Running, deletion time 2019-09-20 01:50:41 +0000 UTC
I0920 01:50:11.738595      16 controller_utils.go:810] Ignoring inactive pod emptydir-wrapper-7751/wrapped-volume-race-5f1d2353-ab99-452a-815b-4433d80e1d50-6zvfj in state Running, deletion time 2019-09-20 01:50:41 +0000 UTC
Sep 20 01:50:11.738: INFO: Terminating ReplicationController wrapped-volume-race-5f1d2353-ab99-452a-815b-4433d80e1d50 pods took: 403.199377ms
I0920 01:50:13.860635      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:50:54.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7751" for this suite.
Sep 20 01:51:02.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:51:02.281: INFO: namespace emptydir-wrapper-7751 deletion completed in 8.242343998s

• [SLOW TEST:187.834 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:51:02.285: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6410
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-mrnd
STEP: Creating a pod to test atomic-volume-subpath
Sep 20 01:51:02.490: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-mrnd" in namespace "subpath-6410" to be "success or failure"
Sep 20 01:51:02.496: INFO: Pod "pod-subpath-test-secret-mrnd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.826936ms
Sep 20 01:51:04.509: INFO: Pod "pod-subpath-test-secret-mrnd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018566549s
Sep 20 01:51:06.516: INFO: Pod "pod-subpath-test-secret-mrnd": Phase="Running", Reason="", readiness=true. Elapsed: 4.026078846s
Sep 20 01:51:08.524: INFO: Pod "pod-subpath-test-secret-mrnd": Phase="Running", Reason="", readiness=true. Elapsed: 6.033236409s
Sep 20 01:51:10.529: INFO: Pod "pod-subpath-test-secret-mrnd": Phase="Running", Reason="", readiness=true. Elapsed: 8.039076954s
Sep 20 01:51:12.539: INFO: Pod "pod-subpath-test-secret-mrnd": Phase="Running", Reason="", readiness=true. Elapsed: 10.048661477s
Sep 20 01:51:14.546: INFO: Pod "pod-subpath-test-secret-mrnd": Phase="Running", Reason="", readiness=true. Elapsed: 12.055350896s
Sep 20 01:51:16.552: INFO: Pod "pod-subpath-test-secret-mrnd": Phase="Running", Reason="", readiness=true. Elapsed: 14.061442791s
Sep 20 01:51:18.558: INFO: Pod "pod-subpath-test-secret-mrnd": Phase="Running", Reason="", readiness=true. Elapsed: 16.067804933s
Sep 20 01:51:20.564: INFO: Pod "pod-subpath-test-secret-mrnd": Phase="Running", Reason="", readiness=true. Elapsed: 18.07370786s
Sep 20 01:51:22.571: INFO: Pod "pod-subpath-test-secret-mrnd": Phase="Running", Reason="", readiness=true. Elapsed: 20.08074778s
Sep 20 01:51:24.577: INFO: Pod "pod-subpath-test-secret-mrnd": Phase="Running", Reason="", readiness=true. Elapsed: 22.087025193s
Sep 20 01:51:26.583: INFO: Pod "pod-subpath-test-secret-mrnd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.093169047s
STEP: Saw pod success
Sep 20 01:51:26.584: INFO: Pod "pod-subpath-test-secret-mrnd" satisfied condition "success or failure"
Sep 20 01:51:26.590: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod pod-subpath-test-secret-mrnd container test-container-subpath-secret-mrnd: <nil>
STEP: delete the pod
Sep 20 01:51:26.634: INFO: Waiting for pod pod-subpath-test-secret-mrnd to disappear
Sep 20 01:51:26.638: INFO: Pod pod-subpath-test-secret-mrnd no longer exists
STEP: Deleting pod pod-subpath-test-secret-mrnd
Sep 20 01:51:26.638: INFO: Deleting pod "pod-subpath-test-secret-mrnd" in namespace "subpath-6410"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:51:26.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6410" for this suite.
Sep 20 01:51:32.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:51:32.888: INFO: namespace subpath-6410 deletion completed in 6.238436194s

• [SLOW TEST:30.604 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:51:32.894: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6184
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:51:44.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6184" for this suite.
Sep 20 01:51:50.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:51:50.337: INFO: namespace resourcequota-6184 deletion completed in 6.204893555s

• [SLOW TEST:17.444 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:51:50.342: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-7330
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 01:51:50.515: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Creating first CR 
Sep 20 01:51:50.678: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-09-20T01:51:50Z generation:1 name:name1 resourceVersion:81202 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:cc34277a-5e9c-4eb5-8950-3ab71102ad3a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Sep 20 01:52:00.690: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-09-20T01:52:00Z generation:1 name:name2 resourceVersion:81231 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:f4d5bacc-b229-49c2-bd83-5378abe316d8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Sep 20 01:52:10.703: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-09-20T01:51:50Z generation:2 name:name1 resourceVersion:81259 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:cc34277a-5e9c-4eb5-8950-3ab71102ad3a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Sep 20 01:52:20.715: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-09-20T01:52:00Z generation:2 name:name2 resourceVersion:81287 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:f4d5bacc-b229-49c2-bd83-5378abe316d8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Sep 20 01:52:30.728: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-09-20T01:51:50Z generation:2 name:name1 resourceVersion:81316 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:cc34277a-5e9c-4eb5-8950-3ab71102ad3a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Sep 20 01:52:40.742: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-09-20T01:52:00Z generation:2 name:name2 resourceVersion:81346 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:f4d5bacc-b229-49c2-bd83-5378abe316d8] num:map[num1:9223372036854775807 num2:1000000]]}
I0920 01:52:44.690498      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:52:51.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-7330" for this suite.
Sep 20 01:52:57.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:52:57.535: INFO: namespace crd-watch-7330 deletion completed in 6.267378572s

• [SLOW TEST:67.193 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:52:57.539: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7752
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 20 01:52:57.780: INFO: Number of nodes with available pods: 0
Sep 20 01:52:57.780: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 01:52:58.793: INFO: Number of nodes with available pods: 0
Sep 20 01:52:58.793: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 01:52:59.795: INFO: Number of nodes with available pods: 0
Sep 20 01:52:59.795: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 01:53:00.803: INFO: Number of nodes with available pods: 5
Sep 20 01:53:00.803: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep 20 01:53:00.848: INFO: Number of nodes with available pods: 4
Sep 20 01:53:00.848: INFO: Node worker-wqshf-7859ffd555-r6tnq is running more than one daemon pod
Sep 20 01:53:01.863: INFO: Number of nodes with available pods: 4
Sep 20 01:53:01.863: INFO: Node worker-wqshf-7859ffd555-r6tnq is running more than one daemon pod
Sep 20 01:53:02.861: INFO: Number of nodes with available pods: 4
Sep 20 01:53:02.861: INFO: Node worker-wqshf-7859ffd555-r6tnq is running more than one daemon pod
Sep 20 01:53:03.861: INFO: Number of nodes with available pods: 5
Sep 20 01:53:03.861: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7752, will wait for the garbage collector to delete the pods
I0920 01:53:03.876047      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 01:53:03.876323      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 01:53:03.938: INFO: Deleting DaemonSet.extensions daemon-set took: 11.953419ms
I0920 01:53:04.345133      16 controller_utils.go:810] Ignoring inactive pod daemonsets-7752/daemon-set-s7mqc in state Running, deletion time 2019-09-20 01:53:34 +0000 UTC
I0920 01:53:04.345356      16 controller_utils.go:810] Ignoring inactive pod daemonsets-7752/daemon-set-kmhvf in state Running, deletion time 2019-09-20 01:53:34 +0000 UTC
I0920 01:53:04.345487      16 controller_utils.go:810] Ignoring inactive pod daemonsets-7752/daemon-set-zlwwm in state Running, deletion time 2019-09-20 01:53:34 +0000 UTC
I0920 01:53:04.345598      16 controller_utils.go:810] Ignoring inactive pod daemonsets-7752/daemon-set-g2kmt in state Running, deletion time 2019-09-20 01:53:34 +0000 UTC
I0920 01:53:04.345708      16 controller_utils.go:810] Ignoring inactive pod daemonsets-7752/daemon-set-pjpbs in state Running, deletion time 2019-09-20 01:53:34 +0000 UTC
Sep 20 01:53:04.345: INFO: Terminating DaemonSet.extensions daemon-set pods took: 407.532758ms
Sep 20 01:53:16.951: INFO: Number of nodes with available pods: 0
Sep 20 01:53:16.951: INFO: Number of running nodes: 0, number of available pods: 0
Sep 20 01:53:16.955: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7752/daemonsets","resourceVersion":"81553"},"items":null}

Sep 20 01:53:16.960: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7752/pods","resourceVersion":"81553"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:53:16.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7752" for this suite.
Sep 20 01:53:23.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:53:23.244: INFO: namespace daemonsets-7752 deletion completed in 6.244601198s

• [SLOW TEST:25.705 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:53:23.245: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-80
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 01:53:23.425: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f7ed7d24-3cf9-4c52-a3c0-6efb00524f1b" in namespace "projected-80" to be "success or failure"
Sep 20 01:53:23.432: INFO: Pod "downwardapi-volume-f7ed7d24-3cf9-4c52-a3c0-6efb00524f1b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.370631ms
Sep 20 01:53:25.438: INFO: Pod "downwardapi-volume-f7ed7d24-3cf9-4c52-a3c0-6efb00524f1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013343926s
Sep 20 01:53:27.448: INFO: Pod "downwardapi-volume-f7ed7d24-3cf9-4c52-a3c0-6efb00524f1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023074846s
STEP: Saw pod success
Sep 20 01:53:27.448: INFO: Pod "downwardapi-volume-f7ed7d24-3cf9-4c52-a3c0-6efb00524f1b" satisfied condition "success or failure"
Sep 20 01:53:27.457: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod downwardapi-volume-f7ed7d24-3cf9-4c52-a3c0-6efb00524f1b container client-container: <nil>
STEP: delete the pod
Sep 20 01:53:27.497: INFO: Waiting for pod downwardapi-volume-f7ed7d24-3cf9-4c52-a3c0-6efb00524f1b to disappear
Sep 20 01:53:27.501: INFO: Pod downwardapi-volume-f7ed7d24-3cf9-4c52-a3c0-6efb00524f1b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:53:27.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-80" for this suite.
Sep 20 01:53:33.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:53:33.751: INFO: namespace projected-80 deletion completed in 6.24444744s

• [SLOW TEST:10.507 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:53:33.761: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4103
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 20 01:53:42.040: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 20 01:53:42.046: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 20 01:53:44.047: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 20 01:53:44.055: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 20 01:53:46.047: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 20 01:53:46.052: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 20 01:53:48.047: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 20 01:53:48.053: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 20 01:53:50.046: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 20 01:53:50.052: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 20 01:53:52.047: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 20 01:53:52.055: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 20 01:53:54.047: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 20 01:53:54.053: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:53:54.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4103" for this suite.
Sep 20 01:54:22.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:54:22.282: INFO: namespace container-lifecycle-hook-4103 deletion completed in 28.220615704s

• [SLOW TEST:48.522 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:54:22.293: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-8554
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep 20 01:54:27.518: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:54:27.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8554" for this suite.
Sep 20 01:54:55.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:54:55.763: INFO: namespace replicaset-8554 deletion completed in 28.213950194s

• [SLOW TEST:33.471 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:54:55.778: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9642
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-aab04d18-80aa-4c24-8195-4d87efa6cd00
STEP: Creating a pod to test consume configMaps
Sep 20 01:54:55.969: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2e7d7232-9f0d-4e63-94a0-f730ef4423a1" in namespace "projected-9642" to be "success or failure"
Sep 20 01:54:55.978: INFO: Pod "pod-projected-configmaps-2e7d7232-9f0d-4e63-94a0-f730ef4423a1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.937367ms
Sep 20 01:54:57.987: INFO: Pod "pod-projected-configmaps-2e7d7232-9f0d-4e63-94a0-f730ef4423a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017735206s
Sep 20 01:54:59.993: INFO: Pod "pod-projected-configmaps-2e7d7232-9f0d-4e63-94a0-f730ef4423a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02415286s
STEP: Saw pod success
Sep 20 01:54:59.994: INFO: Pod "pod-projected-configmaps-2e7d7232-9f0d-4e63-94a0-f730ef4423a1" satisfied condition "success or failure"
Sep 20 01:54:59.999: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod pod-projected-configmaps-2e7d7232-9f0d-4e63-94a0-f730ef4423a1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 01:55:00.076: INFO: Waiting for pod pod-projected-configmaps-2e7d7232-9f0d-4e63-94a0-f730ef4423a1 to disappear
Sep 20 01:55:00.082: INFO: Pod pod-projected-configmaps-2e7d7232-9f0d-4e63-94a0-f730ef4423a1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:55:00.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9642" for this suite.
Sep 20 01:55:06.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:55:06.303: INFO: namespace projected-9642 deletion completed in 6.214079037s

• [SLOW TEST:10.525 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:55:06.304: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4188
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-a76fbdf9-f002-4d66-a860-934c4c2bd4cd
STEP: Creating a pod to test consume configMaps
Sep 20 01:55:06.493: INFO: Waiting up to 5m0s for pod "pod-configmaps-6f943411-089c-4844-aa92-6625a1c7430f" in namespace "configmap-4188" to be "success or failure"
Sep 20 01:55:06.498: INFO: Pod "pod-configmaps-6f943411-089c-4844-aa92-6625a1c7430f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.744453ms
Sep 20 01:55:08.505: INFO: Pod "pod-configmaps-6f943411-089c-4844-aa92-6625a1c7430f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011656103s
Sep 20 01:55:10.511: INFO: Pod "pod-configmaps-6f943411-089c-4844-aa92-6625a1c7430f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018125707s
STEP: Saw pod success
Sep 20 01:55:10.511: INFO: Pod "pod-configmaps-6f943411-089c-4844-aa92-6625a1c7430f" satisfied condition "success or failure"
Sep 20 01:55:10.518: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod pod-configmaps-6f943411-089c-4844-aa92-6625a1c7430f container configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 01:55:10.589: INFO: Waiting for pod pod-configmaps-6f943411-089c-4844-aa92-6625a1c7430f to disappear
Sep 20 01:55:10.596: INFO: Pod pod-configmaps-6f943411-089c-4844-aa92-6625a1c7430f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:55:10.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4188" for this suite.
Sep 20 01:55:16.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:55:16.819: INFO: namespace configmap-4188 deletion completed in 6.217900809s

• [SLOW TEST:10.515 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:55:16.821: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5640
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 01:55:17.350: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 01:55:19.368: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704541317, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704541317, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704541317, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704541317, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 01:55:22.401: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:55:22.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5640" for this suite.
Sep 20 01:55:34.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:55:35.090: INFO: namespace webhook-5640 deletion completed in 12.30458406s
STEP: Destroying namespace "webhook-5640-markers" for this suite.
Sep 20 01:55:41.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:55:41.331: INFO: namespace webhook-5640-markers deletion completed in 6.240499225s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:24.534 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:55:41.355: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9775
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 01:55:41.840: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 01:55:43.856: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704541341, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704541341, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704541341, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704541341, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 01:55:46.895: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 01:55:46.901: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2128-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:55:48.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9775" for this suite.
Sep 20 01:55:54.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:55:54.784: INFO: namespace webhook-9775 deletion completed in 6.225532401s
STEP: Destroying namespace "webhook-9775-markers" for this suite.
Sep 20 01:56:00.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:56:00.983: INFO: namespace webhook-9775-markers deletion completed in 6.199148755s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:19.663 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:56:01.020: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1333
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep 20 01:56:01.229: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1333 /api/v1/namespaces/watch-1333/configmaps/e2e-watch-test-label-changed 4a0ca0bc-c4d3-4a2a-9cca-afd938d3411c 82396 0 2019-09-20 01:56:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 20 01:56:01.229: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1333 /api/v1/namespaces/watch-1333/configmaps/e2e-watch-test-label-changed 4a0ca0bc-c4d3-4a2a-9cca-afd938d3411c 82397 0 2019-09-20 01:56:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep 20 01:56:01.230: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1333 /api/v1/namespaces/watch-1333/configmaps/e2e-watch-test-label-changed 4a0ca0bc-c4d3-4a2a-9cca-afd938d3411c 82398 0 2019-09-20 01:56:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
I0920 01:56:06.869939      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep 20 01:56:11.275: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1333 /api/v1/namespaces/watch-1333/configmaps/e2e-watch-test-label-changed 4a0ca0bc-c4d3-4a2a-9cca-afd938d3411c 82427 0 2019-09-20 01:56:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 20 01:56:11.276: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1333 /api/v1/namespaces/watch-1333/configmaps/e2e-watch-test-label-changed 4a0ca0bc-c4d3-4a2a-9cca-afd938d3411c 82428 0 2019-09-20 01:56:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Sep 20 01:56:11.276: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1333 /api/v1/namespaces/watch-1333/configmaps/e2e-watch-test-label-changed 4a0ca0bc-c4d3-4a2a-9cca-afd938d3411c 82429 0 2019-09-20 01:56:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:56:11.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1333" for this suite.
Sep 20 01:56:17.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:56:17.488: INFO: namespace watch-1333 deletion completed in 6.203078197s

• [SLOW TEST:16.468 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:56:17.490: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4108
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4108
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4108
STEP: creating replication controller externalsvc in namespace services-4108
I0920 01:56:17.709874      16 runners.go:184] Created replication controller with name: externalsvc, namespace: services-4108, replica count: 2
I0920 01:56:17.709985      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 01:56:17.710238      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 01:56:20.760344      16 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Sep 20 01:56:20.792: INFO: Creating new exec pod
Sep 20 01:56:24.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=services-4108 execpodcddqw -- /bin/sh -x -c nslookup nodeport-service'
Sep 20 01:56:25.915: INFO: stderr: "+ nslookup nodeport-service\n"
Sep 20 01:56:25.915: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nnodeport-service.services-4108.svc.cluster.local\tcanonical name = externalsvc.services-4108.svc.cluster.local.\nName:\texternalsvc.services-4108.svc.cluster.local\nAddress: 10.240.27.12\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4108, will wait for the garbage collector to delete the pods
I0920 01:56:25.920881      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 01:56:25.920930      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 01:56:25.983: INFO: Deleting ReplicationController externalsvc took: 12.256115ms
Sep 20 01:56:26.383: INFO: Terminating ReplicationController externalsvc pods took: 400.503868ms
I0920 01:56:26.383705      16 controller_utils.go:810] Ignoring inactive pod services-4108/externalsvc-zr94v in state Running, deletion time 2019-09-20 01:56:27 +0000 UTC
I0920 01:56:26.383766      16 controller_utils.go:810] Ignoring inactive pod services-4108/externalsvc-szmpb in state Running, deletion time 2019-09-20 01:56:27 +0000 UTC
Sep 20 01:56:36.206: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:56:36.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4108" for this suite.
Sep 20 01:56:42.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:56:42.453: INFO: namespace services-4108 deletion completed in 6.223726361s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:24.963 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:56:42.453: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4031
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 01:57:06.646: INFO: Container started at 2019-09-20 01:56:44 +0000 UTC, pod became ready at 2019-09-20 01:57:05 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:57:06.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4031" for this suite.
Sep 20 01:57:34.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:57:34.895: INFO: namespace container-probe-4031 deletion completed in 28.241252468s

• [SLOW TEST:52.442 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:57:34.896: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1265
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 01:57:35.343: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 01:57:37.360: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704541455, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704541455, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704541455, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704541455, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 01:57:40.384: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:57:40.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1265" for this suite.
Sep 20 01:57:46.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:57:46.981: INFO: namespace webhook-1265 deletion completed in 6.222439213s
STEP: Destroying namespace "webhook-1265-markers" for this suite.
Sep 20 01:57:53.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:57:53.186: INFO: namespace webhook-1265-markers deletion completed in 6.20557566s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.317 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:57:53.218: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7594
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 01:57:53.395: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 01:57:57.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7594" for this suite.
Sep 20 01:58:49.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 01:58:49.738: INFO: namespace pods-7594 deletion completed in 52.229901425s

• [SLOW TEST:56.520 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 01:58:49.748: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5771
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Sep 20 01:58:49.921: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 20 01:58:49.941: INFO: Waiting for terminating namespaces to be deleted...
Sep 20 01:58:49.951: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-kqpmz before test
Sep 20 01:58:49.978: INFO: node-local-dns-tppj5 from kube-system started at 2019-09-20 00:59:43 +0000 UTC (1 container statuses recorded)
Sep 20 01:58:49.978: INFO: 	Container node-cache ready: true, restart count 0
Sep 20 01:58:49.978: INFO: kube-proxy-xmcqx from kube-system started at 2019-09-19 20:33:51 +0000 UTC (1 container statuses recorded)
Sep 20 01:58:49.978: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 01:58:49.979: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-2ks25 from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 01:58:49.979: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 20 01:58:49.979: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 20 01:58:49.979: INFO: canal-pwz75 from kube-system started at 2019-09-19 20:33:51 +0000 UTC (2 container statuses recorded)
Sep 20 01:58:49.979: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 01:58:49.979: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 01:58:49.979: INFO: node-exporter-rtkgz from kube-system started at 2019-09-19 20:33:51 +0000 UTC (2 container statuses recorded)
Sep 20 01:58:49.979: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 01:58:49.979: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 01:58:49.979: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-ldlfc before test
Sep 20 01:58:50.077: INFO: node-local-dns-rj6pt from kube-system started at 2019-09-19 21:38:47 +0000 UTC (1 container statuses recorded)
Sep 20 01:58:50.078: INFO: 	Container node-cache ready: true, restart count 0
Sep 20 01:58:50.078: INFO: kube-proxy-zvhzd from kube-system started at 2019-09-19 20:33:57 +0000 UTC (1 container statuses recorded)
Sep 20 01:58:50.078: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 01:58:50.078: INFO: node-exporter-47p98 from kube-system started at 2019-09-19 20:33:57 +0000 UTC (2 container statuses recorded)
Sep 20 01:58:50.078: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 01:58:50.078: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 01:58:50.078: INFO: canal-hf9f7 from kube-system started at 2019-09-19 20:33:57 +0000 UTC (2 container statuses recorded)
Sep 20 01:58:50.078: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 01:58:50.078: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 01:58:50.078: INFO: sonobuoy-e2e-job-8f5b7ab9f6954bcb from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 01:58:50.078: INFO: 	Container e2e ready: true, restart count 0
Sep 20 01:58:50.078: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 01:58:50.078: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-djczp from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 01:58:50.078: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 20 01:58:50.078: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 20 01:58:50.078: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-lmjf4 before test
Sep 20 01:58:50.169: INFO: canal-s8rlz from kube-system started at 2019-09-19 20:34:06 +0000 UTC (2 container statuses recorded)
Sep 20 01:58:50.169: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 01:58:50.169: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 01:58:50.169: INFO: node-exporter-65hp9 from kube-system started at 2019-09-19 20:34:06 +0000 UTC (2 container statuses recorded)
Sep 20 01:58:50.169: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 01:58:50.169: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 01:58:50.169: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-n9zxp from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 01:58:50.169: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 20 01:58:50.169: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 20 01:58:50.170: INFO: openvpn-client-84ccd8596d-s64vh from kube-system started at 2019-09-20 00:44:53 +0000 UTC (2 container statuses recorded)
Sep 20 01:58:50.170: INFO: 	Container dnat-controller ready: true, restart count 0
Sep 20 01:58:50.170: INFO: 	Container openvpn-client ready: true, restart count 0
Sep 20 01:58:50.170: INFO: kube-proxy-mhstl from kube-system started at 2019-09-19 20:34:06 +0000 UTC (1 container statuses recorded)
Sep 20 01:58:50.170: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 01:58:50.170: INFO: node-local-dns-bx7jl from kube-system started at 2019-09-19 20:34:26 +0000 UTC (1 container statuses recorded)
Sep 20 01:58:50.170: INFO: 	Container node-cache ready: true, restart count 0
Sep 20 01:58:50.170: INFO: coredns-57f944bd9f-rp6nc from kube-system started at 2019-09-19 21:35:33 +0000 UTC (1 container statuses recorded)
Sep 20 01:58:50.170: INFO: 	Container coredns ready: true, restart count 0
Sep 20 01:58:50.170: INFO: kubernetes-dashboard-7d5fb85f7f-jqgq6 from kube-system started at 2019-09-20 00:59:15 +0000 UTC (1 container statuses recorded)
Sep 20 01:58:50.170: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Sep 20 01:58:50.170: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-mxxqz before test
Sep 20 01:58:50.263: INFO: kube-proxy-dtrjq from kube-system started at 2019-09-19 20:34:07 +0000 UTC (1 container statuses recorded)
Sep 20 01:58:50.264: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 01:58:50.264: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-7brnz from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 01:58:50.265: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 20 01:58:50.265: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 20 01:58:50.265: INFO: canal-kdmx8 from kube-system started at 2019-09-19 20:34:07 +0000 UTC (2 container statuses recorded)
Sep 20 01:58:50.266: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 01:58:50.266: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 01:58:50.266: INFO: node-exporter-xq8tf from kube-system started at 2019-09-19 20:34:07 +0000 UTC (2 container statuses recorded)
Sep 20 01:58:50.267: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 01:58:50.267: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 01:58:50.267: INFO: node-local-dns-bxghq from kube-system started at 2019-09-19 20:34:27 +0000 UTC (1 container statuses recorded)
Sep 20 01:58:50.267: INFO: 	Container node-cache ready: true, restart count 0
Sep 20 01:58:50.268: INFO: sonobuoy from sonobuoy started at 2019-09-19 20:40:21 +0000 UTC (1 container statuses recorded)
Sep 20 01:58:50.268: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 20 01:58:50.268: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-r6tnq before test
Sep 20 01:58:50.346: INFO: kube-proxy-p4g4f from kube-system started at 2019-09-19 20:33:57 +0000 UTC (1 container statuses recorded)
Sep 20 01:58:50.347: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 01:58:50.347: INFO: coredns-57f944bd9f-6qlrr from kube-system started at 2019-09-20 00:59:15 +0000 UTC (1 container statuses recorded)
Sep 20 01:58:50.348: INFO: 	Container coredns ready: true, restart count 0
Sep 20 01:58:50.348: INFO: canal-8lwhv from kube-system started at 2019-09-19 20:33:57 +0000 UTC (2 container statuses recorded)
Sep 20 01:58:50.348: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 01:58:50.349: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 01:58:50.349: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-bph2c from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 01:58:50.349: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 20 01:58:50.350: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 20 01:58:50.350: INFO: node-exporter-ld8fb from kube-system started at 2019-09-19 20:33:57 +0000 UTC (2 container statuses recorded)
Sep 20 01:58:50.350: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 01:58:50.351: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 01:58:50.351: INFO: node-local-dns-2krxh from kube-system started at 2019-09-20 00:44:53 +0000 UTC (1 container statuses recorded)
Sep 20 01:58:50.351: INFO: 	Container node-cache ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-f99502d3-0206-489f-a7de-ff2208ad6893 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
I0920 02:00:30.708653      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
STEP: removing the label kubernetes.io/e2e-f99502d3-0206-489f-a7de-ff2208ad6893 off the node worker-wqshf-7859ffd555-kqpmz
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f99502d3-0206-489f-a7de-ff2208ad6893
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:03:58.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5771" for this suite.
Sep 20 02:04:18.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:04:18.813: INFO: namespace sched-pred-5771 deletion completed in 20.287797033s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
I0920 02:04:18.813454      16 request.go:706] Error in request: resource name may not be empty

• [SLOW TEST:329.065 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:04:18.815: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4555
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:04:26.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4555" for this suite.
Sep 20 02:04:32.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:04:32.274: INFO: namespace resourcequota-4555 deletion completed in 6.23476152s

• [SLOW TEST:13.459 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:04:32.277: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7418
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 02:04:32.945: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 02:04:34.960: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704541872, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704541872, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704541872, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704541872, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 02:04:37.986: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:04:38.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7418" for this suite.
Sep 20 02:04:44.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:04:44.661: INFO: namespace webhook-7418 deletion completed in 6.304025573s
STEP: Destroying namespace "webhook-7418-markers" for this suite.
Sep 20 02:04:50.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:04:50.907: INFO: namespace webhook-7418-markers deletion completed in 6.245736349s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.656 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:04:50.936: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8797
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-8797
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8797 to expose endpoints map[]
Sep 20 02:04:51.132: INFO: successfully validated that service endpoint-test2 in namespace services-8797 exposes endpoints map[] (4.970793ms elapsed)
STEP: Creating pod pod1 in namespace services-8797
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8797 to expose endpoints map[pod1:[80]]
Sep 20 02:04:54.188: INFO: successfully validated that service endpoint-test2 in namespace services-8797 exposes endpoints map[pod1:[80]] (3.043521589s elapsed)
STEP: Creating pod pod2 in namespace services-8797
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8797 to expose endpoints map[pod1:[80] pod2:[80]]
Sep 20 02:04:57.274: INFO: successfully validated that service endpoint-test2 in namespace services-8797 exposes endpoints map[pod1:[80] pod2:[80]] (3.075801832s elapsed)
STEP: Deleting pod pod1 in namespace services-8797
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8797 to expose endpoints map[pod2:[80]]
Sep 20 02:04:57.296: INFO: successfully validated that service endpoint-test2 in namespace services-8797 exposes endpoints map[pod2:[80]] (10.957052ms elapsed)
STEP: Deleting pod pod2 in namespace services-8797
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8797 to expose endpoints map[]
Sep 20 02:04:57.313: INFO: successfully validated that service endpoint-test2 in namespace services-8797 exposes endpoints map[] (6.979725ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:04:57.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8797" for this suite.
Sep 20 02:05:09.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:05:09.583: INFO: namespace services-8797 deletion completed in 12.230245897s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:18.648 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:05:09.591: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4069
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 02:05:10.509: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 02:05:12.531: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704541910, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704541910, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704541910, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704541910, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 02:05:15.550: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:05:16.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4069" for this suite.
Sep 20 02:05:22.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:05:22.308: INFO: namespace webhook-4069 deletion completed in 6.232355936s
STEP: Destroying namespace "webhook-4069-markers" for this suite.
Sep 20 02:05:28.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:05:28.519: INFO: namespace webhook-4069-markers deletion completed in 6.210417635s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.960 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:05:28.556: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3136
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-3136
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 20 02:05:28.722: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 20 02:05:52.901: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.108:8080/dial?request=hostName&protocol=http&host=172.25.2.19&port=8080&tries=1'] Namespace:pod-network-test-3136 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 02:05:52.901: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 02:05:53.540: INFO: Waiting for endpoints: map[]
Sep 20 02:05:53.546: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.108:8080/dial?request=hostName&protocol=http&host=172.25.4.25&port=8080&tries=1'] Namespace:pod-network-test-3136 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 02:05:53.546: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 02:05:54.193: INFO: Waiting for endpoints: map[]
Sep 20 02:05:54.199: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.108:8080/dial?request=hostName&protocol=http&host=172.25.0.107&port=8080&tries=1'] Namespace:pod-network-test-3136 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 02:05:54.199: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 02:05:54.858: INFO: Waiting for endpoints: map[]
Sep 20 02:05:54.863: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.108:8080/dial?request=hostName&protocol=http&host=172.25.3.73&port=8080&tries=1'] Namespace:pod-network-test-3136 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 02:05:54.864: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 02:05:55.543: INFO: Waiting for endpoints: map[]
Sep 20 02:05:55.551: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.108:8080/dial?request=hostName&protocol=http&host=172.25.1.96&port=8080&tries=1'] Namespace:pod-network-test-3136 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 02:05:55.551: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 02:05:56.345: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:05:56.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3136" for this suite.
I0920 02:05:57.881482      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
Sep 20 02:06:08.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:06:08.577: INFO: namespace pod-network-test-3136 deletion completed in 12.223188569s

• [SLOW TEST:40.022 seconds]
[sig-network] Networking
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:06:08.585: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5183
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:06:12.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5183" for this suite.
I0920 02:06:55.720471      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
Sep 20 02:06:56.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:06:57.085: INFO: namespace kubelet-test-5183 deletion completed in 44.2355783s

• [SLOW TEST:48.501 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:06:57.085: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9452
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 02:06:57.310: INFO: Waiting up to 5m0s for pod "busybox-user-65534-d5d75979-1656-4512-b858-eec2b57dbe99" in namespace "security-context-test-9452" to be "success or failure"
Sep 20 02:06:57.320: INFO: Pod "busybox-user-65534-d5d75979-1656-4512-b858-eec2b57dbe99": Phase="Pending", Reason="", readiness=false. Elapsed: 9.914317ms
Sep 20 02:06:59.331: INFO: Pod "busybox-user-65534-d5d75979-1656-4512-b858-eec2b57dbe99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0210004s
Sep 20 02:07:01.338: INFO: Pod "busybox-user-65534-d5d75979-1656-4512-b858-eec2b57dbe99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027763915s
Sep 20 02:07:01.338: INFO: Pod "busybox-user-65534-d5d75979-1656-4512-b858-eec2b57dbe99" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:07:01.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9452" for this suite.
Sep 20 02:07:07.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:07:07.571: INFO: namespace security-context-test-9452 deletion completed in 6.226872041s

• [SLOW TEST:10.486 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:07:07.574: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6680
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-6680
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 20 02:07:07.742: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 20 02:07:33.943: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.0.110 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6680 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 02:07:33.943: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 02:07:35.591: INFO: Found all expected endpoints: [netserver-0]
Sep 20 02:07:35.596: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.4.26 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6680 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 02:07:35.597: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 02:07:37.361: INFO: Found all expected endpoints: [netserver-1]
Sep 20 02:07:37.366: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.3.75 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6680 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 02:07:37.367: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 02:07:38.996: INFO: Found all expected endpoints: [netserver-2]
Sep 20 02:07:39.005: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.2.20 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6680 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 02:07:39.005: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 02:07:40.665: INFO: Found all expected endpoints: [netserver-3]
Sep 20 02:07:40.673: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.1.97 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6680 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 02:07:40.673: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 02:07:42.326: INFO: Found all expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:07:42.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6680" for this suite.
Sep 20 02:07:54.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:07:54.585: INFO: namespace pod-network-test-6680 deletion completed in 12.248361185s

• [SLOW TEST:47.012 seconds]
[sig-network] Networking
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:07:54.599: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5063
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 20 02:07:54.783: INFO: Waiting up to 5m0s for pod "pod-b8a12466-cc3b-4e29-9a91-465339fc9c8f" in namespace "emptydir-5063" to be "success or failure"
Sep 20 02:07:54.788: INFO: Pod "pod-b8a12466-cc3b-4e29-9a91-465339fc9c8f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.318099ms
Sep 20 02:07:56.794: INFO: Pod "pod-b8a12466-cc3b-4e29-9a91-465339fc9c8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010906768s
Sep 20 02:07:58.802: INFO: Pod "pod-b8a12466-cc3b-4e29-9a91-465339fc9c8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018222712s
STEP: Saw pod success
Sep 20 02:07:58.802: INFO: Pod "pod-b8a12466-cc3b-4e29-9a91-465339fc9c8f" satisfied condition "success or failure"
Sep 20 02:07:58.807: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod pod-b8a12466-cc3b-4e29-9a91-465339fc9c8f container test-container: <nil>
STEP: delete the pod
Sep 20 02:07:58.847: INFO: Waiting for pod pod-b8a12466-cc3b-4e29-9a91-465339fc9c8f to disappear
Sep 20 02:07:58.851: INFO: Pod pod-b8a12466-cc3b-4e29-9a91-465339fc9c8f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:07:58.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5063" for this suite.
Sep 20 02:08:04.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:08:05.090: INFO: namespace emptydir-5063 deletion completed in 6.232016835s

• [SLOW TEST:10.493 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:08:05.099: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4317
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:08:18.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4317" for this suite.
Sep 20 02:08:24.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:08:24.628: INFO: namespace resourcequota-4317 deletion completed in 6.236528091s

• [SLOW TEST:19.530 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:08:24.630: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5488
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 02:08:24.822: INFO: Waiting up to 5m0s for pod "downwardapi-volume-69cf514c-1052-4b23-9141-9e5fb241d23f" in namespace "downward-api-5488" to be "success or failure"
Sep 20 02:08:24.837: INFO: Pod "downwardapi-volume-69cf514c-1052-4b23-9141-9e5fb241d23f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.514966ms
Sep 20 02:08:26.843: INFO: Pod "downwardapi-volume-69cf514c-1052-4b23-9141-9e5fb241d23f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018645325s
Sep 20 02:08:28.849: INFO: Pod "downwardapi-volume-69cf514c-1052-4b23-9141-9e5fb241d23f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02489905s
STEP: Saw pod success
Sep 20 02:08:28.849: INFO: Pod "downwardapi-volume-69cf514c-1052-4b23-9141-9e5fb241d23f" satisfied condition "success or failure"
Sep 20 02:08:28.854: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod downwardapi-volume-69cf514c-1052-4b23-9141-9e5fb241d23f container client-container: <nil>
STEP: delete the pod
Sep 20 02:08:28.931: INFO: Waiting for pod downwardapi-volume-69cf514c-1052-4b23-9141-9e5fb241d23f to disappear
Sep 20 02:08:28.935: INFO: Pod downwardapi-volume-69cf514c-1052-4b23-9141-9e5fb241d23f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:08:28.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5488" for this suite.
Sep 20 02:08:34.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:08:35.167: INFO: namespace downward-api-5488 deletion completed in 6.224371958s

• [SLOW TEST:10.538 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:08:35.168: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3558
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 20 02:08:38.424: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:08:38.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3558" for this suite.
Sep 20 02:08:44.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:08:44.681: INFO: namespace container-runtime-3558 deletion completed in 6.230515678s

• [SLOW TEST:9.513 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:08:44.682: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9620
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-681
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7220
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:09:00.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9620" for this suite.
Sep 20 02:09:06.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:09:06.468: INFO: namespace namespaces-9620 deletion completed in 6.218730674s
STEP: Destroying namespace "nsdeletetest-681" for this suite.
Sep 20 02:09:06.474: INFO: Namespace nsdeletetest-681 was already deleted
STEP: Destroying namespace "nsdeletetest-7220" for this suite.
Sep 20 02:09:12.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:09:12.682: INFO: namespace nsdeletetest-7220 deletion completed in 6.208383849s

• [SLOW TEST:28.001 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:09:12.683: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8435
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Sep 20 02:09:12.855: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Sep 20 02:09:25.679: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 02:09:28.626: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:09:41.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8435" for this suite.
Sep 20 02:09:47.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:09:47.342: INFO: namespace crd-publish-openapi-8435 deletion completed in 6.254299905s

• [SLOW TEST:34.660 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:09:47.343: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2599
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-632d8b82-dd7f-4046-bd09-ede09a709ab8 in namespace container-probe-2599
Sep 20 02:09:51.545: INFO: Started pod busybox-632d8b82-dd7f-4046-bd09-ede09a709ab8 in namespace container-probe-2599
STEP: checking the pod's current state and verifying that restartCount is present
Sep 20 02:09:51.553: INFO: Initial restart count of pod busybox-632d8b82-dd7f-4046-bd09-ede09a709ab8 is 0
I0920 02:11:57.897793      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:13:52.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2599" for this suite.
Sep 20 02:13:58.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:13:58.684: INFO: namespace container-probe-2599 deletion completed in 6.241148139s

• [SLOW TEST:251.341 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:13:58.685: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1680
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0920 02:14:08.910712      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 20 02:14:08.910: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:14:08.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1680" for this suite.
Sep 20 02:14:14.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:14:15.161: INFO: namespace gc-1680 deletion completed in 6.242836197s

• [SLOW TEST:16.476 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:14:15.162: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6853
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-6853
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 20 02:14:15.326: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 20 02:14:41.516: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.1.102:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6853 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 02:14:41.516: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 02:14:42.166: INFO: Found all expected endpoints: [netserver-0]
Sep 20 02:14:42.172: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.4.27:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6853 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 02:14:42.172: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 02:14:42.881: INFO: Found all expected endpoints: [netserver-1]
Sep 20 02:14:42.887: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.3.77:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6853 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 02:14:42.887: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 02:14:43.556: INFO: Found all expected endpoints: [netserver-2]
Sep 20 02:14:43.562: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.0.114:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6853 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 02:14:43.562: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 02:14:44.249: INFO: Found all expected endpoints: [netserver-3]
Sep 20 02:14:44.257: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.2.21:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6853 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 20 02:14:44.257: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 02:14:44.975: INFO: Found all expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:14:44.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6853" for this suite.
Sep 20 02:14:57.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:14:57.235: INFO: namespace pod-network-test-6853 deletion completed in 12.254416357s

• [SLOW TEST:42.074 seconds]
[sig-network] Networking
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:14:57.242: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4508
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Sep 20 02:14:57.412: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
Sep 20 02:15:00.996: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
I0920 02:15:06.732498      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:15:13.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4508" for this suite.
Sep 20 02:15:19.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:15:20.191: INFO: namespace crd-publish-openapi-4508 deletion completed in 6.217313189s

• [SLOW TEST:22.949 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:15:20.196: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9745
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 02:15:20.393: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep 20 02:15:25.401: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 20 02:15:25.401: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Sep 20 02:15:29.447: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9745 /apis/apps/v1/namespaces/deployment-9745/deployments/test-cleanup-deployment 18855124-c645-4866-bd91-9f6bb7eac1b2 86927 1 2019-09-20 02:15:25 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0037ebe88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-09-20 02:15:25 +0000 UTC,LastTransitionTime:2019-09-20 02:15:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-65db99849b" has successfully progressed.,LastUpdateTime:2019-09-20 02:15:27 +0000 UTC,LastTransitionTime:2019-09-20 02:15:25 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 20 02:15:29.454: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-9745 /apis/apps/v1/namespaces/deployment-9745/replicasets/test-cleanup-deployment-65db99849b 324c87b9-704a-4688-b022-00489de8f0c5 86917 1 2019-09-20 02:15:25 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 18855124-c645-4866-bd91-9f6bb7eac1b2 0xc004496327 0xc004496328}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0044963a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 20 02:15:29.460: INFO: Pod "test-cleanup-deployment-65db99849b-zqmsv" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-65db99849b-zqmsv test-cleanup-deployment-65db99849b- deployment-9745 /api/v1/namespaces/deployment-9745/pods/test-cleanup-deployment-65db99849b-zqmsv e12ae89c-7ad3-4db7-9b26-e8dfab6626c9 86916 0 2019-09-20 02:15:25 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[cni.projectcalico.org/podIP:172.25.1.103/32] [{apps/v1 ReplicaSet test-cleanup-deployment-65db99849b 324c87b9-704a-4688-b022-00489de8f0c5 0xc004496817 0xc004496818}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6t8kx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6t8kx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6t8kx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-r6tnq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 02:15:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 02:15:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 02:15:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 02:15:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.207,PodIP:172.25.1.103,StartTime:2019-09-20 02:15:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 02:15:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://905c8a1af781e3b452f7413d702ba403d7a27f60bb7b0038129f38cbad42829d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:15:29.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9745" for this suite.
Sep 20 02:15:35.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:15:35.670: INFO: namespace deployment-9745 deletion completed in 6.202784429s

• [SLOW TEST:15.475 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:15:35.678: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1403
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 02:15:39.899: INFO: Waiting up to 5m0s for pod "client-envvars-75ace70f-294d-4746-a7d7-69c5f0c56d6b" in namespace "pods-1403" to be "success or failure"
Sep 20 02:15:39.910: INFO: Pod "client-envvars-75ace70f-294d-4746-a7d7-69c5f0c56d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.542947ms
Sep 20 02:15:41.917: INFO: Pod "client-envvars-75ace70f-294d-4746-a7d7-69c5f0c56d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017178628s
Sep 20 02:15:43.925: INFO: Pod "client-envvars-75ace70f-294d-4746-a7d7-69c5f0c56d6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025779042s
STEP: Saw pod success
Sep 20 02:15:43.925: INFO: Pod "client-envvars-75ace70f-294d-4746-a7d7-69c5f0c56d6b" satisfied condition "success or failure"
Sep 20 02:15:43.930: INFO: Trying to get logs from node worker-wqshf-7859ffd555-lmjf4 pod client-envvars-75ace70f-294d-4746-a7d7-69c5f0c56d6b container env3cont: <nil>
STEP: delete the pod
Sep 20 02:15:43.982: INFO: Waiting for pod client-envvars-75ace70f-294d-4746-a7d7-69c5f0c56d6b to disappear
Sep 20 02:15:43.990: INFO: Pod client-envvars-75ace70f-294d-4746-a7d7-69c5f0c56d6b no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:15:43.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1403" for this suite.
Sep 20 02:16:12.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:16:12.202: INFO: namespace pods-1403 deletion completed in 28.204757514s

• [SLOW TEST:36.525 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:16:12.205: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8239
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 02:16:12.989: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 02:16:15.008: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704542573, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704542573, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704542573, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704542573, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 02:16:18.040: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Sep 20 02:16:18.281: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:16:18.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8239" for this suite.
Sep 20 02:16:24.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:16:24.621: INFO: namespace webhook-8239 deletion completed in 6.225628192s
STEP: Destroying namespace "webhook-8239-markers" for this suite.
Sep 20 02:16:30.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:16:30.841: INFO: namespace webhook-8239-markers deletion completed in 6.220214881s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.659 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:16:30.865: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9723
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-2aa6502c-30cb-4d46-a87e-cf862d07acdd
STEP: Creating a pod to test consume secrets
Sep 20 02:16:31.056: INFO: Waiting up to 5m0s for pod "pod-secrets-7b15083f-341f-4c0b-ad8e-36836c2ed87f" in namespace "secrets-9723" to be "success or failure"
Sep 20 02:16:31.063: INFO: Pod "pod-secrets-7b15083f-341f-4c0b-ad8e-36836c2ed87f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.315138ms
Sep 20 02:16:33.069: INFO: Pod "pod-secrets-7b15083f-341f-4c0b-ad8e-36836c2ed87f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013614352s
Sep 20 02:16:35.078: INFO: Pod "pod-secrets-7b15083f-341f-4c0b-ad8e-36836c2ed87f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022428714s
STEP: Saw pod success
Sep 20 02:16:35.078: INFO: Pod "pod-secrets-7b15083f-341f-4c0b-ad8e-36836c2ed87f" satisfied condition "success or failure"
Sep 20 02:16:35.087: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod pod-secrets-7b15083f-341f-4c0b-ad8e-36836c2ed87f container secret-volume-test: <nil>
STEP: delete the pod
Sep 20 02:16:35.133: INFO: Waiting for pod pod-secrets-7b15083f-341f-4c0b-ad8e-36836c2ed87f to disappear
Sep 20 02:16:35.138: INFO: Pod pod-secrets-7b15083f-341f-4c0b-ad8e-36836c2ed87f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:16:35.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9723" for this suite.
Sep 20 02:16:41.166: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:16:41.357: INFO: namespace secrets-9723 deletion completed in 6.212385615s

• [SLOW TEST:10.492 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:16:41.364: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3620
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 02:16:41.541: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 20 02:16:44.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 --namespace=crd-publish-openapi-3620 create -f -'
Sep 20 02:16:45.069: INFO: stderr: ""
Sep 20 02:16:45.069: INFO: stdout: "e2e-test-crd-publish-openapi-9184-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 20 02:16:45.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 --namespace=crd-publish-openapi-3620 delete e2e-test-crd-publish-openapi-9184-crds test-cr'
Sep 20 02:16:45.191: INFO: stderr: ""
Sep 20 02:16:45.191: INFO: stdout: "e2e-test-crd-publish-openapi-9184-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Sep 20 02:16:45.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 --namespace=crd-publish-openapi-3620 apply -f -'
Sep 20 02:16:45.372: INFO: stderr: ""
Sep 20 02:16:45.372: INFO: stdout: "e2e-test-crd-publish-openapi-9184-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 20 02:16:45.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 --namespace=crd-publish-openapi-3620 delete e2e-test-crd-publish-openapi-9184-crds test-cr'
Sep 20 02:16:45.509: INFO: stderr: ""
Sep 20 02:16:45.509: INFO: stdout: "e2e-test-crd-publish-openapi-9184-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Sep 20 02:16:45.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 explain e2e-test-crd-publish-openapi-9184-crds'
Sep 20 02:16:45.695: INFO: stderr: ""
Sep 20 02:16:45.695: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9184-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:16:48.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3620" for this suite.
Sep 20 02:16:54.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:16:54.887: INFO: namespace crd-publish-openapi-3620 deletion completed in 6.235969638s

• [SLOW TEST:13.524 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:16:54.891: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8832
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:17:00.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8832" for this suite.
Sep 20 02:17:06.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:17:06.551: INFO: namespace watch-8832 deletion completed in 6.293390397s

• [SLOW TEST:11.660 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:17:06.552: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-9572
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:17:10.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9572" for this suite.
Sep 20 02:17:16.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:17:17.027: INFO: namespace emptydir-wrapper-9572 deletion completed in 6.205712247s

• [SLOW TEST:10.475 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:17:17.030: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9512
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:17:21.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9512" for this suite.
Sep 20 02:18:05.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:18:05.491: INFO: namespace kubelet-test-9512 deletion completed in 44.221822381s

• [SLOW TEST:48.462 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:18:05.494: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-350
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Sep 20 02:18:05.665: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Sep 20 02:18:05.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 create -f - --namespace=kubectl-350'
Sep 20 02:18:05.952: INFO: stderr: ""
Sep 20 02:18:05.952: INFO: stdout: "service/redis-slave created\n"
Sep 20 02:18:05.952: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Sep 20 02:18:05.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 create -f - --namespace=kubectl-350'
Sep 20 02:18:06.142: INFO: stderr: ""
Sep 20 02:18:06.143: INFO: stdout: "service/redis-master created\n"
Sep 20 02:18:06.143: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep 20 02:18:06.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 create -f - --namespace=kubectl-350'
Sep 20 02:18:06.319: INFO: stderr: ""
Sep 20 02:18:06.319: INFO: stdout: "service/frontend created\n"
Sep 20 02:18:06.319: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Sep 20 02:18:06.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 create -f - --namespace=kubectl-350'
Sep 20 02:18:06.493: INFO: stderr: ""
Sep 20 02:18:06.493: INFO: stdout: "deployment.apps/frontend created\n"
Sep 20 02:18:06.493: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 20 02:18:06.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 create -f - --namespace=kubectl-350'
Sep 20 02:18:06.667: INFO: stderr: ""
Sep 20 02:18:06.667: INFO: stdout: "deployment.apps/redis-master created\n"
Sep 20 02:18:06.667: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Sep 20 02:18:06.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 create -f - --namespace=kubectl-350'
Sep 20 02:18:06.867: INFO: stderr: ""
Sep 20 02:18:06.867: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Sep 20 02:18:06.867: INFO: Waiting for all frontend pods to be Running.
I0920 02:18:06.868127      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 02:18:06.868153      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 02:18:26.919: INFO: Waiting for frontend to serve content.
Sep 20 02:18:27.019: INFO: Trying to add a new entry to the guestbook.
Sep 20 02:18:27.149: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Sep 20 02:18:27.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 delete --grace-period=0 --force -f - --namespace=kubectl-350'
Sep 20 02:18:27.435: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 02:18:27.435: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Sep 20 02:18:27.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 delete --grace-period=0 --force -f - --namespace=kubectl-350'
Sep 20 02:18:27.549: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 02:18:27.549: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep 20 02:18:27.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 delete --grace-period=0 --force -f - --namespace=kubectl-350'
Sep 20 02:18:27.654: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 02:18:27.654: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 20 02:18:27.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 delete --grace-period=0 --force -f - --namespace=kubectl-350'
Sep 20 02:18:27.745: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 02:18:27.745: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 20 02:18:27.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 delete --grace-period=0 --force -f - --namespace=kubectl-350'
Sep 20 02:18:27.839: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 02:18:27.839: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep 20 02:18:27.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 delete --grace-period=0 --force -f - --namespace=kubectl-350'
Sep 20 02:18:27.928: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 02:18:27.928: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:18:27.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-350" for this suite.
Sep 20 02:18:55.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:18:56.176: INFO: namespace kubectl-350 deletion completed in 28.235627536s

• [SLOW TEST:50.683 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:18:56.188: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6856
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 02:18:56.368: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ceeee532-e37b-4540-908b-036fa8439ea8" in namespace "projected-6856" to be "success or failure"
Sep 20 02:18:56.379: INFO: Pod "downwardapi-volume-ceeee532-e37b-4540-908b-036fa8439ea8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.970142ms
Sep 20 02:18:58.385: INFO: Pod "downwardapi-volume-ceeee532-e37b-4540-908b-036fa8439ea8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016235323s
Sep 20 02:19:00.392: INFO: Pod "downwardapi-volume-ceeee532-e37b-4540-908b-036fa8439ea8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022994806s
STEP: Saw pod success
Sep 20 02:19:00.392: INFO: Pod "downwardapi-volume-ceeee532-e37b-4540-908b-036fa8439ea8" satisfied condition "success or failure"
Sep 20 02:19:00.399: INFO: Trying to get logs from node worker-wqshf-7859ffd555-lmjf4 pod downwardapi-volume-ceeee532-e37b-4540-908b-036fa8439ea8 container client-container: <nil>
STEP: delete the pod
Sep 20 02:19:00.482: INFO: Waiting for pod downwardapi-volume-ceeee532-e37b-4540-908b-036fa8439ea8 to disappear
Sep 20 02:19:00.487: INFO: Pod downwardapi-volume-ceeee532-e37b-4540-908b-036fa8439ea8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:19:00.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6856" for this suite.
Sep 20 02:19:06.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:19:06.719: INFO: namespace projected-6856 deletion completed in 6.226683524s

• [SLOW TEST:10.532 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:19:06.723: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2259
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep 20 02:19:11.448: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2259 pod-service-account-498590b6-362e-4e4d-87ce-2158d076b8e2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep 20 02:19:12.274: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2259 pod-service-account-498590b6-362e-4e4d-87ce-2158d076b8e2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep 20 02:19:12.964: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2259 pod-service-account-498590b6-362e-4e4d-87ce-2158d076b8e2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:19:13.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2259" for this suite.
Sep 20 02:19:19.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:19:19.886: INFO: namespace svcaccounts-2259 deletion completed in 6.226603696s

• [SLOW TEST:13.164 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:19:19.887: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7639
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-d0d54efc-30d0-4c5b-a355-20ba622ed6ed
STEP: Creating secret with name s-test-opt-upd-3ef50efd-34ca-4b64-b9cd-fd0485e1b0c9
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-d0d54efc-30d0-4c5b-a355-20ba622ed6ed
STEP: Updating secret s-test-opt-upd-3ef50efd-34ca-4b64-b9cd-fd0485e1b0c9
STEP: Creating secret with name s-test-opt-create-7770e2ff-72b4-46d9-a4b1-e1a3ad0d6e61
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:19:26.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7639" for this suite.
Sep 20 02:19:38.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:19:38.844: INFO: namespace projected-7639 deletion completed in 12.21803371s

• [SLOW TEST:18.957 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:19:38.845: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-10
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Sep 20 02:19:39.035: INFO: Waiting up to 5m0s for pod "downward-api-a027e1b3-88dd-4964-8789-f887d975a713" in namespace "downward-api-10" to be "success or failure"
Sep 20 02:19:39.043: INFO: Pod "downward-api-a027e1b3-88dd-4964-8789-f887d975a713": Phase="Pending", Reason="", readiness=false. Elapsed: 7.108852ms
Sep 20 02:19:41.048: INFO: Pod "downward-api-a027e1b3-88dd-4964-8789-f887d975a713": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012645344s
Sep 20 02:19:43.056: INFO: Pod "downward-api-a027e1b3-88dd-4964-8789-f887d975a713": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020598686s
STEP: Saw pod success
Sep 20 02:19:43.057: INFO: Pod "downward-api-a027e1b3-88dd-4964-8789-f887d975a713" satisfied condition "success or failure"
Sep 20 02:19:43.067: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod downward-api-a027e1b3-88dd-4964-8789-f887d975a713 container dapi-container: <nil>
STEP: delete the pod
Sep 20 02:19:43.110: INFO: Waiting for pod downward-api-a027e1b3-88dd-4964-8789-f887d975a713 to disappear
Sep 20 02:19:43.118: INFO: Pod downward-api-a027e1b3-88dd-4964-8789-f887d975a713 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:19:43.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-10" for this suite.
Sep 20 02:19:49.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:19:49.396: INFO: namespace downward-api-10 deletion completed in 6.269640532s

• [SLOW TEST:10.551 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:19:49.406: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3665
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:19:53.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3665" for this suite.
Sep 20 02:20:37.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:20:37.968: INFO: namespace kubelet-test-3665 deletion completed in 44.323217808s

• [SLOW TEST:48.561 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:20:37.968: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6633
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-bwnc
STEP: Creating a pod to test atomic-volume-subpath
Sep 20 02:20:38.162: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-bwnc" in namespace "subpath-6633" to be "success or failure"
Sep 20 02:20:38.167: INFO: Pod "pod-subpath-test-configmap-bwnc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.689492ms
Sep 20 02:20:40.173: INFO: Pod "pod-subpath-test-configmap-bwnc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010417869s
Sep 20 02:20:42.197: INFO: Pod "pod-subpath-test-configmap-bwnc": Phase="Running", Reason="", readiness=true. Elapsed: 4.034621408s
Sep 20 02:20:44.205: INFO: Pod "pod-subpath-test-configmap-bwnc": Phase="Running", Reason="", readiness=true. Elapsed: 6.043314863s
Sep 20 02:20:46.214: INFO: Pod "pod-subpath-test-configmap-bwnc": Phase="Running", Reason="", readiness=true. Elapsed: 8.051675509s
I0920 02:20:46.906501      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
Sep 20 02:20:48.220: INFO: Pod "pod-subpath-test-configmap-bwnc": Phase="Running", Reason="", readiness=true. Elapsed: 10.058221777s
Sep 20 02:20:50.229: INFO: Pod "pod-subpath-test-configmap-bwnc": Phase="Running", Reason="", readiness=true. Elapsed: 12.066629403s
Sep 20 02:20:52.301: INFO: Pod "pod-subpath-test-configmap-bwnc": Phase="Running", Reason="", readiness=true. Elapsed: 14.138655147s
Sep 20 02:20:54.307: INFO: Pod "pod-subpath-test-configmap-bwnc": Phase="Running", Reason="", readiness=true. Elapsed: 16.144724169s
Sep 20 02:20:56.314: INFO: Pod "pod-subpath-test-configmap-bwnc": Phase="Running", Reason="", readiness=true. Elapsed: 18.152016685s
Sep 20 02:20:58.326: INFO: Pod "pod-subpath-test-configmap-bwnc": Phase="Running", Reason="", readiness=true. Elapsed: 20.163942691s
Sep 20 02:21:00.334: INFO: Pod "pod-subpath-test-configmap-bwnc": Phase="Running", Reason="", readiness=true. Elapsed: 22.171746401s
Sep 20 02:21:02.343: INFO: Pod "pod-subpath-test-configmap-bwnc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.1803521s
STEP: Saw pod success
Sep 20 02:21:02.343: INFO: Pod "pod-subpath-test-configmap-bwnc" satisfied condition "success or failure"
Sep 20 02:21:02.349: INFO: Trying to get logs from node worker-wqshf-7859ffd555-lmjf4 pod pod-subpath-test-configmap-bwnc container test-container-subpath-configmap-bwnc: <nil>
STEP: delete the pod
Sep 20 02:21:30.337: INFO: Waiting for pod pod-subpath-test-configmap-bwnc to disappear
Sep 20 02:21:30.343: INFO: Pod pod-subpath-test-configmap-bwnc no longer exists
STEP: Deleting pod pod-subpath-test-configmap-bwnc
Sep 20 02:21:30.343: INFO: Deleting pod "pod-subpath-test-configmap-bwnc" in namespace "subpath-6633"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:21:30.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6633" for this suite.
Sep 20 02:21:36.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:21:36.642: INFO: namespace subpath-6633 deletion completed in 6.287805785s

• [SLOW TEST:58.674 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:21:36.650: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-813
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Sep 20 02:21:36.842: INFO: Waiting up to 5m0s for pod "downward-api-72363e54-2916-4105-8fab-6dcc5a5bcc60" in namespace "downward-api-813" to be "success or failure"
Sep 20 02:21:36.847: INFO: Pod "downward-api-72363e54-2916-4105-8fab-6dcc5a5bcc60": Phase="Pending", Reason="", readiness=false. Elapsed: 5.121309ms
Sep 20 02:21:38.896: INFO: Pod "downward-api-72363e54-2916-4105-8fab-6dcc5a5bcc60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05425486s
Sep 20 02:21:40.904: INFO: Pod "downward-api-72363e54-2916-4105-8fab-6dcc5a5bcc60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062172069s
STEP: Saw pod success
Sep 20 02:21:40.904: INFO: Pod "downward-api-72363e54-2916-4105-8fab-6dcc5a5bcc60" satisfied condition "success or failure"
Sep 20 02:21:40.912: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod downward-api-72363e54-2916-4105-8fab-6dcc5a5bcc60 container dapi-container: <nil>
STEP: delete the pod
Sep 20 02:21:40.998: INFO: Waiting for pod downward-api-72363e54-2916-4105-8fab-6dcc5a5bcc60 to disappear
Sep 20 02:21:41.003: INFO: Pod downward-api-72363e54-2916-4105-8fab-6dcc5a5bcc60 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:21:41.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-813" for this suite.
Sep 20 02:21:47.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:21:47.243: INFO: namespace downward-api-813 deletion completed in 6.232118456s

• [SLOW TEST:10.594 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:21:47.245: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5355
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 02:21:47.436: INFO: Waiting up to 5m0s for pod "downwardapi-volume-94cd9a2b-4e6e-40a4-9f02-53aba2dc9ec6" in namespace "projected-5355" to be "success or failure"
Sep 20 02:21:47.443: INFO: Pod "downwardapi-volume-94cd9a2b-4e6e-40a4-9f02-53aba2dc9ec6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.444312ms
Sep 20 02:21:49.451: INFO: Pod "downwardapi-volume-94cd9a2b-4e6e-40a4-9f02-53aba2dc9ec6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014048749s
Sep 20 02:21:51.460: INFO: Pod "downwardapi-volume-94cd9a2b-4e6e-40a4-9f02-53aba2dc9ec6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023386905s
STEP: Saw pod success
Sep 20 02:21:51.460: INFO: Pod "downwardapi-volume-94cd9a2b-4e6e-40a4-9f02-53aba2dc9ec6" satisfied condition "success or failure"
Sep 20 02:21:51.467: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod downwardapi-volume-94cd9a2b-4e6e-40a4-9f02-53aba2dc9ec6 container client-container: <nil>
STEP: delete the pod
Sep 20 02:21:51.509: INFO: Waiting for pod downwardapi-volume-94cd9a2b-4e6e-40a4-9f02-53aba2dc9ec6 to disappear
Sep 20 02:21:51.523: INFO: Pod downwardapi-volume-94cd9a2b-4e6e-40a4-9f02-53aba2dc9ec6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:21:51.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5355" for this suite.
Sep 20 02:21:57.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:21:57.759: INFO: namespace projected-5355 deletion completed in 6.227630838s

• [SLOW TEST:10.515 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:21:57.768: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9065
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-9065/secret-test-77e33c03-579d-4f49-a2dd-a7b7ca5bb43f
STEP: Creating a pod to test consume secrets
Sep 20 02:21:57.957: INFO: Waiting up to 5m0s for pod "pod-configmaps-015ef651-df71-4c3d-8632-e29e1ef025e5" in namespace "secrets-9065" to be "success or failure"
Sep 20 02:21:57.966: INFO: Pod "pod-configmaps-015ef651-df71-4c3d-8632-e29e1ef025e5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.29141ms
I0920 02:21:58.776569      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
Sep 20 02:21:59.972: INFO: Pod "pod-configmaps-015ef651-df71-4c3d-8632-e29e1ef025e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015455369s
Sep 20 02:22:01.979: INFO: Pod "pod-configmaps-015ef651-df71-4c3d-8632-e29e1ef025e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022001887s
STEP: Saw pod success
Sep 20 02:22:01.980: INFO: Pod "pod-configmaps-015ef651-df71-4c3d-8632-e29e1ef025e5" satisfied condition "success or failure"
Sep 20 02:22:01.985: INFO: Trying to get logs from node worker-wqshf-7859ffd555-lmjf4 pod pod-configmaps-015ef651-df71-4c3d-8632-e29e1ef025e5 container env-test: <nil>
STEP: delete the pod
Sep 20 02:22:02.104: INFO: Waiting for pod pod-configmaps-015ef651-df71-4c3d-8632-e29e1ef025e5 to disappear
Sep 20 02:22:02.111: INFO: Pod pod-configmaps-015ef651-df71-4c3d-8632-e29e1ef025e5 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:22:02.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9065" for this suite.
Sep 20 02:22:08.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:22:09.071: INFO: namespace secrets-9065 deletion completed in 6.938009246s

• [SLOW TEST:11.303 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:22:09.072: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8550
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 20 02:22:09.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-8550'
Sep 20 02:22:09.369: INFO: stderr: ""
Sep 20 02:22:09.369: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
I0920 02:22:09.369746      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 02:22:09.369790      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
STEP: verifying the pod e2e-test-httpd-pod was created
Sep 20 02:22:14.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pod e2e-test-httpd-pod --namespace=kubectl-8550 -o json'
Sep 20 02:22:14.521: INFO: stderr: ""
Sep 20 02:22:14.521: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.25.1.108/32\"\n        },\n        \"creationTimestamp\": \"2019-09-20T02:22:09Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8550\",\n        \"resourceVersion\": \"88935\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-8550/pods/e2e-test-httpd-pod\",\n        \"uid\": \"b4bab2cf-195a-4508-a931-7c85a8e2f0ae\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-h2tdv\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker-wqshf-7859ffd555-r6tnq\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-h2tdv\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-h2tdv\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-20T02:22:09Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-20T02:22:11Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-20T02:22:11Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-20T02:22:09Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://c6de677af5b661fe806fe3f81e31e75d4777987dcc6ed25792764c5e485afd73\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-09-20T02:22:11Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"104.248.240.207\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.25.1.108\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.25.1.108\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-09-20T02:22:09Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep 20 02:22:14.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 replace -f - --namespace=kubectl-8550'
Sep 20 02:22:14.706: INFO: stderr: ""
Sep 20 02:22:14.706: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Sep 20 02:22:14.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 delete pods e2e-test-httpd-pod --namespace=kubectl-8550'
Sep 20 02:22:23.584: INFO: stderr: ""
Sep 20 02:22:23.584: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:22:23.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8550" for this suite.
Sep 20 02:22:29.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:22:29.823: INFO: namespace kubectl-8550 deletion completed in 6.231443801s

• [SLOW TEST:20.751 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:22:29.824: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5100
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:23:30.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5100" for this suite.
Sep 20 02:23:58.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:23:58.235: INFO: namespace container-probe-5100 deletion completed in 28.212906309s

• [SLOW TEST:88.411 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:23:58.240: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2170
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 20 02:23:58.586: INFO: Number of nodes with available pods: 0
Sep 20 02:23:58.586: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 02:23:59.599: INFO: Number of nodes with available pods: 0
Sep 20 02:23:59.599: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 02:24:00.599: INFO: Number of nodes with available pods: 0
Sep 20 02:24:00.600: INFO: Node worker-wqshf-7859ffd555-kqpmz is running more than one daemon pod
Sep 20 02:24:01.608: INFO: Number of nodes with available pods: 5
Sep 20 02:24:01.608: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep 20 02:24:01.660: INFO: Number of nodes with available pods: 4
Sep 20 02:24:01.660: INFO: Node worker-wqshf-7859ffd555-ldlfc is running more than one daemon pod
Sep 20 02:24:02.675: INFO: Number of nodes with available pods: 4
Sep 20 02:24:02.675: INFO: Node worker-wqshf-7859ffd555-ldlfc is running more than one daemon pod
Sep 20 02:24:03.673: INFO: Number of nodes with available pods: 4
Sep 20 02:24:03.673: INFO: Node worker-wqshf-7859ffd555-ldlfc is running more than one daemon pod
Sep 20 02:24:04.675: INFO: Number of nodes with available pods: 4
Sep 20 02:24:04.675: INFO: Node worker-wqshf-7859ffd555-ldlfc is running more than one daemon pod
Sep 20 02:24:05.682: INFO: Number of nodes with available pods: 4
Sep 20 02:24:05.682: INFO: Node worker-wqshf-7859ffd555-ldlfc is running more than one daemon pod
Sep 20 02:24:06.676: INFO: Number of nodes with available pods: 4
Sep 20 02:24:06.676: INFO: Node worker-wqshf-7859ffd555-ldlfc is running more than one daemon pod
Sep 20 02:24:07.682: INFO: Number of nodes with available pods: 5
Sep 20 02:24:07.682: INFO: Number of running nodes: 5, number of available pods: 5
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2170, will wait for the garbage collector to delete the pods
I0920 02:24:07.692916      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 02:24:07.692969      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 02:24:07.754: INFO: Deleting DaemonSet.extensions daemon-set took: 10.925249ms
I0920 02:24:08.154422      16 controller_utils.go:810] Ignoring inactive pod daemonsets-2170/daemon-set-gt7sw in state Running, deletion time 2019-09-20 02:24:38 +0000 UTC
I0920 02:24:08.154484      16 controller_utils.go:810] Ignoring inactive pod daemonsets-2170/daemon-set-vbqqk in state Running, deletion time 2019-09-20 02:24:38 +0000 UTC
I0920 02:24:08.154501      16 controller_utils.go:810] Ignoring inactive pod daemonsets-2170/daemon-set-dpgnt in state Running, deletion time 2019-09-20 02:24:38 +0000 UTC
I0920 02:24:08.154514      16 controller_utils.go:810] Ignoring inactive pod daemonsets-2170/daemon-set-tchbx in state Running, deletion time 2019-09-20 02:24:38 +0000 UTC
I0920 02:24:08.154526      16 controller_utils.go:810] Ignoring inactive pod daemonsets-2170/daemon-set-pqgch in state Running, deletion time 2019-09-20 02:24:38 +0000 UTC
Sep 20 02:24:08.154: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.76764ms
Sep 20 02:24:16.660: INFO: Number of nodes with available pods: 0
Sep 20 02:24:16.661: INFO: Number of running nodes: 0, number of available pods: 0
Sep 20 02:24:16.666: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2170/daemonsets","resourceVersion":"89435"},"items":null}

Sep 20 02:24:16.673: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2170/pods","resourceVersion":"89435"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:24:16.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2170" for this suite.
Sep 20 02:24:22.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:24:22.934: INFO: namespace daemonsets-2170 deletion completed in 6.213833918s

• [SLOW TEST:24.694 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:24:22.935: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5406
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 02:24:23.400: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 02:24:25.418: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543063, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543063, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543063, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543063, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 02:24:28.440: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 02:24:28.502: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3230-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:24:29.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5406" for this suite.
Sep 20 02:24:36.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:24:36.427: INFO: namespace webhook-5406 deletion completed in 6.436766114s
STEP: Destroying namespace "webhook-5406-markers" for this suite.
Sep 20 02:24:42.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:24:42.637: INFO: namespace webhook-5406-markers deletion completed in 6.20934494s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:19.728 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:24:42.669: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8782
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep 20 02:24:42.859: INFO: Pod name pod-release: Found 0 pods out of 1
Sep 20 02:24:47.869: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:24:48.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8782" for this suite.
Sep 20 02:24:54.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:24:55.146: INFO: namespace replication-controller-8782 deletion completed in 6.240874963s

• [SLOW TEST:12.478 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:24:55.153: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4527
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 02:24:55.343: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f797d096-4b74-4566-9b73-24a3890d89a1" in namespace "downward-api-4527" to be "success or failure"
Sep 20 02:24:55.349: INFO: Pod "downwardapi-volume-f797d096-4b74-4566-9b73-24a3890d89a1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.539627ms
Sep 20 02:24:57.356: INFO: Pod "downwardapi-volume-f797d096-4b74-4566-9b73-24a3890d89a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012273078s
Sep 20 02:24:59.363: INFO: Pod "downwardapi-volume-f797d096-4b74-4566-9b73-24a3890d89a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019218442s
STEP: Saw pod success
Sep 20 02:24:59.363: INFO: Pod "downwardapi-volume-f797d096-4b74-4566-9b73-24a3890d89a1" satisfied condition "success or failure"
Sep 20 02:24:59.369: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod downwardapi-volume-f797d096-4b74-4566-9b73-24a3890d89a1 container client-container: <nil>
STEP: delete the pod
Sep 20 02:24:59.422: INFO: Waiting for pod downwardapi-volume-f797d096-4b74-4566-9b73-24a3890d89a1 to disappear
Sep 20 02:24:59.430: INFO: Pod downwardapi-volume-f797d096-4b74-4566-9b73-24a3890d89a1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:24:59.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4527" for this suite.
Sep 20 02:25:05.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:25:05.650: INFO: namespace downward-api-4527 deletion completed in 6.21364039s

• [SLOW TEST:10.498 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:25:05.651: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-1997
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:25:05.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-1997" for this suite.
Sep 20 02:25:11.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:25:12.075: INFO: namespace tables-1997 deletion completed in 6.240383513s

• [SLOW TEST:6.424 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:25:12.075: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1583
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-9cfe07e3-66f8-4f5d-9d90-940c92d37708
STEP: Creating a pod to test consume secrets
Sep 20 02:25:12.268: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cd6b17d6-420e-4743-b5bb-2f0f573a2aa1" in namespace "projected-1583" to be "success or failure"
Sep 20 02:25:12.275: INFO: Pod "pod-projected-secrets-cd6b17d6-420e-4743-b5bb-2f0f573a2aa1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.079574ms
Sep 20 02:25:14.282: INFO: Pod "pod-projected-secrets-cd6b17d6-420e-4743-b5bb-2f0f573a2aa1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014274119s
Sep 20 02:25:16.289: INFO: Pod "pod-projected-secrets-cd6b17d6-420e-4743-b5bb-2f0f573a2aa1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021172445s
STEP: Saw pod success
Sep 20 02:25:16.289: INFO: Pod "pod-projected-secrets-cd6b17d6-420e-4743-b5bb-2f0f573a2aa1" satisfied condition "success or failure"
Sep 20 02:25:16.296: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod pod-projected-secrets-cd6b17d6-420e-4743-b5bb-2f0f573a2aa1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 20 02:25:16.339: INFO: Waiting for pod pod-projected-secrets-cd6b17d6-420e-4743-b5bb-2f0f573a2aa1 to disappear
Sep 20 02:25:16.347: INFO: Pod pod-projected-secrets-cd6b17d6-420e-4743-b5bb-2f0f573a2aa1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:25:16.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1583" for this suite.
Sep 20 02:25:22.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:25:22.572: INFO: namespace projected-1583 deletion completed in 6.215259811s

• [SLOW TEST:10.498 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:25:22.579: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3283
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-6f81ca9d-0e27-4710-974c-75e8be1c39fe
STEP: Creating a pod to test consume configMaps
Sep 20 02:25:22.779: INFO: Waiting up to 5m0s for pod "pod-configmaps-94437100-a3e0-49da-8a1d-e579a3c26ad7" in namespace "configmap-3283" to be "success or failure"
Sep 20 02:25:22.783: INFO: Pod "pod-configmaps-94437100-a3e0-49da-8a1d-e579a3c26ad7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012371ms
Sep 20 02:25:24.791: INFO: Pod "pod-configmaps-94437100-a3e0-49da-8a1d-e579a3c26ad7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011698636s
Sep 20 02:25:26.799: INFO: Pod "pod-configmaps-94437100-a3e0-49da-8a1d-e579a3c26ad7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020207563s
STEP: Saw pod success
Sep 20 02:25:26.800: INFO: Pod "pod-configmaps-94437100-a3e0-49da-8a1d-e579a3c26ad7" satisfied condition "success or failure"
Sep 20 02:25:26.805: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod pod-configmaps-94437100-a3e0-49da-8a1d-e579a3c26ad7 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 02:25:26.846: INFO: Waiting for pod pod-configmaps-94437100-a3e0-49da-8a1d-e579a3c26ad7 to disappear
Sep 20 02:25:26.851: INFO: Pod pod-configmaps-94437100-a3e0-49da-8a1d-e579a3c26ad7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:25:26.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3283" for this suite.
Sep 20 02:25:32.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:25:33.061: INFO: namespace configmap-3283 deletion completed in 6.203951471s

• [SLOW TEST:10.484 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:25:33.069: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9782
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 02:25:33.241: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a0ac4c29-c40f-4c05-8609-8d410be196ea" in namespace "downward-api-9782" to be "success or failure"
Sep 20 02:25:33.248: INFO: Pod "downwardapi-volume-a0ac4c29-c40f-4c05-8609-8d410be196ea": Phase="Pending", Reason="", readiness=false. Elapsed: 7.527838ms
Sep 20 02:25:35.254: INFO: Pod "downwardapi-volume-a0ac4c29-c40f-4c05-8609-8d410be196ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01350662s
Sep 20 02:25:37.266: INFO: Pod "downwardapi-volume-a0ac4c29-c40f-4c05-8609-8d410be196ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024775004s
STEP: Saw pod success
Sep 20 02:25:37.266: INFO: Pod "downwardapi-volume-a0ac4c29-c40f-4c05-8609-8d410be196ea" satisfied condition "success or failure"
Sep 20 02:25:37.272: INFO: Trying to get logs from node worker-wqshf-7859ffd555-lmjf4 pod downwardapi-volume-a0ac4c29-c40f-4c05-8609-8d410be196ea container client-container: <nil>
STEP: delete the pod
Sep 20 02:25:37.317: INFO: Waiting for pod downwardapi-volume-a0ac4c29-c40f-4c05-8609-8d410be196ea to disappear
Sep 20 02:25:37.323: INFO: Pod downwardapi-volume-a0ac4c29-c40f-4c05-8609-8d410be196ea no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:25:37.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9782" for this suite.
Sep 20 02:25:43.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:25:43.614: INFO: namespace downward-api-9782 deletion completed in 6.282602079s

• [SLOW TEST:10.545 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:25:43.625: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4420
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 20 02:25:43.815: INFO: Waiting up to 5m0s for pod "pod-c6ac8030-9f30-489c-ab19-4f7408bb3257" in namespace "emptydir-4420" to be "success or failure"
Sep 20 02:25:43.821: INFO: Pod "pod-c6ac8030-9f30-489c-ab19-4f7408bb3257": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070932ms
Sep 20 02:25:45.830: INFO: Pod "pod-c6ac8030-9f30-489c-ab19-4f7408bb3257": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014989164s
Sep 20 02:25:47.837: INFO: Pod "pod-c6ac8030-9f30-489c-ab19-4f7408bb3257": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022002518s
STEP: Saw pod success
Sep 20 02:25:47.837: INFO: Pod "pod-c6ac8030-9f30-489c-ab19-4f7408bb3257" satisfied condition "success or failure"
Sep 20 02:25:47.843: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod pod-c6ac8030-9f30-489c-ab19-4f7408bb3257 container test-container: <nil>
STEP: delete the pod
Sep 20 02:25:47.904: INFO: Waiting for pod pod-c6ac8030-9f30-489c-ab19-4f7408bb3257 to disappear
Sep 20 02:25:47.911: INFO: Pod pod-c6ac8030-9f30-489c-ab19-4f7408bb3257 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:25:47.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4420" for this suite.
Sep 20 02:25:53.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:25:54.130: INFO: namespace emptydir-4420 deletion completed in 6.213440141s

• [SLOW TEST:10.506 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:25:54.131: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3539
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 20 02:25:54.307: INFO: Waiting up to 5m0s for pod "pod-12d01997-1886-4150-8406-19ad6a77a94d" in namespace "emptydir-3539" to be "success or failure"
Sep 20 02:25:54.314: INFO: Pod "pod-12d01997-1886-4150-8406-19ad6a77a94d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.035341ms
Sep 20 02:25:56.322: INFO: Pod "pod-12d01997-1886-4150-8406-19ad6a77a94d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014968075s
Sep 20 02:25:58.329: INFO: Pod "pod-12d01997-1886-4150-8406-19ad6a77a94d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02177908s
STEP: Saw pod success
Sep 20 02:25:58.329: INFO: Pod "pod-12d01997-1886-4150-8406-19ad6a77a94d" satisfied condition "success or failure"
Sep 20 02:25:58.335: INFO: Trying to get logs from node worker-wqshf-7859ffd555-lmjf4 pod pod-12d01997-1886-4150-8406-19ad6a77a94d container test-container: <nil>
STEP: delete the pod
Sep 20 02:25:58.378: INFO: Waiting for pod pod-12d01997-1886-4150-8406-19ad6a77a94d to disappear
Sep 20 02:25:58.392: INFO: Pod pod-12d01997-1886-4150-8406-19ad6a77a94d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:25:58.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3539" for this suite.
Sep 20 02:26:04.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:26:04.620: INFO: namespace emptydir-3539 deletion completed in 6.222928886s

• [SLOW TEST:10.490 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:26:04.626: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2269
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 02:26:04.819: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2e9e74b3-d2f9-4b77-b023-ac2057d9c4ab" in namespace "projected-2269" to be "success or failure"
Sep 20 02:26:04.828: INFO: Pod "downwardapi-volume-2e9e74b3-d2f9-4b77-b023-ac2057d9c4ab": Phase="Pending", Reason="", readiness=false. Elapsed: 8.669756ms
Sep 20 02:26:06.838: INFO: Pod "downwardapi-volume-2e9e74b3-d2f9-4b77-b023-ac2057d9c4ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018566296s
Sep 20 02:26:08.845: INFO: Pod "downwardapi-volume-2e9e74b3-d2f9-4b77-b023-ac2057d9c4ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025268065s
STEP: Saw pod success
Sep 20 02:26:08.845: INFO: Pod "downwardapi-volume-2e9e74b3-d2f9-4b77-b023-ac2057d9c4ab" satisfied condition "success or failure"
Sep 20 02:26:08.849: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod downwardapi-volume-2e9e74b3-d2f9-4b77-b023-ac2057d9c4ab container client-container: <nil>
STEP: delete the pod
Sep 20 02:26:08.890: INFO: Waiting for pod downwardapi-volume-2e9e74b3-d2f9-4b77-b023-ac2057d9c4ab to disappear
Sep 20 02:26:08.894: INFO: Pod downwardapi-volume-2e9e74b3-d2f9-4b77-b023-ac2057d9c4ab no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:26:08.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2269" for this suite.
Sep 20 02:26:14.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:26:15.151: INFO: namespace projected-2269 deletion completed in 6.249863455s

• [SLOW TEST:10.525 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:26:15.151: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9687
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Sep 20 02:26:15.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 create -f - --namespace=kubectl-9687'
Sep 20 02:26:15.608: INFO: stderr: ""
Sep 20 02:26:15.608: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep 20 02:26:16.615: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 02:26:16.615: INFO: Found 0 / 1
Sep 20 02:26:17.615: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 02:26:17.615: INFO: Found 0 / 1
Sep 20 02:26:18.614: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 02:26:18.614: INFO: Found 1 / 1
Sep 20 02:26:18.614: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep 20 02:26:18.621: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 02:26:18.622: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 20 02:26:18.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 patch pod redis-master-4k795 --namespace=kubectl-9687 -p {"metadata":{"annotations":{"x":"y"}}}'
Sep 20 02:26:18.741: INFO: stderr: ""
Sep 20 02:26:18.741: INFO: stdout: "pod/redis-master-4k795 patched\n"
STEP: checking annotations
Sep 20 02:26:18.747: INFO: Selector matched 1 pods for map[app:redis]
Sep 20 02:26:18.747: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:26:18.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9687" for this suite.
Sep 20 02:26:46.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:26:47.008: INFO: namespace kubectl-9687 deletion completed in 28.227076929s

• [SLOW TEST:31.857 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:26:47.013: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7969
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Sep 20 02:26:47.202: INFO: Waiting up to 5m0s for pod "downward-api-4e9c6ef0-6ba7-467e-86f0-a468cb184861" in namespace "downward-api-7969" to be "success or failure"
Sep 20 02:26:47.211: INFO: Pod "downward-api-4e9c6ef0-6ba7-467e-86f0-a468cb184861": Phase="Pending", Reason="", readiness=false. Elapsed: 8.522316ms
Sep 20 02:26:49.219: INFO: Pod "downward-api-4e9c6ef0-6ba7-467e-86f0-a468cb184861": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016998612s
Sep 20 02:26:51.229: INFO: Pod "downward-api-4e9c6ef0-6ba7-467e-86f0-a468cb184861": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026678885s
STEP: Saw pod success
Sep 20 02:26:51.229: INFO: Pod "downward-api-4e9c6ef0-6ba7-467e-86f0-a468cb184861" satisfied condition "success or failure"
Sep 20 02:26:51.238: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod downward-api-4e9c6ef0-6ba7-467e-86f0-a468cb184861 container dapi-container: <nil>
STEP: delete the pod
Sep 20 02:26:51.280: INFO: Waiting for pod downward-api-4e9c6ef0-6ba7-467e-86f0-a468cb184861 to disappear
Sep 20 02:26:51.285: INFO: Pod downward-api-4e9c6ef0-6ba7-467e-86f0-a468cb184861 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:26:51.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7969" for this suite.
Sep 20 02:26:57.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:26:57.503: INFO: namespace downward-api-7969 deletion completed in 6.20855845s

• [SLOW TEST:10.491 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:26:57.505: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1622
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Sep 20 02:26:57.671: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:27:02.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1622" for this suite.
Sep 20 02:27:08.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:27:08.536: INFO: namespace init-container-1622 deletion completed in 6.242309642s

• [SLOW TEST:11.032 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:27:08.539: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9866
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Sep 20 02:27:08.716: INFO: Waiting up to 5m0s for pod "downward-api-fa9b37e5-20d1-45c0-85c4-0846d6163d83" in namespace "downward-api-9866" to be "success or failure"
Sep 20 02:27:08.728: INFO: Pod "downward-api-fa9b37e5-20d1-45c0-85c4-0846d6163d83": Phase="Pending", Reason="", readiness=false. Elapsed: 12.179822ms
Sep 20 02:27:10.735: INFO: Pod "downward-api-fa9b37e5-20d1-45c0-85c4-0846d6163d83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018938524s
Sep 20 02:27:12.742: INFO: Pod "downward-api-fa9b37e5-20d1-45c0-85c4-0846d6163d83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02627832s
STEP: Saw pod success
Sep 20 02:27:12.743: INFO: Pod "downward-api-fa9b37e5-20d1-45c0-85c4-0846d6163d83" satisfied condition "success or failure"
Sep 20 02:27:12.755: INFO: Trying to get logs from node worker-wqshf-7859ffd555-lmjf4 pod downward-api-fa9b37e5-20d1-45c0-85c4-0846d6163d83 container dapi-container: <nil>
STEP: delete the pod
Sep 20 02:27:12.806: INFO: Waiting for pod downward-api-fa9b37e5-20d1-45c0-85c4-0846d6163d83 to disappear
Sep 20 02:27:12.813: INFO: Pod downward-api-fa9b37e5-20d1-45c0-85c4-0846d6163d83 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:27:12.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9866" for this suite.
I0920 02:27:13.914521      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
Sep 20 02:27:18.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:27:19.116: INFO: namespace downward-api-9866 deletion completed in 6.29048199s

• [SLOW TEST:10.577 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:27:19.121: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9997
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Sep 20 02:27:19.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 create -f - --namespace=kubectl-9997'
Sep 20 02:27:19.787: INFO: stderr: ""
Sep 20 02:27:19.787: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 20 02:27:19.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9997'
Sep 20 02:27:19.883: INFO: stderr: ""
Sep 20 02:27:19.883: INFO: stdout: "update-demo-nautilus-5ll28 update-demo-nautilus-cdzps "
Sep 20 02:27:19.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-nautilus-5ll28 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9997'
Sep 20 02:27:19.991: INFO: stderr: ""
Sep 20 02:27:19.991: INFO: stdout: ""
Sep 20 02:27:19.991: INFO: update-demo-nautilus-5ll28 is created but not running
Sep 20 02:27:24.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9997'
Sep 20 02:27:25.084: INFO: stderr: ""
Sep 20 02:27:25.084: INFO: stdout: "update-demo-nautilus-5ll28 update-demo-nautilus-cdzps "
Sep 20 02:27:25.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-nautilus-5ll28 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9997'
Sep 20 02:27:25.168: INFO: stderr: ""
Sep 20 02:27:25.168: INFO: stdout: "true"
Sep 20 02:27:25.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-nautilus-5ll28 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9997'
Sep 20 02:27:25.264: INFO: stderr: ""
Sep 20 02:27:25.264: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 20 02:27:25.264: INFO: validating pod update-demo-nautilus-5ll28
Sep 20 02:27:25.361: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 02:27:25.362: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 02:27:25.362: INFO: update-demo-nautilus-5ll28 is verified up and running
Sep 20 02:27:25.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-nautilus-cdzps -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9997'
Sep 20 02:27:25.460: INFO: stderr: ""
Sep 20 02:27:25.460: INFO: stdout: "true"
Sep 20 02:27:25.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-nautilus-cdzps -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9997'
Sep 20 02:27:25.544: INFO: stderr: ""
Sep 20 02:27:25.544: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 20 02:27:25.544: INFO: validating pod update-demo-nautilus-cdzps
Sep 20 02:27:25.644: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 02:27:25.644: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 02:27:25.644: INFO: update-demo-nautilus-cdzps is verified up and running
STEP: rolling-update to new replication controller
Sep 20 02:27:25.650: INFO: scanned /root for discovery docs: <nil>
Sep 20 02:27:25.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-9997'
Sep 20 02:27:49.221: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep 20 02:27:49.221: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 20 02:27:49.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9997'
Sep 20 02:27:49.313: INFO: stderr: ""
Sep 20 02:27:49.313: INFO: stdout: "update-demo-kitten-b5qw2 update-demo-kitten-x8rcp "
Sep 20 02:27:49.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-kitten-b5qw2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9997'
Sep 20 02:27:49.397: INFO: stderr: ""
Sep 20 02:27:49.397: INFO: stdout: "true"
Sep 20 02:27:49.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-kitten-b5qw2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9997'
Sep 20 02:27:49.477: INFO: stderr: ""
Sep 20 02:27:49.477: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep 20 02:27:49.477: INFO: validating pod update-demo-kitten-b5qw2
Sep 20 02:27:49.577: INFO: got data: {
  "image": "kitten.jpg"
}

Sep 20 02:27:49.577: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep 20 02:27:49.577: INFO: update-demo-kitten-b5qw2 is verified up and running
Sep 20 02:27:49.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-kitten-x8rcp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9997'
Sep 20 02:27:49.679: INFO: stderr: ""
Sep 20 02:27:49.679: INFO: stdout: "true"
Sep 20 02:27:49.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods update-demo-kitten-x8rcp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9997'
Sep 20 02:27:49.763: INFO: stderr: ""
Sep 20 02:27:49.763: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep 20 02:27:49.763: INFO: validating pod update-demo-kitten-x8rcp
Sep 20 02:27:49.863: INFO: got data: {
  "image": "kitten.jpg"
}

Sep 20 02:27:49.863: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep 20 02:27:49.863: INFO: update-demo-kitten-x8rcp is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:27:49.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9997" for this suite.
Sep 20 02:28:17.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:28:18.089: INFO: namespace kubectl-9997 deletion completed in 28.217812373s

• [SLOW TEST:58.968 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:28:18.097: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8209
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Sep 20 02:28:22.845: INFO: Successfully updated pod "labelsupdatec143c0d7-d308-4c8f-82cd-4e953cd5c3d6"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:28:24.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8209" for this suite.
Sep 20 02:28:36.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:28:37.140: INFO: namespace projected-8209 deletion completed in 12.230078625s

• [SLOW TEST:19.043 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:28:37.158: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6686
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 02:28:37.345: INFO: Waiting up to 5m0s for pod "downwardapi-volume-50e66ef4-4a4d-4ff3-a79e-6713a1769e03" in namespace "downward-api-6686" to be "success or failure"
Sep 20 02:28:37.353: INFO: Pod "downwardapi-volume-50e66ef4-4a4d-4ff3-a79e-6713a1769e03": Phase="Pending", Reason="", readiness=false. Elapsed: 8.566291ms
Sep 20 02:28:39.360: INFO: Pod "downwardapi-volume-50e66ef4-4a4d-4ff3-a79e-6713a1769e03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015264063s
Sep 20 02:28:41.367: INFO: Pod "downwardapi-volume-50e66ef4-4a4d-4ff3-a79e-6713a1769e03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022147235s
STEP: Saw pod success
Sep 20 02:28:41.367: INFO: Pod "downwardapi-volume-50e66ef4-4a4d-4ff3-a79e-6713a1769e03" satisfied condition "success or failure"
Sep 20 02:28:41.371: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod downwardapi-volume-50e66ef4-4a4d-4ff3-a79e-6713a1769e03 container client-container: <nil>
STEP: delete the pod
Sep 20 02:28:41.413: INFO: Waiting for pod downwardapi-volume-50e66ef4-4a4d-4ff3-a79e-6713a1769e03 to disappear
Sep 20 02:28:41.421: INFO: Pod downwardapi-volume-50e66ef4-4a4d-4ff3-a79e-6713a1769e03 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:28:41.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6686" for this suite.
Sep 20 02:28:47.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:28:47.655: INFO: namespace downward-api-6686 deletion completed in 6.222798264s

• [SLOW TEST:10.498 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:28:47.666: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5281
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0920 02:28:53.881431      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 20 02:28:53.881: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:28:53.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5281" for this suite.
Sep 20 02:28:59.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:29:00.090: INFO: namespace gc-5281 deletion completed in 6.202797134s

• [SLOW TEST:12.424 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:29:00.098: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-625
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-f405c00d-19eb-41b2-b716-55275a0310c1 in namespace container-probe-625
Sep 20 02:29:04.291: INFO: Started pod test-webserver-f405c00d-19eb-41b2-b716-55275a0310c1 in namespace container-probe-625
STEP: checking the pod's current state and verifying that restartCount is present
Sep 20 02:29:04.300: INFO: Initial restart count of pod test-webserver-f405c00d-19eb-41b2-b716-55275a0310c1 is 0
I0920 02:31:41.784737      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:33:05.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-625" for this suite.
I0920 02:33:37.920941      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
Sep 20 02:34:45.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:34:46.618: INFO: namespace container-probe-625 deletion completed in 1m40.703481335s

• [SLOW TEST:346.520 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:34:46.618: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3523
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 20 02:34:49.923: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:34:49.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3523" for this suite.
Sep 20 02:34:55.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:34:56.328: INFO: namespace container-runtime-3523 deletion completed in 6.372416205s

• [SLOW TEST:9.710 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:34:56.332: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9067
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Sep 20 02:34:56.590: INFO: Waiting up to 5m0s for pod "client-containers-2b625a4f-64ad-4ab2-9f37-6013990332f0" in namespace "containers-9067" to be "success or failure"
Sep 20 02:34:56.614: INFO: Pod "client-containers-2b625a4f-64ad-4ab2-9f37-6013990332f0": Phase="Pending", Reason="", readiness=false. Elapsed: 23.928935ms
Sep 20 02:34:58.627: INFO: Pod "client-containers-2b625a4f-64ad-4ab2-9f37-6013990332f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037279492s
Sep 20 02:35:00.638: INFO: Pod "client-containers-2b625a4f-64ad-4ab2-9f37-6013990332f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04804027s
STEP: Saw pod success
Sep 20 02:35:00.638: INFO: Pod "client-containers-2b625a4f-64ad-4ab2-9f37-6013990332f0" satisfied condition "success or failure"
Sep 20 02:35:00.643: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod client-containers-2b625a4f-64ad-4ab2-9f37-6013990332f0 container test-container: <nil>
STEP: delete the pod
Sep 20 02:35:00.731: INFO: Waiting for pod client-containers-2b625a4f-64ad-4ab2-9f37-6013990332f0 to disappear
Sep 20 02:35:00.736: INFO: Pod client-containers-2b625a4f-64ad-4ab2-9f37-6013990332f0 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:35:00.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9067" for this suite.
Sep 20 02:35:06.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:35:07.064: INFO: namespace containers-9067 deletion completed in 6.319305297s

• [SLOW TEST:10.732 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:35:07.064: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-4907
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep 20 02:35:11.293: INFO: &Pod{ObjectMeta:{send-events-05156b71-591e-4e6d-826f-2706fe5e4cc6  events-4907 /api/v1/namespaces/events-4907/pods/send-events-05156b71-591e-4e6d-826f-2706fe5e4cc6 7824e2fa-6228-4e73-ada8-ab95ce9747fb 92172 0 2019-09-20 02:35:07 +0000 UTC <nil> <nil> map[name:foo time:253140392] map[cni.projectcalico.org/podIP:172.25.1.119/32] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zjkh9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zjkh9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zjkh9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-wqshf-7859ffd555-r6tnq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 02:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 02:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 02:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-09-20 02:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:104.248.240.207,PodIP:172.25.1.119,StartTime:2019-09-20 02:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-09-20 02:35:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:docker://0521dcc8a3687a239ce1fd5f821ca5dc3afabc1f14e5b114270e54d7e39e14f8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Sep 20 02:35:13.301: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep 20 02:35:15.312: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:35:15.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4907" for this suite.
Sep 20 02:35:59.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:35:59.588: INFO: namespace events-4907 deletion completed in 44.252021814s

• [SLOW TEST:52.524 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:35:59.591: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-7501
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep 20 02:36:00.282: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Sep 20 02:36:02.302: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543760, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543760, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543760, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543760, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 02:36:05.327: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 02:36:05.336: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:36:06.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7501" for this suite.
Sep 20 02:36:13.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:36:13.250: INFO: namespace crd-webhook-7501 deletion completed in 6.247523255s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:13.689 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:36:13.282: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7848
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-00183735-6014-49c5-8f8b-d14a9a24495c
STEP: Creating a pod to test consume secrets
Sep 20 02:36:13.481: INFO: Waiting up to 5m0s for pod "pod-secrets-bfc0a1d1-5454-4626-918d-0098fcf86680" in namespace "secrets-7848" to be "success or failure"
Sep 20 02:36:13.488: INFO: Pod "pod-secrets-bfc0a1d1-5454-4626-918d-0098fcf86680": Phase="Pending", Reason="", readiness=false. Elapsed: 7.030162ms
Sep 20 02:36:15.498: INFO: Pod "pod-secrets-bfc0a1d1-5454-4626-918d-0098fcf86680": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016705243s
Sep 20 02:36:17.505: INFO: Pod "pod-secrets-bfc0a1d1-5454-4626-918d-0098fcf86680": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02410295s
STEP: Saw pod success
Sep 20 02:36:17.505: INFO: Pod "pod-secrets-bfc0a1d1-5454-4626-918d-0098fcf86680" satisfied condition "success or failure"
Sep 20 02:36:17.514: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod pod-secrets-bfc0a1d1-5454-4626-918d-0098fcf86680 container secret-volume-test: <nil>
STEP: delete the pod
Sep 20 02:36:17.563: INFO: Waiting for pod pod-secrets-bfc0a1d1-5454-4626-918d-0098fcf86680 to disappear
Sep 20 02:36:17.568: INFO: Pod pod-secrets-bfc0a1d1-5454-4626-918d-0098fcf86680 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:36:17.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7848" for this suite.
Sep 20 02:36:23.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:36:23.828: INFO: namespace secrets-7848 deletion completed in 6.253533359s

• [SLOW TEST:10.545 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:36:23.828: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5114
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 20 02:36:27.067: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:36:27.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5114" for this suite.
Sep 20 02:36:33.123: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:36:33.329: INFO: namespace container-runtime-5114 deletion completed in 6.227412815s

• [SLOW TEST:9.501 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:36:33.336: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1027
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-e41104e1-211b-4758-a6a7-465949b986d8
STEP: Creating a pod to test consume secrets
Sep 20 02:36:33.533: INFO: Waiting up to 5m0s for pod "pod-secrets-9482ac1c-1a76-41ac-9ecb-4a5bc08ad030" in namespace "secrets-1027" to be "success or failure"
Sep 20 02:36:33.538: INFO: Pod "pod-secrets-9482ac1c-1a76-41ac-9ecb-4a5bc08ad030": Phase="Pending", Reason="", readiness=false. Elapsed: 5.152204ms
Sep 20 02:36:35.544: INFO: Pod "pod-secrets-9482ac1c-1a76-41ac-9ecb-4a5bc08ad030": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01122217s
Sep 20 02:36:37.552: INFO: Pod "pod-secrets-9482ac1c-1a76-41ac-9ecb-4a5bc08ad030": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018804658s
STEP: Saw pod success
Sep 20 02:36:37.552: INFO: Pod "pod-secrets-9482ac1c-1a76-41ac-9ecb-4a5bc08ad030" satisfied condition "success or failure"
Sep 20 02:36:37.561: INFO: Trying to get logs from node worker-wqshf-7859ffd555-lmjf4 pod pod-secrets-9482ac1c-1a76-41ac-9ecb-4a5bc08ad030 container secret-volume-test: <nil>
STEP: delete the pod
Sep 20 02:36:37.740: INFO: Waiting for pod pod-secrets-9482ac1c-1a76-41ac-9ecb-4a5bc08ad030 to disappear
Sep 20 02:36:37.744: INFO: Pod pod-secrets-9482ac1c-1a76-41ac-9ecb-4a5bc08ad030 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:36:37.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1027" for this suite.
Sep 20 02:36:43.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:36:44.018: INFO: namespace secrets-1027 deletion completed in 6.264010937s

• [SLOW TEST:10.683 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:36:44.021: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9996
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-9996
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-9996
Sep 20 02:36:44.225: INFO: Found 0 stateful pods, waiting for 1
Sep 20 02:36:54.236: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Sep 20 02:36:54.279: INFO: Deleting all statefulset in ns statefulset-9996
Sep 20 02:36:54.291: INFO: Scaling statefulset ss to 0
Sep 20 02:37:14.331: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 02:37:14.339: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:37:14.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9996" for this suite.
Sep 20 02:37:20.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:37:20.618: INFO: namespace statefulset-9996 deletion completed in 6.248810526s

• [SLOW TEST:36.597 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:37:20.620: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3894
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-6e02236d-9f9a-4f75-886f-8ee7ce2cb31d
STEP: Creating a pod to test consume secrets
Sep 20 02:37:20.811: INFO: Waiting up to 5m0s for pod "pod-secrets-c567be59-761b-46ef-90d2-62028564cce2" in namespace "secrets-3894" to be "success or failure"
Sep 20 02:37:20.820: INFO: Pod "pod-secrets-c567be59-761b-46ef-90d2-62028564cce2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.147765ms
Sep 20 02:37:22.826: INFO: Pod "pod-secrets-c567be59-761b-46ef-90d2-62028564cce2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014624793s
Sep 20 02:37:24.833: INFO: Pod "pod-secrets-c567be59-761b-46ef-90d2-62028564cce2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021390022s
STEP: Saw pod success
Sep 20 02:37:24.833: INFO: Pod "pod-secrets-c567be59-761b-46ef-90d2-62028564cce2" satisfied condition "success or failure"
Sep 20 02:37:24.838: INFO: Trying to get logs from node worker-wqshf-7859ffd555-lmjf4 pod pod-secrets-c567be59-761b-46ef-90d2-62028564cce2 container secret-volume-test: <nil>
STEP: delete the pod
Sep 20 02:37:24.881: INFO: Waiting for pod pod-secrets-c567be59-761b-46ef-90d2-62028564cce2 to disappear
Sep 20 02:37:24.885: INFO: Pod pod-secrets-c567be59-761b-46ef-90d2-62028564cce2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:37:24.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3894" for this suite.
Sep 20 02:37:30.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:37:31.110: INFO: namespace secrets-3894 deletion completed in 6.217769129s

• [SLOW TEST:10.490 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:37:31.111: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9303
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Sep 20 02:37:31.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 create -f - --namespace=kubectl-9303'
Sep 20 02:37:31.881: INFO: stderr: ""
Sep 20 02:37:31.881: INFO: stdout: "pod/pause created\n"
Sep 20 02:37:31.881: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep 20 02:37:31.881: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9303" to be "running and ready"
Sep 20 02:37:31.890: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 9.572015ms
Sep 20 02:37:33.897: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016508601s
Sep 20 02:37:35.904: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.022772214s
Sep 20 02:37:35.904: INFO: Pod "pause" satisfied condition "running and ready"
Sep 20 02:37:35.904: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Sep 20 02:37:35.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 label pods pause testing-label=testing-label-value --namespace=kubectl-9303'
Sep 20 02:37:36.020: INFO: stderr: ""
Sep 20 02:37:36.021: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep 20 02:37:36.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pod pause -L testing-label --namespace=kubectl-9303'
Sep 20 02:37:36.109: INFO: stderr: ""
Sep 20 02:37:36.109: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep 20 02:37:36.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 label pods pause testing-label- --namespace=kubectl-9303'
Sep 20 02:37:36.207: INFO: stderr: ""
Sep 20 02:37:36.207: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep 20 02:37:36.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pod pause -L testing-label --namespace=kubectl-9303'
Sep 20 02:37:36.293: INFO: stderr: ""
Sep 20 02:37:36.293: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Sep 20 02:37:36.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 delete --grace-period=0 --force -f - --namespace=kubectl-9303'
Sep 20 02:37:36.388: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 02:37:36.388: INFO: stdout: "pod \"pause\" force deleted\n"
Sep 20 02:37:36.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get rc,svc -l name=pause --no-headers --namespace=kubectl-9303'
Sep 20 02:37:36.488: INFO: stderr: "No resources found in kubectl-9303 namespace.\n"
Sep 20 02:37:36.488: INFO: stdout: ""
Sep 20 02:37:36.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods -l name=pause --namespace=kubectl-9303 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 20 02:37:36.589: INFO: stderr: ""
Sep 20 02:37:36.589: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:37:36.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9303" for this suite.
Sep 20 02:37:42.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:37:42.848: INFO: namespace kubectl-9303 deletion completed in 6.250843776s

• [SLOW TEST:11.737 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:37:42.848: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9805
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-11e6ade8-07fe-4d8d-b46f-7b74bbcf3244
STEP: Creating a pod to test consume configMaps
Sep 20 02:37:43.026: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-88b656ee-6f11-486c-bec8-aee099030a3e" in namespace "projected-9805" to be "success or failure"
Sep 20 02:37:43.033: INFO: Pod "pod-projected-configmaps-88b656ee-6f11-486c-bec8-aee099030a3e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.893761ms
I0920 02:37:43.788967      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
Sep 20 02:37:45.040: INFO: Pod "pod-projected-configmaps-88b656ee-6f11-486c-bec8-aee099030a3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012769041s
Sep 20 02:37:47.047: INFO: Pod "pod-projected-configmaps-88b656ee-6f11-486c-bec8-aee099030a3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019975233s
STEP: Saw pod success
Sep 20 02:37:47.047: INFO: Pod "pod-projected-configmaps-88b656ee-6f11-486c-bec8-aee099030a3e" satisfied condition "success or failure"
Sep 20 02:37:47.055: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod pod-projected-configmaps-88b656ee-6f11-486c-bec8-aee099030a3e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 02:37:47.233: INFO: Waiting for pod pod-projected-configmaps-88b656ee-6f11-486c-bec8-aee099030a3e to disappear
Sep 20 02:37:47.241: INFO: Pod pod-projected-configmaps-88b656ee-6f11-486c-bec8-aee099030a3e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:37:47.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9805" for this suite.
Sep 20 02:37:53.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:37:53.465: INFO: namespace projected-9805 deletion completed in 6.213422881s

• [SLOW TEST:10.617 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:37:53.484: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6336
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-dfs2
STEP: Creating a pod to test atomic-volume-subpath
Sep 20 02:37:53.685: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-dfs2" in namespace "subpath-6336" to be "success or failure"
Sep 20 02:37:53.691: INFO: Pod "pod-subpath-test-downwardapi-dfs2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.599897ms
Sep 20 02:37:55.698: INFO: Pod "pod-subpath-test-downwardapi-dfs2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012105723s
Sep 20 02:37:57.704: INFO: Pod "pod-subpath-test-downwardapi-dfs2": Phase="Running", Reason="", readiness=true. Elapsed: 4.018705956s
Sep 20 02:37:59.711: INFO: Pod "pod-subpath-test-downwardapi-dfs2": Phase="Running", Reason="", readiness=true. Elapsed: 6.025466121s
Sep 20 02:38:01.718: INFO: Pod "pod-subpath-test-downwardapi-dfs2": Phase="Running", Reason="", readiness=true. Elapsed: 8.032632967s
Sep 20 02:38:03.726: INFO: Pod "pod-subpath-test-downwardapi-dfs2": Phase="Running", Reason="", readiness=true. Elapsed: 10.040719581s
Sep 20 02:38:05.734: INFO: Pod "pod-subpath-test-downwardapi-dfs2": Phase="Running", Reason="", readiness=true. Elapsed: 12.048704148s
Sep 20 02:38:07.741: INFO: Pod "pod-subpath-test-downwardapi-dfs2": Phase="Running", Reason="", readiness=true. Elapsed: 14.05575924s
Sep 20 02:38:09.748: INFO: Pod "pod-subpath-test-downwardapi-dfs2": Phase="Running", Reason="", readiness=true. Elapsed: 16.062528715s
Sep 20 02:38:11.759: INFO: Pod "pod-subpath-test-downwardapi-dfs2": Phase="Running", Reason="", readiness=true. Elapsed: 18.073208099s
Sep 20 02:38:13.766: INFO: Pod "pod-subpath-test-downwardapi-dfs2": Phase="Running", Reason="", readiness=true. Elapsed: 20.08051905s
Sep 20 02:38:15.772: INFO: Pod "pod-subpath-test-downwardapi-dfs2": Phase="Running", Reason="", readiness=true. Elapsed: 22.087080508s
Sep 20 02:38:17.779: INFO: Pod "pod-subpath-test-downwardapi-dfs2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.093883226s
STEP: Saw pod success
Sep 20 02:38:17.779: INFO: Pod "pod-subpath-test-downwardapi-dfs2" satisfied condition "success or failure"
Sep 20 02:38:17.786: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod pod-subpath-test-downwardapi-dfs2 container test-container-subpath-downwardapi-dfs2: <nil>
STEP: delete the pod
Sep 20 02:38:17.825: INFO: Waiting for pod pod-subpath-test-downwardapi-dfs2 to disappear
Sep 20 02:38:17.830: INFO: Pod pod-subpath-test-downwardapi-dfs2 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-dfs2
Sep 20 02:38:17.830: INFO: Deleting pod "pod-subpath-test-downwardapi-dfs2" in namespace "subpath-6336"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:38:17.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6336" for this suite.
Sep 20 02:38:23.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:38:24.068: INFO: namespace subpath-6336 deletion completed in 6.22750062s

• [SLOW TEST:30.584 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:38:24.069: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-5935
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Sep 20 02:38:24.245: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Sep 20 02:38:24.572: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep 20 02:38:26.646: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543904, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543904, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543904, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543904, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 02:38:28.653: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543904, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543904, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543904, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543904, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 02:38:30.653: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543904, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543904, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543904, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543904, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 02:38:32.653: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543904, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543904, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543904, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543904, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 02:38:34.653: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543904, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543904, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543904, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704543904, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 02:38:37.892: INFO: Waited 1.227851792s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:38:38.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5935" for this suite.
Sep 20 02:38:45.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:38:45.274: INFO: namespace aggregator-5935 deletion completed in 6.258593179s

• [SLOW TEST:21.206 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:38:45.276: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8559
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-20d362ad-5346-492b-843c-5839d6524270
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:38:45.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8559" for this suite.
Sep 20 02:38:51.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:38:51.711: INFO: namespace secrets-8559 deletion completed in 6.254014454s

• [SLOW TEST:6.435 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:38:51.715: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1517
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-90a737a1-6993-4329-a640-002ebe9ac5fe
STEP: Creating a pod to test consume secrets
Sep 20 02:38:51.909: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-138954b7-945f-4509-922d-93f3262b1067" in namespace "projected-1517" to be "success or failure"
Sep 20 02:38:51.922: INFO: Pod "pod-projected-secrets-138954b7-945f-4509-922d-93f3262b1067": Phase="Pending", Reason="", readiness=false. Elapsed: 12.887417ms
Sep 20 02:38:53.928: INFO: Pod "pod-projected-secrets-138954b7-945f-4509-922d-93f3262b1067": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019519522s
Sep 20 02:38:55.936: INFO: Pod "pod-projected-secrets-138954b7-945f-4509-922d-93f3262b1067": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027428616s
STEP: Saw pod success
Sep 20 02:38:55.936: INFO: Pod "pod-projected-secrets-138954b7-945f-4509-922d-93f3262b1067" satisfied condition "success or failure"
Sep 20 02:38:55.942: INFO: Trying to get logs from node worker-wqshf-7859ffd555-lmjf4 pod pod-projected-secrets-138954b7-945f-4509-922d-93f3262b1067 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 20 02:38:56.219: INFO: Waiting for pod pod-projected-secrets-138954b7-945f-4509-922d-93f3262b1067 to disappear
Sep 20 02:38:56.224: INFO: Pod pod-projected-secrets-138954b7-945f-4509-922d-93f3262b1067 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:38:56.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1517" for this suite.
Sep 20 02:39:02.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:39:02.471: INFO: namespace projected-1517 deletion completed in 6.240338076s

• [SLOW TEST:10.757 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:39:02.473: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5273
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5273.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5273.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5273.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5273.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5273.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5273.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5273.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5273.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5273.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5273.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5273.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5273.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5273.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5273.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5273.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5273.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5273.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5273.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 20 02:39:06.809: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5273.svc.cluster.local from pod dns-5273/dns-test-2c8e2512-c018-459d-91c2-c9d8c77927c0: the server could not find the requested resource (get pods dns-test-2c8e2512-c018-459d-91c2-c9d8c77927c0)
Sep 20 02:39:06.855: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5273.svc.cluster.local from pod dns-5273/dns-test-2c8e2512-c018-459d-91c2-c9d8c77927c0: the server could not find the requested resource (get pods dns-test-2c8e2512-c018-459d-91c2-c9d8c77927c0)
Sep 20 02:39:06.868: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5273.svc.cluster.local from pod dns-5273/dns-test-2c8e2512-c018-459d-91c2-c9d8c77927c0: the server could not find the requested resource (get pods dns-test-2c8e2512-c018-459d-91c2-c9d8c77927c0)
Sep 20 02:39:06.879: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5273.svc.cluster.local from pod dns-5273/dns-test-2c8e2512-c018-459d-91c2-c9d8c77927c0: the server could not find the requested resource (get pods dns-test-2c8e2512-c018-459d-91c2-c9d8c77927c0)
Sep 20 02:39:07.072: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5273.svc.cluster.local from pod dns-5273/dns-test-2c8e2512-c018-459d-91c2-c9d8c77927c0: the server could not find the requested resource (get pods dns-test-2c8e2512-c018-459d-91c2-c9d8c77927c0)
Sep 20 02:39:07.121: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5273.svc.cluster.local from pod dns-5273/dns-test-2c8e2512-c018-459d-91c2-c9d8c77927c0: the server could not find the requested resource (get pods dns-test-2c8e2512-c018-459d-91c2-c9d8c77927c0)
Sep 20 02:39:07.139: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5273.svc.cluster.local from pod dns-5273/dns-test-2c8e2512-c018-459d-91c2-c9d8c77927c0: the server could not find the requested resource (get pods dns-test-2c8e2512-c018-459d-91c2-c9d8c77927c0)
Sep 20 02:39:07.149: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5273.svc.cluster.local from pod dns-5273/dns-test-2c8e2512-c018-459d-91c2-c9d8c77927c0: the server could not find the requested resource (get pods dns-test-2c8e2512-c018-459d-91c2-c9d8c77927c0)
Sep 20 02:39:07.292: INFO: Lookups using dns-5273/dns-test-2c8e2512-c018-459d-91c2-c9d8c77927c0 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5273.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5273.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5273.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5273.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5273.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5273.svc.cluster.local jessie_udp@dns-test-service-2.dns-5273.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5273.svc.cluster.local]

Sep 20 02:39:13.312: INFO: DNS probes using dns-5273/dns-test-2c8e2512-c018-459d-91c2-c9d8c77927c0 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:39:13.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5273" for this suite.
Sep 20 02:39:19.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:39:19.595: INFO: namespace dns-5273 deletion completed in 6.226558359s

• [SLOW TEST:17.122 seconds]
[sig-network] DNS
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:39:19.602: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1348
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Sep 20 02:39:19.783: INFO: Waiting up to 5m0s for pod "var-expansion-8b88582f-a1a6-4241-b2a2-26bbb5282bc3" in namespace "var-expansion-1348" to be "success or failure"
Sep 20 02:39:19.790: INFO: Pod "var-expansion-8b88582f-a1a6-4241-b2a2-26bbb5282bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.669411ms
Sep 20 02:39:21.797: INFO: Pod "var-expansion-8b88582f-a1a6-4241-b2a2-26bbb5282bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01402822s
Sep 20 02:39:23.803: INFO: Pod "var-expansion-8b88582f-a1a6-4241-b2a2-26bbb5282bc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020566987s
STEP: Saw pod success
Sep 20 02:39:23.803: INFO: Pod "var-expansion-8b88582f-a1a6-4241-b2a2-26bbb5282bc3" satisfied condition "success or failure"
Sep 20 02:39:23.810: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod var-expansion-8b88582f-a1a6-4241-b2a2-26bbb5282bc3 container dapi-container: <nil>
STEP: delete the pod
Sep 20 02:39:23.990: INFO: Waiting for pod var-expansion-8b88582f-a1a6-4241-b2a2-26bbb5282bc3 to disappear
Sep 20 02:39:24.004: INFO: Pod var-expansion-8b88582f-a1a6-4241-b2a2-26bbb5282bc3 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:39:24.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1348" for this suite.
Sep 20 02:39:30.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:39:30.216: INFO: namespace var-expansion-1348 deletion completed in 6.204986022s

• [SLOW TEST:10.614 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:39:30.230: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-146
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-146/configmap-test-cde01cc8-526e-4b30-b3d5-91be3ac139c8
STEP: Creating a pod to test consume configMaps
Sep 20 02:39:30.427: INFO: Waiting up to 5m0s for pod "pod-configmaps-d895f6b0-7f76-447a-bcbd-9caf7ead1fd3" in namespace "configmap-146" to be "success or failure"
Sep 20 02:39:30.442: INFO: Pod "pod-configmaps-d895f6b0-7f76-447a-bcbd-9caf7ead1fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.444393ms
Sep 20 02:39:32.449: INFO: Pod "pod-configmaps-d895f6b0-7f76-447a-bcbd-9caf7ead1fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021433329s
Sep 20 02:39:34.456: INFO: Pod "pod-configmaps-d895f6b0-7f76-447a-bcbd-9caf7ead1fd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028301985s
STEP: Saw pod success
Sep 20 02:39:34.456: INFO: Pod "pod-configmaps-d895f6b0-7f76-447a-bcbd-9caf7ead1fd3" satisfied condition "success or failure"
Sep 20 02:39:34.462: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod pod-configmaps-d895f6b0-7f76-447a-bcbd-9caf7ead1fd3 container env-test: <nil>
STEP: delete the pod
Sep 20 02:39:34.505: INFO: Waiting for pod pod-configmaps-d895f6b0-7f76-447a-bcbd-9caf7ead1fd3 to disappear
Sep 20 02:39:34.510: INFO: Pod pod-configmaps-d895f6b0-7f76-447a-bcbd-9caf7ead1fd3 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:39:34.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-146" for this suite.
Sep 20 02:39:40.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:39:40.746: INFO: namespace configmap-146 deletion completed in 6.228769835s

• [SLOW TEST:10.517 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:39:40.756: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-490
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 02:39:40.952: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Sep 20 02:39:44.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 --namespace=crd-publish-openapi-490 create -f -'
Sep 20 02:39:45.039: INFO: stderr: ""
Sep 20 02:39:45.039: INFO: stdout: "e2e-test-crd-publish-openapi-4969-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 20 02:39:45.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 --namespace=crd-publish-openapi-490 delete e2e-test-crd-publish-openapi-4969-crds test-foo'
Sep 20 02:39:45.151: INFO: stderr: ""
Sep 20 02:39:45.151: INFO: stdout: "e2e-test-crd-publish-openapi-4969-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Sep 20 02:39:45.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 --namespace=crd-publish-openapi-490 apply -f -'
Sep 20 02:39:45.371: INFO: stderr: ""
Sep 20 02:39:45.371: INFO: stdout: "e2e-test-crd-publish-openapi-4969-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 20 02:39:45.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 --namespace=crd-publish-openapi-490 delete e2e-test-crd-publish-openapi-4969-crds test-foo'
Sep 20 02:39:45.485: INFO: stderr: ""
Sep 20 02:39:45.485: INFO: stdout: "e2e-test-crd-publish-openapi-4969-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Sep 20 02:39:45.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 --namespace=crd-publish-openapi-490 create -f -'
Sep 20 02:39:45.664: INFO: rc: 1
Sep 20 02:39:45.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 --namespace=crd-publish-openapi-490 apply -f -'
Sep 20 02:39:45.818: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Sep 20 02:39:45.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 --namespace=crd-publish-openapi-490 create -f -'
Sep 20 02:39:45.977: INFO: rc: 1
Sep 20 02:39:45.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 --namespace=crd-publish-openapi-490 apply -f -'
Sep 20 02:39:46.137: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Sep 20 02:39:46.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 explain e2e-test-crd-publish-openapi-4969-crds'
Sep 20 02:39:46.309: INFO: stderr: ""
Sep 20 02:39:46.309: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4969-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Sep 20 02:39:46.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 explain e2e-test-crd-publish-openapi-4969-crds.metadata'
Sep 20 02:39:46.487: INFO: stderr: ""
Sep 20 02:39:46.487: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4969-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Sep 20 02:39:46.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 explain e2e-test-crd-publish-openapi-4969-crds.spec'
Sep 20 02:39:46.660: INFO: stderr: ""
Sep 20 02:39:46.660: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4969-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Sep 20 02:39:46.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 explain e2e-test-crd-publish-openapi-4969-crds.spec.bars'
Sep 20 02:39:46.873: INFO: stderr: ""
Sep 20 02:39:46.873: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4969-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Sep 20 02:39:46.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 explain e2e-test-crd-publish-openapi-4969-crds.spec.bars2'
Sep 20 02:39:47.044: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:39:50.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-490" for this suite.
Sep 20 02:39:56.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:39:56.349: INFO: namespace crd-publish-openapi-490 deletion completed in 6.305186629s

• [SLOW TEST:15.593 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:39:56.350: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9366
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 20 02:39:56.564: INFO: Waiting up to 5m0s for pod "pod-86d02d74-dd43-42b3-b4cf-104fd8f17cc7" in namespace "emptydir-9366" to be "success or failure"
Sep 20 02:39:56.572: INFO: Pod "pod-86d02d74-dd43-42b3-b4cf-104fd8f17cc7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.146546ms
Sep 20 02:39:58.578: INFO: Pod "pod-86d02d74-dd43-42b3-b4cf-104fd8f17cc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013969552s
Sep 20 02:40:00.585: INFO: Pod "pod-86d02d74-dd43-42b3-b4cf-104fd8f17cc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020416312s
STEP: Saw pod success
Sep 20 02:40:00.585: INFO: Pod "pod-86d02d74-dd43-42b3-b4cf-104fd8f17cc7" satisfied condition "success or failure"
Sep 20 02:40:00.590: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod pod-86d02d74-dd43-42b3-b4cf-104fd8f17cc7 container test-container: <nil>
STEP: delete the pod
Sep 20 02:40:00.638: INFO: Waiting for pod pod-86d02d74-dd43-42b3-b4cf-104fd8f17cc7 to disappear
Sep 20 02:40:00.645: INFO: Pod pod-86d02d74-dd43-42b3-b4cf-104fd8f17cc7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:40:00.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9366" for this suite.
Sep 20 02:40:06.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:40:06.898: INFO: namespace emptydir-9366 deletion completed in 6.242825973s

• [SLOW TEST:10.548 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:40:06.899: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6935
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:40:15.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6935" for this suite.
Sep 20 02:40:21.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:40:21.331: INFO: namespace job-6935 deletion completed in 6.239418524s

• [SLOW TEST:14.433 seconds]
[sig-apps] Job
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:40:21.337: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7439
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 02:40:21.522: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-92dd2e24-281a-426d-a77c-b4459b5e0488" in namespace "security-context-test-7439" to be "success or failure"
Sep 20 02:40:21.528: INFO: Pod "busybox-privileged-false-92dd2e24-281a-426d-a77c-b4459b5e0488": Phase="Pending", Reason="", readiness=false. Elapsed: 5.595069ms
Sep 20 02:40:23.535: INFO: Pod "busybox-privileged-false-92dd2e24-281a-426d-a77c-b4459b5e0488": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012809487s
Sep 20 02:40:25.542: INFO: Pod "busybox-privileged-false-92dd2e24-281a-426d-a77c-b4459b5e0488": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019910886s
Sep 20 02:40:25.542: INFO: Pod "busybox-privileged-false-92dd2e24-281a-426d-a77c-b4459b5e0488" satisfied condition "success or failure"
Sep 20 02:40:25.571: INFO: Got logs for pod "busybox-privileged-false-92dd2e24-281a-426d-a77c-b4459b5e0488": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:40:25.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7439" for this suite.
Sep 20 02:40:31.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:40:31.823: INFO: namespace security-context-test-7439 deletion completed in 6.24508796s

• [SLOW TEST:10.487 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:40:31.838: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5071
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-097768c3-1217-44ca-9db8-e486bf5eb1d5 in namespace container-probe-5071
Sep 20 02:40:36.048: INFO: Started pod busybox-097768c3-1217-44ca-9db8-e486bf5eb1d5 in namespace container-probe-5071
STEP: checking the pod's current state and verifying that restartCount is present
Sep 20 02:40:36.109: INFO: Initial restart count of pod busybox-097768c3-1217-44ca-9db8-e486bf5eb1d5 is 0
Sep 20 02:41:26.290: INFO: Restart count of pod container-probe-5071/busybox-097768c3-1217-44ca-9db8-e486bf5eb1d5 is now 1 (50.180941612s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:41:26.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5071" for this suite.
Sep 20 02:41:32.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:41:32.545: INFO: namespace container-probe-5071 deletion completed in 6.225253936s

• [SLOW TEST:60.708 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:41:32.553: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8488
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-8488
STEP: creating replication controller nodeport-test in namespace services-8488
I0920 02:41:32.754782      16 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-8488, replica count: 2
I0920 02:41:32.755589      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 02:41:32.755651      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
Sep 20 02:41:35.806: INFO: Creating new exec pod
I0920 02:41:35.806123      16 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 02:41:39.843158      16 reflector.go:120] Starting reflector *v1.Endpoints (0s) from k8s.io/kubernetes/test/e2e/framework/service/jig.go:389
I0920 02:41:39.843325      16 reflector.go:158] Listing and watching *v1.Endpoints from k8s.io/kubernetes/test/e2e/framework/service/jig.go:389
Sep 20 02:41:40.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=services-8488 execpodx4ggs -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Sep 20 02:41:41.597: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep 20 02:41:41.597: INFO: stdout: ""
Sep 20 02:41:41.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=services-8488 execpodx4ggs -- /bin/sh -x -c nc -zv -t -w 2 10.240.25.98 80'
Sep 20 02:41:42.358: INFO: stderr: "+ nc -zv -t -w 2 10.240.25.98 80\nConnection to 10.240.25.98 80 port [tcp/http] succeeded!\n"
Sep 20 02:41:42.358: INFO: stdout: ""
Sep 20 02:41:42.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=services-8488 execpodx4ggs -- /bin/sh -x -c nc -zv -t -w 2 104.248.240.176 30908'
Sep 20 02:41:43.095: INFO: stderr: "+ nc -zv -t -w 2 104.248.240.176 30908\nConnection to 104.248.240.176 30908 port [tcp/30908] succeeded!\n"
Sep 20 02:41:43.095: INFO: stdout: ""
Sep 20 02:41:43.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 exec --namespace=services-8488 execpodx4ggs -- /bin/sh -x -c nc -zv -t -w 2 104.248.240.179 30908'
Sep 20 02:41:43.818: INFO: stderr: "+ nc -zv -t -w 2 104.248.240.179 30908\nConnection to 104.248.240.179 30908 port [tcp/30908] succeeded!\n"
Sep 20 02:41:43.818: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:41:43.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8488" for this suite.
Sep 20 02:41:49.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:41:50.050: INFO: namespace services-8488 deletion completed in 6.220872168s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:17.499 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:41:50.066: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3386
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-bc432670-62b8-45b7-b241-11b708f98843
STEP: Creating a pod to test consume configMaps
Sep 20 02:41:50.253: INFO: Waiting up to 5m0s for pod "pod-configmaps-1a795574-d9a8-4e6c-bc6e-029eeb86c096" in namespace "configmap-3386" to be "success or failure"
Sep 20 02:41:50.258: INFO: Pod "pod-configmaps-1a795574-d9a8-4e6c-bc6e-029eeb86c096": Phase="Pending", Reason="", readiness=false. Elapsed: 5.025517ms
Sep 20 02:41:52.265: INFO: Pod "pod-configmaps-1a795574-d9a8-4e6c-bc6e-029eeb86c096": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012055487s
Sep 20 02:41:54.275: INFO: Pod "pod-configmaps-1a795574-d9a8-4e6c-bc6e-029eeb86c096": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021530926s
STEP: Saw pod success
Sep 20 02:41:54.275: INFO: Pod "pod-configmaps-1a795574-d9a8-4e6c-bc6e-029eeb86c096" satisfied condition "success or failure"
Sep 20 02:41:54.284: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod pod-configmaps-1a795574-d9a8-4e6c-bc6e-029eeb86c096 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 02:41:54.326: INFO: Waiting for pod pod-configmaps-1a795574-d9a8-4e6c-bc6e-029eeb86c096 to disappear
Sep 20 02:41:54.331: INFO: Pod pod-configmaps-1a795574-d9a8-4e6c-bc6e-029eeb86c096 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:41:54.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3386" for this suite.
Sep 20 02:42:00.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:42:00.563: INFO: namespace configmap-3386 deletion completed in 6.221490508s

• [SLOW TEST:10.498 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:42:00.569: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9676
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 02:42:00.745: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b2cd6871-4b27-4454-a6d2-6ea07df978a8" in namespace "projected-9676" to be "success or failure"
Sep 20 02:42:00.755: INFO: Pod "downwardapi-volume-b2cd6871-4b27-4454-a6d2-6ea07df978a8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.251599ms
Sep 20 02:42:02.763: INFO: Pod "downwardapi-volume-b2cd6871-4b27-4454-a6d2-6ea07df978a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017107963s
Sep 20 02:42:04.769: INFO: Pod "downwardapi-volume-b2cd6871-4b27-4454-a6d2-6ea07df978a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023443519s
STEP: Saw pod success
Sep 20 02:42:04.769: INFO: Pod "downwardapi-volume-b2cd6871-4b27-4454-a6d2-6ea07df978a8" satisfied condition "success or failure"
Sep 20 02:42:04.775: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod downwardapi-volume-b2cd6871-4b27-4454-a6d2-6ea07df978a8 container client-container: <nil>
STEP: delete the pod
Sep 20 02:42:04.812: INFO: Waiting for pod downwardapi-volume-b2cd6871-4b27-4454-a6d2-6ea07df978a8 to disappear
Sep 20 02:42:04.825: INFO: Pod downwardapi-volume-b2cd6871-4b27-4454-a6d2-6ea07df978a8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:42:04.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9676" for this suite.
Sep 20 02:42:10.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:42:11.062: INFO: namespace projected-9676 deletion completed in 6.230969036s

• [SLOW TEST:10.493 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:42:11.062: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8104
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-8104
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-8104
STEP: Creating statefulset with conflicting port in namespace statefulset-8104
STEP: Waiting until pod test-pod will start running in namespace statefulset-8104
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8104
Sep 20 02:42:15.279: INFO: Observed stateful pod in namespace: statefulset-8104, name: ss-0, uid: c66a40c0-42dc-46c8-8702-25888bccc821, status phase: Pending. Waiting for statefulset controller to delete.
Sep 20 02:42:15.663: INFO: Observed stateful pod in namespace: statefulset-8104, name: ss-0, uid: c66a40c0-42dc-46c8-8702-25888bccc821, status phase: Failed. Waiting for statefulset controller to delete.
Sep 20 02:42:15.672: INFO: Observed stateful pod in namespace: statefulset-8104, name: ss-0, uid: c66a40c0-42dc-46c8-8702-25888bccc821, status phase: Failed. Waiting for statefulset controller to delete.
Sep 20 02:42:15.679: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8104
STEP: Removing pod with conflicting port in namespace statefulset-8104
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8104 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Sep 20 02:42:21.730: INFO: Deleting all statefulset in ns statefulset-8104
Sep 20 02:42:21.736: INFO: Scaling statefulset ss to 0
Sep 20 02:42:41.766: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 02:42:41.771: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:42:41.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8104" for this suite.
Sep 20 02:42:47.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:42:48.026: INFO: namespace statefulset-8104 deletion completed in 6.222398785s

• [SLOW TEST:36.964 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:42:48.027: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2273
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-2273
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2273 to expose endpoints map[]
Sep 20 02:42:48.221: INFO: successfully validated that service multi-endpoint-test in namespace services-2273 exposes endpoints map[] (8.332271ms elapsed)
STEP: Creating pod pod1 in namespace services-2273
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2273 to expose endpoints map[pod1:[100]]
Sep 20 02:42:51.288: INFO: successfully validated that service multi-endpoint-test in namespace services-2273 exposes endpoints map[pod1:[100]] (3.053047182s elapsed)
STEP: Creating pod pod2 in namespace services-2273
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2273 to expose endpoints map[pod1:[100] pod2:[101]]
Sep 20 02:42:54.378: INFO: successfully validated that service multi-endpoint-test in namespace services-2273 exposes endpoints map[pod1:[100] pod2:[101]] (3.081097621s elapsed)
STEP: Deleting pod pod1 in namespace services-2273
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2273 to expose endpoints map[pod2:[101]]
Sep 20 02:42:54.404: INFO: successfully validated that service multi-endpoint-test in namespace services-2273 exposes endpoints map[pod2:[101]] (15.313352ms elapsed)
STEP: Deleting pod pod2 in namespace services-2273
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2273 to expose endpoints map[]
Sep 20 02:42:54.426: INFO: successfully validated that service multi-endpoint-test in namespace services-2273 exposes endpoints map[] (7.075452ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:42:54.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2273" for this suite.
I0920 02:42:57.925053      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
Sep 20 02:43:06.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:43:06.693: INFO: namespace services-2273 deletion completed in 12.233813389s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:18.667 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:43:06.695: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4781
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:43:06.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4781" for this suite.
Sep 20 02:43:12.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:43:13.137: INFO: namespace services-4781 deletion completed in 6.237672084s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:6.442 seconds]
[sig-network] Services
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:43:13.146: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4667
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 02:43:13.314: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: creating the pod
STEP: submitting the pod to kubernetes
I0920 02:43:15.791551      16 reflector.go:383] k8s.io/kubernetes/test/e2e/scheduling/taints.go:146: Watch close - *v1.Pod total 1 items received
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:43:17.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4667" for this suite.
Sep 20 02:44:05.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:44:05.968: INFO: namespace pods-4667 deletion completed in 48.215455337s

• [SLOW TEST:52.822 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:44:05.973: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6034
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-fa4ae4d5-b12c-41df-89da-3305fa1290b9
STEP: Creating a pod to test consume configMaps
Sep 20 02:44:06.235: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c9f00b07-cdd3-46f1-b3bc-ee13b805a7dc" in namespace "projected-6034" to be "success or failure"
Sep 20 02:44:06.241: INFO: Pod "pod-projected-configmaps-c9f00b07-cdd3-46f1-b3bc-ee13b805a7dc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.54661ms
Sep 20 02:44:08.264: INFO: Pod "pod-projected-configmaps-c9f00b07-cdd3-46f1-b3bc-ee13b805a7dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029023618s
Sep 20 02:44:10.270: INFO: Pod "pod-projected-configmaps-c9f00b07-cdd3-46f1-b3bc-ee13b805a7dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034798812s
STEP: Saw pod success
Sep 20 02:44:10.270: INFO: Pod "pod-projected-configmaps-c9f00b07-cdd3-46f1-b3bc-ee13b805a7dc" satisfied condition "success or failure"
Sep 20 02:44:10.276: INFO: Trying to get logs from node worker-wqshf-7859ffd555-lmjf4 pod pod-projected-configmaps-c9f00b07-cdd3-46f1-b3bc-ee13b805a7dc container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 02:44:10.460: INFO: Waiting for pod pod-projected-configmaps-c9f00b07-cdd3-46f1-b3bc-ee13b805a7dc to disappear
Sep 20 02:44:10.464: INFO: Pod pod-projected-configmaps-c9f00b07-cdd3-46f1-b3bc-ee13b805a7dc no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:44:10.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6034" for this suite.
Sep 20 02:44:16.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:44:16.712: INFO: namespace projected-6034 deletion completed in 6.241613275s

• [SLOW TEST:10.740 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:44:16.726: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2828
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 02:44:17.220: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 02:44:19.243: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704544257, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704544257, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704544257, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704544257, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 02:44:22.276: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 02:44:22.282: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:44:23.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2828" for this suite.
Sep 20 02:44:29.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:44:29.918: INFO: namespace webhook-2828 deletion completed in 6.231092931s
STEP: Destroying namespace "webhook-2828-markers" for this suite.
Sep 20 02:44:35.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:44:36.218: INFO: namespace webhook-2828-markers deletion completed in 6.299781766s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:19.520 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:44:36.255: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2866
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-936b71d8-da3e-4d2a-aa19-62d19c6b7874
STEP: Creating a pod to test consume secrets
Sep 20 02:44:36.445: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-184e2bc8-f13c-44f1-95ba-b72f7ebf53db" in namespace "projected-2866" to be "success or failure"
Sep 20 02:44:36.449: INFO: Pod "pod-projected-secrets-184e2bc8-f13c-44f1-95ba-b72f7ebf53db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.332046ms
Sep 20 02:44:38.457: INFO: Pod "pod-projected-secrets-184e2bc8-f13c-44f1-95ba-b72f7ebf53db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011680853s
Sep 20 02:44:40.465: INFO: Pod "pod-projected-secrets-184e2bc8-f13c-44f1-95ba-b72f7ebf53db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019811184s
STEP: Saw pod success
Sep 20 02:44:40.465: INFO: Pod "pod-projected-secrets-184e2bc8-f13c-44f1-95ba-b72f7ebf53db" satisfied condition "success or failure"
Sep 20 02:44:40.471: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod pod-projected-secrets-184e2bc8-f13c-44f1-95ba-b72f7ebf53db container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 20 02:44:40.652: INFO: Waiting for pod pod-projected-secrets-184e2bc8-f13c-44f1-95ba-b72f7ebf53db to disappear
Sep 20 02:44:40.657: INFO: Pod pod-projected-secrets-184e2bc8-f13c-44f1-95ba-b72f7ebf53db no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:44:40.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2866" for this suite.
Sep 20 02:44:46.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:44:47.026: INFO: namespace projected-2866 deletion completed in 6.343739436s

• [SLOW TEST:10.772 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:44:47.032: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2104
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 20 02:44:47.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-2104'
Sep 20 02:44:47.337: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 20 02:44:47.337: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Sep 20 02:44:47.350: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Sep 20 02:44:47.358: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Sep 20 02:44:47.371: INFO: scanned /root for discovery docs: <nil>
Sep 20 02:44:47.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-2104'
Sep 20 02:45:03.267: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep 20 02:45:03.267: INFO: stdout: "Created e2e-test-httpd-rc-0851d4c62124dc1ba8b30734c106dc2a\nScaling up e2e-test-httpd-rc-0851d4c62124dc1ba8b30734c106dc2a from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-0851d4c62124dc1ba8b30734c106dc2a up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-0851d4c62124dc1ba8b30734c106dc2a to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Sep 20 02:45:03.267: INFO: stdout: "Created e2e-test-httpd-rc-0851d4c62124dc1ba8b30734c106dc2a\nScaling up e2e-test-httpd-rc-0851d4c62124dc1ba8b30734c106dc2a from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-0851d4c62124dc1ba8b30734c106dc2a up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-0851d4c62124dc1ba8b30734c106dc2a to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Sep 20 02:45:03.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-2104'
Sep 20 02:45:03.363: INFO: stderr: ""
Sep 20 02:45:03.363: INFO: stdout: "e2e-test-httpd-rc-0851d4c62124dc1ba8b30734c106dc2a-jssxq "
Sep 20 02:45:03.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods e2e-test-httpd-rc-0851d4c62124dc1ba8b30734c106dc2a-jssxq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2104'
Sep 20 02:45:03.449: INFO: stderr: ""
Sep 20 02:45:03.449: INFO: stdout: "true"
Sep 20 02:45:03.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 get pods e2e-test-httpd-rc-0851d4c62124dc1ba8b30734c106dc2a-jssxq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2104'
Sep 20 02:45:03.560: INFO: stderr: ""
Sep 20 02:45:03.560: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Sep 20 02:45:03.560: INFO: e2e-test-httpd-rc-0851d4c62124dc1ba8b30734c106dc2a-jssxq is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Sep 20 02:45:03.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 delete rc e2e-test-httpd-rc --namespace=kubectl-2104'
Sep 20 02:45:03.654: INFO: stderr: ""
Sep 20 02:45:03.654: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:45:03.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2104" for this suite.
Sep 20 02:45:15.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:45:15.880: INFO: namespace kubectl-2104 deletion completed in 12.217696671s

• [SLOW TEST:28.850 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:45:15.889: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8193
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-8193
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Sep 20 02:45:16.105: INFO: Found 0 stateful pods, waiting for 3
Sep 20 02:45:26.113: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 02:45:26.114: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 02:45:26.114: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Sep 20 02:45:26.154: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep 20 02:45:36.210: INFO: Updating stateful set ss2
Sep 20 02:45:36.223: INFO: Waiting for Pod statefulset-8193/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Sep 20 02:45:46.281: INFO: Found 2 stateful pods, waiting for 3
Sep 20 02:45:56.295: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 02:45:56.295: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 02:45:56.295: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep 20 02:45:56.333: INFO: Updating stateful set ss2
Sep 20 02:45:56.344: INFO: Waiting for Pod statefulset-8193/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 20 02:46:06.382: INFO: Updating stateful set ss2
Sep 20 02:46:06.394: INFO: Waiting for StatefulSet statefulset-8193/ss2 to complete update
Sep 20 02:46:06.395: INFO: Waiting for Pod statefulset-8193/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Sep 20 02:46:16.413: INFO: Deleting all statefulset in ns statefulset-8193
Sep 20 02:46:16.424: INFO: Scaling statefulset ss2 to 0
Sep 20 02:46:46.458: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 02:46:46.463: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:46:46.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8193" for this suite.
Sep 20 02:46:52.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:46:52.742: INFO: namespace statefulset-8193 deletion completed in 6.238711976s

• [SLOW TEST:96.854 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:46:52.750: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2228
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:46:52.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2228" for this suite.
Sep 20 02:46:58.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:46:59.160: INFO: namespace custom-resource-definition-2228 deletion completed in 6.221402672s

• [SLOW TEST:6.411 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:46:59.162: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6799
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 02:46:59.463: INFO: (0) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 120.624597ms)
Sep 20 02:46:59.474: INFO: (1) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.238667ms)
Sep 20 02:46:59.487: INFO: (2) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.426743ms)
Sep 20 02:46:59.498: INFO: (3) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.834477ms)
Sep 20 02:46:59.510: INFO: (4) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.293055ms)
Sep 20 02:46:59.521: INFO: (5) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.516458ms)
Sep 20 02:46:59.532: INFO: (6) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.954004ms)
Sep 20 02:46:59.542: INFO: (7) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.864726ms)
Sep 20 02:46:59.552: INFO: (8) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.282512ms)
Sep 20 02:46:59.565: INFO: (9) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.343131ms)
Sep 20 02:46:59.576: INFO: (10) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.06181ms)
Sep 20 02:46:59.589: INFO: (11) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 13.136233ms)
Sep 20 02:46:59.602: INFO: (12) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.610557ms)
Sep 20 02:46:59.614: INFO: (13) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.619703ms)
Sep 20 02:46:59.629: INFO: (14) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 15.158668ms)
Sep 20 02:46:59.641: INFO: (15) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.234481ms)
Sep 20 02:46:59.653: INFO: (16) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.01062ms)
Sep 20 02:46:59.664: INFO: (17) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.014712ms)
Sep 20 02:46:59.675: INFO: (18) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.494772ms)
Sep 20 02:46:59.693: INFO: (19) /api/v1/nodes/worker-wqshf-7859ffd555-kqpmz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 17.391045ms)
[AfterEach] version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:46:59.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6799" for this suite.
Sep 20 02:47:05.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:47:05.928: INFO: namespace proxy-6799 deletion completed in 6.227618655s

• [SLOW TEST:6.767 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:47:05.928: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2504
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 20 02:47:06.123: INFO: Waiting up to 5m0s for pod "pod-e039bfb4-29b1-4f0c-b49f-c4ad1f140ff4" in namespace "emptydir-2504" to be "success or failure"
Sep 20 02:47:06.130: INFO: Pod "pod-e039bfb4-29b1-4f0c-b49f-c4ad1f140ff4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.895278ms
Sep 20 02:47:08.138: INFO: Pod "pod-e039bfb4-29b1-4f0c-b49f-c4ad1f140ff4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013999766s
Sep 20 02:47:10.148: INFO: Pod "pod-e039bfb4-29b1-4f0c-b49f-c4ad1f140ff4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024271085s
STEP: Saw pod success
Sep 20 02:47:10.149: INFO: Pod "pod-e039bfb4-29b1-4f0c-b49f-c4ad1f140ff4" satisfied condition "success or failure"
Sep 20 02:47:10.156: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod pod-e039bfb4-29b1-4f0c-b49f-c4ad1f140ff4 container test-container: <nil>
STEP: delete the pod
Sep 20 02:47:10.335: INFO: Waiting for pod pod-e039bfb4-29b1-4f0c-b49f-c4ad1f140ff4 to disappear
Sep 20 02:47:10.341: INFO: Pod pod-e039bfb4-29b1-4f0c-b49f-c4ad1f140ff4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:47:10.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2504" for this suite.
Sep 20 02:47:16.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:47:16.592: INFO: namespace emptydir-2504 deletion completed in 6.242926438s

• [SLOW TEST:10.664 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:47:16.593: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8879
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Sep 20 02:47:16.808: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f5d075f-d04a-4a5b-bd1e-0c997eb417de" in namespace "downward-api-8879" to be "success or failure"
Sep 20 02:47:16.823: INFO: Pod "downwardapi-volume-3f5d075f-d04a-4a5b-bd1e-0c997eb417de": Phase="Pending", Reason="", readiness=false. Elapsed: 15.142039ms
Sep 20 02:47:18.830: INFO: Pod "downwardapi-volume-3f5d075f-d04a-4a5b-bd1e-0c997eb417de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022545748s
Sep 20 02:47:20.836: INFO: Pod "downwardapi-volume-3f5d075f-d04a-4a5b-bd1e-0c997eb417de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02839942s
STEP: Saw pod success
Sep 20 02:47:20.836: INFO: Pod "downwardapi-volume-3f5d075f-d04a-4a5b-bd1e-0c997eb417de" satisfied condition "success or failure"
Sep 20 02:47:20.843: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod downwardapi-volume-3f5d075f-d04a-4a5b-bd1e-0c997eb417de container client-container: <nil>
STEP: delete the pod
Sep 20 02:47:20.898: INFO: Waiting for pod downwardapi-volume-3f5d075f-d04a-4a5b-bd1e-0c997eb417de to disappear
Sep 20 02:47:20.902: INFO: Pod downwardapi-volume-3f5d075f-d04a-4a5b-bd1e-0c997eb417de no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:47:20.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8879" for this suite.
Sep 20 02:47:26.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:47:27.177: INFO: namespace downward-api-8879 deletion completed in 6.267306776s

• [SLOW TEST:10.584 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:47:27.180: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1902
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-c3e5a10a-ebb7-4d86-9ed6-12021c9e2201
STEP: Creating a pod to test consume configMaps
Sep 20 02:47:27.374: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-529d5ce1-f35b-4f84-bd9a-87207a9e0ed2" in namespace "projected-1902" to be "success or failure"
Sep 20 02:47:27.380: INFO: Pod "pod-projected-configmaps-529d5ce1-f35b-4f84-bd9a-87207a9e0ed2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.201509ms
Sep 20 02:47:29.387: INFO: Pod "pod-projected-configmaps-529d5ce1-f35b-4f84-bd9a-87207a9e0ed2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012092107s
Sep 20 02:47:31.395: INFO: Pod "pod-projected-configmaps-529d5ce1-f35b-4f84-bd9a-87207a9e0ed2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020936003s
STEP: Saw pod success
Sep 20 02:47:31.396: INFO: Pod "pod-projected-configmaps-529d5ce1-f35b-4f84-bd9a-87207a9e0ed2" satisfied condition "success or failure"
Sep 20 02:47:31.402: INFO: Trying to get logs from node worker-wqshf-7859ffd555-lmjf4 pod pod-projected-configmaps-529d5ce1-f35b-4f84-bd9a-87207a9e0ed2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 20 02:47:31.614: INFO: Waiting for pod pod-projected-configmaps-529d5ce1-f35b-4f84-bd9a-87207a9e0ed2 to disappear
Sep 20 02:47:31.624: INFO: Pod pod-projected-configmaps-529d5ce1-f35b-4f84-bd9a-87207a9e0ed2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:47:31.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1902" for this suite.
Sep 20 02:47:37.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:47:37.857: INFO: namespace projected-1902 deletion completed in 6.222392933s

• [SLOW TEST:10.678 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:47:37.864: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4643
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:47:38.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4643" for this suite.
Sep 20 02:47:44.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:47:44.335: INFO: namespace resourcequota-4643 deletion completed in 6.24160096s

• [SLOW TEST:6.472 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:47:44.337: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2051
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 02:47:44.508: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 20 02:47:47.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 --namespace=crd-publish-openapi-2051 create -f -'
Sep 20 02:47:48.539: INFO: stderr: ""
Sep 20 02:47:48.539: INFO: stdout: "e2e-test-crd-publish-openapi-2381-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 20 02:47:48.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 --namespace=crd-publish-openapi-2051 delete e2e-test-crd-publish-openapi-2381-crds test-cr'
Sep 20 02:47:48.637: INFO: stderr: ""
Sep 20 02:47:48.637: INFO: stdout: "e2e-test-crd-publish-openapi-2381-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Sep 20 02:47:48.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 --namespace=crd-publish-openapi-2051 apply -f -'
Sep 20 02:47:48.828: INFO: stderr: ""
Sep 20 02:47:48.828: INFO: stdout: "e2e-test-crd-publish-openapi-2381-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 20 02:47:48.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 --namespace=crd-publish-openapi-2051 delete e2e-test-crd-publish-openapi-2381-crds test-cr'
Sep 20 02:47:48.920: INFO: stderr: ""
Sep 20 02:47:48.920: INFO: stdout: "e2e-test-crd-publish-openapi-2381-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep 20 02:47:48.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 explain e2e-test-crd-publish-openapi-2381-crds'
Sep 20 02:47:49.099: INFO: stderr: ""
Sep 20 02:47:49.099: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2381-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:47:52.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2051" for this suite.
Sep 20 02:47:58.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:47:58.871: INFO: namespace crd-publish-openapi-2051 deletion completed in 6.25399196s

• [SLOW TEST:14.534 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:47:58.874: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6310
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Sep 20 02:47:59.044: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:48:03.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6310" for this suite.
Sep 20 02:48:09.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:48:10.066: INFO: namespace custom-resource-definition-6310 deletion completed in 6.229002249s

• [SLOW TEST:11.192 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:48:10.067: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9309
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 20 02:48:14.787: INFO: Successfully updated pod "pod-update-activedeadlineseconds-20c6a21d-18a3-4438-a0eb-d2aab1efd935"
Sep 20 02:48:14.787: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-20c6a21d-18a3-4438-a0eb-d2aab1efd935" in namespace "pods-9309" to be "terminated due to deadline exceeded"
Sep 20 02:48:14.793: INFO: Pod "pod-update-activedeadlineseconds-20c6a21d-18a3-4438-a0eb-d2aab1efd935": Phase="Running", Reason="", readiness=true. Elapsed: 5.25214ms
Sep 20 02:48:16.800: INFO: Pod "pod-update-activedeadlineseconds-20c6a21d-18a3-4438-a0eb-d2aab1efd935": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.012041735s
Sep 20 02:48:16.800: INFO: Pod "pod-update-activedeadlineseconds-20c6a21d-18a3-4438-a0eb-d2aab1efd935" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:48:16.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9309" for this suite.
Sep 20 02:48:22.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:48:23.047: INFO: namespace pods-9309 deletion completed in 6.239734305s

• [SLOW TEST:12.980 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:48:23.047: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5859
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 20 02:48:23.779: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 02:48:25.798: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704544503, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704544503, loc:(*time.Location)(0x84be2c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704544503, loc:(*time.Location)(0x84be2c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704544503, loc:(*time.Location)(0x84be2c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 20 02:48:28.821: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:48:28.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5859" for this suite.
Sep 20 02:48:34.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:48:35.073: INFO: namespace webhook-5859 deletion completed in 6.219825436s
STEP: Destroying namespace "webhook-5859-markers" for this suite.
Sep 20 02:48:41.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:48:41.304: INFO: namespace webhook-5859-markers deletion completed in 6.230809543s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.280 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:48:41.336: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1189
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Sep 20 02:48:46.088: INFO: Successfully updated pod "annotationupdatedf1eccdd-fc25-48b8-b9a5-aeb06626a93a"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:48:48.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1189" for this suite.
Sep 20 02:49:16.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:49:16.392: INFO: namespace downward-api-1189 deletion completed in 28.232180711s

• [SLOW TEST:35.056 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:49:16.394: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-163
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-163
I0920 02:49:16.569915      16 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-163, replica count: 1
I0920 02:49:16.570673      16 reflector.go:120] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 02:49:16.570726      16 reflector.go:158] Listing and watching *v1.Pod from k8s.io/kubernetes/test/utils/pod_store.go:56
I0920 02:49:17.621043      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 02:49:18.621569      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 02:49:19.621957      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 02:49:19.623145      16 reflector.go:120] Starting reflector *v1.Endpoints (0s) from k8s.io/kubernetes/test/e2e/network/service_latency.go:323
I0920 02:49:19.623195      16 reflector.go:158] Listing and watching *v1.Endpoints from k8s.io/kubernetes/test/e2e/network/service_latency.go:323
Sep 20 02:49:19.739: INFO: Created: latency-svc-tp6wj
Sep 20 02:49:19.748: INFO: Got endpoints: latency-svc-tp6wj [24.317062ms]
Sep 20 02:49:19.766: INFO: Created: latency-svc-vs5dt
Sep 20 02:49:19.772: INFO: Got endpoints: latency-svc-vs5dt [23.309845ms]
Sep 20 02:49:19.776: INFO: Created: latency-svc-tdnjp
Sep 20 02:49:19.782: INFO: Got endpoints: latency-svc-tdnjp [32.353761ms]
Sep 20 02:49:19.784: INFO: Created: latency-svc-4wltq
Sep 20 02:49:19.790: INFO: Got endpoints: latency-svc-4wltq [39.537962ms]
Sep 20 02:49:19.794: INFO: Created: latency-svc-6822n
Sep 20 02:49:19.802: INFO: Created: latency-svc-hzbqd
Sep 20 02:49:19.802: INFO: Got endpoints: latency-svc-6822n [51.467077ms]
Sep 20 02:49:19.812: INFO: Created: latency-svc-v4hnz
Sep 20 02:49:19.812: INFO: Got endpoints: latency-svc-hzbqd [61.986511ms]
Sep 20 02:49:19.817: INFO: Got endpoints: latency-svc-v4hnz [65.925288ms]
Sep 20 02:49:19.826: INFO: Created: latency-svc-j7bzw
Sep 20 02:49:19.826: INFO: Got endpoints: latency-svc-j7bzw [75.515994ms]
Sep 20 02:49:19.831: INFO: Created: latency-svc-kl4dj
Sep 20 02:49:19.839: INFO: Got endpoints: latency-svc-kl4dj [88.162892ms]
Sep 20 02:49:19.847: INFO: Created: latency-svc-rwsxt
Sep 20 02:49:19.852: INFO: Got endpoints: latency-svc-rwsxt [101.051155ms]
Sep 20 02:49:19.860: INFO: Created: latency-svc-hkhtf
Sep 20 02:49:19.863: INFO: Got endpoints: latency-svc-hkhtf [111.804633ms]
Sep 20 02:49:19.867: INFO: Created: latency-svc-sjlxn
Sep 20 02:49:19.873: INFO: Got endpoints: latency-svc-sjlxn [121.743837ms]
Sep 20 02:49:19.879: INFO: Created: latency-svc-whp72
Sep 20 02:49:19.884: INFO: Got endpoints: latency-svc-whp72 [132.912205ms]
Sep 20 02:49:19.892: INFO: Created: latency-svc-bmrp7
Sep 20 02:49:19.901: INFO: Got endpoints: latency-svc-bmrp7 [149.18541ms]
Sep 20 02:49:19.902: INFO: Created: latency-svc-g59vd
Sep 20 02:49:19.909: INFO: Got endpoints: latency-svc-g59vd [156.899093ms]
Sep 20 02:49:19.910: INFO: Created: latency-svc-wkb7c
Sep 20 02:49:19.917: INFO: Got endpoints: latency-svc-wkb7c [165.141839ms]
Sep 20 02:49:19.920: INFO: Created: latency-svc-627bx
Sep 20 02:49:19.928: INFO: Got endpoints: latency-svc-627bx [155.797526ms]
Sep 20 02:49:19.929: INFO: Created: latency-svc-9vtp9
Sep 20 02:49:19.937: INFO: Created: latency-svc-j9b2z
Sep 20 02:49:19.938: INFO: Got endpoints: latency-svc-9vtp9 [29.588829ms]
Sep 20 02:49:19.944: INFO: Got endpoints: latency-svc-j9b2z [161.715053ms]
Sep 20 02:49:19.951: INFO: Created: latency-svc-4gj5x
Sep 20 02:49:19.962: INFO: Got endpoints: latency-svc-4gj5x [172.339667ms]
Sep 20 02:49:19.964: INFO: Created: latency-svc-gbj2r
Sep 20 02:49:19.971: INFO: Created: latency-svc-9nb4p
Sep 20 02:49:19.971: INFO: Got endpoints: latency-svc-gbj2r [167.879646ms]
Sep 20 02:49:19.974: INFO: Got endpoints: latency-svc-9nb4p [161.274957ms]
Sep 20 02:49:19.978: INFO: Created: latency-svc-r4w6p
Sep 20 02:49:19.987: INFO: Got endpoints: latency-svc-r4w6p [170.165637ms]
Sep 20 02:49:19.988: INFO: Created: latency-svc-4fw7q
Sep 20 02:49:19.994: INFO: Got endpoints: latency-svc-4fw7q [167.73956ms]
Sep 20 02:49:19.996: INFO: Created: latency-svc-r5lwn
Sep 20 02:49:19.998: INFO: Got endpoints: latency-svc-r5lwn [158.809444ms]
Sep 20 02:49:20.000: INFO: Created: latency-svc-2vjts
Sep 20 02:49:20.007: INFO: Got endpoints: latency-svc-2vjts [154.504876ms]
Sep 20 02:49:20.012: INFO: Created: latency-svc-pps98
Sep 20 02:49:20.019: INFO: Got endpoints: latency-svc-pps98 [156.019556ms]
Sep 20 02:49:20.020: INFO: Created: latency-svc-qb6p6
Sep 20 02:49:20.024: INFO: Got endpoints: latency-svc-qb6p6 [150.408836ms]
Sep 20 02:49:20.029: INFO: Created: latency-svc-2f6kk
Sep 20 02:49:20.031: INFO: Created: latency-svc-9pc5l
Sep 20 02:49:20.035: INFO: Got endpoints: latency-svc-2f6kk [150.496519ms]
Sep 20 02:49:20.038: INFO: Got endpoints: latency-svc-9pc5l [136.694485ms]
Sep 20 02:49:20.048: INFO: Created: latency-svc-btm2v
Sep 20 02:49:20.053: INFO: Got endpoints: latency-svc-btm2v [136.070598ms]
Sep 20 02:49:20.058: INFO: Created: latency-svc-fznbd
Sep 20 02:49:20.063: INFO: Got endpoints: latency-svc-fznbd [134.797395ms]
Sep 20 02:49:20.067: INFO: Created: latency-svc-kw4fd
Sep 20 02:49:20.074: INFO: Got endpoints: latency-svc-kw4fd [135.871293ms]
Sep 20 02:49:20.075: INFO: Created: latency-svc-hmzxc
Sep 20 02:49:20.079: INFO: Got endpoints: latency-svc-hmzxc [135.184099ms]
Sep 20 02:49:20.082: INFO: Created: latency-svc-g2d4h
Sep 20 02:49:20.087: INFO: Got endpoints: latency-svc-g2d4h [124.559625ms]
Sep 20 02:49:20.089: INFO: Created: latency-svc-gb6l5
Sep 20 02:49:20.096: INFO: Got endpoints: latency-svc-gb6l5 [124.738115ms]
Sep 20 02:49:20.097: INFO: Created: latency-svc-qcwm2
Sep 20 02:49:20.103: INFO: Created: latency-svc-bg7r5
Sep 20 02:49:20.109: INFO: Got endpoints: latency-svc-qcwm2 [134.900745ms]
Sep 20 02:49:20.111: INFO: Created: latency-svc-cdkzh
Sep 20 02:49:20.120: INFO: Created: latency-svc-ggwph
Sep 20 02:49:20.126: INFO: Created: latency-svc-l67c4
Sep 20 02:49:20.133: INFO: Created: latency-svc-7gvnw
Sep 20 02:49:20.143: INFO: Created: latency-svc-k2v5d
Sep 20 02:49:20.150: INFO: Got endpoints: latency-svc-bg7r5 [162.707068ms]
Sep 20 02:49:20.150: INFO: Created: latency-svc-bp7ts
Sep 20 02:49:20.156: INFO: Created: latency-svc-xkh2r
Sep 20 02:49:20.161: INFO: Created: latency-svc-sq6p6
Sep 20 02:49:20.167: INFO: Created: latency-svc-4szjl
Sep 20 02:49:20.175: INFO: Created: latency-svc-cvtqr
Sep 20 02:49:20.185: INFO: Created: latency-svc-cxwjx
Sep 20 02:49:20.191: INFO: Created: latency-svc-xh4gv
Sep 20 02:49:20.198: INFO: Created: latency-svc-wrsn6
Sep 20 02:49:20.199: INFO: Got endpoints: latency-svc-cdkzh [204.193275ms]
Sep 20 02:49:20.209: INFO: Created: latency-svc-rbh7l
Sep 20 02:49:20.217: INFO: Created: latency-svc-n2k97
Sep 20 02:49:20.226: INFO: Created: latency-svc-dns76
Sep 20 02:49:20.250: INFO: Got endpoints: latency-svc-ggwph [251.361198ms]
Sep 20 02:49:20.262: INFO: Created: latency-svc-t5hxr
Sep 20 02:49:20.299: INFO: Got endpoints: latency-svc-l67c4 [292.419589ms]
Sep 20 02:49:20.318: INFO: Created: latency-svc-mvdlb
Sep 20 02:49:20.348: INFO: Got endpoints: latency-svc-7gvnw [327.970909ms]
Sep 20 02:49:20.362: INFO: Created: latency-svc-vj26c
Sep 20 02:49:20.400: INFO: Got endpoints: latency-svc-k2v5d [376.538008ms]
Sep 20 02:49:20.415: INFO: Created: latency-svc-hxbnk
Sep 20 02:49:20.448: INFO: Got endpoints: latency-svc-bp7ts [412.588497ms]
Sep 20 02:49:20.462: INFO: Created: latency-svc-v9vq6
Sep 20 02:49:20.497: INFO: Got endpoints: latency-svc-xkh2r [458.749133ms]
Sep 20 02:49:20.511: INFO: Created: latency-svc-lq5zb
Sep 20 02:49:20.547: INFO: Got endpoints: latency-svc-sq6p6 [493.610452ms]
Sep 20 02:49:20.562: INFO: Created: latency-svc-jp87h
Sep 20 02:49:20.597: INFO: Got endpoints: latency-svc-4szjl [534.282499ms]
Sep 20 02:49:20.611: INFO: Created: latency-svc-sjlv4
Sep 20 02:49:20.647: INFO: Got endpoints: latency-svc-cvtqr [572.73541ms]
Sep 20 02:49:20.664: INFO: Created: latency-svc-k5mdh
Sep 20 02:49:20.699: INFO: Got endpoints: latency-svc-cxwjx [619.416915ms]
Sep 20 02:49:20.714: INFO: Created: latency-svc-6x6m2
Sep 20 02:49:20.750: INFO: Got endpoints: latency-svc-xh4gv [662.204735ms]
Sep 20 02:49:20.764: INFO: Created: latency-svc-qjq25
Sep 20 02:49:20.800: INFO: Got endpoints: latency-svc-wrsn6 [703.94722ms]
Sep 20 02:49:20.822: INFO: Created: latency-svc-7mgh7
Sep 20 02:49:20.849: INFO: Got endpoints: latency-svc-rbh7l [739.917684ms]
Sep 20 02:49:20.862: INFO: Created: latency-svc-86hbq
Sep 20 02:49:20.897: INFO: Got endpoints: latency-svc-n2k97 [746.809191ms]
Sep 20 02:49:20.910: INFO: Created: latency-svc-pcz9r
Sep 20 02:49:20.949: INFO: Got endpoints: latency-svc-dns76 [749.685176ms]
Sep 20 02:49:20.963: INFO: Created: latency-svc-nqqqt
Sep 20 02:49:20.999: INFO: Got endpoints: latency-svc-t5hxr [749.269832ms]
Sep 20 02:49:21.015: INFO: Created: latency-svc-qvjdm
Sep 20 02:49:21.046: INFO: Got endpoints: latency-svc-mvdlb [746.930747ms]
Sep 20 02:49:21.064: INFO: Created: latency-svc-56g6h
Sep 20 02:49:21.097: INFO: Got endpoints: latency-svc-vj26c [749.111091ms]
Sep 20 02:49:21.112: INFO: Created: latency-svc-dnzlw
Sep 20 02:49:21.146: INFO: Got endpoints: latency-svc-hxbnk [745.864664ms]
Sep 20 02:49:21.159: INFO: Created: latency-svc-hk75f
Sep 20 02:49:21.207: INFO: Got endpoints: latency-svc-v9vq6 [759.170619ms]
Sep 20 02:49:21.221: INFO: Created: latency-svc-2tm42
Sep 20 02:49:21.253: INFO: Got endpoints: latency-svc-lq5zb [755.692854ms]
Sep 20 02:49:21.268: INFO: Created: latency-svc-wt7z9
Sep 20 02:49:21.300: INFO: Got endpoints: latency-svc-jp87h [752.054819ms]
Sep 20 02:49:21.318: INFO: Created: latency-svc-fflxv
Sep 20 02:49:21.349: INFO: Got endpoints: latency-svc-sjlv4 [751.763182ms]
Sep 20 02:49:21.363: INFO: Created: latency-svc-vvd6s
Sep 20 02:49:21.398: INFO: Got endpoints: latency-svc-k5mdh [750.756843ms]
Sep 20 02:49:21.416: INFO: Created: latency-svc-rbmhw
Sep 20 02:49:21.448: INFO: Got endpoints: latency-svc-6x6m2 [748.475972ms]
Sep 20 02:49:21.461: INFO: Created: latency-svc-bmfcb
Sep 20 02:49:21.501: INFO: Got endpoints: latency-svc-qjq25 [751.261619ms]
Sep 20 02:49:21.519: INFO: Created: latency-svc-kzn8g
Sep 20 02:49:21.548: INFO: Got endpoints: latency-svc-7mgh7 [747.862376ms]
Sep 20 02:49:21.565: INFO: Created: latency-svc-2jm8k
Sep 20 02:49:21.598: INFO: Got endpoints: latency-svc-86hbq [748.383357ms]
Sep 20 02:49:21.622: INFO: Created: latency-svc-rnmnt
Sep 20 02:49:21.648: INFO: Got endpoints: latency-svc-pcz9r [750.960903ms]
Sep 20 02:49:21.665: INFO: Created: latency-svc-h9dlb
Sep 20 02:49:21.696: INFO: Got endpoints: latency-svc-nqqqt [747.076195ms]
Sep 20 02:49:21.712: INFO: Created: latency-svc-x67m7
Sep 20 02:49:21.750: INFO: Got endpoints: latency-svc-qvjdm [750.080694ms]
Sep 20 02:49:21.766: INFO: Created: latency-svc-qzrjt
Sep 20 02:49:21.797: INFO: Got endpoints: latency-svc-56g6h [750.369116ms]
Sep 20 02:49:21.813: INFO: Created: latency-svc-zzs4p
Sep 20 02:49:21.847: INFO: Got endpoints: latency-svc-dnzlw [750.317661ms]
Sep 20 02:49:21.864: INFO: Created: latency-svc-spscm
Sep 20 02:49:21.898: INFO: Got endpoints: latency-svc-hk75f [751.917347ms]
Sep 20 02:49:21.917: INFO: Created: latency-svc-xhrdq
Sep 20 02:49:21.948: INFO: Got endpoints: latency-svc-2tm42 [740.452826ms]
Sep 20 02:49:21.961: INFO: Created: latency-svc-8xvwj
Sep 20 02:49:21.999: INFO: Got endpoints: latency-svc-wt7z9 [745.344256ms]
Sep 20 02:49:22.016: INFO: Created: latency-svc-hrrcr
Sep 20 02:49:22.047: INFO: Got endpoints: latency-svc-fflxv [746.961078ms]
Sep 20 02:49:22.059: INFO: Created: latency-svc-kwf2g
Sep 20 02:49:22.098: INFO: Got endpoints: latency-svc-vvd6s [748.665346ms]
Sep 20 02:49:22.111: INFO: Created: latency-svc-9s9pz
Sep 20 02:49:22.146: INFO: Got endpoints: latency-svc-rbmhw [747.876643ms]
Sep 20 02:49:22.167: INFO: Created: latency-svc-fjrzv
Sep 20 02:49:22.196: INFO: Got endpoints: latency-svc-bmfcb [748.246542ms]
Sep 20 02:49:22.213: INFO: Created: latency-svc-w7pg4
Sep 20 02:49:22.246: INFO: Got endpoints: latency-svc-kzn8g [744.531471ms]
Sep 20 02:49:22.262: INFO: Created: latency-svc-ztbhz
Sep 20 02:49:22.296: INFO: Got endpoints: latency-svc-2jm8k [747.492361ms]
Sep 20 02:49:22.312: INFO: Created: latency-svc-fc8l6
Sep 20 02:49:22.347: INFO: Got endpoints: latency-svc-rnmnt [747.997325ms]
Sep 20 02:49:22.363: INFO: Created: latency-svc-s9xf2
Sep 20 02:49:22.396: INFO: Got endpoints: latency-svc-h9dlb [748.514272ms]
Sep 20 02:49:22.412: INFO: Created: latency-svc-nd9s5
Sep 20 02:49:22.446: INFO: Got endpoints: latency-svc-x67m7 [749.163848ms]
Sep 20 02:49:22.458: INFO: Created: latency-svc-s2v6n
Sep 20 02:49:22.501: INFO: Got endpoints: latency-svc-qzrjt [751.12069ms]
Sep 20 02:49:22.516: INFO: Created: latency-svc-nsf8h
Sep 20 02:49:22.546: INFO: Got endpoints: latency-svc-zzs4p [748.743493ms]
Sep 20 02:49:22.561: INFO: Created: latency-svc-sjfqr
Sep 20 02:49:22.601: INFO: Got endpoints: latency-svc-spscm [753.801202ms]
Sep 20 02:49:22.617: INFO: Created: latency-svc-scrsb
Sep 20 02:49:22.650: INFO: Got endpoints: latency-svc-xhrdq [751.470248ms]
Sep 20 02:49:22.665: INFO: Created: latency-svc-2g7qv
Sep 20 02:49:22.697: INFO: Got endpoints: latency-svc-8xvwj [748.629071ms]
Sep 20 02:49:22.712: INFO: Created: latency-svc-kj8ft
Sep 20 02:49:22.748: INFO: Got endpoints: latency-svc-hrrcr [748.825242ms]
Sep 20 02:49:22.762: INFO: Created: latency-svc-xflk5
Sep 20 02:49:22.798: INFO: Got endpoints: latency-svc-kwf2g [751.019183ms]
Sep 20 02:49:22.818: INFO: Created: latency-svc-6v82x
Sep 20 02:49:22.848: INFO: Got endpoints: latency-svc-9s9pz [750.535856ms]
Sep 20 02:49:22.876: INFO: Created: latency-svc-wtkqj
Sep 20 02:49:22.896: INFO: Got endpoints: latency-svc-fjrzv [749.444138ms]
Sep 20 02:49:22.909: INFO: Created: latency-svc-nz2lk
Sep 20 02:49:22.950: INFO: Got endpoints: latency-svc-w7pg4 [753.692338ms]
Sep 20 02:49:22.968: INFO: Created: latency-svc-xtr65
Sep 20 02:49:22.998: INFO: Got endpoints: latency-svc-ztbhz [752.292089ms]
Sep 20 02:49:23.014: INFO: Created: latency-svc-w5fvq
Sep 20 02:49:23.046: INFO: Got endpoints: latency-svc-fc8l6 [750.438466ms]
Sep 20 02:49:23.061: INFO: Created: latency-svc-xqcrp
Sep 20 02:49:23.098: INFO: Got endpoints: latency-svc-s9xf2 [750.8412ms]
Sep 20 02:49:23.110: INFO: Created: latency-svc-h8h52
Sep 20 02:49:23.147: INFO: Got endpoints: latency-svc-nd9s5 [750.841723ms]
Sep 20 02:49:23.162: INFO: Created: latency-svc-pm9qt
Sep 20 02:49:23.198: INFO: Got endpoints: latency-svc-s2v6n [751.533808ms]
Sep 20 02:49:23.213: INFO: Created: latency-svc-nnmcp
Sep 20 02:49:23.248: INFO: Got endpoints: latency-svc-nsf8h [747.044918ms]
Sep 20 02:49:23.264: INFO: Created: latency-svc-fgkvq
Sep 20 02:49:23.297: INFO: Got endpoints: latency-svc-sjfqr [750.76801ms]
Sep 20 02:49:23.311: INFO: Created: latency-svc-4mtq6
Sep 20 02:49:23.347: INFO: Got endpoints: latency-svc-scrsb [745.036929ms]
Sep 20 02:49:23.362: INFO: Created: latency-svc-6wvm4
Sep 20 02:49:23.398: INFO: Got endpoints: latency-svc-2g7qv [747.481778ms]
Sep 20 02:49:23.412: INFO: Created: latency-svc-q6bhf
Sep 20 02:49:23.449: INFO: Got endpoints: latency-svc-kj8ft [751.121305ms]
Sep 20 02:49:23.464: INFO: Created: latency-svc-ztxks
Sep 20 02:49:23.496: INFO: Got endpoints: latency-svc-xflk5 [748.415778ms]
Sep 20 02:49:23.511: INFO: Created: latency-svc-wxcw4
Sep 20 02:49:23.549: INFO: Got endpoints: latency-svc-6v82x [750.564792ms]
Sep 20 02:49:23.563: INFO: Created: latency-svc-qvmfm
Sep 20 02:49:23.600: INFO: Got endpoints: latency-svc-wtkqj [751.643313ms]
Sep 20 02:49:23.614: INFO: Created: latency-svc-c9rfz
Sep 20 02:49:23.647: INFO: Got endpoints: latency-svc-nz2lk [750.900099ms]
Sep 20 02:49:23.663: INFO: Created: latency-svc-b5frs
Sep 20 02:49:23.698: INFO: Got endpoints: latency-svc-xtr65 [747.741394ms]
Sep 20 02:49:23.727: INFO: Created: latency-svc-t6wj5
Sep 20 02:49:23.749: INFO: Got endpoints: latency-svc-w5fvq [750.16064ms]
Sep 20 02:49:23.762: INFO: Created: latency-svc-z99hc
Sep 20 02:49:23.798: INFO: Got endpoints: latency-svc-xqcrp [751.805348ms]
Sep 20 02:49:23.813: INFO: Created: latency-svc-55w4p
Sep 20 02:49:23.847: INFO: Got endpoints: latency-svc-h8h52 [749.826974ms]
Sep 20 02:49:23.863: INFO: Created: latency-svc-q6zfj
Sep 20 02:49:23.898: INFO: Got endpoints: latency-svc-pm9qt [750.450764ms]
Sep 20 02:49:23.912: INFO: Created: latency-svc-4lksl
Sep 20 02:49:23.949: INFO: Got endpoints: latency-svc-nnmcp [750.372712ms]
Sep 20 02:49:23.964: INFO: Created: latency-svc-v9btb
Sep 20 02:49:23.997: INFO: Got endpoints: latency-svc-fgkvq [749.027874ms]
Sep 20 02:49:24.017: INFO: Created: latency-svc-9qd24
Sep 20 02:49:24.048: INFO: Got endpoints: latency-svc-4mtq6 [750.39277ms]
Sep 20 02:49:24.061: INFO: Created: latency-svc-jtnn2
Sep 20 02:49:24.097: INFO: Got endpoints: latency-svc-6wvm4 [749.47275ms]
Sep 20 02:49:24.112: INFO: Created: latency-svc-jpswk
Sep 20 02:49:24.150: INFO: Got endpoints: latency-svc-q6bhf [752.001738ms]
Sep 20 02:49:24.169: INFO: Created: latency-svc-c72q4
Sep 20 02:49:24.199: INFO: Got endpoints: latency-svc-ztxks [750.076028ms]
Sep 20 02:49:24.217: INFO: Created: latency-svc-x58pk
Sep 20 02:49:24.247: INFO: Got endpoints: latency-svc-wxcw4 [750.134594ms]
Sep 20 02:49:24.264: INFO: Created: latency-svc-5psq6
Sep 20 02:49:24.298: INFO: Got endpoints: latency-svc-qvmfm [748.571132ms]
Sep 20 02:49:24.316: INFO: Created: latency-svc-xcxqq
Sep 20 02:49:24.349: INFO: Got endpoints: latency-svc-c9rfz [748.937181ms]
Sep 20 02:49:24.366: INFO: Created: latency-svc-np94t
Sep 20 02:49:24.399: INFO: Got endpoints: latency-svc-b5frs [751.875988ms]
Sep 20 02:49:24.414: INFO: Created: latency-svc-wc7l6
Sep 20 02:49:24.452: INFO: Got endpoints: latency-svc-t6wj5 [753.67545ms]
Sep 20 02:49:24.469: INFO: Created: latency-svc-kmx8g
Sep 20 02:49:24.498: INFO: Got endpoints: latency-svc-z99hc [749.039374ms]
Sep 20 02:49:24.516: INFO: Created: latency-svc-k48zv
Sep 20 02:49:24.547: INFO: Got endpoints: latency-svc-55w4p [748.064604ms]
Sep 20 02:49:24.562: INFO: Created: latency-svc-gvb96
Sep 20 02:49:24.598: INFO: Got endpoints: latency-svc-q6zfj [750.137037ms]
Sep 20 02:49:24.615: INFO: Created: latency-svc-lkkrq
Sep 20 02:49:24.648: INFO: Got endpoints: latency-svc-4lksl [749.966881ms]
Sep 20 02:49:24.663: INFO: Created: latency-svc-kt7bm
Sep 20 02:49:24.696: INFO: Got endpoints: latency-svc-v9btb [747.731131ms]
Sep 20 02:49:24.711: INFO: Created: latency-svc-klvhl
Sep 20 02:49:24.748: INFO: Got endpoints: latency-svc-9qd24 [750.772583ms]
Sep 20 02:49:24.762: INFO: Created: latency-svc-c6c2g
Sep 20 02:49:24.799: INFO: Got endpoints: latency-svc-jtnn2 [750.642923ms]
Sep 20 02:49:24.815: INFO: Created: latency-svc-nxfgh
Sep 20 02:49:24.847: INFO: Got endpoints: latency-svc-jpswk [749.763386ms]
Sep 20 02:49:24.861: INFO: Created: latency-svc-xtcwk
Sep 20 02:49:24.898: INFO: Got endpoints: latency-svc-c72q4 [748.469301ms]
Sep 20 02:49:24.914: INFO: Created: latency-svc-hs5dc
Sep 20 02:49:24.948: INFO: Got endpoints: latency-svc-x58pk [749.155108ms]
Sep 20 02:49:25.000: INFO: Got endpoints: latency-svc-5psq6 [753.081491ms]
Sep 20 02:49:25.010: INFO: Created: latency-svc-v658k
Sep 20 02:49:25.019: INFO: Created: latency-svc-nxjcm
Sep 20 02:49:25.049: INFO: Got endpoints: latency-svc-xcxqq [751.486924ms]
Sep 20 02:49:25.063: INFO: Created: latency-svc-gw4w7
Sep 20 02:49:25.098: INFO: Got endpoints: latency-svc-np94t [748.647102ms]
Sep 20 02:49:25.111: INFO: Created: latency-svc-hw6c9
Sep 20 02:49:25.148: INFO: Got endpoints: latency-svc-wc7l6 [748.94396ms]
Sep 20 02:49:25.164: INFO: Created: latency-svc-hzlqm
Sep 20 02:49:25.198: INFO: Got endpoints: latency-svc-kmx8g [745.98329ms]
Sep 20 02:49:25.215: INFO: Created: latency-svc-m8ccf
Sep 20 02:49:25.248: INFO: Got endpoints: latency-svc-k48zv [749.365648ms]
Sep 20 02:49:25.272: INFO: Created: latency-svc-kwf7k
Sep 20 02:49:25.299: INFO: Got endpoints: latency-svc-gvb96 [752.322163ms]
Sep 20 02:49:25.314: INFO: Created: latency-svc-wmj85
Sep 20 02:49:25.348: INFO: Got endpoints: latency-svc-lkkrq [749.574354ms]
Sep 20 02:49:25.364: INFO: Created: latency-svc-8qst4
Sep 20 02:49:25.398: INFO: Got endpoints: latency-svc-kt7bm [749.564056ms]
Sep 20 02:49:25.416: INFO: Created: latency-svc-7cfvz
Sep 20 02:49:25.447: INFO: Got endpoints: latency-svc-klvhl [750.250494ms]
Sep 20 02:49:25.463: INFO: Created: latency-svc-j6w2m
Sep 20 02:49:25.500: INFO: Got endpoints: latency-svc-c6c2g [751.805429ms]
Sep 20 02:49:25.514: INFO: Created: latency-svc-zscng
Sep 20 02:49:25.549: INFO: Got endpoints: latency-svc-nxfgh [749.995441ms]
Sep 20 02:49:25.564: INFO: Created: latency-svc-kdkg2
Sep 20 02:49:25.598: INFO: Got endpoints: latency-svc-xtcwk [751.094682ms]
Sep 20 02:49:25.614: INFO: Created: latency-svc-wn2hl
Sep 20 02:49:25.648: INFO: Got endpoints: latency-svc-hs5dc [749.99243ms]
Sep 20 02:49:25.662: INFO: Created: latency-svc-pl87t
Sep 20 02:49:25.697: INFO: Got endpoints: latency-svc-v658k [748.901142ms]
Sep 20 02:49:25.711: INFO: Created: latency-svc-hv9sv
Sep 20 02:49:25.748: INFO: Got endpoints: latency-svc-nxjcm [747.349432ms]
Sep 20 02:49:25.763: INFO: Created: latency-svc-2fm7f
Sep 20 02:49:25.798: INFO: Got endpoints: latency-svc-gw4w7 [748.048537ms]
Sep 20 02:49:25.811: INFO: Created: latency-svc-ns68n
Sep 20 02:49:25.847: INFO: Got endpoints: latency-svc-hw6c9 [748.800577ms]
Sep 20 02:49:25.863: INFO: Created: latency-svc-h2h9h
Sep 20 02:49:25.896: INFO: Got endpoints: latency-svc-hzlqm [748.625988ms]
Sep 20 02:49:25.911: INFO: Created: latency-svc-58vvf
Sep 20 02:49:25.949: INFO: Got endpoints: latency-svc-m8ccf [750.390978ms]
Sep 20 02:49:25.962: INFO: Created: latency-svc-p45jw
Sep 20 02:49:25.997: INFO: Got endpoints: latency-svc-kwf7k [749.002468ms]
Sep 20 02:49:26.012: INFO: Created: latency-svc-p86pq
Sep 20 02:49:26.049: INFO: Got endpoints: latency-svc-wmj85 [749.80168ms]
Sep 20 02:49:26.065: INFO: Created: latency-svc-mpjcw
Sep 20 02:49:26.097: INFO: Got endpoints: latency-svc-8qst4 [749.131209ms]
Sep 20 02:49:26.113: INFO: Created: latency-svc-dt4tr
Sep 20 02:49:26.148: INFO: Got endpoints: latency-svc-7cfvz [750.05789ms]
Sep 20 02:49:26.163: INFO: Created: latency-svc-jc62b
Sep 20 02:49:26.196: INFO: Got endpoints: latency-svc-j6w2m [749.072935ms]
Sep 20 02:49:26.210: INFO: Created: latency-svc-87pc2
Sep 20 02:49:26.247: INFO: Got endpoints: latency-svc-zscng [746.922069ms]
Sep 20 02:49:26.262: INFO: Created: latency-svc-7s47x
Sep 20 02:49:26.297: INFO: Got endpoints: latency-svc-kdkg2 [748.27223ms]
Sep 20 02:49:26.312: INFO: Created: latency-svc-69mkv
Sep 20 02:49:26.348: INFO: Got endpoints: latency-svc-wn2hl [749.881127ms]
Sep 20 02:49:26.363: INFO: Created: latency-svc-xfdh5
Sep 20 02:49:26.398: INFO: Got endpoints: latency-svc-pl87t [749.148805ms]
Sep 20 02:49:26.413: INFO: Created: latency-svc-z4fjl
Sep 20 02:49:26.449: INFO: Got endpoints: latency-svc-hv9sv [751.327324ms]
Sep 20 02:49:26.480: INFO: Created: latency-svc-l5wng
Sep 20 02:49:26.501: INFO: Got endpoints: latency-svc-2fm7f [753.451253ms]
Sep 20 02:49:26.517: INFO: Created: latency-svc-4lmj6
Sep 20 02:49:26.550: INFO: Got endpoints: latency-svc-ns68n [751.390341ms]
Sep 20 02:49:26.565: INFO: Created: latency-svc-w5qb4
Sep 20 02:49:26.598: INFO: Got endpoints: latency-svc-h2h9h [750.244954ms]
Sep 20 02:49:26.615: INFO: Created: latency-svc-vzhjx
Sep 20 02:49:26.647: INFO: Got endpoints: latency-svc-58vvf [750.546078ms]
Sep 20 02:49:26.667: INFO: Created: latency-svc-bxvvn
Sep 20 02:49:26.697: INFO: Got endpoints: latency-svc-p45jw [748.782304ms]
Sep 20 02:49:26.716: INFO: Created: latency-svc-l484j
Sep 20 02:49:26.748: INFO: Got endpoints: latency-svc-p86pq [750.882508ms]
Sep 20 02:49:26.764: INFO: Created: latency-svc-26kjb
Sep 20 02:49:26.801: INFO: Got endpoints: latency-svc-mpjcw [750.930312ms]
Sep 20 02:49:26.816: INFO: Created: latency-svc-f2hst
Sep 20 02:49:26.849: INFO: Got endpoints: latency-svc-dt4tr [751.036996ms]
Sep 20 02:49:26.869: INFO: Created: latency-svc-5p4dj
Sep 20 02:49:26.898: INFO: Got endpoints: latency-svc-jc62b [749.649918ms]
Sep 20 02:49:26.912: INFO: Created: latency-svc-lcf2g
Sep 20 02:49:26.947: INFO: Got endpoints: latency-svc-87pc2 [751.086393ms]
Sep 20 02:49:26.962: INFO: Created: latency-svc-qzfgb
Sep 20 02:49:26.999: INFO: Got endpoints: latency-svc-7s47x [751.999006ms]
Sep 20 02:49:27.017: INFO: Created: latency-svc-59rnt
Sep 20 02:49:27.047: INFO: Got endpoints: latency-svc-69mkv [749.03407ms]
Sep 20 02:49:27.062: INFO: Created: latency-svc-r5fnk
Sep 20 02:49:27.097: INFO: Got endpoints: latency-svc-xfdh5 [749.084625ms]
Sep 20 02:49:27.114: INFO: Created: latency-svc-f6gn6
Sep 20 02:49:27.156: INFO: Got endpoints: latency-svc-z4fjl [758.574444ms]
Sep 20 02:49:27.177: INFO: Created: latency-svc-fdnwp
Sep 20 02:49:27.196: INFO: Got endpoints: latency-svc-l5wng [747.155719ms]
Sep 20 02:49:27.210: INFO: Created: latency-svc-7p6pl
Sep 20 02:49:27.258: INFO: Got endpoints: latency-svc-4lmj6 [756.295924ms]
Sep 20 02:49:27.277: INFO: Created: latency-svc-c5k9x
Sep 20 02:49:27.297: INFO: Got endpoints: latency-svc-w5qb4 [746.94669ms]
Sep 20 02:49:27.313: INFO: Created: latency-svc-j5tg8
Sep 20 02:49:27.347: INFO: Got endpoints: latency-svc-vzhjx [748.808136ms]
Sep 20 02:49:27.365: INFO: Created: latency-svc-gjvgl
Sep 20 02:49:27.398: INFO: Got endpoints: latency-svc-bxvvn [750.358595ms]
Sep 20 02:49:27.412: INFO: Created: latency-svc-qm6pj
Sep 20 02:49:27.449: INFO: Got endpoints: latency-svc-l484j [751.166584ms]
Sep 20 02:49:27.463: INFO: Created: latency-svc-9grpn
Sep 20 02:49:27.497: INFO: Got endpoints: latency-svc-26kjb [749.303721ms]
Sep 20 02:49:27.511: INFO: Created: latency-svc-47jt8
Sep 20 02:49:27.549: INFO: Got endpoints: latency-svc-f2hst [748.114221ms]
Sep 20 02:49:27.562: INFO: Created: latency-svc-r8t7l
Sep 20 02:49:27.597: INFO: Got endpoints: latency-svc-5p4dj [747.675389ms]
Sep 20 02:49:27.662: INFO: Got endpoints: latency-svc-lcf2g [764.362504ms]
Sep 20 02:49:27.697: INFO: Got endpoints: latency-svc-qzfgb [749.249539ms]
Sep 20 02:49:27.748: INFO: Got endpoints: latency-svc-59rnt [748.000327ms]
Sep 20 02:49:27.798: INFO: Got endpoints: latency-svc-r5fnk [750.599155ms]
Sep 20 02:49:27.847: INFO: Got endpoints: latency-svc-f6gn6 [749.52442ms]
Sep 20 02:49:27.897: INFO: Got endpoints: latency-svc-fdnwp [740.313018ms]
Sep 20 02:49:27.948: INFO: Got endpoints: latency-svc-7p6pl [751.670611ms]
Sep 20 02:49:27.996: INFO: Got endpoints: latency-svc-c5k9x [737.629273ms]
Sep 20 02:49:28.047: INFO: Got endpoints: latency-svc-j5tg8 [750.551522ms]
Sep 20 02:49:28.097: INFO: Got endpoints: latency-svc-gjvgl [750.049342ms]
Sep 20 02:49:28.147: INFO: Got endpoints: latency-svc-qm6pj [749.119428ms]
Sep 20 02:49:28.197: INFO: Got endpoints: latency-svc-9grpn [748.566136ms]
Sep 20 02:49:28.247: INFO: Got endpoints: latency-svc-47jt8 [748.790682ms]
Sep 20 02:49:28.296: INFO: Got endpoints: latency-svc-r8t7l [747.53438ms]
Sep 20 02:49:28.297: INFO: Latencies: [23.309845ms 29.588829ms 32.353761ms 39.537962ms 51.467077ms 61.986511ms 65.925288ms 75.515994ms 88.162892ms 101.051155ms 111.804633ms 121.743837ms 124.559625ms 124.738115ms 132.912205ms 134.797395ms 134.900745ms 135.184099ms 135.871293ms 136.070598ms 136.694485ms 149.18541ms 150.408836ms 150.496519ms 154.504876ms 155.797526ms 156.019556ms 156.899093ms 158.809444ms 161.274957ms 161.715053ms 162.707068ms 165.141839ms 167.73956ms 167.879646ms 170.165637ms 172.339667ms 204.193275ms 251.361198ms 292.419589ms 327.970909ms 376.538008ms 412.588497ms 458.749133ms 493.610452ms 534.282499ms 572.73541ms 619.416915ms 662.204735ms 703.94722ms 737.629273ms 739.917684ms 740.313018ms 740.452826ms 744.531471ms 745.036929ms 745.344256ms 745.864664ms 745.98329ms 746.809191ms 746.922069ms 746.930747ms 746.94669ms 746.961078ms 747.044918ms 747.076195ms 747.155719ms 747.349432ms 747.481778ms 747.492361ms 747.53438ms 747.675389ms 747.731131ms 747.741394ms 747.862376ms 747.876643ms 747.997325ms 748.000327ms 748.048537ms 748.064604ms 748.114221ms 748.246542ms 748.27223ms 748.383357ms 748.415778ms 748.469301ms 748.475972ms 748.514272ms 748.566136ms 748.571132ms 748.625988ms 748.629071ms 748.647102ms 748.665346ms 748.743493ms 748.782304ms 748.790682ms 748.800577ms 748.808136ms 748.825242ms 748.901142ms 748.937181ms 748.94396ms 749.002468ms 749.027874ms 749.03407ms 749.039374ms 749.072935ms 749.084625ms 749.111091ms 749.119428ms 749.131209ms 749.148805ms 749.155108ms 749.163848ms 749.249539ms 749.269832ms 749.303721ms 749.365648ms 749.444138ms 749.47275ms 749.52442ms 749.564056ms 749.574354ms 749.649918ms 749.685176ms 749.763386ms 749.80168ms 749.826974ms 749.881127ms 749.966881ms 749.99243ms 749.995441ms 750.049342ms 750.05789ms 750.076028ms 750.080694ms 750.134594ms 750.137037ms 750.16064ms 750.244954ms 750.250494ms 750.317661ms 750.358595ms 750.369116ms 750.372712ms 750.390978ms 750.39277ms 750.438466ms 750.450764ms 750.535856ms 750.546078ms 750.551522ms 750.564792ms 750.599155ms 750.642923ms 750.756843ms 750.76801ms 750.772583ms 750.8412ms 750.841723ms 750.882508ms 750.900099ms 750.930312ms 750.960903ms 751.019183ms 751.036996ms 751.086393ms 751.094682ms 751.12069ms 751.121305ms 751.166584ms 751.261619ms 751.327324ms 751.390341ms 751.470248ms 751.486924ms 751.533808ms 751.643313ms 751.670611ms 751.763182ms 751.805348ms 751.805429ms 751.875988ms 751.917347ms 751.999006ms 752.001738ms 752.054819ms 752.292089ms 752.322163ms 753.081491ms 753.451253ms 753.67545ms 753.692338ms 753.801202ms 755.692854ms 756.295924ms 758.574444ms 759.170619ms 764.362504ms]
Sep 20 02:49:28.298: INFO: 50 %ile: 748.901142ms
Sep 20 02:49:28.298: INFO: 90 %ile: 751.763182ms
Sep 20 02:49:28.298: INFO: 99 %ile: 759.170619ms
Sep 20 02:49:28.298: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:49:28.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-163" for this suite.
Sep 20 02:49:40.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:49:40.545: INFO: namespace svc-latency-163 deletion completed in 12.239396707s

• [SLOW TEST:24.152 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:49:40.550: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6387
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-3d0ca663-a054-4f4c-a4e2-d76786ff293c
STEP: Creating a pod to test consume secrets
Sep 20 02:49:40.735: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ea545f7c-40e3-4156-b17a-e0673809276f" in namespace "projected-6387" to be "success or failure"
Sep 20 02:49:40.745: INFO: Pod "pod-projected-secrets-ea545f7c-40e3-4156-b17a-e0673809276f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.785363ms
Sep 20 02:49:42.751: INFO: Pod "pod-projected-secrets-ea545f7c-40e3-4156-b17a-e0673809276f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016353172s
Sep 20 02:49:44.759: INFO: Pod "pod-projected-secrets-ea545f7c-40e3-4156-b17a-e0673809276f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023713305s
STEP: Saw pod success
Sep 20 02:49:44.759: INFO: Pod "pod-projected-secrets-ea545f7c-40e3-4156-b17a-e0673809276f" satisfied condition "success or failure"
Sep 20 02:49:44.768: INFO: Trying to get logs from node worker-wqshf-7859ffd555-kqpmz pod pod-projected-secrets-ea545f7c-40e3-4156-b17a-e0673809276f container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 20 02:49:44.826: INFO: Waiting for pod pod-projected-secrets-ea545f7c-40e3-4156-b17a-e0673809276f to disappear
Sep 20 02:49:44.830: INFO: Pod pod-projected-secrets-ea545f7c-40e3-4156-b17a-e0673809276f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:49:44.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6387" for this suite.
Sep 20 02:49:50.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:49:51.184: INFO: namespace projected-6387 deletion completed in 6.347046784s

• [SLOW TEST:10.634 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:49:51.190: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9903
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-713c3f04-4ac7-46ae-8ff8-7e4266ff6946
STEP: Creating a pod to test consume secrets
Sep 20 02:49:51.389: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-93fe3072-0961-4ca4-a1a7-dd08e3b67418" in namespace "projected-9903" to be "success or failure"
Sep 20 02:49:51.398: INFO: Pod "pod-projected-secrets-93fe3072-0961-4ca4-a1a7-dd08e3b67418": Phase="Pending", Reason="", readiness=false. Elapsed: 9.418888ms
Sep 20 02:49:53.406: INFO: Pod "pod-projected-secrets-93fe3072-0961-4ca4-a1a7-dd08e3b67418": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017005103s
Sep 20 02:49:55.412: INFO: Pod "pod-projected-secrets-93fe3072-0961-4ca4-a1a7-dd08e3b67418": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022874742s
STEP: Saw pod success
Sep 20 02:49:55.412: INFO: Pod "pod-projected-secrets-93fe3072-0961-4ca4-a1a7-dd08e3b67418" satisfied condition "success or failure"
Sep 20 02:49:55.417: INFO: Trying to get logs from node worker-wqshf-7859ffd555-r6tnq pod pod-projected-secrets-93fe3072-0961-4ca4-a1a7-dd08e3b67418 container secret-volume-test: <nil>
STEP: delete the pod
Sep 20 02:49:55.608: INFO: Waiting for pod pod-projected-secrets-93fe3072-0961-4ca4-a1a7-dd08e3b67418 to disappear
Sep 20 02:49:55.613: INFO: Pod pod-projected-secrets-93fe3072-0961-4ca4-a1a7-dd08e3b67418 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:49:55.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9903" for this suite.
Sep 20 02:50:01.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:50:01.852: INFO: namespace projected-9903 deletion completed in 6.232393518s

• [SLOW TEST:10.663 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:50:01.855: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-749
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Sep 20 02:50:02.027: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 20 02:50:02.051: INFO: Waiting for terminating namespaces to be deleted...
Sep 20 02:50:02.058: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-kqpmz before test
Sep 20 02:50:02.082: INFO: canal-pwz75 from kube-system started at 2019-09-19 20:33:51 +0000 UTC (2 container statuses recorded)
Sep 20 02:50:02.083: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 02:50:02.083: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 02:50:02.084: INFO: node-exporter-rtkgz from kube-system started at 2019-09-19 20:33:51 +0000 UTC (2 container statuses recorded)
Sep 20 02:50:02.084: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 02:50:02.085: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 02:50:02.085: INFO: kube-proxy-xmcqx from kube-system started at 2019-09-19 20:33:51 +0000 UTC (1 container statuses recorded)
Sep 20 02:50:02.086: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 02:50:02.086: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-2ks25 from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 02:50:02.086: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Sep 20 02:50:02.087: INFO: 	Container systemd-logs ready: true, restart count 2
Sep 20 02:50:02.087: INFO: node-local-dns-tppj5 from kube-system started at 2019-09-20 00:59:43 +0000 UTC (1 container statuses recorded)
Sep 20 02:50:02.088: INFO: 	Container node-cache ready: true, restart count 0
Sep 20 02:50:02.088: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-ldlfc before test
Sep 20 02:50:02.291: INFO: canal-hf9f7 from kube-system started at 2019-09-19 20:33:57 +0000 UTC (2 container statuses recorded)
Sep 20 02:50:02.291: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 02:50:02.291: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 02:50:02.291: INFO: sonobuoy-e2e-job-8f5b7ab9f6954bcb from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 02:50:02.291: INFO: 	Container e2e ready: true, restart count 0
Sep 20 02:50:02.291: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 02:50:02.291: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-djczp from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 02:50:02.292: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Sep 20 02:50:02.292: INFO: 	Container systemd-logs ready: true, restart count 2
Sep 20 02:50:02.292: INFO: kube-proxy-zvhzd from kube-system started at 2019-09-19 20:33:57 +0000 UTC (1 container statuses recorded)
Sep 20 02:50:02.292: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 02:50:02.292: INFO: node-exporter-47p98 from kube-system started at 2019-09-19 20:33:57 +0000 UTC (2 container statuses recorded)
Sep 20 02:50:02.292: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 02:50:02.292: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 02:50:02.292: INFO: node-local-dns-rj6pt from kube-system started at 2019-09-19 21:38:47 +0000 UTC (1 container statuses recorded)
Sep 20 02:50:02.292: INFO: 	Container node-cache ready: true, restart count 0
Sep 20 02:50:02.292: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-lmjf4 before test
Sep 20 02:50:02.481: INFO: canal-s8rlz from kube-system started at 2019-09-19 20:34:06 +0000 UTC (2 container statuses recorded)
Sep 20 02:50:02.481: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 02:50:02.481: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 02:50:02.481: INFO: node-exporter-65hp9 from kube-system started at 2019-09-19 20:34:06 +0000 UTC (2 container statuses recorded)
Sep 20 02:50:02.482: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 02:50:02.482: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 02:50:02.482: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-n9zxp from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 02:50:02.482: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Sep 20 02:50:02.482: INFO: 	Container systemd-logs ready: true, restart count 2
Sep 20 02:50:02.482: INFO: openvpn-client-84ccd8596d-s64vh from kube-system started at 2019-09-20 00:44:53 +0000 UTC (2 container statuses recorded)
Sep 20 02:50:02.482: INFO: 	Container dnat-controller ready: true, restart count 0
Sep 20 02:50:02.482: INFO: 	Container openvpn-client ready: true, restart count 1
Sep 20 02:50:02.482: INFO: kube-proxy-mhstl from kube-system started at 2019-09-19 20:34:06 +0000 UTC (1 container statuses recorded)
Sep 20 02:50:02.482: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 02:50:02.482: INFO: node-local-dns-bx7jl from kube-system started at 2019-09-19 20:34:26 +0000 UTC (1 container statuses recorded)
Sep 20 02:50:02.482: INFO: 	Container node-cache ready: true, restart count 0
Sep 20 02:50:02.482: INFO: coredns-57f944bd9f-rp6nc from kube-system started at 2019-09-19 21:35:33 +0000 UTC (1 container statuses recorded)
Sep 20 02:50:02.482: INFO: 	Container coredns ready: true, restart count 0
Sep 20 02:50:02.482: INFO: kubernetes-dashboard-7d5fb85f7f-jqgq6 from kube-system started at 2019-09-20 00:59:15 +0000 UTC (1 container statuses recorded)
Sep 20 02:50:02.482: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Sep 20 02:50:02.482: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-mxxqz before test
Sep 20 02:50:02.645: INFO: kube-proxy-dtrjq from kube-system started at 2019-09-19 20:34:07 +0000 UTC (1 container statuses recorded)
Sep 20 02:50:02.645: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 02:50:02.645: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-7brnz from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 02:50:02.645: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Sep 20 02:50:02.645: INFO: 	Container systemd-logs ready: true, restart count 2
Sep 20 02:50:02.645: INFO: canal-kdmx8 from kube-system started at 2019-09-19 20:34:07 +0000 UTC (2 container statuses recorded)
Sep 20 02:50:02.645: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 02:50:02.645: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 02:50:02.645: INFO: node-exporter-xq8tf from kube-system started at 2019-09-19 20:34:07 +0000 UTC (2 container statuses recorded)
Sep 20 02:50:02.645: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 02:50:02.645: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 02:50:02.645: INFO: node-local-dns-bxghq from kube-system started at 2019-09-19 20:34:27 +0000 UTC (1 container statuses recorded)
Sep 20 02:50:02.645: INFO: 	Container node-cache ready: true, restart count 0
Sep 20 02:50:02.645: INFO: sonobuoy from sonobuoy started at 2019-09-19 20:40:21 +0000 UTC (1 container statuses recorded)
Sep 20 02:50:02.645: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 20 02:50:02.645: INFO: 
Logging pods the kubelet thinks is on node worker-wqshf-7859ffd555-r6tnq before test
Sep 20 02:50:02.739: INFO: coredns-57f944bd9f-6qlrr from kube-system started at 2019-09-20 00:59:15 +0000 UTC (1 container statuses recorded)
Sep 20 02:50:02.739: INFO: 	Container coredns ready: true, restart count 0
Sep 20 02:50:02.739: INFO: canal-8lwhv from kube-system started at 2019-09-19 20:33:57 +0000 UTC (2 container statuses recorded)
Sep 20 02:50:02.739: INFO: 	Container calico-node ready: true, restart count 0
Sep 20 02:50:02.740: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 02:50:02.740: INFO: kube-proxy-p4g4f from kube-system started at 2019-09-19 20:33:57 +0000 UTC (1 container statuses recorded)
Sep 20 02:50:02.740: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 20 02:50:02.740: INFO: node-exporter-ld8fb from kube-system started at 2019-09-19 20:33:57 +0000 UTC (2 container statuses recorded)
Sep 20 02:50:02.740: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 20 02:50:02.740: INFO: 	Container node-exporter ready: true, restart count 0
Sep 20 02:50:02.740: INFO: node-local-dns-2krxh from kube-system started at 2019-09-20 00:44:53 +0000 UTC (1 container statuses recorded)
Sep 20 02:50:02.740: INFO: 	Container node-cache ready: true, restart count 0
Sep 20 02:50:02.740: INFO: sonobuoy-systemd-logs-daemon-set-ee9e868abc9f4adf-bph2c from sonobuoy started at 2019-09-20 00:34:51 +0000 UTC (2 container statuses recorded)
Sep 20 02:50:02.740: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Sep 20 02:50:02.740: INFO: 	Container systemd-logs ready: true, restart count 2
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-beb805d1-e70a-4f1d-807a-c0b84ac76773 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-beb805d1-e70a-4f1d-807a-c0b84ac76773 off the node worker-wqshf-7859ffd555-lmjf4
STEP: verifying the node doesn't have the label kubernetes.io/e2e-beb805d1-e70a-4f1d-807a-c0b84ac76773
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:50:10.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-749" for this suite.
Sep 20 02:50:18.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:50:19.114: INFO: namespace sched-pred-749 deletion completed in 8.213583219s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
I0920 02:50:19.114616      16 request.go:706] Error in request: resource name may not be empty

• [SLOW TEST:17.260 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 20 02:50:19.116: INFO: >>> kubeConfig: /tmp/kubeconfig-342770709
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-616
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 20 02:50:19.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-616'
Sep 20 02:50:19.398: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 20 02:50:19.398: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Sep 20 02:50:19.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-342770709 delete deployment e2e-test-httpd-deployment --namespace=kubectl-616'
Sep 20 02:50:19.507: INFO: stderr: ""
Sep 20 02:50:19.507: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 20 02:50:19.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-616" for this suite.
Sep 20 02:50:25.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 20 02:50:25.732: INFO: namespace kubectl-616 deletion completed in 6.215839379s

• [SLOW TEST:6.617 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSep 20 02:50:25.733: INFO: Running AfterSuite actions on all nodes
Sep 20 02:50:25.734: INFO: Running AfterSuite actions on node 1
Sep 20 02:50:25.734: INFO: Skipping dumping logs from cluster

Ran 276 of 4897 Specs in 8117.953 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4621 Skipped
PASS

Ginkgo ran 1 suite in 2h15m20.305559838s
Test Suite Passed
