I1220 10:42:35.582325      20 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-773140870
I1220 10:42:35.582499      20 e2e.go:92] Starting e2e run "da69ec06-09a7-47a8-b943-b2e026b5a8ee" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1576838553 - Will randomize all specs
Will run 274 of 4732 specs

Dec 20 10:42:35.658: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 10:42:35.664: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec 20 10:42:35.692: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec 20 10:42:35.749: INFO: 17 / 17 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec 20 10:42:35.749: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Dec 20 10:42:35.749: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec 20 10:42:35.765: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Dec 20 10:42:35.765: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec 20 10:42:35.765: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Dec 20 10:42:35.765: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Dec 20 10:42:35.765: INFO: e2e test version: v1.16.3
Dec 20 10:42:35.768: INFO: kube-apiserver version: v1.16.3
Dec 20 10:42:35.768: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 10:42:35.777: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:42:35.777: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename job
Dec 20 10:42:35.856: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Dec 20 10:42:35.897: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-5722
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-5722, will wait for the garbage collector to delete the pods
Dec 20 10:42:47.049: INFO: Deleting Job.batch foo took: 398.341787ms
Dec 20 10:42:47.650: INFO: Terminating Job.batch foo pods took: 600.590835ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:43:29.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5722" for this suite.
Dec 20 10:43:35.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:43:35.605: INFO: namespace job-5722 deletion completed in 6.333077088s

• [SLOW TEST:59.827 seconds]
[sig-apps] Job
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:43:35.607: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-925
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-925.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-925.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-925.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-925.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-925.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-925.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 20 10:43:58.548: INFO: DNS probes using dns-925/dns-test-35174aa8-7410-48dd-909b-286bde42a749 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:43:58.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-925" for this suite.
Dec 20 10:44:06.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:44:06.904: INFO: namespace dns-925 deletion completed in 8.258899791s

• [SLOW TEST:31.297 seconds]
[sig-network] DNS
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:44:06.914: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7274
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-05ad5091-6064-41a2-b194-03857e9c19bd
STEP: Creating a pod to test consume configMaps
Dec 20 10:44:07.179: INFO: Waiting up to 5m0s for pod "pod-configmaps-897b279e-febe-45fc-93be-5407d5d624ba" in namespace "configmap-7274" to be "success or failure"
Dec 20 10:44:07.194: INFO: Pod "pod-configmaps-897b279e-febe-45fc-93be-5407d5d624ba": Phase="Pending", Reason="", readiness=false. Elapsed: 14.733917ms
Dec 20 10:44:09.201: INFO: Pod "pod-configmaps-897b279e-febe-45fc-93be-5407d5d624ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021713036s
Dec 20 10:44:11.209: INFO: Pod "pod-configmaps-897b279e-febe-45fc-93be-5407d5d624ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029512636s
Dec 20 10:44:13.219: INFO: Pod "pod-configmaps-897b279e-febe-45fc-93be-5407d5d624ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039750357s
STEP: Saw pod success
Dec 20 10:44:13.219: INFO: Pod "pod-configmaps-897b279e-febe-45fc-93be-5407d5d624ba" satisfied condition "success or failure"
Dec 20 10:44:13.230: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-configmaps-897b279e-febe-45fc-93be-5407d5d624ba container configmap-volume-test: <nil>
STEP: delete the pod
Dec 20 10:44:13.431: INFO: Waiting for pod pod-configmaps-897b279e-febe-45fc-93be-5407d5d624ba to disappear
Dec 20 10:44:13.436: INFO: Pod pod-configmaps-897b279e-febe-45fc-93be-5407d5d624ba no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:44:13.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7274" for this suite.
Dec 20 10:44:21.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:44:21.728: INFO: namespace configmap-7274 deletion completed in 8.284921251s

• [SLOW TEST:14.815 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:44:21.729: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-442
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 20 10:44:23.409: INFO: Number of nodes with available pods: 0
Dec 20 10:44:23.409: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 10:44:24.434: INFO: Number of nodes with available pods: 0
Dec 20 10:44:24.435: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 10:44:25.429: INFO: Number of nodes with available pods: 0
Dec 20 10:44:25.429: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 10:44:26.445: INFO: Number of nodes with available pods: 0
Dec 20 10:44:26.445: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 10:44:27.423: INFO: Number of nodes with available pods: 0
Dec 20 10:44:27.423: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 10:44:28.432: INFO: Number of nodes with available pods: 0
Dec 20 10:44:28.432: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 10:44:29.428: INFO: Number of nodes with available pods: 0
Dec 20 10:44:29.428: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 10:44:30.425: INFO: Number of nodes with available pods: 0
Dec 20 10:44:30.425: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 10:44:33.883: INFO: Number of nodes with available pods: 0
Dec 20 10:44:33.884: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 10:44:34.429: INFO: Number of nodes with available pods: 0
Dec 20 10:44:34.429: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 10:44:35.429: INFO: Number of nodes with available pods: 0
Dec 20 10:44:35.429: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 10:44:36.434: INFO: Number of nodes with available pods: 1
Dec 20 10:44:36.434: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 10:44:37.425: INFO: Number of nodes with available pods: 1
Dec 20 10:44:37.425: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 10:44:38.680: INFO: Number of nodes with available pods: 1
Dec 20 10:44:38.680: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 10:44:39.426: INFO: Number of nodes with available pods: 1
Dec 20 10:44:39.426: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 10:44:40.424: INFO: Number of nodes with available pods: 1
Dec 20 10:44:40.424: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 10:44:41.423: INFO: Number of nodes with available pods: 1
Dec 20 10:44:41.423: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 10:44:42.423: INFO: Number of nodes with available pods: 2
Dec 20 10:44:42.423: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 10:44:43.427: INFO: Number of nodes with available pods: 2
Dec 20 10:44:43.427: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 10:44:44.441: INFO: Number of nodes with available pods: 2
Dec 20 10:44:44.441: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 10:44:45.429: INFO: Number of nodes with available pods: 3
Dec 20 10:44:45.429: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec 20 10:44:45.495: INFO: Number of nodes with available pods: 2
Dec 20 10:44:45.496: INFO: Node metakube-worker-457fd-578f899757-zpxc2 is running more than one daemon pod
Dec 20 10:44:46.519: INFO: Number of nodes with available pods: 2
Dec 20 10:44:46.520: INFO: Node metakube-worker-457fd-578f899757-zpxc2 is running more than one daemon pod
Dec 20 10:44:47.509: INFO: Number of nodes with available pods: 2
Dec 20 10:44:47.509: INFO: Node metakube-worker-457fd-578f899757-zpxc2 is running more than one daemon pod
Dec 20 10:44:48.512: INFO: Number of nodes with available pods: 3
Dec 20 10:44:48.512: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-442, will wait for the garbage collector to delete the pods
Dec 20 10:44:48.626: INFO: Deleting DaemonSet.extensions daemon-set took: 41.029091ms
Dec 20 10:44:49.527: INFO: Terminating DaemonSet.extensions daemon-set pods took: 901.167925ms
Dec 20 10:45:02.535: INFO: Number of nodes with available pods: 0
Dec 20 10:45:02.536: INFO: Number of running nodes: 0, number of available pods: 0
Dec 20 10:45:02.545: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-442/daemonsets","resourceVersion":"5101"},"items":null}

Dec 20 10:45:02.551: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-442/pods","resourceVersion":"5101"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:45:02.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-442" for this suite.
Dec 20 10:45:10.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:45:10.873: INFO: namespace daemonsets-442 deletion completed in 8.281911603s

• [SLOW TEST:49.144 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:45:10.874: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7128
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 20 10:45:11.123: INFO: Waiting up to 5m0s for pod "pod-b8c0e9ca-4d4b-4e3f-856b-6ee0e1067d5a" in namespace "emptydir-7128" to be "success or failure"
Dec 20 10:45:11.128: INFO: Pod "pod-b8c0e9ca-4d4b-4e3f-856b-6ee0e1067d5a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.311745ms
Dec 20 10:45:13.146: INFO: Pod "pod-b8c0e9ca-4d4b-4e3f-856b-6ee0e1067d5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023125793s
Dec 20 10:45:15.153: INFO: Pod "pod-b8c0e9ca-4d4b-4e3f-856b-6ee0e1067d5a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030119029s
Dec 20 10:45:17.159: INFO: Pod "pod-b8c0e9ca-4d4b-4e3f-856b-6ee0e1067d5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036438974s
STEP: Saw pod success
Dec 20 10:45:17.159: INFO: Pod "pod-b8c0e9ca-4d4b-4e3f-856b-6ee0e1067d5a" satisfied condition "success or failure"
Dec 20 10:45:17.168: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-b8c0e9ca-4d4b-4e3f-856b-6ee0e1067d5a container test-container: <nil>
STEP: delete the pod
Dec 20 10:45:17.364: INFO: Waiting for pod pod-b8c0e9ca-4d4b-4e3f-856b-6ee0e1067d5a to disappear
Dec 20 10:45:17.373: INFO: Pod pod-b8c0e9ca-4d4b-4e3f-856b-6ee0e1067d5a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:45:17.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7128" for this suite.
Dec 20 10:45:23.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:45:23.704: INFO: namespace emptydir-7128 deletion completed in 6.323099297s

• [SLOW TEST:12.830 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:45:23.705: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3008
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 10:45:23.909: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 20 10:45:27.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 --namespace=crd-publish-openapi-3008 create -f -'
Dec 20 10:45:31.257: INFO: stderr: ""
Dec 20 10:45:31.257: INFO: stdout: "e2e-test-crd-publish-openapi-6981-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 20 10:45:31.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 --namespace=crd-publish-openapi-3008 delete e2e-test-crd-publish-openapi-6981-crds test-cr'
Dec 20 10:45:31.406: INFO: stderr: ""
Dec 20 10:45:31.406: INFO: stdout: "e2e-test-crd-publish-openapi-6981-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Dec 20 10:45:31.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 --namespace=crd-publish-openapi-3008 apply -f -'
Dec 20 10:45:31.738: INFO: stderr: ""
Dec 20 10:45:31.738: INFO: stdout: "e2e-test-crd-publish-openapi-6981-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 20 10:45:31.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 --namespace=crd-publish-openapi-3008 delete e2e-test-crd-publish-openapi-6981-crds test-cr'
Dec 20 10:45:31.893: INFO: stderr: ""
Dec 20 10:45:31.894: INFO: stdout: "e2e-test-crd-publish-openapi-6981-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 20 10:45:31.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 explain e2e-test-crd-publish-openapi-6981-crds'
Dec 20 10:45:32.207: INFO: stderr: ""
Dec 20 10:45:32.207: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6981-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:45:36.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3008" for this suite.
Dec 20 10:45:42.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:45:42.499: INFO: namespace crd-publish-openapi-3008 deletion completed in 6.291104523s

• [SLOW TEST:18.795 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:45:42.508: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2990
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec 20 10:45:42.767: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2990 /api/v1/namespaces/watch-2990/configmaps/e2e-watch-test-label-changed e66c26ba-9164-48bd-998d-5188bef7e9b9 5317 0 2019-12-20 10:45:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 20 10:45:42.768: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2990 /api/v1/namespaces/watch-2990/configmaps/e2e-watch-test-label-changed e66c26ba-9164-48bd-998d-5188bef7e9b9 5318 0 2019-12-20 10:45:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec 20 10:45:42.768: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2990 /api/v1/namespaces/watch-2990/configmaps/e2e-watch-test-label-changed e66c26ba-9164-48bd-998d-5188bef7e9b9 5319 0 2019-12-20 10:45:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec 20 10:45:52.981: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2990 /api/v1/namespaces/watch-2990/configmaps/e2e-watch-test-label-changed e66c26ba-9164-48bd-998d-5188bef7e9b9 5351 0 2019-12-20 10:45:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 20 10:45:52.982: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2990 /api/v1/namespaces/watch-2990/configmaps/e2e-watch-test-label-changed e66c26ba-9164-48bd-998d-5188bef7e9b9 5353 0 2019-12-20 10:45:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Dec 20 10:45:52.982: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2990 /api/v1/namespaces/watch-2990/configmaps/e2e-watch-test-label-changed e66c26ba-9164-48bd-998d-5188bef7e9b9 5355 0 2019-12-20 10:45:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:45:52.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2990" for this suite.
Dec 20 10:45:59.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:45:59.456: INFO: namespace watch-2990 deletion completed in 6.465210104s

• [SLOW TEST:16.949 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:45:59.467: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6146
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Dec 20 10:45:59.790: INFO: PodSpec: initContainers in spec.initContainers
Dec 20 10:46:57.274: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-8ac8b9cf-b3da-4038-b85e-d5cd547054af", GenerateName:"", Namespace:"init-container-6146", SelfLink:"/api/v1/namespaces/init-container-6146/pods/pod-init-8ac8b9cf-b3da-4038-b85e-d5cd547054af", UID:"43b9a40a-6154-4c0c-b644-8e0b0c8f052d", ResourceVersion:"5592", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63712435559, loc:(*time.Location)(0x84bfb00)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"790964138"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.25.2.8/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-bppr2", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00498ef80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-bppr2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-bppr2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-bppr2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004e80d88), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"metakube-worker-457fd-578f899757-zpxc2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0023c89c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004e80e00)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004e80e20)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004e80e28), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004e80e2c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712435560, loc:(*time.Location)(0x84bfb00)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712435560, loc:(*time.Location)(0x84bfb00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712435560, loc:(*time.Location)(0x84bfb00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712435559, loc:(*time.Location)(0x84bfb00)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.1.11", PodIP:"172.25.2.8", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.25.2.8"}}, StartTime:(*v1.Time)(0xc0012e8080), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0032015e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003201650)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://c81699095f9b24a5aec969a76828c2f479f13e140d114775ffccd960c764f541", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0012e80c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0012e80a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc004e80eaf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:46:57.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6146" for this suite.
Dec 20 10:47:26.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:47:27.208: INFO: namespace init-container-6146 deletion completed in 29.781921518s

• [SLOW TEST:87.741 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:47:27.211: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1811
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:47:27.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1811" for this suite.
Dec 20 10:47:33.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:47:33.736: INFO: namespace custom-resource-definition-1811 deletion completed in 6.274568248s

• [SLOW TEST:6.526 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:47:33.741: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1980
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 20 10:47:34.009: INFO: Waiting up to 5m0s for pod "pod-f35aec54-4498-4678-94cc-228583515c0c" in namespace "emptydir-1980" to be "success or failure"
Dec 20 10:47:34.015: INFO: Pod "pod-f35aec54-4498-4678-94cc-228583515c0c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.722203ms
Dec 20 10:47:36.026: INFO: Pod "pod-f35aec54-4498-4678-94cc-228583515c0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016953903s
Dec 20 10:47:38.033: INFO: Pod "pod-f35aec54-4498-4678-94cc-228583515c0c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024343995s
Dec 20 10:47:40.043: INFO: Pod "pod-f35aec54-4498-4678-94cc-228583515c0c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033922364s
Dec 20 10:47:42.058: INFO: Pod "pod-f35aec54-4498-4678-94cc-228583515c0c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.048879592s
Dec 20 10:47:44.066: INFO: Pod "pod-f35aec54-4498-4678-94cc-228583515c0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.057527028s
STEP: Saw pod success
Dec 20 10:47:44.067: INFO: Pod "pod-f35aec54-4498-4678-94cc-228583515c0c" satisfied condition "success or failure"
Dec 20 10:47:44.072: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-f35aec54-4498-4678-94cc-228583515c0c container test-container: <nil>
STEP: delete the pod
Dec 20 10:47:44.148: INFO: Waiting for pod pod-f35aec54-4498-4678-94cc-228583515c0c to disappear
Dec 20 10:47:44.156: INFO: Pod pod-f35aec54-4498-4678-94cc-228583515c0c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:47:44.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1980" for this suite.
Dec 20 10:47:50.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:47:50.478: INFO: namespace emptydir-1980 deletion completed in 6.302407668s

• [SLOW TEST:16.737 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:47:50.479: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4801
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 20 10:47:50.694: INFO: Waiting up to 5m0s for pod "pod-e03824c7-5cbf-41d7-94a6-dcdbcd831dc8" in namespace "emptydir-4801" to be "success or failure"
Dec 20 10:47:50.700: INFO: Pod "pod-e03824c7-5cbf-41d7-94a6-dcdbcd831dc8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.380003ms
Dec 20 10:47:52.707: INFO: Pod "pod-e03824c7-5cbf-41d7-94a6-dcdbcd831dc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012782594s
Dec 20 10:47:54.714: INFO: Pod "pod-e03824c7-5cbf-41d7-94a6-dcdbcd831dc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020079936s
STEP: Saw pod success
Dec 20 10:47:54.714: INFO: Pod "pod-e03824c7-5cbf-41d7-94a6-dcdbcd831dc8" satisfied condition "success or failure"
Dec 20 10:47:54.720: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-e03824c7-5cbf-41d7-94a6-dcdbcd831dc8 container test-container: <nil>
STEP: delete the pod
Dec 20 10:47:54.801: INFO: Waiting for pod pod-e03824c7-5cbf-41d7-94a6-dcdbcd831dc8 to disappear
Dec 20 10:47:54.813: INFO: Pod pod-e03824c7-5cbf-41d7-94a6-dcdbcd831dc8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:47:54.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4801" for this suite.
Dec 20 10:48:00.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:48:01.106: INFO: namespace emptydir-4801 deletion completed in 6.284379215s

• [SLOW TEST:10.627 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:48:01.106: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9280
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 20 10:48:01.348: INFO: Waiting up to 5m0s for pod "pod-7096111b-1409-4e77-8f63-60b138e0c717" in namespace "emptydir-9280" to be "success or failure"
Dec 20 10:48:01.357: INFO: Pod "pod-7096111b-1409-4e77-8f63-60b138e0c717": Phase="Pending", Reason="", readiness=false. Elapsed: 8.301902ms
Dec 20 10:48:03.365: INFO: Pod "pod-7096111b-1409-4e77-8f63-60b138e0c717": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016812605s
Dec 20 10:48:05.507: INFO: Pod "pod-7096111b-1409-4e77-8f63-60b138e0c717": Phase="Pending", Reason="", readiness=false. Elapsed: 4.158548122s
Dec 20 10:48:07.514: INFO: Pod "pod-7096111b-1409-4e77-8f63-60b138e0c717": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.166156475s
STEP: Saw pod success
Dec 20 10:48:07.515: INFO: Pod "pod-7096111b-1409-4e77-8f63-60b138e0c717" satisfied condition "success or failure"
Dec 20 10:48:07.522: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-7096111b-1409-4e77-8f63-60b138e0c717 container test-container: <nil>
STEP: delete the pod
Dec 20 10:48:07.591: INFO: Waiting for pod pod-7096111b-1409-4e77-8f63-60b138e0c717 to disappear
Dec 20 10:48:07.599: INFO: Pod pod-7096111b-1409-4e77-8f63-60b138e0c717 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:48:07.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9280" for this suite.
Dec 20 10:48:13.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:48:13.878: INFO: namespace emptydir-9280 deletion completed in 6.271675252s

• [SLOW TEST:12.772 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:48:13.881: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-830
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 10:48:14.102: INFO: (0) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 15.899153ms)
Dec 20 10:48:14.152: INFO: (1) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 50.194363ms)
Dec 20 10:48:14.167: INFO: (2) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 14.393946ms)
Dec 20 10:48:14.185: INFO: (3) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 18.048776ms)
Dec 20 10:48:14.195: INFO: (4) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.972568ms)
Dec 20 10:48:14.206: INFO: (5) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.794478ms)
Dec 20 10:48:14.217: INFO: (6) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.406701ms)
Dec 20 10:48:14.228: INFO: (7) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.490785ms)
Dec 20 10:48:14.239: INFO: (8) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.067416ms)
Dec 20 10:48:14.250: INFO: (9) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.291353ms)
Dec 20 10:48:14.260: INFO: (10) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.687338ms)
Dec 20 10:48:14.271: INFO: (11) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.394281ms)
Dec 20 10:48:14.283: INFO: (12) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.263121ms)
Dec 20 10:48:14.293: INFO: (13) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.520766ms)
Dec 20 10:48:14.304: INFO: (14) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.359086ms)
Dec 20 10:48:14.314: INFO: (15) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.545896ms)
Dec 20 10:48:14.325: INFO: (16) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.969083ms)
Dec 20 10:48:14.335: INFO: (17) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.706695ms)
Dec 20 10:48:14.347: INFO: (18) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.709324ms)
Dec 20 10:48:14.358: INFO: (19) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.400081ms)
[AfterEach] version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:48:14.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-830" for this suite.
Dec 20 10:48:20.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:48:20.668: INFO: namespace proxy-830 deletion completed in 6.301279877s

• [SLOW TEST:6.788 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:48:20.672: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-914
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:48:25.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-914" for this suite.
Dec 20 10:48:31.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:48:31.356: INFO: namespace emptydir-wrapper-914 deletion completed in 6.273694088s

• [SLOW TEST:10.684 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:48:31.356: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1455
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-4ac33ad6-0b38-462a-b6c4-7c1feaf4deea
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:48:31.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1455" for this suite.
Dec 20 10:48:37.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:48:37.956: INFO: namespace secrets-1455 deletion completed in 6.340957739s

• [SLOW TEST:6.600 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:48:37.957: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9151
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with configMap that has name projected-configmap-test-upd-04cb35c8-8262-4a0f-a586-0ac4320341a0
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-04cb35c8-8262-4a0f-a586-0ac4320341a0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:49:48.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9151" for this suite.
Dec 20 10:50:06.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:50:06.470: INFO: namespace projected-9151 deletion completed in 18.279513005s

• [SLOW TEST:88.513 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:50:06.471: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-427
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 20 10:50:06.723: INFO: Waiting up to 5m0s for pod "downward-api-0835720b-bbba-4e35-b2eb-04c220a288f5" in namespace "downward-api-427" to be "success or failure"
Dec 20 10:50:06.729: INFO: Pod "downward-api-0835720b-bbba-4e35-b2eb-04c220a288f5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.258518ms
Dec 20 10:50:08.736: INFO: Pod "downward-api-0835720b-bbba-4e35-b2eb-04c220a288f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012786795s
Dec 20 10:50:10.748: INFO: Pod "downward-api-0835720b-bbba-4e35-b2eb-04c220a288f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024311484s
STEP: Saw pod success
Dec 20 10:50:10.748: INFO: Pod "downward-api-0835720b-bbba-4e35-b2eb-04c220a288f5" satisfied condition "success or failure"
Dec 20 10:50:10.754: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod downward-api-0835720b-bbba-4e35-b2eb-04c220a288f5 container dapi-container: <nil>
STEP: delete the pod
Dec 20 10:50:10.819: INFO: Waiting for pod downward-api-0835720b-bbba-4e35-b2eb-04c220a288f5 to disappear
Dec 20 10:50:10.828: INFO: Pod downward-api-0835720b-bbba-4e35-b2eb-04c220a288f5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:50:10.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-427" for this suite.
Dec 20 10:50:16.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:50:17.125: INFO: namespace downward-api-427 deletion completed in 6.286256052s

• [SLOW TEST:10.654 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:50:17.126: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1712
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-30f45ea2-45e0-4bff-bc7f-675252a63866
STEP: Creating a pod to test consume configMaps
Dec 20 10:50:17.378: INFO: Waiting up to 5m0s for pod "pod-configmaps-1d7db517-00d3-46fe-88a8-eca37ed39622" in namespace "configmap-1712" to be "success or failure"
Dec 20 10:50:17.384: INFO: Pod "pod-configmaps-1d7db517-00d3-46fe-88a8-eca37ed39622": Phase="Pending", Reason="", readiness=false. Elapsed: 5.87803ms
Dec 20 10:50:19.391: INFO: Pod "pod-configmaps-1d7db517-00d3-46fe-88a8-eca37ed39622": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012599428s
Dec 20 10:50:21.407: INFO: Pod "pod-configmaps-1d7db517-00d3-46fe-88a8-eca37ed39622": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029431835s
Dec 20 10:50:23.420: INFO: Pod "pod-configmaps-1d7db517-00d3-46fe-88a8-eca37ed39622": Phase="Pending", Reason="", readiness=false. Elapsed: 6.042077248s
Dec 20 10:50:25.428: INFO: Pod "pod-configmaps-1d7db517-00d3-46fe-88a8-eca37ed39622": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.050251844s
STEP: Saw pod success
Dec 20 10:50:25.428: INFO: Pod "pod-configmaps-1d7db517-00d3-46fe-88a8-eca37ed39622" satisfied condition "success or failure"
Dec 20 10:50:25.435: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-configmaps-1d7db517-00d3-46fe-88a8-eca37ed39622 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 20 10:50:25.706: INFO: Waiting for pod pod-configmaps-1d7db517-00d3-46fe-88a8-eca37ed39622 to disappear
Dec 20 10:50:25.713: INFO: Pod pod-configmaps-1d7db517-00d3-46fe-88a8-eca37ed39622 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:50:25.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1712" for this suite.
Dec 20 10:50:33.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:50:34.034: INFO: namespace configmap-1712 deletion completed in 8.308950961s

• [SLOW TEST:16.909 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:50:34.040: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9191
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 20 10:50:34.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-9191'
Dec 20 10:50:34.474: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 20 10:50:34.474: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Dec 20 10:50:36.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 delete deployment e2e-test-httpd-deployment --namespace=kubectl-9191'
Dec 20 10:50:36.694: INFO: stderr: ""
Dec 20 10:50:36.694: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:50:36.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9191" for this suite.
Dec 20 10:51:04.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:51:04.965: INFO: namespace kubectl-9191 deletion completed in 28.261601894s

• [SLOW TEST:30.926 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:51:04.971: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6238
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-9534e94f-fe0b-41bd-a43b-5bbbdfa265f6
STEP: Creating a pod to test consume secrets
Dec 20 10:51:05.240: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3cc6c192-b0e7-493c-be27-825b88b92c04" in namespace "projected-6238" to be "success or failure"
Dec 20 10:51:05.261: INFO: Pod "pod-projected-secrets-3cc6c192-b0e7-493c-be27-825b88b92c04": Phase="Pending", Reason="", readiness=false. Elapsed: 20.463473ms
Dec 20 10:51:07.271: INFO: Pod "pod-projected-secrets-3cc6c192-b0e7-493c-be27-825b88b92c04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031261068s
Dec 20 10:51:09.295: INFO: Pod "pod-projected-secrets-3cc6c192-b0e7-493c-be27-825b88b92c04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055003261s
STEP: Saw pod success
Dec 20 10:51:09.295: INFO: Pod "pod-projected-secrets-3cc6c192-b0e7-493c-be27-825b88b92c04" satisfied condition "success or failure"
Dec 20 10:51:09.301: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-projected-secrets-3cc6c192-b0e7-493c-be27-825b88b92c04 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 20 10:51:09.625: INFO: Waiting for pod pod-projected-secrets-3cc6c192-b0e7-493c-be27-825b88b92c04 to disappear
Dec 20 10:51:09.632: INFO: Pod pod-projected-secrets-3cc6c192-b0e7-493c-be27-825b88b92c04 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:51:09.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6238" for this suite.
Dec 20 10:51:15.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:51:15.954: INFO: namespace projected-6238 deletion completed in 6.302852366s

• [SLOW TEST:10.984 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:51:15.956: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2998
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 20 10:51:17.035: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 20 10:51:19.056: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712435877, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712435877, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712435877, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712435877, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 10:51:21.084: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712435877, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712435877, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712435877, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712435877, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 10:51:23.065: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712435877, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712435877, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712435877, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712435877, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 10:51:25.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712435877, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712435877, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712435877, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712435877, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 10:51:27.079: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712435877, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712435877, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712435877, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712435877, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 20 10:51:30.094: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:51:41.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2998" for this suite.
Dec 20 10:51:49.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:51:50.060: INFO: namespace webhook-2998 deletion completed in 8.291412056s
STEP: Destroying namespace "webhook-2998-markers" for this suite.
Dec 20 10:52:00.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:52:00.348: INFO: namespace webhook-2998-markers deletion completed in 10.287732837s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:44.424 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:52:00.385: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9113
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 20 10:52:13.400: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 20 10:52:13.415: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 20 10:52:15.415: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 20 10:52:15.422: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 20 10:52:17.415: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 20 10:52:17.421: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:52:17.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9113" for this suite.
Dec 20 10:52:29.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:52:29.790: INFO: namespace container-lifecycle-hook-9113 deletion completed in 12.340961734s

• [SLOW TEST:29.406 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:52:29.792: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8364
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:52:48.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8364" for this suite.
Dec 20 10:52:54.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:52:55.176: INFO: namespace resourcequota-8364 deletion completed in 6.355045676s

• [SLOW TEST:25.384 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:52:55.176: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3506
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:53:00.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3506" for this suite.
Dec 20 10:53:06.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:53:06.443: INFO: namespace watch-3506 deletion completed in 6.391601625s

• [SLOW TEST:11.266 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:53:06.445: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7150
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-8aebc978-35b4-4d59-a72a-014c5c923a3b in namespace container-probe-7150
Dec 20 10:53:10.681: INFO: Started pod busybox-8aebc978-35b4-4d59-a72a-014c5c923a3b in namespace container-probe-7150
STEP: checking the pod's current state and verifying that restartCount is present
Dec 20 10:53:10.688: INFO: Initial restart count of pod busybox-8aebc978-35b4-4d59-a72a-014c5c923a3b is 0
Dec 20 10:54:02.929: INFO: Restart count of pod container-probe-7150/busybox-8aebc978-35b4-4d59-a72a-014c5c923a3b is now 1 (52.240589674s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:54:02.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7150" for this suite.
Dec 20 10:54:09.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:54:09.545: INFO: namespace container-probe-7150 deletion completed in 6.273728251s

• [SLOW TEST:63.100 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:54:09.547: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9090
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9090.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9090.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9090.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9090.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9090.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9090.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9090.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9090.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9090.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9090.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9090.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9090.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9090.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9090.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9090.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9090.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9090.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9090.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 20 10:54:13.995: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9090.svc.cluster.local from pod dns-9090/dns-test-015936a6-0406-413b-a8ea-33aa09a94f31: the server could not find the requested resource (get pods dns-test-015936a6-0406-413b-a8ea-33aa09a94f31)
Dec 20 10:54:14.039: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9090.svc.cluster.local from pod dns-9090/dns-test-015936a6-0406-413b-a8ea-33aa09a94f31: the server could not find the requested resource (get pods dns-test-015936a6-0406-413b-a8ea-33aa09a94f31)
Dec 20 10:54:14.050: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9090.svc.cluster.local from pod dns-9090/dns-test-015936a6-0406-413b-a8ea-33aa09a94f31: the server could not find the requested resource (get pods dns-test-015936a6-0406-413b-a8ea-33aa09a94f31)
Dec 20 10:54:14.063: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9090.svc.cluster.local from pod dns-9090/dns-test-015936a6-0406-413b-a8ea-33aa09a94f31: the server could not find the requested resource (get pods dns-test-015936a6-0406-413b-a8ea-33aa09a94f31)
Dec 20 10:54:14.209: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9090.svc.cluster.local from pod dns-9090/dns-test-015936a6-0406-413b-a8ea-33aa09a94f31: the server could not find the requested resource (get pods dns-test-015936a6-0406-413b-a8ea-33aa09a94f31)
Dec 20 10:54:14.220: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9090.svc.cluster.local from pod dns-9090/dns-test-015936a6-0406-413b-a8ea-33aa09a94f31: the server could not find the requested resource (get pods dns-test-015936a6-0406-413b-a8ea-33aa09a94f31)
Dec 20 10:54:14.233: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9090.svc.cluster.local from pod dns-9090/dns-test-015936a6-0406-413b-a8ea-33aa09a94f31: the server could not find the requested resource (get pods dns-test-015936a6-0406-413b-a8ea-33aa09a94f31)
Dec 20 10:54:14.243: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9090.svc.cluster.local from pod dns-9090/dns-test-015936a6-0406-413b-a8ea-33aa09a94f31: the server could not find the requested resource (get pods dns-test-015936a6-0406-413b-a8ea-33aa09a94f31)
Dec 20 10:54:14.392: INFO: Lookups using dns-9090/dns-test-015936a6-0406-413b-a8ea-33aa09a94f31 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9090.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9090.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9090.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9090.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9090.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9090.svc.cluster.local jessie_udp@dns-test-service-2.dns-9090.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9090.svc.cluster.local]

Dec 20 10:54:20.315: INFO: DNS probes using dns-9090/dns-test-015936a6-0406-413b-a8ea-33aa09a94f31 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:54:21.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9090" for this suite.
Dec 20 10:54:33.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:54:34.194: INFO: namespace dns-9090 deletion completed in 12.965937738s

• [SLOW TEST:24.648 seconds]
[sig-network] DNS
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:54:34.196: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-2471
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-2471
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-2471
STEP: Deleting pre-stop pod
Dec 20 10:54:49.681: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:54:49.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2471" for this suite.
Dec 20 10:55:36.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:55:36.451: INFO: namespace prestop-2471 deletion completed in 46.576642576s

• [SLOW TEST:62.255 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:55:36.453: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5199
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 20 10:55:36.726: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ad65420b-4d89-42c6-9fd3-b1af46f2a443" in namespace "projected-5199" to be "success or failure"
Dec 20 10:55:36.732: INFO: Pod "downwardapi-volume-ad65420b-4d89-42c6-9fd3-b1af46f2a443": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084977ms
Dec 20 10:55:38.744: INFO: Pod "downwardapi-volume-ad65420b-4d89-42c6-9fd3-b1af46f2a443": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01778105s
Dec 20 10:55:40.750: INFO: Pod "downwardapi-volume-ad65420b-4d89-42c6-9fd3-b1af46f2a443": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024169302s
STEP: Saw pod success
Dec 20 10:55:40.750: INFO: Pod "downwardapi-volume-ad65420b-4d89-42c6-9fd3-b1af46f2a443" satisfied condition "success or failure"
Dec 20 10:55:40.760: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod downwardapi-volume-ad65420b-4d89-42c6-9fd3-b1af46f2a443 container client-container: <nil>
STEP: delete the pod
Dec 20 10:55:40.814: INFO: Waiting for pod downwardapi-volume-ad65420b-4d89-42c6-9fd3-b1af46f2a443 to disappear
Dec 20 10:55:40.820: INFO: Pod downwardapi-volume-ad65420b-4d89-42c6-9fd3-b1af46f2a443 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:55:40.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5199" for this suite.
Dec 20 10:55:46.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:55:47.145: INFO: namespace projected-5199 deletion completed in 6.314725993s

• [SLOW TEST:10.692 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:55:47.146: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2080
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 10:55:47.366: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Dec 20 10:55:48.445: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:55:49.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2080" for this suite.
Dec 20 10:55:55.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:55:55.850: INFO: namespace replication-controller-2080 deletion completed in 6.375982224s

• [SLOW TEST:8.705 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:55:55.851: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3269
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 20 10:55:57.307: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 20 10:55:59.330: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436157, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436157, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436157, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436157, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 10:56:01.341: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436157, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436157, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436157, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436157, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 20 10:56:04.680: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:56:05.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3269" for this suite.
Dec 20 10:56:11.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:56:11.922: INFO: namespace webhook-3269 deletion completed in 6.306826521s
STEP: Destroying namespace "webhook-3269-markers" for this suite.
Dec 20 10:56:17.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:56:18.210: INFO: namespace webhook-3269-markers deletion completed in 6.287513882s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:22.398 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:56:18.252: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-114
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-9795
STEP: Creating secret with name secret-test-b4272c14-410b-45d5-b1fa-2f85708253f8
STEP: Creating a pod to test consume secrets
Dec 20 10:56:18.738: INFO: Waiting up to 5m0s for pod "pod-secrets-516c79ef-5357-4228-9252-209c5a4c5c57" in namespace "secrets-114" to be "success or failure"
Dec 20 10:56:18.746: INFO: Pod "pod-secrets-516c79ef-5357-4228-9252-209c5a4c5c57": Phase="Pending", Reason="", readiness=false. Elapsed: 7.903554ms
Dec 20 10:56:20.763: INFO: Pod "pod-secrets-516c79ef-5357-4228-9252-209c5a4c5c57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025623822s
Dec 20 10:56:22.770: INFO: Pod "pod-secrets-516c79ef-5357-4228-9252-209c5a4c5c57": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032091374s
Dec 20 10:56:25.165: INFO: Pod "pod-secrets-516c79ef-5357-4228-9252-209c5a4c5c57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.427008277s
STEP: Saw pod success
Dec 20 10:56:25.165: INFO: Pod "pod-secrets-516c79ef-5357-4228-9252-209c5a4c5c57" satisfied condition "success or failure"
Dec 20 10:56:25.183: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-secrets-516c79ef-5357-4228-9252-209c5a4c5c57 container secret-volume-test: <nil>
STEP: delete the pod
Dec 20 10:56:25.292: INFO: Waiting for pod pod-secrets-516c79ef-5357-4228-9252-209c5a4c5c57 to disappear
Dec 20 10:56:25.686: INFO: Pod pod-secrets-516c79ef-5357-4228-9252-209c5a4c5c57 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:56:25.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-114" for this suite.
Dec 20 10:56:31.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:56:31.991: INFO: namespace secrets-114 deletion completed in 6.296272515s
STEP: Destroying namespace "secret-namespace-9795" for this suite.
Dec 20 10:56:40.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:56:40.287: INFO: namespace secret-namespace-9795 deletion completed in 8.294878805s

• [SLOW TEST:22.035 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:56:40.292: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6485
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-0916e47a-dcdd-40c7-82b4-5e9b9c44780a
STEP: Creating secret with name s-test-opt-upd-2f5a9bf1-d36a-4a7b-9dda-2bf20cdb7164
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-0916e47a-dcdd-40c7-82b4-5e9b9c44780a
STEP: Updating secret s-test-opt-upd-2f5a9bf1-d36a-4a7b-9dda-2bf20cdb7164
STEP: Creating secret with name s-test-opt-create-7db47aff-cacb-49cf-ae7f-87e0a07d9f72
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:58:16.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6485" for this suite.
Dec 20 10:58:28.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:58:28.580: INFO: namespace projected-6485 deletion completed in 12.298397562s

• [SLOW TEST:108.288 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:58:28.584: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8537
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-8537
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8537 to expose endpoints map[]
Dec 20 10:58:28.863: INFO: Get endpoints failed (11.7382ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Dec 20 10:58:29.870: INFO: successfully validated that service endpoint-test2 in namespace services-8537 exposes endpoints map[] (1.018793166s elapsed)
STEP: Creating pod pod1 in namespace services-8537
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8537 to expose endpoints map[pod1:[80]]
Dec 20 10:58:34.046: INFO: successfully validated that service endpoint-test2 in namespace services-8537 exposes endpoints map[pod1:[80]] (4.157040826s elapsed)
STEP: Creating pod pod2 in namespace services-8537
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8537 to expose endpoints map[pod1:[80] pod2:[80]]
Dec 20 10:58:37.506: INFO: successfully validated that service endpoint-test2 in namespace services-8537 exposes endpoints map[pod1:[80] pod2:[80]] (3.450689883s elapsed)
STEP: Deleting pod pod1 in namespace services-8537
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8537 to expose endpoints map[pod2:[80]]
Dec 20 10:58:37.554: INFO: successfully validated that service endpoint-test2 in namespace services-8537 exposes endpoints map[pod2:[80]] (28.367616ms elapsed)
STEP: Deleting pod pod2 in namespace services-8537
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8537 to expose endpoints map[]
Dec 20 10:58:39.043: INFO: successfully validated that service endpoint-test2 in namespace services-8537 exposes endpoints map[] (1.020300652s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:58:39.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8537" for this suite.
Dec 20 10:58:51.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:58:51.774: INFO: namespace services-8537 deletion completed in 12.665739422s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:23.191 seconds]
[sig-network] Services
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:58:51.775: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-823
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 20 10:58:52.015: INFO: Waiting up to 5m0s for pod "pod-3db204a1-1b65-4585-9367-4420ec9b1967" in namespace "emptydir-823" to be "success or failure"
Dec 20 10:58:52.027: INFO: Pod "pod-3db204a1-1b65-4585-9367-4420ec9b1967": Phase="Pending", Reason="", readiness=false. Elapsed: 11.562842ms
Dec 20 10:58:54.034: INFO: Pod "pod-3db204a1-1b65-4585-9367-4420ec9b1967": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01901767s
Dec 20 10:58:56.041: INFO: Pod "pod-3db204a1-1b65-4585-9367-4420ec9b1967": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025906946s
STEP: Saw pod success
Dec 20 10:58:56.041: INFO: Pod "pod-3db204a1-1b65-4585-9367-4420ec9b1967" satisfied condition "success or failure"
Dec 20 10:58:56.048: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-3db204a1-1b65-4585-9367-4420ec9b1967 container test-container: <nil>
STEP: delete the pod
Dec 20 10:58:56.151: INFO: Waiting for pod pod-3db204a1-1b65-4585-9367-4420ec9b1967 to disappear
Dec 20 10:58:56.160: INFO: Pod pod-3db204a1-1b65-4585-9367-4420ec9b1967 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:58:56.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-823" for this suite.
Dec 20 10:59:02.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:59:02.526: INFO: namespace emptydir-823 deletion completed in 6.356896438s

• [SLOW TEST:10.751 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:59:02.528: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6977
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Dec 20 10:59:02.777: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 10:59:06.687: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:59:22.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6977" for this suite.
Dec 20 10:59:28.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:59:28.520: INFO: namespace crd-publish-openapi-6977 deletion completed in 6.319993917s

• [SLOW TEST:25.993 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:59:28.521: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9312
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Dec 20 10:59:29.322: INFO: created pod pod-service-account-defaultsa
Dec 20 10:59:29.322: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec 20 10:59:29.332: INFO: created pod pod-service-account-mountsa
Dec 20 10:59:29.332: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec 20 10:59:29.358: INFO: created pod pod-service-account-nomountsa
Dec 20 10:59:29.358: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec 20 10:59:29.381: INFO: created pod pod-service-account-defaultsa-mountspec
Dec 20 10:59:29.381: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec 20 10:59:29.414: INFO: created pod pod-service-account-mountsa-mountspec
Dec 20 10:59:29.414: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec 20 10:59:29.431: INFO: created pod pod-service-account-nomountsa-mountspec
Dec 20 10:59:29.432: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec 20 10:59:29.445: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec 20 10:59:29.445: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec 20 10:59:29.498: INFO: created pod pod-service-account-mountsa-nomountspec
Dec 20 10:59:29.498: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec 20 10:59:29.510: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec 20 10:59:29.511: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:59:29.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9312" for this suite.
Dec 20 10:59:41.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:59:41.855: INFO: namespace svcaccounts-9312 deletion completed in 12.322667537s

• [SLOW TEST:13.334 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:59:41.858: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7322
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-7322/configmap-test-e133382b-03ae-4743-acf2-626ddfe58364
STEP: Creating a pod to test consume configMaps
Dec 20 10:59:42.114: INFO: Waiting up to 5m0s for pod "pod-configmaps-4f70c762-8c0b-4a2a-bf69-45f7822cbd49" in namespace "configmap-7322" to be "success or failure"
Dec 20 10:59:42.138: INFO: Pod "pod-configmaps-4f70c762-8c0b-4a2a-bf69-45f7822cbd49": Phase="Pending", Reason="", readiness=false. Elapsed: 23.349176ms
Dec 20 10:59:44.144: INFO: Pod "pod-configmaps-4f70c762-8c0b-4a2a-bf69-45f7822cbd49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030064675s
Dec 20 10:59:46.268: INFO: Pod "pod-configmaps-4f70c762-8c0b-4a2a-bf69-45f7822cbd49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.153850089s
STEP: Saw pod success
Dec 20 10:59:46.268: INFO: Pod "pod-configmaps-4f70c762-8c0b-4a2a-bf69-45f7822cbd49" satisfied condition "success or failure"
Dec 20 10:59:46.277: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-configmaps-4f70c762-8c0b-4a2a-bf69-45f7822cbd49 container env-test: <nil>
STEP: delete the pod
Dec 20 10:59:46.668: INFO: Waiting for pod pod-configmaps-4f70c762-8c0b-4a2a-bf69-45f7822cbd49 to disappear
Dec 20 10:59:46.674: INFO: Pod pod-configmaps-4f70c762-8c0b-4a2a-bf69-45f7822cbd49 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:59:46.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7322" for this suite.
Dec 20 10:59:54.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 10:59:55.036: INFO: namespace configmap-7322 deletion completed in 8.354359591s

• [SLOW TEST:13.179 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 10:59:55.040: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7561
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-56299457-cd5e-465d-ac3a-e6a618757b9d
STEP: Creating a pod to test consume secrets
Dec 20 10:59:55.329: INFO: Waiting up to 5m0s for pod "pod-secrets-39647ca6-cdcd-4946-a97e-44c2e1a4dc4f" in namespace "secrets-7561" to be "success or failure"
Dec 20 10:59:55.340: INFO: Pod "pod-secrets-39647ca6-cdcd-4946-a97e-44c2e1a4dc4f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.31678ms
Dec 20 10:59:57.364: INFO: Pod "pod-secrets-39647ca6-cdcd-4946-a97e-44c2e1a4dc4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0352792s
Dec 20 10:59:59.371: INFO: Pod "pod-secrets-39647ca6-cdcd-4946-a97e-44c2e1a4dc4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04196809s
STEP: Saw pod success
Dec 20 10:59:59.371: INFO: Pod "pod-secrets-39647ca6-cdcd-4946-a97e-44c2e1a4dc4f" satisfied condition "success or failure"
Dec 20 10:59:59.383: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-secrets-39647ca6-cdcd-4946-a97e-44c2e1a4dc4f container secret-volume-test: <nil>
STEP: delete the pod
Dec 20 10:59:59.445: INFO: Waiting for pod pod-secrets-39647ca6-cdcd-4946-a97e-44c2e1a4dc4f to disappear
Dec 20 10:59:59.452: INFO: Pod pod-secrets-39647ca6-cdcd-4946-a97e-44c2e1a4dc4f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 10:59:59.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7561" for this suite.
Dec 20 11:00:07.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:00:07.998: INFO: namespace secrets-7561 deletion completed in 8.539408106s

• [SLOW TEST:12.958 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:00:08.001: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1106
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 11:00:08.404: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:00:20.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1106" for this suite.
Dec 20 11:01:06.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:01:06.908: INFO: namespace pods-1106 deletion completed in 46.264676393s

• [SLOW TEST:58.907 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:01:06.908: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-879
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 11:01:07.132: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec 20 11:01:07.165: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 20 11:01:12.173: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 20 11:01:12.174: INFO: Creating deployment "test-rolling-update-deployment"
Dec 20 11:01:12.185: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec 20 11:01:12.200: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec 20 11:01:14.678: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec 20 11:01:14.689: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436472, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436472, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436473, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436472, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 11:01:17.101: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436472, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436472, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436473, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436472, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 11:01:18.704: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436472, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436472, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436473, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436472, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 11:01:20.701: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436472, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436472, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436473, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436472, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 11:01:22.700: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436472, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436472, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436473, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436472, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 11:01:24.696: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436472, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436472, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436473, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436472, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 11:01:26.696: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436472, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436472, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436473, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712436472, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 11:01:28.697: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 20 11:01:28.717: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-879 /apis/apps/v1/namespaces/deployment-879/deployments/test-rolling-update-deployment 9b3b0172-a511-4fd6-b90d-acee9a3badd1 9624 1 2019-12-20 11:01:12 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0020ccd68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-12-20 11:01:12 +0000 UTC,LastTransitionTime:2019-12-20 11:01:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2019-12-20 11:01:27 +0000 UTC,LastTransitionTime:2019-12-20 11:01:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 20 11:01:28.724: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-879 /apis/apps/v1/namespaces/deployment-879/replicasets/test-rolling-update-deployment-55d946486 bb925611-531f-4bef-9026-798f8b5ac35f 9614 1 2019-12-20 11:01:12 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 9b3b0172-a511-4fd6-b90d-acee9a3badd1 0xc002a70670 0xc002a70671}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002a706d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 20 11:01:28.725: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec 20 11:01:28.725: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-879 /apis/apps/v1/namespaces/deployment-879/replicasets/test-rolling-update-controller 112b386c-dc5d-419a-9d5a-75922c0b5be6 9623 2 2019-12-20 11:01:07 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 9b3b0172-a511-4fd6-b90d-acee9a3badd1 0xc002a7053f 0xc002a70570}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002a70618 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 20 11:01:28.732: INFO: Pod "test-rolling-update-deployment-55d946486-trrq6" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-trrq6 test-rolling-update-deployment-55d946486- deployment-879 /api/v1/namespaces/deployment-879/pods/test-rolling-update-deployment-55d946486-trrq6 ba60e01b-c70c-4783-b23f-ffb94bb290af 9613 0 2019-12-20 11:01:12 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[cni.projectcalico.org/podIP:172.25.1.29/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 bb925611-531f-4bef-9026-798f8b5ac35f 0xc002a71000 0xc002a71001}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9bm8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9bm8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9bm8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-428w5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:01:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:01:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:01:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:01:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.1.29,StartTime:2019-12-20 11:01:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-20 11:01:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://a3057b9a535f57a5e3d899e70f1d6afb6eb80107349d460165f93324db310c3c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:01:28.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-879" for this suite.
Dec 20 11:01:38.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:01:39.013: INFO: namespace deployment-879 deletion completed in 10.272504089s

• [SLOW TEST:32.106 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:01:39.018: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8922
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 20 11:01:39.955: INFO: Waiting up to 5m0s for pod "pod-288d9831-3de9-4180-ae36-fcbbce65b1ee" in namespace "emptydir-8922" to be "success or failure"
Dec 20 11:01:39.961: INFO: Pod "pod-288d9831-3de9-4180-ae36-fcbbce65b1ee": Phase="Pending", Reason="", readiness=false. Elapsed: 6.185933ms
Dec 20 11:01:41.969: INFO: Pod "pod-288d9831-3de9-4180-ae36-fcbbce65b1ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014304953s
Dec 20 11:01:43.975: INFO: Pod "pod-288d9831-3de9-4180-ae36-fcbbce65b1ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020537855s
STEP: Saw pod success
Dec 20 11:01:43.976: INFO: Pod "pod-288d9831-3de9-4180-ae36-fcbbce65b1ee" satisfied condition "success or failure"
Dec 20 11:01:43.980: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-288d9831-3de9-4180-ae36-fcbbce65b1ee container test-container: <nil>
STEP: delete the pod
Dec 20 11:01:44.101: INFO: Waiting for pod pod-288d9831-3de9-4180-ae36-fcbbce65b1ee to disappear
Dec 20 11:01:44.110: INFO: Pod pod-288d9831-3de9-4180-ae36-fcbbce65b1ee no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:01:44.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8922" for this suite.
Dec 20 11:01:50.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:01:50.477: INFO: namespace emptydir-8922 deletion completed in 6.309038896s

• [SLOW TEST:11.459 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:01:50.477: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8143
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 20 11:01:50.727: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a67ef1fd-0928-46a5-9355-d5f291d9e571" in namespace "downward-api-8143" to be "success or failure"
Dec 20 11:01:50.746: INFO: Pod "downwardapi-volume-a67ef1fd-0928-46a5-9355-d5f291d9e571": Phase="Pending", Reason="", readiness=false. Elapsed: 18.274633ms
Dec 20 11:01:52.752: INFO: Pod "downwardapi-volume-a67ef1fd-0928-46a5-9355-d5f291d9e571": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024840853s
Dec 20 11:01:54.759: INFO: Pod "downwardapi-volume-a67ef1fd-0928-46a5-9355-d5f291d9e571": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031386494s
STEP: Saw pod success
Dec 20 11:01:54.759: INFO: Pod "downwardapi-volume-a67ef1fd-0928-46a5-9355-d5f291d9e571" satisfied condition "success or failure"
Dec 20 11:01:54.766: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod downwardapi-volume-a67ef1fd-0928-46a5-9355-d5f291d9e571 container client-container: <nil>
STEP: delete the pod
Dec 20 11:01:54.817: INFO: Waiting for pod downwardapi-volume-a67ef1fd-0928-46a5-9355-d5f291d9e571 to disappear
Dec 20 11:01:54.822: INFO: Pod downwardapi-volume-a67ef1fd-0928-46a5-9355-d5f291d9e571 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:01:54.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8143" for this suite.
Dec 20 11:02:00.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:02:01.124: INFO: namespace downward-api-8143 deletion completed in 6.295125738s

• [SLOW TEST:10.647 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:02:01.128: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4517
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-4517/secret-test-15d7a855-2f38-446f-a753-c71051f46a12
STEP: Creating a pod to test consume secrets
Dec 20 11:02:01.371: INFO: Waiting up to 5m0s for pod "pod-configmaps-f5e2c4d4-3662-4b58-8639-e42a0404c9cd" in namespace "secrets-4517" to be "success or failure"
Dec 20 11:02:01.377: INFO: Pod "pod-configmaps-f5e2c4d4-3662-4b58-8639-e42a0404c9cd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.523683ms
Dec 20 11:02:03.384: INFO: Pod "pod-configmaps-f5e2c4d4-3662-4b58-8639-e42a0404c9cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012961376s
Dec 20 11:02:05.823: INFO: Pod "pod-configmaps-f5e2c4d4-3662-4b58-8639-e42a0404c9cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.452339936s
STEP: Saw pod success
Dec 20 11:02:05.824: INFO: Pod "pod-configmaps-f5e2c4d4-3662-4b58-8639-e42a0404c9cd" satisfied condition "success or failure"
Dec 20 11:02:05.830: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-configmaps-f5e2c4d4-3662-4b58-8639-e42a0404c9cd container env-test: <nil>
STEP: delete the pod
Dec 20 11:02:05.921: INFO: Waiting for pod pod-configmaps-f5e2c4d4-3662-4b58-8639-e42a0404c9cd to disappear
Dec 20 11:02:05.929: INFO: Pod pod-configmaps-f5e2c4d4-3662-4b58-8639-e42a0404c9cd no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:02:05.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4517" for this suite.
Dec 20 11:02:14.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:02:14.580: INFO: namespace secrets-4517 deletion completed in 8.611535615s

• [SLOW TEST:13.453 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:02:14.582: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8223
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 11:02:15.049: INFO: Creating deployment "webserver-deployment"
Dec 20 11:02:15.061: INFO: Waiting for observed generation 1
Dec 20 11:02:17.103: INFO: Waiting for all required pods to come up
Dec 20 11:02:17.113: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec 20 11:02:21.130: INFO: Waiting for deployment "webserver-deployment" to complete
Dec 20 11:02:21.157: INFO: Updating deployment "webserver-deployment" with a non-existent image
Dec 20 11:02:21.194: INFO: Updating deployment webserver-deployment
Dec 20 11:02:21.194: INFO: Waiting for observed generation 2
Dec 20 11:02:23.206: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec 20 11:02:23.213: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec 20 11:02:23.223: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 20 11:02:23.258: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec 20 11:02:23.258: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec 20 11:02:23.264: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 20 11:02:23.279: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Dec 20 11:02:23.279: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Dec 20 11:02:23.303: INFO: Updating deployment webserver-deployment
Dec 20 11:02:23.304: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Dec 20 11:02:23.323: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec 20 11:02:23.340: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 20 11:02:23.408: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8223 /apis/apps/v1/namespaces/deployment-8223/deployments/webserver-deployment 759ca5b3-03eb-441b-8949-fee1cecb75d9 10110 3 2019-12-20 11:02:15 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0044da148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2019-12-20 11:02:21 +0000 UTC,LastTransitionTime:2019-12-20 11:02:15 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2019-12-20 11:02:23 +0000 UTC,LastTransitionTime:2019-12-20 11:02:23 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Dec 20 11:02:23.462: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-8223 /apis/apps/v1/namespaces/deployment-8223/replicasets/webserver-deployment-c7997dcc8 6e97c744-ab27-4eb9-acd9-bd311b1b7344 10102 3 2019-12-20 11:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 759ca5b3-03eb-441b-8949-fee1cecb75d9 0xc0044da667 0xc0044da668}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0044da6d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 20 11:02:23.462: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Dec 20 11:02:23.462: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-8223 /apis/apps/v1/namespaces/deployment-8223/replicasets/webserver-deployment-595b5b9587 c1af7131-e279-4e49-aa37-de2a27edaa2c 10100 3 2019-12-20 11:02:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 759ca5b3-03eb-441b-8949-fee1cecb75d9 0xc0044da5a7 0xc0044da5a8}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0044da608 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Dec 20 11:02:23.529: INFO: Pod "webserver-deployment-595b5b9587-8lkg4" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-8lkg4 webserver-deployment-595b5b9587- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-595b5b9587-8lkg4 beaac015-1616-42c0-b333-f1f319aa7448 10158 0 2019-12-20 11:02:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c1af7131-e279-4e49-aa37-de2a27edaa2c 0xc003ba34c7 0xc003ba34c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-zpxc2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.529: INFO: Pod "webserver-deployment-595b5b9587-98jhf" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-98jhf webserver-deployment-595b5b9587- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-595b5b9587-98jhf 18d6c2c0-b937-4003-bb71-87b6615671a4 10118 0 2019-12-20 11:02:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c1af7131-e279-4e49-aa37-de2a27edaa2c 0xc003ba35d0 0xc003ba35d1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-dsqmn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.529: INFO: Pod "webserver-deployment-595b5b9587-b5rvd" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-b5rvd webserver-deployment-595b5b9587- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-595b5b9587-b5rvd 10dadfd9-7437-4db5-aec6-278ce76ebf59 10111 0 2019-12-20 11:02:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c1af7131-e279-4e49-aa37-de2a27edaa2c 0xc003ba36d0 0xc003ba36d1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-dsqmn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.529: INFO: Pod "webserver-deployment-595b5b9587-b9wxc" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-b9wxc webserver-deployment-595b5b9587- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-595b5b9587-b9wxc 3b9129a0-d73a-4e8d-8eb4-8c98cb82ed3b 10005 0 2019-12-20 11:02:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.2.26/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c1af7131-e279-4e49-aa37-de2a27edaa2c 0xc003ba37e0 0xc003ba37e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-zpxc2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.11,PodIP:172.25.2.26,StartTime:2019-12-20 11:02:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-20 11:02:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://2515d321a3cd048753f38d83fc83f46df073e2c6b77fdc296b5dbd91cd70c721,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.26,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.529: INFO: Pod "webserver-deployment-595b5b9587-bc7px" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bc7px webserver-deployment-595b5b9587- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-595b5b9587-bc7px d36cb5f0-98eb-44e6-a553-5761a9b9764c 10147 0 2019-12-20 11:02:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c1af7131-e279-4e49-aa37-de2a27edaa2c 0xc003ba3940 0xc003ba3941}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-dsqmn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.529: INFO: Pod "webserver-deployment-595b5b9587-c4tfd" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-c4tfd webserver-deployment-595b5b9587- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-595b5b9587-c4tfd d235c698-3c80-4afb-989e-66898eab0870 10153 0 2019-12-20 11:02:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c1af7131-e279-4e49-aa37-de2a27edaa2c 0xc003ba3a40 0xc003ba3a41}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-zpxc2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.530: INFO: Pod "webserver-deployment-595b5b9587-cgns5" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-cgns5 webserver-deployment-595b5b9587- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-595b5b9587-cgns5 e4e7e9a0-9c2b-43a5-9cca-8bd5b7627783 9997 0 2019-12-20 11:02:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.2.25/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c1af7131-e279-4e49-aa37-de2a27edaa2c 0xc003ba3b50 0xc003ba3b51}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-zpxc2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.11,PodIP:172.25.2.25,StartTime:2019-12-20 11:02:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-20 11:02:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f3ac9380d2a52d2ab38446257e335554db4f5d16f9024547da40cc429cf71b4f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.25,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.530: INFO: Pod "webserver-deployment-595b5b9587-dnhq8" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-dnhq8 webserver-deployment-595b5b9587- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-595b5b9587-dnhq8 635aa868-2e8a-4791-9e89-730e4590f0d2 9984 0 2019-12-20 11:02:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.0.12/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c1af7131-e279-4e49-aa37-de2a27edaa2c 0xc003ba3cd0 0xc003ba3cd1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-dsqmn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.6,PodIP:172.25.0.12,StartTime:2019-12-20 11:02:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-20 11:02:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://e4da8562d5232733c88876cb0963b44beb909ec8676c7d96d7f793891ebeda99,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.12,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.530: INFO: Pod "webserver-deployment-595b5b9587-fnpsv" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fnpsv webserver-deployment-595b5b9587- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-595b5b9587-fnpsv 334ad7d2-5232-4e38-a37d-cc83640cffdc 10108 0 2019-12-20 11:02:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c1af7131-e279-4e49-aa37-de2a27edaa2c 0xc003ba3e30 0xc003ba3e31}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-428w5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.530: INFO: Pod "webserver-deployment-595b5b9587-hlnfk" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-hlnfk webserver-deployment-595b5b9587- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-595b5b9587-hlnfk 3ca55a30-82cf-4fb4-992b-85466222fbfd 10002 0 2019-12-20 11:02:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.2.27/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c1af7131-e279-4e49-aa37-de2a27edaa2c 0xc003ba3f40 0xc003ba3f41}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-zpxc2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.11,PodIP:172.25.2.27,StartTime:2019-12-20 11:02:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-20 11:02:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://cfaa8ce1d862b68eae92b442ae2d0ee7f944813880e65b29e9d6441278df5ad3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.27,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.531: INFO: Pod "webserver-deployment-595b5b9587-hm7t9" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-hm7t9 webserver-deployment-595b5b9587- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-595b5b9587-hm7t9 16e1c109-eac3-4672-81ef-d2df3cace9c1 10127 0 2019-12-20 11:02:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c1af7131-e279-4e49-aa37-de2a27edaa2c 0xc0049e80a0 0xc0049e80a1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-zpxc2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.531: INFO: Pod "webserver-deployment-595b5b9587-j8vb4" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-j8vb4 webserver-deployment-595b5b9587- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-595b5b9587-j8vb4 f363d9c4-806a-4aff-aed9-abba02f65541 9979 0 2019-12-20 11:02:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.0.11/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c1af7131-e279-4e49-aa37-de2a27edaa2c 0xc0049e81b0 0xc0049e81b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-dsqmn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.6,PodIP:172.25.0.11,StartTime:2019-12-20 11:02:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-20 11:02:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://16e73a689cab644b3170358c9450921bd60988b10f8612577257888333120e65,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.531: INFO: Pod "webserver-deployment-595b5b9587-js5ql" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-js5ql webserver-deployment-595b5b9587- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-595b5b9587-js5ql 19a0a32b-1678-460d-a315-47341ce8fa13 10010 0 2019-12-20 11:02:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.1.34/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c1af7131-e279-4e49-aa37-de2a27edaa2c 0xc0049e8320 0xc0049e8321}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-428w5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.1.34,StartTime:2019-12-20 11:02:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-20 11:02:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://7cf87ac2e050c233ee836c64b51672019b84b678ffcb233ff35413c174779dbc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.34,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.531: INFO: Pod "webserver-deployment-595b5b9587-nhnxw" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-nhnxw webserver-deployment-595b5b9587- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-595b5b9587-nhnxw 4b8206c7-e7c1-42e9-8d2a-041a0d074b5f 10129 0 2019-12-20 11:02:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c1af7131-e279-4e49-aa37-de2a27edaa2c 0xc0049e8480 0xc0049e8481}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-zpxc2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.531: INFO: Pod "webserver-deployment-595b5b9587-q5cwp" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-q5cwp webserver-deployment-595b5b9587- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-595b5b9587-q5cwp 7654391a-b856-4b59-beb8-08de12bd3511 10011 0 2019-12-20 11:02:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.1.33/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c1af7131-e279-4e49-aa37-de2a27edaa2c 0xc0049e8590 0xc0049e8591}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-428w5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.1.33,StartTime:2019-12-20 11:02:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-20 11:02:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ef32e439cec51ab5d8d0b384646c178f3315deb0ef0cd7ae3e8d68a9ccc22a3e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.532: INFO: Pod "webserver-deployment-595b5b9587-qm94s" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qm94s webserver-deployment-595b5b9587- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-595b5b9587-qm94s cb3cffb5-5b5a-42ec-882f-cbfa5edc8f65 10148 0 2019-12-20 11:02:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c1af7131-e279-4e49-aa37-de2a27edaa2c 0xc0049e86f0 0xc0049e86f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-zpxc2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.532: INFO: Pod "webserver-deployment-595b5b9587-qvlv5" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qvlv5 webserver-deployment-595b5b9587- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-595b5b9587-qvlv5 b961a174-45d4-4117-bf5b-533a62086281 10021 0 2019-12-20 11:02:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.1.36/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c1af7131-e279-4e49-aa37-de2a27edaa2c 0xc0049e8800 0xc0049e8801}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-428w5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.1.36,StartTime:2019-12-20 11:02:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-20 11:02:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f6fbf9b48a76715e86896a78a19a03952c8431479167d60be3ebf1a1c7a8a6e2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.36,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.532: INFO: Pod "webserver-deployment-595b5b9587-shcj6" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-shcj6 webserver-deployment-595b5b9587- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-595b5b9587-shcj6 e8e91a6a-2386-4abd-a4a0-778386ae1a1c 10149 0 2019-12-20 11:02:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c1af7131-e279-4e49-aa37-de2a27edaa2c 0xc0049e8970 0xc0049e8971}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-428w5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.532: INFO: Pod "webserver-deployment-595b5b9587-tlq6f" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-tlq6f webserver-deployment-595b5b9587- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-595b5b9587-tlq6f 0f85f6fe-0457-4c4e-b4fe-f90da0cad87b 10126 0 2019-12-20 11:02:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c1af7131-e279-4e49-aa37-de2a27edaa2c 0xc0049e8a70 0xc0049e8a71}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-428w5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.532: INFO: Pod "webserver-deployment-595b5b9587-xvbqq" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xvbqq webserver-deployment-595b5b9587- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-595b5b9587-xvbqq 687461ab-19af-42d5-8012-ca4e935ea286 10128 0 2019-12-20 11:02:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 c1af7131-e279-4e49-aa37-de2a27edaa2c 0xc0049e8b70 0xc0049e8b71}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-428w5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.533: INFO: Pod "webserver-deployment-c7997dcc8-6sqpz" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-6sqpz webserver-deployment-c7997dcc8- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-c7997dcc8-6sqpz b596d777-355b-4db7-bd6e-e2554f92f8b8 10161 0 2019-12-20 11:02:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6e97c744-ab27-4eb9-acd9-bd311b1b7344 0xc0049e8c70 0xc0049e8c71}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-428w5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.533: INFO: Pod "webserver-deployment-c7997dcc8-7gcwh" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-7gcwh webserver-deployment-c7997dcc8- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-c7997dcc8-7gcwh 365f384f-d79a-4fbc-b90c-0603be06d7d2 10151 0 2019-12-20 11:02:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6e97c744-ab27-4eb9-acd9-bd311b1b7344 0xc0049e8d80 0xc0049e8d81}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-dsqmn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.533: INFO: Pod "webserver-deployment-c7997dcc8-8ht94" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-8ht94 webserver-deployment-c7997dcc8- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-c7997dcc8-8ht94 59eeb8ae-2163-4c9e-86aa-59e843089661 10096 0 2019-12-20 11:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.2.28/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6e97c744-ab27-4eb9-acd9-bd311b1b7344 0xc0049e8ea0 0xc0049e8ea1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-zpxc2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.11,PodIP:,StartTime:2019-12-20 11:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.533: INFO: Pod "webserver-deployment-c7997dcc8-c7j2c" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-c7j2c webserver-deployment-c7997dcc8- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-c7997dcc8-c7j2c 6f98671a-0943-4b72-a82c-2d9dc6ccf390 10160 0 2019-12-20 11:02:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6e97c744-ab27-4eb9-acd9-bd311b1b7344 0xc0049e9007 0xc0049e9008}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-zpxc2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.533: INFO: Pod "webserver-deployment-c7997dcc8-gvv2f" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-gvv2f webserver-deployment-c7997dcc8- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-c7997dcc8-gvv2f c1b98934-1756-4af8-8773-167010a35a06 10089 0 2019-12-20 11:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.0.13/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6e97c744-ab27-4eb9-acd9-bd311b1b7344 0xc0049e9130 0xc0049e9131}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-dsqmn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.6,PodIP:,StartTime:2019-12-20 11:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.533: INFO: Pod "webserver-deployment-c7997dcc8-j7z66" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-j7z66 webserver-deployment-c7997dcc8- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-c7997dcc8-j7z66 7bd45f02-0dee-47d0-8a82-fac15461e5a4 10156 0 2019-12-20 11:02:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6e97c744-ab27-4eb9-acd9-bd311b1b7344 0xc0049e9290 0xc0049e9291}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.533: INFO: Pod "webserver-deployment-c7997dcc8-jhnzf" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-jhnzf webserver-deployment-c7997dcc8- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-c7997dcc8-jhnzf f2071edc-8023-498e-8fbc-5b6e940af8b1 10152 0 2019-12-20 11:02:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6e97c744-ab27-4eb9-acd9-bd311b1b7344 0xc0049e9387 0xc0049e9388}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-dsqmn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.534: INFO: Pod "webserver-deployment-c7997dcc8-jjkvm" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-jjkvm webserver-deployment-c7997dcc8- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-c7997dcc8-jjkvm 6442d846-a47f-44a7-9f3d-6094f665086a 10095 0 2019-12-20 11:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.1.39/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6e97c744-ab27-4eb9-acd9-bd311b1b7344 0xc0049e94b0 0xc0049e94b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-428w5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2019-12-20 11:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.534: INFO: Pod "webserver-deployment-c7997dcc8-k6xgj" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-k6xgj webserver-deployment-c7997dcc8- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-c7997dcc8-k6xgj c6d707d6-2fa3-4b9e-b3ee-40f2716fdc61 10124 0 2019-12-20 11:02:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6e97c744-ab27-4eb9-acd9-bd311b1b7344 0xc0049e9610 0xc0049e9611}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-zpxc2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.534: INFO: Pod "webserver-deployment-c7997dcc8-plsgc" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-plsgc webserver-deployment-c7997dcc8- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-c7997dcc8-plsgc 2c303c92-9fdb-4c59-af3f-78d7e6cc1e57 10082 0 2019-12-20 11:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6e97c744-ab27-4eb9-acd9-bd311b1b7344 0xc0049e9720 0xc0049e9721}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-zpxc2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.11,PodIP:,StartTime:2019-12-20 11:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.534: INFO: Pod "webserver-deployment-c7997dcc8-qfrfw" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qfrfw webserver-deployment-c7997dcc8- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-c7997dcc8-qfrfw a088c76b-bea8-4618-b08b-0706344bb912 10123 0 2019-12-20 11:02:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6e97c744-ab27-4eb9-acd9-bd311b1b7344 0xc0049e9887 0xc0049e9888}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-dsqmn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.534: INFO: Pod "webserver-deployment-c7997dcc8-w7bpv" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-w7bpv webserver-deployment-c7997dcc8- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-c7997dcc8-w7bpv 7fda4455-a2a9-4e66-8efb-f3785556adf0 10130 0 2019-12-20 11:02:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6e97c744-ab27-4eb9-acd9-bd311b1b7344 0xc0049e99a0 0xc0049e99a1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-428w5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 20 11:02:23.534: INFO: Pod "webserver-deployment-c7997dcc8-wq8m4" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-wq8m4 webserver-deployment-c7997dcc8- deployment-8223 /api/v1/namespaces/deployment-8223/pods/webserver-deployment-c7997dcc8-wq8m4 b255d53a-d0c5-4730-8285-bef02dba0556 10091 0 2019-12-20 11:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.1.38/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6e97c744-ab27-4eb9-acd9-bd311b1b7344 0xc0049e9ac0 0xc0049e9ac1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvk9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvk9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvk9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-428w5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:,StartTime:2019-12-20 11:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:02:23.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8223" for this suite.
Dec 20 11:02:33.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:02:33.890: INFO: namespace deployment-8223 deletion completed in 10.292557829s

• [SLOW TEST:19.308 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:02:33.892: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2350
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Dec 20 11:02:44.189: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-773140870 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Dec 20 11:02:54.389: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:02:54.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2350" for this suite.
Dec 20 11:03:00.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:03:00.724: INFO: namespace pods-2350 deletion completed in 6.307893619s

• [SLOW TEST:26.832 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:03:00.724: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-9953
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec 20 11:03:13.047: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9953 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 20 11:03:13.047: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 11:03:13.628: INFO: Exec stderr: ""
Dec 20 11:03:13.628: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9953 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 20 11:03:13.628: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 11:03:14.120: INFO: Exec stderr: ""
Dec 20 11:03:14.120: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9953 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 20 11:03:14.121: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 11:03:14.668: INFO: Exec stderr: ""
Dec 20 11:03:14.669: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9953 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 20 11:03:14.669: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 11:03:15.091: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec 20 11:03:15.092: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9953 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 20 11:03:15.092: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 11:03:15.592: INFO: Exec stderr: ""
Dec 20 11:03:15.592: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9953 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 20 11:03:15.592: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 11:03:16.099: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec 20 11:03:16.099: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9953 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 20 11:03:16.099: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 11:03:16.657: INFO: Exec stderr: ""
Dec 20 11:03:16.657: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9953 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 20 11:03:16.657: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 11:03:17.170: INFO: Exec stderr: ""
Dec 20 11:03:17.170: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9953 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 20 11:03:17.170: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 11:03:17.775: INFO: Exec stderr: ""
Dec 20 11:03:17.775: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9953 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 20 11:03:17.775: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 11:03:18.350: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:03:18.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9953" for this suite.
Dec 20 11:04:06.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:04:06.639: INFO: namespace e2e-kubelet-etc-hosts-9953 deletion completed in 48.277986815s

• [SLOW TEST:65.915 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:04:06.640: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8879
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-4aa0d366-8946-4174-831e-fdf979e887ba
STEP: Creating a pod to test consume configMaps
Dec 20 11:04:06.870: INFO: Waiting up to 5m0s for pod "pod-configmaps-412a18ba-da1d-4d60-ae14-97877c030a74" in namespace "configmap-8879" to be "success or failure"
Dec 20 11:04:06.877: INFO: Pod "pod-configmaps-412a18ba-da1d-4d60-ae14-97877c030a74": Phase="Pending", Reason="", readiness=false. Elapsed: 7.06697ms
Dec 20 11:04:08.884: INFO: Pod "pod-configmaps-412a18ba-da1d-4d60-ae14-97877c030a74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01468975s
Dec 20 11:04:10.891: INFO: Pod "pod-configmaps-412a18ba-da1d-4d60-ae14-97877c030a74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021026579s
STEP: Saw pod success
Dec 20 11:04:10.891: INFO: Pod "pod-configmaps-412a18ba-da1d-4d60-ae14-97877c030a74" satisfied condition "success or failure"
Dec 20 11:04:10.897: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-configmaps-412a18ba-da1d-4d60-ae14-97877c030a74 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 20 11:04:10.951: INFO: Waiting for pod pod-configmaps-412a18ba-da1d-4d60-ae14-97877c030a74 to disappear
Dec 20 11:04:10.957: INFO: Pod pod-configmaps-412a18ba-da1d-4d60-ae14-97877c030a74 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:04:10.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8879" for this suite.
Dec 20 11:04:17.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:04:17.243: INFO: namespace configmap-8879 deletion completed in 6.277588605s

• [SLOW TEST:10.603 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:04:17.245: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3142
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 20 11:04:17.473: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8a80c206-0b2d-44b4-956a-68b79eaf305b" in namespace "downward-api-3142" to be "success or failure"
Dec 20 11:04:17.511: INFO: Pod "downwardapi-volume-8a80c206-0b2d-44b4-956a-68b79eaf305b": Phase="Pending", Reason="", readiness=false. Elapsed: 38.381783ms
Dec 20 11:04:19.519: INFO: Pod "downwardapi-volume-8a80c206-0b2d-44b4-956a-68b79eaf305b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04589434s
Dec 20 11:04:21.526: INFO: Pod "downwardapi-volume-8a80c206-0b2d-44b4-956a-68b79eaf305b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05250547s
STEP: Saw pod success
Dec 20 11:04:21.526: INFO: Pod "downwardapi-volume-8a80c206-0b2d-44b4-956a-68b79eaf305b" satisfied condition "success or failure"
Dec 20 11:04:21.532: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod downwardapi-volume-8a80c206-0b2d-44b4-956a-68b79eaf305b container client-container: <nil>
STEP: delete the pod
Dec 20 11:04:21.580: INFO: Waiting for pod downwardapi-volume-8a80c206-0b2d-44b4-956a-68b79eaf305b to disappear
Dec 20 11:04:21.587: INFO: Pod downwardapi-volume-8a80c206-0b2d-44b4-956a-68b79eaf305b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:04:21.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3142" for this suite.
Dec 20 11:04:27.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:04:27.870: INFO: namespace downward-api-3142 deletion completed in 6.275383496s

• [SLOW TEST:10.625 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:04:27.871: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3730
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-04bdfdc5-1ebe-4868-8362-1ed1c72c36b9
STEP: Creating a pod to test consume secrets
Dec 20 11:04:28.115: INFO: Waiting up to 5m0s for pod "pod-secrets-5d3ee913-df23-4e0b-9671-7d29e32bf3bf" in namespace "secrets-3730" to be "success or failure"
Dec 20 11:04:28.126: INFO: Pod "pod-secrets-5d3ee913-df23-4e0b-9671-7d29e32bf3bf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.886932ms
Dec 20 11:04:30.138: INFO: Pod "pod-secrets-5d3ee913-df23-4e0b-9671-7d29e32bf3bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023172479s
Dec 20 11:04:32.146: INFO: Pod "pod-secrets-5d3ee913-df23-4e0b-9671-7d29e32bf3bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030379221s
STEP: Saw pod success
Dec 20 11:04:32.146: INFO: Pod "pod-secrets-5d3ee913-df23-4e0b-9671-7d29e32bf3bf" satisfied condition "success or failure"
Dec 20 11:04:32.151: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-secrets-5d3ee913-df23-4e0b-9671-7d29e32bf3bf container secret-volume-test: <nil>
STEP: delete the pod
Dec 20 11:04:32.204: INFO: Waiting for pod pod-secrets-5d3ee913-df23-4e0b-9671-7d29e32bf3bf to disappear
Dec 20 11:04:32.213: INFO: Pod pod-secrets-5d3ee913-df23-4e0b-9671-7d29e32bf3bf no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:04:32.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3730" for this suite.
Dec 20 11:04:38.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:04:38.670: INFO: namespace secrets-3730 deletion completed in 6.449108639s

• [SLOW TEST:10.800 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:04:38.671: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5745
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-7ab7c344-13bf-4a3f-9edf-0941c57d8cd5 in namespace container-probe-5745
Dec 20 11:04:42.947: INFO: Started pod busybox-7ab7c344-13bf-4a3f-9edf-0941c57d8cd5 in namespace container-probe-5745
STEP: checking the pod's current state and verifying that restartCount is present
Dec 20 11:04:42.955: INFO: Initial restart count of pod busybox-7ab7c344-13bf-4a3f-9edf-0941c57d8cd5 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:08:43.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5745" for this suite.
Dec 20 11:08:49.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:08:49.610: INFO: namespace container-probe-5745 deletion completed in 6.288035687s

• [SLOW TEST:250.939 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:08:49.613: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3845
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Dec 20 11:08:49.889: INFO: Waiting up to 5m0s for pod "pod-3db2c71a-a660-4e9c-87f7-02d8dc07d401" in namespace "emptydir-3845" to be "success or failure"
Dec 20 11:08:49.901: INFO: Pod "pod-3db2c71a-a660-4e9c-87f7-02d8dc07d401": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008641ms
Dec 20 11:08:51.907: INFO: Pod "pod-3db2c71a-a660-4e9c-87f7-02d8dc07d401": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018430493s
Dec 20 11:08:53.923: INFO: Pod "pod-3db2c71a-a660-4e9c-87f7-02d8dc07d401": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034785204s
Dec 20 11:08:55.932: INFO: Pod "pod-3db2c71a-a660-4e9c-87f7-02d8dc07d401": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043354474s
STEP: Saw pod success
Dec 20 11:08:55.932: INFO: Pod "pod-3db2c71a-a660-4e9c-87f7-02d8dc07d401" satisfied condition "success or failure"
Dec 20 11:08:55.942: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-3db2c71a-a660-4e9c-87f7-02d8dc07d401 container test-container: <nil>
STEP: delete the pod
Dec 20 11:08:56.072: INFO: Waiting for pod pod-3db2c71a-a660-4e9c-87f7-02d8dc07d401 to disappear
Dec 20 11:08:56.077: INFO: Pod pod-3db2c71a-a660-4e9c-87f7-02d8dc07d401 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:08:56.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3845" for this suite.
Dec 20 11:09:02.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:09:02.383: INFO: namespace emptydir-3845 deletion completed in 6.298498521s

• [SLOW TEST:12.770 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:09:02.385: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9675
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 20 11:09:02.586: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 20 11:09:02.638: INFO: Waiting for terminating namespaces to be deleted...
Dec 20 11:09:02.645: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-457fd-578f899757-428w5 before test
Dec 20 11:09:02.676: INFO: sonobuoy-systemd-logs-daemon-set-73919c9b23754985-gvq9z from sonobuoy started at 2019-12-20 10:42:00 +0000 UTC (2 container statuses recorded)
Dec 20 11:09:02.677: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 20 11:09:02.677: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 20 11:09:02.677: INFO: node-exporter-6fmjz from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 11:09:02.677: INFO: 	Container node-exporter ready: true, restart count 0
Dec 20 11:09:02.677: INFO: canal-64h7p from kube-system started at 2019-12-20 10:27:04 +0000 UTC (2 container statuses recorded)
Dec 20 11:09:02.677: INFO: 	Container calico-node ready: true, restart count 0
Dec 20 11:09:02.678: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 20 11:09:02.678: INFO: kube-proxy-nfqd6 from kube-system started at 2019-12-20 10:26:58 +0000 UTC (1 container statuses recorded)
Dec 20 11:09:02.678: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 20 11:09:02.678: INFO: sonobuoy from sonobuoy started at 2019-12-20 10:41:51 +0000 UTC (1 container statuses recorded)
Dec 20 11:09:02.678: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 20 11:09:02.678: INFO: node-local-dns-kj7kj from kube-system started at 2019-12-20 10:26:58 +0000 UTC (1 container statuses recorded)
Dec 20 11:09:02.678: INFO: 	Container node-cache ready: true, restart count 0
Dec 20 11:09:02.679: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-457fd-578f899757-dsqmn before test
Dec 20 11:09:02.741: INFO: tiller-deploy-78f78bb476-6t2mk from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 11:09:02.741: INFO: 	Container tiller ready: true, restart count 0
Dec 20 11:09:02.741: INFO: kube-proxy-wq825 from kube-system started at 2019-12-20 10:26:26 +0000 UTC (1 container statuses recorded)
Dec 20 11:09:02.742: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 20 11:09:02.742: INFO: canal-gcrnc from kube-system started at 2019-12-20 10:26:27 +0000 UTC (2 container statuses recorded)
Dec 20 11:09:02.742: INFO: 	Container calico-node ready: true, restart count 0
Dec 20 11:09:02.742: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 20 11:09:02.742: INFO: openvpn-client-64df8b95c9-wr7dw from kube-system started at 2019-12-20 10:27:01 +0000 UTC (2 container statuses recorded)
Dec 20 11:09:02.742: INFO: 	Container dnat-controller ready: true, restart count 0
Dec 20 11:09:02.742: INFO: 	Container openvpn-client ready: true, restart count 0
Dec 20 11:09:02.742: INFO: coredns-547f89d7d5-5w7wq from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 11:09:02.742: INFO: 	Container coredns ready: true, restart count 0
Dec 20 11:09:02.742: INFO: node-local-dns-7tvth from kube-system started at 2019-12-20 10:26:26 +0000 UTC (1 container statuses recorded)
Dec 20 11:09:02.742: INFO: 	Container node-cache ready: true, restart count 0
Dec 20 11:09:02.743: INFO: node-exporter-qnl6p from kube-system started at 2019-12-20 10:26:26 +0000 UTC (1 container statuses recorded)
Dec 20 11:09:02.743: INFO: 	Container node-exporter ready: true, restart count 0
Dec 20 11:09:02.743: INFO: coredns-547f89d7d5-s26jq from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 11:09:02.743: INFO: 	Container coredns ready: true, restart count 0
Dec 20 11:09:02.743: INFO: cluster-autoscaler-8c65c7d54-9cqhv from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 11:09:02.743: INFO: 	Container cluster-autoscaler ready: true, restart count 0
Dec 20 11:09:02.743: INFO: sonobuoy-systemd-logs-daemon-set-73919c9b23754985-dp62g from sonobuoy started at 2019-12-20 10:42:00 +0000 UTC (2 container statuses recorded)
Dec 20 11:09:02.743: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 20 11:09:02.743: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 20 11:09:02.743: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-457fd-578f899757-zpxc2 before test
Dec 20 11:09:02.801: INFO: node-exporter-wxf2r from kube-system started at 2019-12-20 10:27:10 +0000 UTC (1 container statuses recorded)
Dec 20 11:09:02.801: INFO: 	Container node-exporter ready: true, restart count 0
Dec 20 11:09:02.801: INFO: sonobuoy-e2e-job-1e71c3c7847b472c from sonobuoy started at 2019-12-20 10:42:00 +0000 UTC (2 container statuses recorded)
Dec 20 11:09:02.801: INFO: 	Container e2e ready: true, restart count 0
Dec 20 11:09:02.801: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 20 11:09:02.802: INFO: node-local-dns-4wkt5 from kube-system started at 2019-12-20 10:27:10 +0000 UTC (1 container statuses recorded)
Dec 20 11:09:02.802: INFO: 	Container node-cache ready: true, restart count 0
Dec 20 11:09:02.802: INFO: canal-w56q4 from kube-system started at 2019-12-20 10:27:10 +0000 UTC (2 container statuses recorded)
Dec 20 11:09:02.802: INFO: 	Container calico-node ready: true, restart count 0
Dec 20 11:09:02.802: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 20 11:09:02.802: INFO: sonobuoy-systemd-logs-daemon-set-73919c9b23754985-gwhsz from sonobuoy started at 2019-12-20 10:42:00 +0000 UTC (2 container statuses recorded)
Dec 20 11:09:02.802: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 20 11:09:02.802: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 20 11:09:02.803: INFO: kube-proxy-pml2l from kube-system started at 2019-12-20 10:27:10 +0000 UTC (1 container statuses recorded)
Dec 20 11:09:02.803: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-ed47636e-9161-4cc7-835f-2b2c91340aba 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-ed47636e-9161-4cc7-835f-2b2c91340aba off the node metakube-worker-457fd-578f899757-zpxc2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ed47636e-9161-4cc7-835f-2b2c91340aba
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:14:11.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9675" for this suite.
Dec 20 11:14:33.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:14:34.022: INFO: namespace sched-pred-9675 deletion completed in 22.946680772s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:331.637 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:14:34.024: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-9027
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 20 11:14:37.034: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Dec 20 11:14:39.079: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712437277, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712437277, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712437278, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712437276, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 20 11:14:42.120: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 11:14:42.128: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:14:43.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9027" for this suite.
Dec 20 11:14:49.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:14:49.854: INFO: namespace crd-webhook-9027 deletion completed in 6.275496713s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:15.890 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:14:49.914: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1656
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-72e57bb5-e268-40ec-bca0-9a8636cf99fa
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:14:56.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1656" for this suite.
Dec 20 11:15:24.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:15:24.640: INFO: namespace configmap-1656 deletion completed in 28.311249943s

• [SLOW TEST:34.726 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:15:24.640: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9442
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec 20 11:15:24.920: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9442 /api/v1/namespaces/watch-9442/configmaps/e2e-watch-test-resource-version d9bcadb3-9fd4-43eb-9cba-4c0e23a1b394 13293 0 2019-12-20 11:15:24 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 20 11:15:24.921: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9442 /api/v1/namespaces/watch-9442/configmaps/e2e-watch-test-resource-version d9bcadb3-9fd4-43eb-9cba-4c0e23a1b394 13294 0 2019-12-20 11:15:24 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:15:24.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9442" for this suite.
Dec 20 11:15:30.979: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:15:31.226: INFO: namespace watch-9442 deletion completed in 6.295896658s

• [SLOW TEST:6.586 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:15:31.227: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-2613
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec 20 11:15:32.218: INFO: Pod name wrapped-volume-race-f8a9b801-a834-4d35-bdb6-4ae6e5e0507a: Found 0 pods out of 5
Dec 20 11:15:37.234: INFO: Pod name wrapped-volume-race-f8a9b801-a834-4d35-bdb6-4ae6e5e0507a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f8a9b801-a834-4d35-bdb6-4ae6e5e0507a in namespace emptydir-wrapper-2613, will wait for the garbage collector to delete the pods
Dec 20 11:15:51.379: INFO: Deleting ReplicationController wrapped-volume-race-f8a9b801-a834-4d35-bdb6-4ae6e5e0507a took: 32.129491ms
Dec 20 11:15:51.879: INFO: Terminating ReplicationController wrapped-volume-race-f8a9b801-a834-4d35-bdb6-4ae6e5e0507a pods took: 500.47532ms
STEP: Creating RC which spawns configmap-volume pods
Dec 20 11:16:35.529: INFO: Pod name wrapped-volume-race-2ac11abb-8f69-48f5-98a3-f1794feb005c: Found 0 pods out of 5
Dec 20 11:16:40.541: INFO: Pod name wrapped-volume-race-2ac11abb-8f69-48f5-98a3-f1794feb005c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2ac11abb-8f69-48f5-98a3-f1794feb005c in namespace emptydir-wrapper-2613, will wait for the garbage collector to delete the pods
Dec 20 11:17:00.655: INFO: Deleting ReplicationController wrapped-volume-race-2ac11abb-8f69-48f5-98a3-f1794feb005c took: 15.837697ms
Dec 20 11:17:01.157: INFO: Terminating ReplicationController wrapped-volume-race-2ac11abb-8f69-48f5-98a3-f1794feb005c pods took: 501.905152ms
STEP: Creating RC which spawns configmap-volume pods
Dec 20 11:17:43.103: INFO: Pod name wrapped-volume-race-b55b4321-33cd-4e27-8ba5-0c10d20ce73d: Found 0 pods out of 5
Dec 20 11:17:48.115: INFO: Pod name wrapped-volume-race-b55b4321-33cd-4e27-8ba5-0c10d20ce73d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b55b4321-33cd-4e27-8ba5-0c10d20ce73d in namespace emptydir-wrapper-2613, will wait for the garbage collector to delete the pods
Dec 20 11:18:06.243: INFO: Deleting ReplicationController wrapped-volume-race-b55b4321-33cd-4e27-8ba5-0c10d20ce73d took: 18.545193ms
Dec 20 11:18:06.744: INFO: Terminating ReplicationController wrapped-volume-race-b55b4321-33cd-4e27-8ba5-0c10d20ce73d pods took: 500.441496ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:18:47.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2613" for this suite.
Dec 20 11:19:05.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:19:05.706: INFO: namespace emptydir-wrapper-2613 deletion completed in 18.311309844s

• [SLOW TEST:214.480 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:19:05.708: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5925
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-125c0747-8b0f-43c8-905b-11a4a9310d6d
STEP: Creating secret with name secret-projected-all-test-volume-b326e966-2020-4225-80f9-2f08dcbc31b0
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec 20 11:19:06.090: INFO: Waiting up to 5m0s for pod "projected-volume-b11fef54-13e0-4be4-9ac1-052eeede85c0" in namespace "projected-5925" to be "success or failure"
Dec 20 11:19:06.117: INFO: Pod "projected-volume-b11fef54-13e0-4be4-9ac1-052eeede85c0": Phase="Pending", Reason="", readiness=false. Elapsed: 26.662793ms
Dec 20 11:19:08.128: INFO: Pod "projected-volume-b11fef54-13e0-4be4-9ac1-052eeede85c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037798786s
Dec 20 11:19:10.142: INFO: Pod "projected-volume-b11fef54-13e0-4be4-9ac1-052eeede85c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051238826s
STEP: Saw pod success
Dec 20 11:19:10.142: INFO: Pod "projected-volume-b11fef54-13e0-4be4-9ac1-052eeede85c0" satisfied condition "success or failure"
Dec 20 11:19:10.149: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod projected-volume-b11fef54-13e0-4be4-9ac1-052eeede85c0 container projected-all-volume-test: <nil>
STEP: delete the pod
Dec 20 11:19:10.195: INFO: Waiting for pod projected-volume-b11fef54-13e0-4be4-9ac1-052eeede85c0 to disappear
Dec 20 11:19:10.201: INFO: Pod projected-volume-b11fef54-13e0-4be4-9ac1-052eeede85c0 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:19:10.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5925" for this suite.
Dec 20 11:19:20.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:19:20.872: INFO: namespace projected-5925 deletion completed in 10.655469488s

• [SLOW TEST:15.164 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:19:20.873: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5754
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-4c55c20e-a194-4026-befd-2b54bbef8aa8
STEP: Creating a pod to test consume configMaps
Dec 20 11:19:22.610: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c6ca80f5-4020-4573-aa62-085b16f5ce64" in namespace "projected-5754" to be "success or failure"
Dec 20 11:19:22.631: INFO: Pod "pod-projected-configmaps-c6ca80f5-4020-4573-aa62-085b16f5ce64": Phase="Pending", Reason="", readiness=false. Elapsed: 21.026645ms
Dec 20 11:19:24.638: INFO: Pod "pod-projected-configmaps-c6ca80f5-4020-4573-aa62-085b16f5ce64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028674582s
Dec 20 11:19:26.648: INFO: Pod "pod-projected-configmaps-c6ca80f5-4020-4573-aa62-085b16f5ce64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038452271s
STEP: Saw pod success
Dec 20 11:19:26.649: INFO: Pod "pod-projected-configmaps-c6ca80f5-4020-4573-aa62-085b16f5ce64" satisfied condition "success or failure"
Dec 20 11:19:26.660: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-projected-configmaps-c6ca80f5-4020-4573-aa62-085b16f5ce64 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 20 11:19:26.863: INFO: Waiting for pod pod-projected-configmaps-c6ca80f5-4020-4573-aa62-085b16f5ce64 to disappear
Dec 20 11:19:26.876: INFO: Pod pod-projected-configmaps-c6ca80f5-4020-4573-aa62-085b16f5ce64 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:19:26.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5754" for this suite.
Dec 20 11:19:35.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:19:35.311: INFO: namespace projected-5754 deletion completed in 8.423104548s

• [SLOW TEST:14.438 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:19:35.312: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3303
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Dec 20 11:19:35.499: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:19:58.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3303" for this suite.
Dec 20 11:20:06.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:20:07.995: INFO: namespace crd-publish-openapi-3303 deletion completed in 9.645656445s

• [SLOW TEST:32.683 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:20:07.996: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7614
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Dec 20 11:20:09.649: INFO: Waiting up to 5m0s for pod "client-containers-2ba2d0ac-194f-4c92-b1e3-910afff097e8" in namespace "containers-7614" to be "success or failure"
Dec 20 11:20:09.724: INFO: Pod "client-containers-2ba2d0ac-194f-4c92-b1e3-910afff097e8": Phase="Pending", Reason="", readiness=false. Elapsed: 75.30364ms
Dec 20 11:20:12.462: INFO: Pod "client-containers-2ba2d0ac-194f-4c92-b1e3-910afff097e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.813149049s
Dec 20 11:20:14.468: INFO: Pod "client-containers-2ba2d0ac-194f-4c92-b1e3-910afff097e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.818874105s
STEP: Saw pod success
Dec 20 11:20:14.468: INFO: Pod "client-containers-2ba2d0ac-194f-4c92-b1e3-910afff097e8" satisfied condition "success or failure"
Dec 20 11:20:14.479: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod client-containers-2ba2d0ac-194f-4c92-b1e3-910afff097e8 container test-container: <nil>
STEP: delete the pod
Dec 20 11:20:15.107: INFO: Waiting for pod client-containers-2ba2d0ac-194f-4c92-b1e3-910afff097e8 to disappear
Dec 20 11:20:15.112: INFO: Pod client-containers-2ba2d0ac-194f-4c92-b1e3-910afff097e8 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:20:15.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7614" for this suite.
Dec 20 11:20:23.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:20:23.555: INFO: namespace containers-7614 deletion completed in 8.434073031s

• [SLOW TEST:15.560 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:20:23.559: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6577
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec 20 11:20:29.942: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W1220 11:20:29.942036      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 20 11:20:29.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6577" for this suite.
Dec 20 11:20:37.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:20:38.321: INFO: namespace gc-6577 deletion completed in 8.370148512s

• [SLOW TEST:14.762 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:20:38.326: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3911
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 11:20:38.694: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"68e76b92-eeb2-4e89-b0e3-2c64eb45d98a", Controller:(*bool)(0xc0035ff126), BlockOwnerDeletion:(*bool)(0xc0035ff127)}}
Dec 20 11:20:38.751: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"afa34063-c421-4b69-b8d3-a3a7fa11245d", Controller:(*bool)(0xc0035ff2e6), BlockOwnerDeletion:(*bool)(0xc0035ff2e7)}}
Dec 20 11:20:38.768: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"6b198f02-c36c-48d4-8fa2-1f01a11844f4", Controller:(*bool)(0xc004b75936), BlockOwnerDeletion:(*bool)(0xc004b75937)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:20:43.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3911" for this suite.
Dec 20 11:20:51.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:20:52.629: INFO: namespace gc-3911 deletion completed in 8.798440939s

• [SLOW TEST:14.304 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:20:52.636: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3483
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:20:57.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3483" for this suite.
Dec 20 11:21:14.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:21:15.314: INFO: namespace replication-controller-3483 deletion completed in 17.361188179s

• [SLOW TEST:22.679 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:21:15.317: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9551
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:21:22.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9551" for this suite.
Dec 20 11:22:10.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:22:11.863: INFO: namespace kubelet-test-9551 deletion completed in 49.239831208s

• [SLOW TEST:56.547 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:22:11.864: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7533
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 20 11:22:12.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-7533'
Dec 20 11:22:15.732: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 20 11:22:15.732: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Dec 20 11:22:15.867: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-vfssd]
Dec 20 11:22:15.867: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-vfssd" in namespace "kubectl-7533" to be "running and ready"
Dec 20 11:22:15.873: INFO: Pod "e2e-test-httpd-rc-vfssd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.728479ms
Dec 20 11:22:17.879: INFO: Pod "e2e-test-httpd-rc-vfssd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011704075s
Dec 20 11:22:19.888: INFO: Pod "e2e-test-httpd-rc-vfssd": Phase="Running", Reason="", readiness=true. Elapsed: 4.020542442s
Dec 20 11:22:19.888: INFO: Pod "e2e-test-httpd-rc-vfssd" satisfied condition "running and ready"
Dec 20 11:22:19.888: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-vfssd]
Dec 20 11:22:19.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 logs rc/e2e-test-httpd-rc --namespace=kubectl-7533'
Dec 20 11:22:20.208: INFO: stderr: ""
Dec 20 11:22:20.208: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.25.1.73. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.25.1.73. Set the 'ServerName' directive globally to suppress this message\n[Fri Dec 20 11:22:18.043386 2019] [mpm_event:notice] [pid 1:tid 139891342519144] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Fri Dec 20 11:22:18.043480 2019] [core:notice] [pid 1:tid 139891342519144] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Dec 20 11:22:20.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 delete rc e2e-test-httpd-rc --namespace=kubectl-7533'
Dec 20 11:22:20.444: INFO: stderr: ""
Dec 20 11:22:20.444: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:22:20.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7533" for this suite.
Dec 20 11:22:28.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:22:28.743: INFO: namespace kubectl-7533 deletion completed in 8.290542793s

• [SLOW TEST:16.879 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:22:28.744: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3996
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Dec 20 11:22:29.019: INFO: Waiting up to 5m0s for pod "var-expansion-6c6bd2a3-e3b1-49ef-bc48-4ae56f674d02" in namespace "var-expansion-3996" to be "success or failure"
Dec 20 11:22:29.108: INFO: Pod "var-expansion-6c6bd2a3-e3b1-49ef-bc48-4ae56f674d02": Phase="Pending", Reason="", readiness=false. Elapsed: 88.731103ms
Dec 20 11:22:31.116: INFO: Pod "var-expansion-6c6bd2a3-e3b1-49ef-bc48-4ae56f674d02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.096886803s
Dec 20 11:22:33.123: INFO: Pod "var-expansion-6c6bd2a3-e3b1-49ef-bc48-4ae56f674d02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.103819602s
STEP: Saw pod success
Dec 20 11:22:33.123: INFO: Pod "var-expansion-6c6bd2a3-e3b1-49ef-bc48-4ae56f674d02" satisfied condition "success or failure"
Dec 20 11:22:33.129: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod var-expansion-6c6bd2a3-e3b1-49ef-bc48-4ae56f674d02 container dapi-container: <nil>
STEP: delete the pod
Dec 20 11:22:33.219: INFO: Waiting for pod var-expansion-6c6bd2a3-e3b1-49ef-bc48-4ae56f674d02 to disappear
Dec 20 11:22:33.225: INFO: Pod var-expansion-6c6bd2a3-e3b1-49ef-bc48-4ae56f674d02 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:22:33.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3996" for this suite.
Dec 20 11:22:47.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:22:47.549: INFO: namespace var-expansion-3996 deletion completed in 14.31581082s

• [SLOW TEST:18.805 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:22:47.550: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9171
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 20 11:22:47.794: INFO: Waiting up to 5m0s for pod "downwardapi-volume-42769a75-f95b-4665-a637-e2b1957f6768" in namespace "projected-9171" to be "success or failure"
Dec 20 11:22:47.800: INFO: Pod "downwardapi-volume-42769a75-f95b-4665-a637-e2b1957f6768": Phase="Pending", Reason="", readiness=false. Elapsed: 5.734509ms
Dec 20 11:22:49.807: INFO: Pod "downwardapi-volume-42769a75-f95b-4665-a637-e2b1957f6768": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012675335s
Dec 20 11:22:51.823: INFO: Pod "downwardapi-volume-42769a75-f95b-4665-a637-e2b1957f6768": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028567067s
STEP: Saw pod success
Dec 20 11:22:51.823: INFO: Pod "downwardapi-volume-42769a75-f95b-4665-a637-e2b1957f6768" satisfied condition "success or failure"
Dec 20 11:22:51.829: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod downwardapi-volume-42769a75-f95b-4665-a637-e2b1957f6768 container client-container: <nil>
STEP: delete the pod
Dec 20 11:22:51.877: INFO: Waiting for pod downwardapi-volume-42769a75-f95b-4665-a637-e2b1957f6768 to disappear
Dec 20 11:22:51.883: INFO: Pod downwardapi-volume-42769a75-f95b-4665-a637-e2b1957f6768 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:22:51.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9171" for this suite.
Dec 20 11:22:57.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:22:58.182: INFO: namespace projected-9171 deletion completed in 6.28976175s

• [SLOW TEST:10.631 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:22:58.187: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7375
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:23:09.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7375" for this suite.
Dec 20 11:23:15.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:23:15.840: INFO: namespace resourcequota-7375 deletion completed in 6.33136406s

• [SLOW TEST:17.653 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:23:15.843: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-522
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 20 11:23:16.488: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 20 11:23:18.512: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712437796, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712437796, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712437796, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712437796, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 20 11:23:21.541: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:23:21.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-522" for this suite.
Dec 20 11:23:27.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:23:28.103: INFO: namespace webhook-522 deletion completed in 6.257944898s
STEP: Destroying namespace "webhook-522-markers" for this suite.
Dec 20 11:23:34.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:23:34.394: INFO: namespace webhook-522-markers deletion completed in 6.291097961s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.592 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:23:34.440: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3039
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-dba7143b-1772-4049-9cc9-91c2ef52bfc3
STEP: Creating a pod to test consume configMaps
Dec 20 11:23:34.681: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f2417cef-6a23-4aa7-9e94-a8c2716b9676" in namespace "projected-3039" to be "success or failure"
Dec 20 11:23:34.690: INFO: Pod "pod-projected-configmaps-f2417cef-6a23-4aa7-9e94-a8c2716b9676": Phase="Pending", Reason="", readiness=false. Elapsed: 8.859972ms
Dec 20 11:23:36.698: INFO: Pod "pod-projected-configmaps-f2417cef-6a23-4aa7-9e94-a8c2716b9676": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016269025s
Dec 20 11:23:38.707: INFO: Pod "pod-projected-configmaps-f2417cef-6a23-4aa7-9e94-a8c2716b9676": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025289421s
Dec 20 11:23:40.716: INFO: Pod "pod-projected-configmaps-f2417cef-6a23-4aa7-9e94-a8c2716b9676": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034109691s
STEP: Saw pod success
Dec 20 11:23:40.716: INFO: Pod "pod-projected-configmaps-f2417cef-6a23-4aa7-9e94-a8c2716b9676" satisfied condition "success or failure"
Dec 20 11:23:40.721: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-projected-configmaps-f2417cef-6a23-4aa7-9e94-a8c2716b9676 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 20 11:23:40.769: INFO: Waiting for pod pod-projected-configmaps-f2417cef-6a23-4aa7-9e94-a8c2716b9676 to disappear
Dec 20 11:23:40.783: INFO: Pod pod-projected-configmaps-f2417cef-6a23-4aa7-9e94-a8c2716b9676 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:23:40.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3039" for this suite.
Dec 20 11:23:46.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:23:47.075: INFO: namespace projected-3039 deletion completed in 6.282335388s

• [SLOW TEST:12.636 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:23:47.076: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2708
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 20 11:23:51.864: INFO: Successfully updated pod "pod-update-6a5475f0-c982-4a08-9493-75461de3af68"
STEP: verifying the updated pod is in kubernetes
Dec 20 11:23:51.883: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:23:51.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2708" for this suite.
Dec 20 11:24:19.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:24:20.151: INFO: namespace pods-2708 deletion completed in 28.260312612s

• [SLOW TEST:33.075 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:24:20.153: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3242
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3242
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-3242
I1220 11:24:20.437867      20 runners.go:184] Created replication controller with name: externalname-service, namespace: services-3242, replica count: 2
I1220 11:24:23.488567      20 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 20 11:24:23.489: INFO: Creating new exec pod
Dec 20 11:24:28.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=services-3242 execpod99l44 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec 20 11:24:29.203: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 20 11:24:29.203: INFO: stdout: ""
Dec 20 11:24:29.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=services-3242 execpod99l44 -- /bin/sh -x -c nc -zv -t -w 2 10.240.31.44 80'
Dec 20 11:24:29.810: INFO: stderr: "+ nc -zv -t -w 2 10.240.31.44 80\nConnection to 10.240.31.44 80 port [tcp/http] succeeded!\n"
Dec 20 11:24:29.810: INFO: stdout: ""
Dec 20 11:24:29.810: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:24:29.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3242" for this suite.
Dec 20 11:24:37.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:24:38.134: INFO: namespace services-3242 deletion completed in 8.255317181s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:17.981 seconds]
[sig-network] Services
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:24:38.135: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-910
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-910
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-910
STEP: creating replication controller externalsvc in namespace services-910
I1220 11:24:38.451013      20 runners.go:184] Created replication controller with name: externalsvc, namespace: services-910, replica count: 2
I1220 11:24:41.501867      20 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Dec 20 11:24:43.753: INFO: Creating new exec pod
Dec 20 11:24:49.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=services-910 execpod4dlnm -- /bin/sh -x -c nslookup clusterip-service'
Dec 20 11:24:49.665: INFO: stderr: "+ nslookup clusterip-service\n"
Dec 20 11:24:49.665: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nclusterip-service.services-910.svc.cluster.local\tcanonical name = externalsvc.services-910.svc.cluster.local.\nName:\texternalsvc.services-910.svc.cluster.local\nAddress: 10.240.21.142\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-910, will wait for the garbage collector to delete the pods
Dec 20 11:24:49.741: INFO: Deleting ReplicationController externalsvc took: 18.690085ms
Dec 20 11:24:50.242: INFO: Terminating ReplicationController externalsvc pods took: 500.856536ms
Dec 20 11:25:05.309: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:25:05.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-910" for this suite.
Dec 20 11:25:15.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:25:15.638: INFO: namespace services-910 deletion completed in 10.277107837s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:37.504 seconds]
[sig-network] Services
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:25:15.639: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1951
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 11:25:15.866: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-4abad20b-5cc7-4161-8213-d05a572f384c" in namespace "security-context-test-1951" to be "success or failure"
Dec 20 11:25:15.870: INFO: Pod "alpine-nnp-false-4abad20b-5cc7-4161-8213-d05a572f384c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.560089ms
Dec 20 11:25:17.878: INFO: Pod "alpine-nnp-false-4abad20b-5cc7-4161-8213-d05a572f384c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01278217s
Dec 20 11:25:19.886: INFO: Pod "alpine-nnp-false-4abad20b-5cc7-4161-8213-d05a572f384c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019864885s
Dec 20 11:25:21.898: INFO: Pod "alpine-nnp-false-4abad20b-5cc7-4161-8213-d05a572f384c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032787667s
Dec 20 11:25:23.907: INFO: Pod "alpine-nnp-false-4abad20b-5cc7-4161-8213-d05a572f384c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.041667794s
Dec 20 11:25:23.907: INFO: Pod "alpine-nnp-false-4abad20b-5cc7-4161-8213-d05a572f384c" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:25:23.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1951" for this suite.
Dec 20 11:25:29.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:25:30.227: INFO: namespace security-context-test-1951 deletion completed in 6.288321538s

• [SLOW TEST:14.589 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:25:30.231: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4509
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Dec 20 11:25:30.435: INFO: Waiting up to 5m0s for pod "client-containers-d95867d5-60f9-40d5-aa41-5379c274b409" in namespace "containers-4509" to be "success or failure"
Dec 20 11:25:30.446: INFO: Pod "client-containers-d95867d5-60f9-40d5-aa41-5379c274b409": Phase="Pending", Reason="", readiness=false. Elapsed: 11.667969ms
Dec 20 11:25:32.453: INFO: Pod "client-containers-d95867d5-60f9-40d5-aa41-5379c274b409": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018410671s
Dec 20 11:25:34.462: INFO: Pod "client-containers-d95867d5-60f9-40d5-aa41-5379c274b409": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027407499s
STEP: Saw pod success
Dec 20 11:25:34.462: INFO: Pod "client-containers-d95867d5-60f9-40d5-aa41-5379c274b409" satisfied condition "success or failure"
Dec 20 11:25:34.468: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod client-containers-d95867d5-60f9-40d5-aa41-5379c274b409 container test-container: <nil>
STEP: delete the pod
Dec 20 11:25:34.542: INFO: Waiting for pod client-containers-d95867d5-60f9-40d5-aa41-5379c274b409 to disappear
Dec 20 11:25:34.548: INFO: Pod client-containers-d95867d5-60f9-40d5-aa41-5379c274b409 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:25:34.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4509" for this suite.
Dec 20 11:25:40.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:25:40.815: INFO: namespace containers-4509 deletion completed in 6.256671338s

• [SLOW TEST:10.584 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:25:40.816: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8696
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Dec 20 11:25:41.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 --namespace=kubectl-8696 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Dec 20 11:25:46.899: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Dec 20 11:25:46.899: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:25:48.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8696" for this suite.
Dec 20 11:26:02.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:26:07.448: INFO: namespace kubectl-8696 deletion completed in 18.521481183s

• [SLOW TEST:26.632 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:26:07.449: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-283
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 20 11:26:08.113: INFO: Waiting up to 5m0s for pod "pod-aca29213-53cc-416f-b260-85ce75fda23b" in namespace "emptydir-283" to be "success or failure"
Dec 20 11:26:08.479: INFO: Pod "pod-aca29213-53cc-416f-b260-85ce75fda23b": Phase="Pending", Reason="", readiness=false. Elapsed: 365.911042ms
Dec 20 11:26:10.488: INFO: Pod "pod-aca29213-53cc-416f-b260-85ce75fda23b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.375469785s
Dec 20 11:26:12.496: INFO: Pod "pod-aca29213-53cc-416f-b260-85ce75fda23b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.383765179s
STEP: Saw pod success
Dec 20 11:26:12.497: INFO: Pod "pod-aca29213-53cc-416f-b260-85ce75fda23b" satisfied condition "success or failure"
Dec 20 11:26:12.502: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-aca29213-53cc-416f-b260-85ce75fda23b container test-container: <nil>
STEP: delete the pod
Dec 20 11:26:12.570: INFO: Waiting for pod pod-aca29213-53cc-416f-b260-85ce75fda23b to disappear
Dec 20 11:26:12.575: INFO: Pod pod-aca29213-53cc-416f-b260-85ce75fda23b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:26:12.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-283" for this suite.
Dec 20 11:26:18.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:26:18.904: INFO: namespace emptydir-283 deletion completed in 6.321334884s

• [SLOW TEST:11.455 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:26:18.905: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4165
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Dec 20 11:26:19.107: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Dec 20 11:26:19.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 create -f - --namespace=kubectl-4165'
Dec 20 11:26:19.545: INFO: stderr: ""
Dec 20 11:26:19.545: INFO: stdout: "service/redis-slave created\n"
Dec 20 11:26:19.545: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Dec 20 11:26:19.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 create -f - --namespace=kubectl-4165'
Dec 20 11:26:19.841: INFO: stderr: ""
Dec 20 11:26:19.841: INFO: stdout: "service/redis-master created\n"
Dec 20 11:26:19.841: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec 20 11:26:19.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 create -f - --namespace=kubectl-4165'
Dec 20 11:26:20.315: INFO: stderr: ""
Dec 20 11:26:20.315: INFO: stdout: "service/frontend created\n"
Dec 20 11:26:20.315: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Dec 20 11:26:20.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 create -f - --namespace=kubectl-4165'
Dec 20 11:26:20.653: INFO: stderr: ""
Dec 20 11:26:20.653: INFO: stdout: "deployment.apps/frontend created\n"
Dec 20 11:26:20.654: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 20 11:26:20.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 create -f - --namespace=kubectl-4165'
Dec 20 11:26:20.972: INFO: stderr: ""
Dec 20 11:26:20.973: INFO: stdout: "deployment.apps/redis-master created\n"
Dec 20 11:26:20.973: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Dec 20 11:26:20.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 create -f - --namespace=kubectl-4165'
Dec 20 11:26:21.242: INFO: stderr: ""
Dec 20 11:26:21.242: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Dec 20 11:26:21.242: INFO: Waiting for all frontend pods to be Running.
Dec 20 11:26:51.296: INFO: Waiting for frontend to serve content.
Dec 20 11:26:51.401: INFO: Trying to add a new entry to the guestbook.
Dec 20 11:26:51.534: INFO: Verifying that added entry can be retrieved.
Dec 20 11:26:51.591: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Dec 20 11:26:56.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 delete --grace-period=0 --force -f - --namespace=kubectl-4165'
Dec 20 11:26:56.925: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 20 11:26:56.925: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Dec 20 11:26:56.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 delete --grace-period=0 --force -f - --namespace=kubectl-4165'
Dec 20 11:26:57.109: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 20 11:26:57.109: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 20 11:26:57.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 delete --grace-period=0 --force -f - --namespace=kubectl-4165'
Dec 20 11:26:57.285: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 20 11:26:57.285: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 20 11:26:57.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 delete --grace-period=0 --force -f - --namespace=kubectl-4165'
Dec 20 11:26:57.428: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 20 11:26:57.428: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 20 11:26:57.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 delete --grace-period=0 --force -f - --namespace=kubectl-4165'
Dec 20 11:26:57.566: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 20 11:26:57.566: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 20 11:26:57.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 delete --grace-period=0 --force -f - --namespace=kubectl-4165'
Dec 20 11:26:57.710: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 20 11:26:57.710: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:26:57.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4165" for this suite.
Dec 20 11:27:09.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:27:10.066: INFO: namespace kubectl-4165 deletion completed in 12.346717158s

• [SLOW TEST:51.161 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:27:10.074: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6049
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:27:10.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6049" for this suite.
Dec 20 11:27:16.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:27:16.635: INFO: namespace services-6049 deletion completed in 6.284371081s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:6.562 seconds]
[sig-network] Services
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:27:16.635: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7453
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Dec 20 11:27:16.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 create -f - --namespace=kubectl-7453'
Dec 20 11:27:17.192: INFO: stderr: ""
Dec 20 11:27:17.192: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 20 11:27:17.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7453'
Dec 20 11:27:17.322: INFO: stderr: ""
Dec 20 11:27:17.322: INFO: stdout: "update-demo-nautilus-6n6hd update-demo-nautilus-twgv5 "
Dec 20 11:27:17.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-6n6hd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Dec 20 11:27:17.480: INFO: stderr: ""
Dec 20 11:27:17.480: INFO: stdout: ""
Dec 20 11:27:17.480: INFO: update-demo-nautilus-6n6hd is created but not running
Dec 20 11:27:22.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7453'
Dec 20 11:27:22.596: INFO: stderr: ""
Dec 20 11:27:22.596: INFO: stdout: "update-demo-nautilus-6n6hd update-demo-nautilus-twgv5 "
Dec 20 11:27:22.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-6n6hd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Dec 20 11:27:22.709: INFO: stderr: ""
Dec 20 11:27:22.709: INFO: stdout: "true"
Dec 20 11:27:22.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-6n6hd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Dec 20 11:27:22.828: INFO: stderr: ""
Dec 20 11:27:22.828: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 20 11:27:22.828: INFO: validating pod update-demo-nautilus-6n6hd
Dec 20 11:27:22.932: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 20 11:27:22.932: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 20 11:27:22.932: INFO: update-demo-nautilus-6n6hd is verified up and running
Dec 20 11:27:22.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-twgv5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Dec 20 11:27:23.042: INFO: stderr: ""
Dec 20 11:27:23.042: INFO: stdout: ""
Dec 20 11:27:23.042: INFO: update-demo-nautilus-twgv5 is created but not running
Dec 20 11:27:28.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7453'
Dec 20 11:27:28.162: INFO: stderr: ""
Dec 20 11:27:28.162: INFO: stdout: "update-demo-nautilus-6n6hd update-demo-nautilus-twgv5 "
Dec 20 11:27:28.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-6n6hd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Dec 20 11:27:28.284: INFO: stderr: ""
Dec 20 11:27:28.284: INFO: stdout: "true"
Dec 20 11:27:28.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-6n6hd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Dec 20 11:27:28.391: INFO: stderr: ""
Dec 20 11:27:28.392: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 20 11:27:28.392: INFO: validating pod update-demo-nautilus-6n6hd
Dec 20 11:27:28.405: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 20 11:27:28.405: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 20 11:27:28.405: INFO: update-demo-nautilus-6n6hd is verified up and running
Dec 20 11:27:28.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-twgv5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Dec 20 11:27:28.533: INFO: stderr: ""
Dec 20 11:27:28.533: INFO: stdout: "true"
Dec 20 11:27:28.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-twgv5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Dec 20 11:27:28.646: INFO: stderr: ""
Dec 20 11:27:28.647: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 20 11:27:28.647: INFO: validating pod update-demo-nautilus-twgv5
Dec 20 11:27:28.747: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 20 11:27:28.748: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 20 11:27:28.748: INFO: update-demo-nautilus-twgv5 is verified up and running
STEP: rolling-update to new replication controller
Dec 20 11:27:28.753: INFO: scanned /root for discovery docs: <nil>
Dec 20 11:27:28.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-7453'
Dec 20 11:27:56.196: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec 20 11:27:56.196: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 20 11:27:56.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7453'
Dec 20 11:27:56.322: INFO: stderr: ""
Dec 20 11:27:56.322: INFO: stdout: "update-demo-kitten-6q6dk update-demo-kitten-jnhss "
Dec 20 11:27:56.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-kitten-6q6dk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Dec 20 11:27:56.432: INFO: stderr: ""
Dec 20 11:27:56.432: INFO: stdout: "true"
Dec 20 11:27:56.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-kitten-6q6dk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Dec 20 11:27:56.563: INFO: stderr: ""
Dec 20 11:27:56.563: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec 20 11:27:56.563: INFO: validating pod update-demo-kitten-6q6dk
Dec 20 11:27:56.663: INFO: got data: {
  "image": "kitten.jpg"
}

Dec 20 11:27:56.663: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec 20 11:27:56.663: INFO: update-demo-kitten-6q6dk is verified up and running
Dec 20 11:27:56.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-kitten-jnhss -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Dec 20 11:27:56.787: INFO: stderr: ""
Dec 20 11:27:56.787: INFO: stdout: "true"
Dec 20 11:27:56.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-kitten-jnhss -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Dec 20 11:27:56.912: INFO: stderr: ""
Dec 20 11:27:56.912: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec 20 11:27:56.912: INFO: validating pod update-demo-kitten-jnhss
Dec 20 11:27:57.015: INFO: got data: {
  "image": "kitten.jpg"
}

Dec 20 11:27:57.015: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec 20 11:27:57.015: INFO: update-demo-kitten-jnhss is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:27:57.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7453" for this suite.
Dec 20 11:28:23.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:28:23.327: INFO: namespace kubectl-7453 deletion completed in 26.302180066s

• [SLOW TEST:66.692 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:28:23.328: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-823
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 11:28:23.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 version'
Dec 20 11:28:23.791: INFO: stderr: ""
Dec 20 11:28:23.791: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.3\", GitCommit:\"b3cbbae08ec52a7fc73d334838e18d17e8512749\", GitTreeState:\"clean\", BuildDate:\"2019-11-13T11:23:11Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.3\", GitCommit:\"b3cbbae08ec52a7fc73d334838e18d17e8512749\", GitTreeState:\"clean\", BuildDate:\"2019-11-13T11:13:49Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:28:23.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-823" for this suite.
Dec 20 11:28:29.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:28:30.107: INFO: namespace kubectl-823 deletion completed in 6.299478996s

• [SLOW TEST:6.779 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:28:30.107: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5985
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 11:28:30.365: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec 20 11:28:30.387: INFO: Number of nodes with available pods: 0
Dec 20 11:28:30.387: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec 20 11:28:30.444: INFO: Number of nodes with available pods: 0
Dec 20 11:28:30.445: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 11:28:31.455: INFO: Number of nodes with available pods: 0
Dec 20 11:28:31.455: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 11:28:32.452: INFO: Number of nodes with available pods: 0
Dec 20 11:28:32.453: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 11:28:33.452: INFO: Number of nodes with available pods: 1
Dec 20 11:28:33.452: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec 20 11:28:33.491: INFO: Number of nodes with available pods: 1
Dec 20 11:28:33.491: INFO: Number of running nodes: 0, number of available pods: 1
Dec 20 11:28:34.499: INFO: Number of nodes with available pods: 0
Dec 20 11:28:34.499: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec 20 11:28:34.518: INFO: Number of nodes with available pods: 0
Dec 20 11:28:34.518: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 11:28:35.523: INFO: Number of nodes with available pods: 0
Dec 20 11:28:35.524: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 11:28:36.525: INFO: Number of nodes with available pods: 0
Dec 20 11:28:36.525: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 11:28:37.526: INFO: Number of nodes with available pods: 0
Dec 20 11:28:37.526: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 11:28:38.524: INFO: Number of nodes with available pods: 0
Dec 20 11:28:38.525: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 11:28:39.526: INFO: Number of nodes with available pods: 0
Dec 20 11:28:39.526: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 11:28:40.525: INFO: Number of nodes with available pods: 0
Dec 20 11:28:40.525: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 11:28:41.534: INFO: Number of nodes with available pods: 0
Dec 20 11:28:41.534: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 11:28:42.525: INFO: Number of nodes with available pods: 0
Dec 20 11:28:42.526: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 11:28:43.526: INFO: Number of nodes with available pods: 0
Dec 20 11:28:43.526: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 11:28:44.525: INFO: Number of nodes with available pods: 0
Dec 20 11:28:44.526: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 11:28:45.525: INFO: Number of nodes with available pods: 0
Dec 20 11:28:45.525: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 11:28:46.524: INFO: Number of nodes with available pods: 0
Dec 20 11:28:46.524: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 11:28:47.525: INFO: Number of nodes with available pods: 0
Dec 20 11:28:47.525: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 11:28:48.556: INFO: Number of nodes with available pods: 1
Dec 20 11:28:48.556: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5985, will wait for the garbage collector to delete the pods
Dec 20 11:28:48.656: INFO: Deleting DaemonSet.extensions daemon-set took: 27.464855ms
Dec 20 11:28:49.157: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.397809ms
Dec 20 11:28:51.762: INFO: Number of nodes with available pods: 0
Dec 20 11:28:51.762: INFO: Number of running nodes: 0, number of available pods: 0
Dec 20 11:28:51.771: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5985/daemonsets","resourceVersion":"17975"},"items":null}

Dec 20 11:28:51.777: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5985/pods","resourceVersion":"17975"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:28:51.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5985" for this suite.
Dec 20 11:28:57.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:28:58.219: INFO: namespace daemonsets-5985 deletion completed in 6.37474381s

• [SLOW TEST:28.112 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:28:58.222: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7715
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Dec 20 11:29:03.031: INFO: Successfully updated pod "labelsupdate067d2d8d-3a33-4e03-a296-31e6cb1a7af2"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:29:05.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7715" for this suite.
Dec 20 11:29:17.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:29:17.396: INFO: namespace projected-7715 deletion completed in 12.303687213s

• [SLOW TEST:19.174 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:29:17.399: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-338
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 20 11:29:17.648: INFO: Waiting up to 5m0s for pod "pod-5a67c558-60c5-4e72-9bc6-f8488404caa1" in namespace "emptydir-338" to be "success or failure"
Dec 20 11:29:17.667: INFO: Pod "pod-5a67c558-60c5-4e72-9bc6-f8488404caa1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.36232ms
Dec 20 11:29:19.673: INFO: Pod "pod-5a67c558-60c5-4e72-9bc6-f8488404caa1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024383386s
Dec 20 11:29:21.680: INFO: Pod "pod-5a67c558-60c5-4e72-9bc6-f8488404caa1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032294722s
Dec 20 11:29:23.687: INFO: Pod "pod-5a67c558-60c5-4e72-9bc6-f8488404caa1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039245804s
STEP: Saw pod success
Dec 20 11:29:23.688: INFO: Pod "pod-5a67c558-60c5-4e72-9bc6-f8488404caa1" satisfied condition "success or failure"
Dec 20 11:29:24.120: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-5a67c558-60c5-4e72-9bc6-f8488404caa1 container test-container: <nil>
STEP: delete the pod
Dec 20 11:29:24.407: INFO: Waiting for pod pod-5a67c558-60c5-4e72-9bc6-f8488404caa1 to disappear
Dec 20 11:29:24.641: INFO: Pod pod-5a67c558-60c5-4e72-9bc6-f8488404caa1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:29:24.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-338" for this suite.
Dec 20 11:29:31.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:29:31.234: INFO: namespace emptydir-338 deletion completed in 6.584394849s

• [SLOW TEST:13.835 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:29:31.238: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2310
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2310.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2310.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2310.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2310.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 20 11:29:35.807: INFO: DNS probes using dns-test-5a80d246-1425-496c-9f50-e5a43c50956b succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2310.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2310.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2310.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2310.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 20 11:29:42.162: INFO: DNS probes using dns-test-d30bb508-3ea7-4a0a-b7f2-11533c419403 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2310.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2310.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2310.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2310.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 20 11:29:50.606: INFO: DNS probes using dns-test-77fe9962-0440-4b4b-a1ad-5462ed3082bf succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:29:50.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2310" for this suite.
Dec 20 11:29:59.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:29:59.328: INFO: namespace dns-2310 deletion completed in 8.465243375s

• [SLOW TEST:28.091 seconds]
[sig-network] DNS
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:29:59.331: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4135
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 20 11:29:59.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-4135'
Dec 20 11:29:59.756: INFO: stderr: ""
Dec 20 11:29:59.756: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Dec 20 11:29:59.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 delete pods e2e-test-httpd-pod --namespace=kubectl-4135'
Dec 20 11:30:15.672: INFO: stderr: ""
Dec 20 11:30:15.672: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:30:15.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4135" for this suite.
Dec 20 11:30:21.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:30:21.988: INFO: namespace kubectl-4135 deletion completed in 6.302084387s

• [SLOW TEST:22.658 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:30:21.992: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2617
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Dec 20 11:30:22.210: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Dec 20 11:30:38.387: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 11:30:42.515: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:30:58.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2617" for this suite.
Dec 20 11:31:04.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:31:04.548: INFO: namespace crd-publish-openapi-2617 deletion completed in 6.264162093s

• [SLOW TEST:42.557 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:31:04.549: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5384
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-a5cd6681-15f7-469b-8711-24220a0c8ee2
STEP: Creating a pod to test consume secrets
Dec 20 11:31:04.787: INFO: Waiting up to 5m0s for pod "pod-secrets-65f9f2a4-7b6e-4ed1-8f25-a8ababc77886" in namespace "secrets-5384" to be "success or failure"
Dec 20 11:31:04.797: INFO: Pod "pod-secrets-65f9f2a4-7b6e-4ed1-8f25-a8ababc77886": Phase="Pending", Reason="", readiness=false. Elapsed: 9.80105ms
Dec 20 11:31:06.804: INFO: Pod "pod-secrets-65f9f2a4-7b6e-4ed1-8f25-a8ababc77886": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016295972s
Dec 20 11:31:08.810: INFO: Pod "pod-secrets-65f9f2a4-7b6e-4ed1-8f25-a8ababc77886": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022678614s
STEP: Saw pod success
Dec 20 11:31:08.810: INFO: Pod "pod-secrets-65f9f2a4-7b6e-4ed1-8f25-a8ababc77886" satisfied condition "success or failure"
Dec 20 11:31:08.816: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-secrets-65f9f2a4-7b6e-4ed1-8f25-a8ababc77886 container secret-volume-test: <nil>
STEP: delete the pod
Dec 20 11:31:08.876: INFO: Waiting for pod pod-secrets-65f9f2a4-7b6e-4ed1-8f25-a8ababc77886 to disappear
Dec 20 11:31:08.887: INFO: Pod pod-secrets-65f9f2a4-7b6e-4ed1-8f25-a8ababc77886 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:31:08.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5384" for this suite.
Dec 20 11:31:14.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:31:15.207: INFO: namespace secrets-5384 deletion completed in 6.311033247s

• [SLOW TEST:10.658 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:31:15.208: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1477
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 20 11:31:15.434: INFO: Waiting up to 5m0s for pod "pod-6a4eda51-786e-4946-be9f-4c8daaede569" in namespace "emptydir-1477" to be "success or failure"
Dec 20 11:31:15.440: INFO: Pod "pod-6a4eda51-786e-4946-be9f-4c8daaede569": Phase="Pending", Reason="", readiness=false. Elapsed: 5.036625ms
Dec 20 11:31:17.447: INFO: Pod "pod-6a4eda51-786e-4946-be9f-4c8daaede569": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012307065s
Dec 20 11:31:19.456: INFO: Pod "pod-6a4eda51-786e-4946-be9f-4c8daaede569": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021068461s
Dec 20 11:31:21.463: INFO: Pod "pod-6a4eda51-786e-4946-be9f-4c8daaede569": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028198776s
Dec 20 11:31:23.472: INFO: Pod "pod-6a4eda51-786e-4946-be9f-4c8daaede569": Phase="Running", Reason="", readiness=true. Elapsed: 8.037151358s
Dec 20 11:31:25.481: INFO: Pod "pod-6a4eda51-786e-4946-be9f-4c8daaede569": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.046231983s
STEP: Saw pod success
Dec 20 11:31:25.481: INFO: Pod "pod-6a4eda51-786e-4946-be9f-4c8daaede569" satisfied condition "success or failure"
Dec 20 11:31:25.487: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-6a4eda51-786e-4946-be9f-4c8daaede569 container test-container: <nil>
STEP: delete the pod
Dec 20 11:31:25.581: INFO: Waiting for pod pod-6a4eda51-786e-4946-be9f-4c8daaede569 to disappear
Dec 20 11:31:25.586: INFO: Pod pod-6a4eda51-786e-4946-be9f-4c8daaede569 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:31:25.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1477" for this suite.
Dec 20 11:31:31.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:31:31.879: INFO: namespace emptydir-1477 deletion completed in 6.282342182s

• [SLOW TEST:16.672 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:31:31.880: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8328
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 20 11:31:32.629: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 20 11:31:34.726: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438292, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438292, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438292, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438292, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 20 11:31:38.067: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:31:51.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8328" for this suite.
Dec 20 11:31:57.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:31:57.762: INFO: namespace webhook-8328 deletion completed in 6.303568557s
STEP: Destroying namespace "webhook-8328-markers" for this suite.
Dec 20 11:32:07.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:32:08.096: INFO: namespace webhook-8328-markers deletion completed in 10.333242803s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:36.264 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:32:08.144: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2214
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 20 11:32:09.087: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 20 11:32:11.120: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438329, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438329, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438329, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438329, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 20 11:32:14.150: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
Dec 20 11:32:15.049: INFO: Waiting for webhook configuration to be ready...
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:32:19.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2214" for this suite.
Dec 20 11:32:28.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:32:29.195: INFO: namespace webhook-2214 deletion completed in 9.513145547s
STEP: Destroying namespace "webhook-2214-markers" for this suite.
Dec 20 11:32:35.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:32:35.484: INFO: namespace webhook-2214-markers deletion completed in 6.28897559s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.389 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:32:35.536: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9307
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:32:42.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9307" for this suite.
Dec 20 11:32:48.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:32:49.205: INFO: namespace resourcequota-9307 deletion completed in 6.286748906s

• [SLOW TEST:13.670 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:32:49.207: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2476
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec 20 11:32:49.415: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Dec 20 11:33:00.518: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:33:00.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2476" for this suite.
Dec 20 11:33:06.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:33:06.838: INFO: namespace pods-2476 deletion completed in 6.304127884s

• [SLOW TEST:17.631 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:33:06.841: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4209
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4209
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4209
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4209
Dec 20 11:33:07.112: INFO: Found 0 stateful pods, waiting for 1
Dec 20 11:33:17.120: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec 20 11:33:18.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-4209 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 20 11:33:27.410: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 20 11:33:27.410: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 20 11:33:27.410: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 20 11:33:27.418: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 20 11:33:37.429: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 20 11:33:37.429: INFO: Waiting for statefulset status.replicas updated to 0
Dec 20 11:33:37.503: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999096s
Dec 20 11:33:38.511: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.96216039s
Dec 20 11:33:39.521: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.95423344s
Dec 20 11:33:40.528: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.944350409s
Dec 20 11:33:41.817: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.936395226s
Dec 20 11:33:42.824: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.647906887s
Dec 20 11:33:43.832: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.64051441s
Dec 20 11:33:44.840: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.633161961s
Dec 20 11:33:45.853: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.624690343s
Dec 20 11:33:47.178: INFO: Verifying statefulset ss doesn't scale past 1 for another 612.387527ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4209
Dec 20 11:33:48.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-4209 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 11:33:48.876: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 20 11:33:48.876: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 20 11:33:48.876: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 20 11:33:48.893: INFO: Found 1 stateful pods, waiting for 3
Dec 20 11:33:58.903: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 20 11:33:58.903: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 20 11:33:58.903: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec 20 11:33:58.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-4209 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 20 11:33:59.589: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 20 11:33:59.589: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 20 11:33:59.589: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 20 11:33:59.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-4209 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 20 11:34:00.405: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 20 11:34:00.405: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 20 11:34:00.405: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 20 11:34:00.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-4209 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 20 11:34:01.172: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 20 11:34:01.172: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 20 11:34:01.172: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 20 11:34:01.172: INFO: Waiting for statefulset status.replicas updated to 0
Dec 20 11:34:01.180: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Dec 20 11:34:11.199: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 20 11:34:11.199: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 20 11:34:11.199: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 20 11:34:11.236: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999425s
Dec 20 11:34:12.244: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992702575s
Dec 20 11:34:13.254: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.984223146s
Dec 20 11:34:14.262: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.974999949s
Dec 20 11:34:15.270: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.966118875s
Dec 20 11:34:16.279: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.959096751s
Dec 20 11:34:17.287: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.949911171s
Dec 20 11:34:18.295: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.940968515s
Dec 20 11:34:19.571: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.933744272s
Dec 20 11:34:20.580: INFO: Verifying statefulset ss doesn't scale past 3 for another 657.776568ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4209
Dec 20 11:34:21.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-4209 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 11:34:22.262: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 20 11:34:22.262: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 20 11:34:22.262: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 20 11:34:22.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-4209 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 11:34:22.833: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 20 11:34:22.833: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 20 11:34:22.833: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 20 11:34:22.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-4209 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 11:34:23.466: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 20 11:34:23.466: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 20 11:34:23.467: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 20 11:34:23.467: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 20 11:34:43.534: INFO: Deleting all statefulset in ns statefulset-4209
Dec 20 11:34:43.541: INFO: Scaling statefulset ss to 0
Dec 20 11:34:43.569: INFO: Waiting for statefulset status.replicas updated to 0
Dec 20 11:34:43.575: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:34:43.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4209" for this suite.
Dec 20 11:34:51.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:34:51.895: INFO: namespace statefulset-4209 deletion completed in 8.27918875s

• [SLOW TEST:105.055 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:34:51.903: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7434
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-58af6c81-74db-4d40-ab12-4a9e2f578da1
STEP: Creating a pod to test consume secrets
Dec 20 11:34:52.404: INFO: Waiting up to 5m0s for pod "pod-secrets-55055009-b9ab-4418-99ad-821a4d7768aa" in namespace "secrets-7434" to be "success or failure"
Dec 20 11:34:52.410: INFO: Pod "pod-secrets-55055009-b9ab-4418-99ad-821a4d7768aa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.318245ms
Dec 20 11:34:54.419: INFO: Pod "pod-secrets-55055009-b9ab-4418-99ad-821a4d7768aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015285716s
Dec 20 11:34:56.428: INFO: Pod "pod-secrets-55055009-b9ab-4418-99ad-821a4d7768aa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023693961s
Dec 20 11:34:58.488: INFO: Pod "pod-secrets-55055009-b9ab-4418-99ad-821a4d7768aa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.083949666s
Dec 20 11:35:01.039: INFO: Pod "pod-secrets-55055009-b9ab-4418-99ad-821a4d7768aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.635426709s
STEP: Saw pod success
Dec 20 11:35:01.040: INFO: Pod "pod-secrets-55055009-b9ab-4418-99ad-821a4d7768aa" satisfied condition "success or failure"
Dec 20 11:35:01.233: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-secrets-55055009-b9ab-4418-99ad-821a4d7768aa container secret-env-test: <nil>
STEP: delete the pod
Dec 20 11:35:01.301: INFO: Waiting for pod pod-secrets-55055009-b9ab-4418-99ad-821a4d7768aa to disappear
Dec 20 11:35:01.306: INFO: Pod pod-secrets-55055009-b9ab-4418-99ad-821a4d7768aa no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:35:01.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7434" for this suite.
Dec 20 11:35:09.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:35:10.087: INFO: namespace secrets-7434 deletion completed in 8.772769012s

• [SLOW TEST:18.184 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:35:10.088: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-667
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 11:35:10.359: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Creating first CR 
Dec 20 11:35:11.092: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-20T11:35:11Z generation:1 name:name1 resourceVersion:19906 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:5775c242-6281-429c-9013-3e938dafb475] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Dec 20 11:35:21.117: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-20T11:35:21Z generation:1 name:name2 resourceVersion:19937 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:f1fcd888-89ea-4389-8872-1fc942eabaa0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Dec 20 11:35:31.143: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-20T11:35:11Z generation:2 name:name1 resourceVersion:19969 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:5775c242-6281-429c-9013-3e938dafb475] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Dec 20 11:35:41.158: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-20T11:35:21Z generation:2 name:name2 resourceVersion:20002 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:f1fcd888-89ea-4389-8872-1fc942eabaa0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Dec 20 11:35:51.185: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-20T11:35:11Z generation:2 name:name1 resourceVersion:20034 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:5775c242-6281-429c-9013-3e938dafb475] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Dec 20 11:36:01.206: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-20T11:35:21Z generation:2 name:name2 resourceVersion:20068 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:f1fcd888-89ea-4389-8872-1fc942eabaa0] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:36:11.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-667" for this suite.
Dec 20 11:36:23.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:36:24.584: INFO: namespace crd-watch-667 deletion completed in 12.8366043s

• [SLOW TEST:74.497 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:36:24.585: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-142
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-db828374-58b8-4eaf-aaed-dfc6aa325f3c
STEP: Creating a pod to test consume secrets
Dec 20 11:36:25.173: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-066d3fef-22f9-4577-a32f-377b146cfc06" in namespace "projected-142" to be "success or failure"
Dec 20 11:36:25.178: INFO: Pod "pod-projected-secrets-066d3fef-22f9-4577-a32f-377b146cfc06": Phase="Pending", Reason="", readiness=false. Elapsed: 5.226307ms
Dec 20 11:36:27.185: INFO: Pod "pod-projected-secrets-066d3fef-22f9-4577-a32f-377b146cfc06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012334635s
Dec 20 11:36:29.192: INFO: Pod "pod-projected-secrets-066d3fef-22f9-4577-a32f-377b146cfc06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019312024s
STEP: Saw pod success
Dec 20 11:36:29.193: INFO: Pod "pod-projected-secrets-066d3fef-22f9-4577-a32f-377b146cfc06" satisfied condition "success or failure"
Dec 20 11:36:29.283: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-projected-secrets-066d3fef-22f9-4577-a32f-377b146cfc06 container secret-volume-test: <nil>
STEP: delete the pod
Dec 20 11:36:29.358: INFO: Waiting for pod pod-projected-secrets-066d3fef-22f9-4577-a32f-377b146cfc06 to disappear
Dec 20 11:36:29.364: INFO: Pod pod-projected-secrets-066d3fef-22f9-4577-a32f-377b146cfc06 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:36:29.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-142" for this suite.
Dec 20 11:36:37.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:36:38.152: INFO: namespace projected-142 deletion completed in 8.779699399s

• [SLOW TEST:13.567 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:36:38.156: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6382
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-244c1d8c-e48a-4f80-ba7e-943f27de726c
STEP: Creating a pod to test consume secrets
Dec 20 11:36:38.449: INFO: Waiting up to 5m0s for pod "pod-secrets-3953204a-fa25-40a7-af56-4569debcb9b3" in namespace "secrets-6382" to be "success or failure"
Dec 20 11:36:38.455: INFO: Pod "pod-secrets-3953204a-fa25-40a7-af56-4569debcb9b3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.406047ms
Dec 20 11:36:40.462: INFO: Pod "pod-secrets-3953204a-fa25-40a7-af56-4569debcb9b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01259974s
Dec 20 11:36:42.470: INFO: Pod "pod-secrets-3953204a-fa25-40a7-af56-4569debcb9b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020814305s
STEP: Saw pod success
Dec 20 11:36:42.470: INFO: Pod "pod-secrets-3953204a-fa25-40a7-af56-4569debcb9b3" satisfied condition "success or failure"
Dec 20 11:36:42.477: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-secrets-3953204a-fa25-40a7-af56-4569debcb9b3 container secret-volume-test: <nil>
STEP: delete the pod
Dec 20 11:36:42.543: INFO: Waiting for pod pod-secrets-3953204a-fa25-40a7-af56-4569debcb9b3 to disappear
Dec 20 11:36:42.555: INFO: Pod pod-secrets-3953204a-fa25-40a7-af56-4569debcb9b3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:36:42.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6382" for this suite.
Dec 20 11:36:48.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:36:48.856: INFO: namespace secrets-6382 deletion completed in 6.291201516s

• [SLOW TEST:10.701 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:36:48.858: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4279
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-bb31c863-8953-4126-acfa-096b46408291
STEP: Creating a pod to test consume configMaps
Dec 20 11:36:49.210: INFO: Waiting up to 5m0s for pod "pod-configmaps-79bab1f5-ce26-471a-9a48-e8b5c9575cff" in namespace "configmap-4279" to be "success or failure"
Dec 20 11:36:49.233: INFO: Pod "pod-configmaps-79bab1f5-ce26-471a-9a48-e8b5c9575cff": Phase="Pending", Reason="", readiness=false. Elapsed: 22.465286ms
Dec 20 11:36:51.239: INFO: Pod "pod-configmaps-79bab1f5-ce26-471a-9a48-e8b5c9575cff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029178294s
Dec 20 11:36:53.250: INFO: Pod "pod-configmaps-79bab1f5-ce26-471a-9a48-e8b5c9575cff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039498593s
STEP: Saw pod success
Dec 20 11:36:53.250: INFO: Pod "pod-configmaps-79bab1f5-ce26-471a-9a48-e8b5c9575cff" satisfied condition "success or failure"
Dec 20 11:36:53.256: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-configmaps-79bab1f5-ce26-471a-9a48-e8b5c9575cff container configmap-volume-test: <nil>
STEP: delete the pod
Dec 20 11:36:53.357: INFO: Waiting for pod pod-configmaps-79bab1f5-ce26-471a-9a48-e8b5c9575cff to disappear
Dec 20 11:36:53.364: INFO: Pod pod-configmaps-79bab1f5-ce26-471a-9a48-e8b5c9575cff no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:36:53.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4279" for this suite.
Dec 20 11:36:59.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:36:59.681: INFO: namespace configmap-4279 deletion completed in 6.299710121s

• [SLOW TEST:10.823 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:36:59.686: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-580
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 20 11:36:59.938: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b4d218e4-eda9-45c3-9c60-9c05c4d62c1e" in namespace "projected-580" to be "success or failure"
Dec 20 11:36:59.945: INFO: Pod "downwardapi-volume-b4d218e4-eda9-45c3-9c60-9c05c4d62c1e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.604382ms
Dec 20 11:37:01.962: INFO: Pod "downwardapi-volume-b4d218e4-eda9-45c3-9c60-9c05c4d62c1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023861119s
Dec 20 11:37:03.976: INFO: Pod "downwardapi-volume-b4d218e4-eda9-45c3-9c60-9c05c4d62c1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037918335s
STEP: Saw pod success
Dec 20 11:37:03.976: INFO: Pod "downwardapi-volume-b4d218e4-eda9-45c3-9c60-9c05c4d62c1e" satisfied condition "success or failure"
Dec 20 11:37:03.982: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod downwardapi-volume-b4d218e4-eda9-45c3-9c60-9c05c4d62c1e container client-container: <nil>
STEP: delete the pod
Dec 20 11:37:04.054: INFO: Waiting for pod downwardapi-volume-b4d218e4-eda9-45c3-9c60-9c05c4d62c1e to disappear
Dec 20 11:37:04.064: INFO: Pod downwardapi-volume-b4d218e4-eda9-45c3-9c60-9c05c4d62c1e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:37:04.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-580" for this suite.
Dec 20 11:37:10.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:37:10.345: INFO: namespace projected-580 deletion completed in 6.258986592s

• [SLOW TEST:10.659 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:37:10.347: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2804
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 20 11:37:11.420: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 20 11:37:13.486: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438631, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438631, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438631, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438631, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 20 11:37:16.554: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:37:16.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2804" for this suite.
Dec 20 11:37:29.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:37:29.324: INFO: namespace webhook-2804 deletion completed in 12.328749009s
STEP: Destroying namespace "webhook-2804-markers" for this suite.
Dec 20 11:37:35.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:37:36.368: INFO: namespace webhook-2804-markers deletion completed in 7.044279981s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.068 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:37:36.417: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6168
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Dec 20 11:37:36.690: INFO: Waiting up to 5m0s for pod "client-containers-2a58628d-1cc0-4048-b94b-bbe865dcdf2d" in namespace "containers-6168" to be "success or failure"
Dec 20 11:37:36.696: INFO: Pod "client-containers-2a58628d-1cc0-4048-b94b-bbe865dcdf2d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.002276ms
Dec 20 11:37:38.703: INFO: Pod "client-containers-2a58628d-1cc0-4048-b94b-bbe865dcdf2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013446816s
Dec 20 11:37:40.712: INFO: Pod "client-containers-2a58628d-1cc0-4048-b94b-bbe865dcdf2d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022038946s
Dec 20 11:37:42.721: INFO: Pod "client-containers-2a58628d-1cc0-4048-b94b-bbe865dcdf2d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030974565s
Dec 20 11:37:44.731: INFO: Pod "client-containers-2a58628d-1cc0-4048-b94b-bbe865dcdf2d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.040973783s
Dec 20 11:37:46.742: INFO: Pod "client-containers-2a58628d-1cc0-4048-b94b-bbe865dcdf2d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.051878083s
Dec 20 11:37:48.748: INFO: Pod "client-containers-2a58628d-1cc0-4048-b94b-bbe865dcdf2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.057929509s
STEP: Saw pod success
Dec 20 11:37:48.748: INFO: Pod "client-containers-2a58628d-1cc0-4048-b94b-bbe865dcdf2d" satisfied condition "success or failure"
Dec 20 11:37:48.754: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod client-containers-2a58628d-1cc0-4048-b94b-bbe865dcdf2d container test-container: <nil>
STEP: delete the pod
Dec 20 11:37:48.809: INFO: Waiting for pod client-containers-2a58628d-1cc0-4048-b94b-bbe865dcdf2d to disappear
Dec 20 11:37:48.815: INFO: Pod client-containers-2a58628d-1cc0-4048-b94b-bbe865dcdf2d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:37:48.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6168" for this suite.
Dec 20 11:37:58.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:37:59.284: INFO: namespace containers-6168 deletion completed in 10.463451253s

• [SLOW TEST:22.868 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:37:59.285: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-27
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 20 11:38:04.523: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:38:04.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-27" for this suite.
Dec 20 11:38:10.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:38:10.857: INFO: namespace container-runtime-27 deletion completed in 6.291309021s

• [SLOW TEST:11.572 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:38:10.860: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8826
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 11:38:11.106: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec 20 11:38:16.600: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 20 11:38:16.600: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec 20 11:38:18.607: INFO: Creating deployment "test-rollover-deployment"
Dec 20 11:38:18.622: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec 20 11:38:20.887: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec 20 11:38:20.899: INFO: Ensure that both replica sets have 1 created replica
Dec 20 11:38:20.911: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec 20 11:38:20.935: INFO: Updating deployment test-rollover-deployment
Dec 20 11:38:20.935: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec 20 11:38:22.948: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec 20 11:38:22.961: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec 20 11:38:22.981: INFO: all replica sets need to contain the pod-template-hash label
Dec 20 11:38:22.981: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438699, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438699, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438701, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438699, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 11:38:25.000: INFO: all replica sets need to contain the pod-template-hash label
Dec 20 11:38:25.000: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438699, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438699, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438701, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438699, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 11:38:26.998: INFO: all replica sets need to contain the pod-template-hash label
Dec 20 11:38:26.998: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438699, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438699, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438706, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438699, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 11:38:29.030: INFO: all replica sets need to contain the pod-template-hash label
Dec 20 11:38:29.030: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438699, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438699, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438706, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438699, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 11:38:30.994: INFO: all replica sets need to contain the pod-template-hash label
Dec 20 11:38:30.994: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438699, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438699, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438706, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438699, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 11:38:32.999: INFO: all replica sets need to contain the pod-template-hash label
Dec 20 11:38:32.999: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438699, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438699, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438706, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438699, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 11:38:34.994: INFO: all replica sets need to contain the pod-template-hash label
Dec 20 11:38:34.994: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438699, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438699, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438706, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438699, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 11:38:36.996: INFO: 
Dec 20 11:38:36.996: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 20 11:38:37.018: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8826 /apis/apps/v1/namespaces/deployment-8826/deployments/test-rollover-deployment c08cb25f-5cab-41f8-8b0f-c1622bf30d37 20870 2 2019-12-20 11:38:18 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002a703f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-12-20 11:38:19 +0000 UTC,LastTransitionTime:2019-12-20 11:38:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2019-12-20 11:38:36 +0000 UTC,LastTransitionTime:2019-12-20 11:38:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 20 11:38:37.030: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-8826 /apis/apps/v1/namespaces/deployment-8826/replicasets/test-rollover-deployment-7d7dc6548c 2f9a69d7-feaa-4465-8047-374f97e6415f 20859 2 2019-12-20 11:38:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment c08cb25f-5cab-41f8-8b0f-c1622bf30d37 0xc002a70b77 0xc002a70b78}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002a70c38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 20 11:38:37.030: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec 20 11:38:37.031: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8826 /apis/apps/v1/namespaces/deployment-8826/replicasets/test-rollover-controller 851325f7-d7cb-4a61-a475-e02a8019a54a 20869 2 2019-12-20 11:38:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment c08cb25f-5cab-41f8-8b0f-c1622bf30d37 0xc002a70aa7 0xc002a70aa8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002a70b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 20 11:38:37.031: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-8826 /apis/apps/v1/namespaces/deployment-8826/replicasets/test-rollover-deployment-f6c94f66c 7947656c-0e13-484e-8667-67cf08d52bef 20796 2 2019-12-20 11:38:18 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment c08cb25f-5cab-41f8-8b0f-c1622bf30d37 0xc002a70d60 0xc002a70d61}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002a70e78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 20 11:38:37.040: INFO: Pod "test-rollover-deployment-7d7dc6548c-9ltzj" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-9ltzj test-rollover-deployment-7d7dc6548c- deployment-8826 /api/v1/namespaces/deployment-8826/pods/test-rollover-deployment-7d7dc6548c-9ltzj 418a5c51-d2d8-44a3-bfcd-3383339ccf03 20825 0 2019-12-20 11:38:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[cni.projectcalico.org/podIP:172.25.1.105/32] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c 2f9a69d7-feaa-4465-8047-374f97e6415f 0xc002a715f7 0xc002a715f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2ch96,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2ch96,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2ch96,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-428w5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:38:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:38:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:38:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:38:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.1.105,StartTime:2019-12-20 11:38:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-20 11:38:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://928398470ca7d628433da7f4f19fa7c20b63bd3d3b9f641ce68bb124059bbb4c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:38:37.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8826" for this suite.
Dec 20 11:38:45.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:38:45.405: INFO: namespace deployment-8826 deletion completed in 8.35616677s

• [SLOW TEST:34.545 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:38:45.406: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-444
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 20 11:38:45.815: INFO: Waiting up to 5m0s for pod "downwardapi-volume-041a80d6-71b5-4d81-a92f-5fb014e6bd82" in namespace "projected-444" to be "success or failure"
Dec 20 11:38:45.820: INFO: Pod "downwardapi-volume-041a80d6-71b5-4d81-a92f-5fb014e6bd82": Phase="Pending", Reason="", readiness=false. Elapsed: 5.351297ms
Dec 20 11:38:47.850: INFO: Pod "downwardapi-volume-041a80d6-71b5-4d81-a92f-5fb014e6bd82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035520793s
Dec 20 11:38:49.861: INFO: Pod "downwardapi-volume-041a80d6-71b5-4d81-a92f-5fb014e6bd82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046429362s
STEP: Saw pod success
Dec 20 11:38:49.861: INFO: Pod "downwardapi-volume-041a80d6-71b5-4d81-a92f-5fb014e6bd82" satisfied condition "success or failure"
Dec 20 11:38:49.867: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod downwardapi-volume-041a80d6-71b5-4d81-a92f-5fb014e6bd82 container client-container: <nil>
STEP: delete the pod
Dec 20 11:38:49.942: INFO: Waiting for pod downwardapi-volume-041a80d6-71b5-4d81-a92f-5fb014e6bd82 to disappear
Dec 20 11:38:49.950: INFO: Pod downwardapi-volume-041a80d6-71b5-4d81-a92f-5fb014e6bd82 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:38:49.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-444" for this suite.
Dec 20 11:38:55.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:38:56.277: INFO: namespace projected-444 deletion completed in 6.315816818s

• [SLOW TEST:10.871 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:38:56.280: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-760
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-760
I1220 11:38:56.521404      20 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-760, replica count: 1
I1220 11:38:57.572133      20 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1220 11:38:58.572466      20 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1220 11:38:59.572796      20 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1220 11:39:00.573088      20 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1220 11:39:01.573356      20 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1220 11:39:02.573610      20 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 20 11:39:02.704: INFO: Created: latency-svc-h2vdm
Dec 20 11:39:02.724: INFO: Got endpoints: latency-svc-h2vdm [49.953493ms]
Dec 20 11:39:02.750: INFO: Created: latency-svc-dqk8h
Dec 20 11:39:02.774: INFO: Got endpoints: latency-svc-dqk8h [49.680144ms]
Dec 20 11:39:02.781: INFO: Created: latency-svc-fqqpq
Dec 20 11:39:02.800: INFO: Got endpoints: latency-svc-fqqpq [75.011988ms]
Dec 20 11:39:02.818: INFO: Created: latency-svc-jrjnm
Dec 20 11:39:02.851: INFO: Got endpoints: latency-svc-jrjnm [126.386825ms]
Dec 20 11:39:02.854: INFO: Created: latency-svc-7scvp
Dec 20 11:39:02.875: INFO: Created: latency-svc-dtzs5
Dec 20 11:39:02.889: INFO: Got endpoints: latency-svc-7scvp [163.48442ms]
Dec 20 11:39:02.892: INFO: Got endpoints: latency-svc-dtzs5 [166.319372ms]
Dec 20 11:39:02.902: INFO: Created: latency-svc-q5jts
Dec 20 11:39:02.916: INFO: Got endpoints: latency-svc-q5jts [190.364885ms]
Dec 20 11:39:02.928: INFO: Created: latency-svc-g8vl4
Dec 20 11:39:02.939: INFO: Got endpoints: latency-svc-g8vl4 [214.419102ms]
Dec 20 11:39:02.952: INFO: Created: latency-svc-ng5dd
Dec 20 11:39:02.957: INFO: Got endpoints: latency-svc-ng5dd [230.268746ms]
Dec 20 11:39:02.960: INFO: Created: latency-svc-ngmwk
Dec 20 11:39:02.968: INFO: Got endpoints: latency-svc-ngmwk [241.747306ms]
Dec 20 11:39:02.998: INFO: Created: latency-svc-g4ldt
Dec 20 11:39:03.015: INFO: Got endpoints: latency-svc-g4ldt [288.645282ms]
Dec 20 11:39:03.021: INFO: Created: latency-svc-pbsms
Dec 20 11:39:03.032: INFO: Got endpoints: latency-svc-pbsms [305.88107ms]
Dec 20 11:39:03.053: INFO: Created: latency-svc-xnn5m
Dec 20 11:39:03.068: INFO: Got endpoints: latency-svc-xnn5m [342.010178ms]
Dec 20 11:39:03.069: INFO: Created: latency-svc-5sv8c
Dec 20 11:39:03.078: INFO: Got endpoints: latency-svc-5sv8c [351.325212ms]
Dec 20 11:39:03.096: INFO: Created: latency-svc-w9zrx
Dec 20 11:39:03.108: INFO: Created: latency-svc-lwwhl
Dec 20 11:39:03.112: INFO: Got endpoints: latency-svc-w9zrx [385.934029ms]
Dec 20 11:39:03.121: INFO: Got endpoints: latency-svc-lwwhl [393.97454ms]
Dec 20 11:39:03.161: INFO: Created: latency-svc-xrt4k
Dec 20 11:39:03.171: INFO: Got endpoints: latency-svc-xrt4k [397.587721ms]
Dec 20 11:39:03.173: INFO: Created: latency-svc-zwlb2
Dec 20 11:39:03.202: INFO: Got endpoints: latency-svc-zwlb2 [401.720943ms]
Dec 20 11:39:03.217: INFO: Created: latency-svc-4lwbz
Dec 20 11:39:03.249: INFO: Created: latency-svc-57k7r
Dec 20 11:39:03.249: INFO: Got endpoints: latency-svc-4lwbz [397.92221ms]
Dec 20 11:39:03.263: INFO: Got endpoints: latency-svc-57k7r [374.29314ms]
Dec 20 11:39:03.275: INFO: Created: latency-svc-d4zkx
Dec 20 11:39:03.295: INFO: Got endpoints: latency-svc-d4zkx [403.260734ms]
Dec 20 11:39:03.297: INFO: Created: latency-svc-7zcqv
Dec 20 11:39:03.356: INFO: Created: latency-svc-kj9d6
Dec 20 11:39:03.356: INFO: Got endpoints: latency-svc-7zcqv [440.039069ms]
Dec 20 11:39:03.368: INFO: Got endpoints: latency-svc-kj9d6 [428.675921ms]
Dec 20 11:39:03.376: INFO: Created: latency-svc-qcn9h
Dec 20 11:39:03.390: INFO: Got endpoints: latency-svc-qcn9h [433.05134ms]
Dec 20 11:39:03.408: INFO: Created: latency-svc-z2x2z
Dec 20 11:39:03.436: INFO: Created: latency-svc-2whjx
Dec 20 11:39:03.491: INFO: Got endpoints: latency-svc-2whjx [476.104632ms]
Dec 20 11:39:03.491: INFO: Got endpoints: latency-svc-z2x2z [523.138133ms]
Dec 20 11:39:03.509: INFO: Created: latency-svc-vjhfk
Dec 20 11:39:03.520: INFO: Got endpoints: latency-svc-vjhfk [487.561981ms]
Dec 20 11:39:03.551: INFO: Created: latency-svc-pjfcx
Dec 20 11:39:03.576: INFO: Got endpoints: latency-svc-pjfcx [507.593404ms]
Dec 20 11:39:03.584: INFO: Created: latency-svc-kmwgw
Dec 20 11:39:03.600: INFO: Got endpoints: latency-svc-kmwgw [522.104046ms]
Dec 20 11:39:03.601: INFO: Created: latency-svc-bd7jd
Dec 20 11:39:03.621: INFO: Got endpoints: latency-svc-bd7jd [508.399741ms]
Dec 20 11:39:03.631: INFO: Created: latency-svc-8kbfj
Dec 20 11:39:03.645: INFO: Created: latency-svc-cg6jc
Dec 20 11:39:03.654: INFO: Got endpoints: latency-svc-8kbfj [533.604942ms]
Dec 20 11:39:03.665: INFO: Got endpoints: latency-svc-cg6jc [493.416842ms]
Dec 20 11:39:03.688: INFO: Created: latency-svc-ztqnk
Dec 20 11:39:03.707: INFO: Got endpoints: latency-svc-ztqnk [504.668783ms]
Dec 20 11:39:03.711: INFO: Created: latency-svc-97zkk
Dec 20 11:39:03.729: INFO: Got endpoints: latency-svc-97zkk [479.64173ms]
Dec 20 11:39:03.738: INFO: Created: latency-svc-k9kqh
Dec 20 11:39:03.745: INFO: Got endpoints: latency-svc-k9kqh [480.924381ms]
Dec 20 11:39:03.765: INFO: Created: latency-svc-57jj4
Dec 20 11:39:03.784: INFO: Created: latency-svc-f9q58
Dec 20 11:39:03.786: INFO: Got endpoints: latency-svc-57jj4 [490.094802ms]
Dec 20 11:39:03.797: INFO: Got endpoints: latency-svc-f9q58 [440.016635ms]
Dec 20 11:39:03.803: INFO: Created: latency-svc-gxdx8
Dec 20 11:39:03.823: INFO: Got endpoints: latency-svc-gxdx8 [455.369576ms]
Dec 20 11:39:03.824: INFO: Created: latency-svc-qbrzg
Dec 20 11:39:03.834: INFO: Got endpoints: latency-svc-qbrzg [444.585308ms]
Dec 20 11:39:03.843: INFO: Created: latency-svc-vw7hz
Dec 20 11:39:03.860: INFO: Got endpoints: latency-svc-vw7hz [368.682919ms]
Dec 20 11:39:03.862: INFO: Created: latency-svc-5trwc
Dec 20 11:39:03.874: INFO: Got endpoints: latency-svc-5trwc [382.205075ms]
Dec 20 11:39:03.878: INFO: Created: latency-svc-z2m8t
Dec 20 11:39:03.908: INFO: Created: latency-svc-mtn75
Dec 20 11:39:03.909: INFO: Got endpoints: latency-svc-z2m8t [389.329394ms]
Dec 20 11:39:03.913: INFO: Created: latency-svc-ctv5f
Dec 20 11:39:03.922: INFO: Got endpoints: latency-svc-mtn75 [320.997372ms]
Dec 20 11:39:03.927: INFO: Got endpoints: latency-svc-ctv5f [350.107376ms]
Dec 20 11:39:03.935: INFO: Created: latency-svc-d7k7j
Dec 20 11:39:03.941: INFO: Got endpoints: latency-svc-d7k7j [319.854441ms]
Dec 20 11:39:03.945: INFO: Created: latency-svc-9nv76
Dec 20 11:39:03.953: INFO: Got endpoints: latency-svc-9nv76 [298.081679ms]
Dec 20 11:39:03.962: INFO: Created: latency-svc-h5vwf
Dec 20 11:39:03.982: INFO: Got endpoints: latency-svc-h5vwf [317.246737ms]
Dec 20 11:39:03.987: INFO: Created: latency-svc-srkj6
Dec 20 11:39:04.022: INFO: Got endpoints: latency-svc-srkj6 [315.303938ms]
Dec 20 11:39:04.024: INFO: Created: latency-svc-4pjwd
Dec 20 11:39:04.043: INFO: Got endpoints: latency-svc-4pjwd [313.505693ms]
Dec 20 11:39:04.046: INFO: Created: latency-svc-gf8gm
Dec 20 11:39:04.083: INFO: Got endpoints: latency-svc-gf8gm [338.656194ms]
Dec 20 11:39:04.094: INFO: Created: latency-svc-drl2f
Dec 20 11:39:04.109: INFO: Got endpoints: latency-svc-drl2f [323.021617ms]
Dec 20 11:39:04.134: INFO: Created: latency-svc-sdp8f
Dec 20 11:39:04.138: INFO: Created: latency-svc-q2tj4
Dec 20 11:39:04.146: INFO: Got endpoints: latency-svc-sdp8f [321.769422ms]
Dec 20 11:39:04.146: INFO: Got endpoints: latency-svc-q2tj4 [349.241402ms]
Dec 20 11:39:04.157: INFO: Created: latency-svc-x6qqw
Dec 20 11:39:04.188: INFO: Got endpoints: latency-svc-x6qqw [353.423966ms]
Dec 20 11:39:04.192: INFO: Created: latency-svc-j8lhm
Dec 20 11:39:04.213: INFO: Got endpoints: latency-svc-j8lhm [352.658164ms]
Dec 20 11:39:04.224: INFO: Created: latency-svc-2bb44
Dec 20 11:39:04.240: INFO: Created: latency-svc-rmlqz
Dec 20 11:39:04.240: INFO: Got endpoints: latency-svc-2bb44 [366.05534ms]
Dec 20 11:39:04.264: INFO: Got endpoints: latency-svc-rmlqz [354.518569ms]
Dec 20 11:39:04.272: INFO: Created: latency-svc-fwz46
Dec 20 11:39:04.288: INFO: Created: latency-svc-6fdxx
Dec 20 11:39:04.289: INFO: Got endpoints: latency-svc-fwz46 [367.524348ms]
Dec 20 11:39:04.306: INFO: Got endpoints: latency-svc-6fdxx [379.463018ms]
Dec 20 11:39:04.323: INFO: Created: latency-svc-h8b5q
Dec 20 11:39:04.354: INFO: Created: latency-svc-dfzpr
Dec 20 11:39:04.354: INFO: Got endpoints: latency-svc-h8b5q [413.056438ms]
Dec 20 11:39:04.379: INFO: Created: latency-svc-dzddv
Dec 20 11:39:04.394: INFO: Got endpoints: latency-svc-dzddv [410.734349ms]
Dec 20 11:39:04.394: INFO: Got endpoints: latency-svc-dfzpr [440.640393ms]
Dec 20 11:39:04.421: INFO: Created: latency-svc-tctqd
Dec 20 11:39:04.455: INFO: Got endpoints: latency-svc-tctqd [432.987108ms]
Dec 20 11:39:04.464: INFO: Created: latency-svc-m2ftx
Dec 20 11:39:04.477: INFO: Got endpoints: latency-svc-m2ftx [433.351822ms]
Dec 20 11:39:04.485: INFO: Created: latency-svc-4gqt4
Dec 20 11:39:04.505: INFO: Created: latency-svc-6hk82
Dec 20 11:39:04.509: INFO: Got endpoints: latency-svc-4gqt4 [425.425632ms]
Dec 20 11:39:04.518: INFO: Got endpoints: latency-svc-6hk82 [409.370105ms]
Dec 20 11:39:04.523: INFO: Created: latency-svc-kmx69
Dec 20 11:39:04.541: INFO: Created: latency-svc-qdwm7
Dec 20 11:39:04.560: INFO: Created: latency-svc-h2snj
Dec 20 11:39:04.588: INFO: Got endpoints: latency-svc-kmx69 [442.477642ms]
Dec 20 11:39:04.596: INFO: Created: latency-svc-w9l7d
Dec 20 11:39:04.617: INFO: Created: latency-svc-jr2nv
Dec 20 11:39:04.642: INFO: Got endpoints: latency-svc-qdwm7 [494.824377ms]
Dec 20 11:39:04.660: INFO: Created: latency-svc-8qdkn
Dec 20 11:39:04.680: INFO: Created: latency-svc-h5dpq
Dec 20 11:39:04.680: INFO: Got endpoints: latency-svc-h2snj [492.133718ms]
Dec 20 11:39:04.711: INFO: Created: latency-svc-7rtl6
Dec 20 11:39:04.725: INFO: Got endpoints: latency-svc-w9l7d [511.938125ms]
Dec 20 11:39:04.738: INFO: Created: latency-svc-2ccn4
Dec 20 11:39:04.761: INFO: Created: latency-svc-dkc92
Dec 20 11:39:04.769: INFO: Got endpoints: latency-svc-jr2nv [528.88093ms]
Dec 20 11:39:04.803: INFO: Created: latency-svc-9xkz8
Dec 20 11:39:04.808: INFO: Created: latency-svc-vprkh
Dec 20 11:39:04.827: INFO: Got endpoints: latency-svc-8qdkn [561.906525ms]
Dec 20 11:39:04.833: INFO: Created: latency-svc-xxzzb
Dec 20 11:39:04.866: INFO: Created: latency-svc-x6hjc
Dec 20 11:39:04.869: INFO: Got endpoints: latency-svc-h5dpq [579.711737ms]
Dec 20 11:39:04.904: INFO: Created: latency-svc-8t58z
Dec 20 11:39:04.922: INFO: Got endpoints: latency-svc-7rtl6 [615.555327ms]
Dec 20 11:39:04.928: INFO: Created: latency-svc-5pgcp
Dec 20 11:39:04.945: INFO: Created: latency-svc-lpb5f
Dec 20 11:39:04.964: INFO: Created: latency-svc-8vpwl
Dec 20 11:39:05.002: INFO: Got endpoints: latency-svc-2ccn4 [647.747083ms]
Dec 20 11:39:05.010: INFO: Created: latency-svc-24frr
Dec 20 11:39:05.026: INFO: Got endpoints: latency-svc-dkc92 [631.466784ms]
Dec 20 11:39:05.044: INFO: Created: latency-svc-9zpxk
Dec 20 11:39:05.049: INFO: Created: latency-svc-79wf9
Dec 20 11:39:05.070: INFO: Created: latency-svc-lf4f7
Dec 20 11:39:05.080: INFO: Got endpoints: latency-svc-9xkz8 [686.414851ms]
Dec 20 11:39:05.093: INFO: Created: latency-svc-l4s8t
Dec 20 11:39:05.125: INFO: Created: latency-svc-lccpq
Dec 20 11:39:05.132: INFO: Got endpoints: latency-svc-vprkh [675.997725ms]
Dec 20 11:39:05.141: INFO: Created: latency-svc-pzmcf
Dec 20 11:39:05.157: INFO: Created: latency-svc-jn5mz
Dec 20 11:39:05.167: INFO: Got endpoints: latency-svc-xxzzb [689.750742ms]
Dec 20 11:39:05.177: INFO: Created: latency-svc-s4ql4
Dec 20 11:39:05.189: INFO: Created: latency-svc-cbhqv
Dec 20 11:39:05.225: INFO: Got endpoints: latency-svc-x6hjc [716.683043ms]
Dec 20 11:39:05.249: INFO: Created: latency-svc-zqqqc
Dec 20 11:39:05.268: INFO: Got endpoints: latency-svc-8t58z [749.35083ms]
Dec 20 11:39:05.290: INFO: Created: latency-svc-nlnfl
Dec 20 11:39:05.322: INFO: Got endpoints: latency-svc-5pgcp [732.846906ms]
Dec 20 11:39:05.354: INFO: Created: latency-svc-xq7gw
Dec 20 11:39:05.365: INFO: Got endpoints: latency-svc-lpb5f [718.425321ms]
Dec 20 11:39:05.400: INFO: Created: latency-svc-znr6h
Dec 20 11:39:05.416: INFO: Got endpoints: latency-svc-8vpwl [736.122064ms]
Dec 20 11:39:05.443: INFO: Created: latency-svc-w9wss
Dec 20 11:39:05.470: INFO: Got endpoints: latency-svc-24frr [745.143428ms]
Dec 20 11:39:05.488: INFO: Created: latency-svc-brnzg
Dec 20 11:39:05.523: INFO: Got endpoints: latency-svc-9zpxk [753.454529ms]
Dec 20 11:39:05.543: INFO: Created: latency-svc-9sg9r
Dec 20 11:39:05.570: INFO: Got endpoints: latency-svc-79wf9 [743.212069ms]
Dec 20 11:39:05.609: INFO: Created: latency-svc-vf7nv
Dec 20 11:39:05.616: INFO: Got endpoints: latency-svc-lf4f7 [747.082201ms]
Dec 20 11:39:05.664: INFO: Created: latency-svc-k5n6g
Dec 20 11:39:05.665: INFO: Got endpoints: latency-svc-l4s8t [742.952702ms]
Dec 20 11:39:05.694: INFO: Created: latency-svc-2c9vx
Dec 20 11:39:05.718: INFO: Got endpoints: latency-svc-lccpq [637.895276ms]
Dec 20 11:39:05.744: INFO: Created: latency-svc-7t2tl
Dec 20 11:39:05.779: INFO: Got endpoints: latency-svc-pzmcf [777.088338ms]
Dec 20 11:39:05.808: INFO: Created: latency-svc-dbr4c
Dec 20 11:39:05.825: INFO: Got endpoints: latency-svc-jn5mz [799.346455ms]
Dec 20 11:39:05.852: INFO: Created: latency-svc-rfpd8
Dec 20 11:39:05.881: INFO: Got endpoints: latency-svc-s4ql4 [749.140975ms]
Dec 20 11:39:05.924: INFO: Got endpoints: latency-svc-cbhqv [756.83992ms]
Dec 20 11:39:05.928: INFO: Created: latency-svc-ggwb5
Dec 20 11:39:05.949: INFO: Created: latency-svc-q758n
Dec 20 11:39:05.965: INFO: Got endpoints: latency-svc-zqqqc [738.936592ms]
Dec 20 11:39:05.992: INFO: Created: latency-svc-qvr75
Dec 20 11:39:06.028: INFO: Got endpoints: latency-svc-nlnfl [759.69596ms]
Dec 20 11:39:06.057: INFO: Created: latency-svc-wswvb
Dec 20 11:39:06.067: INFO: Got endpoints: latency-svc-xq7gw [744.913177ms]
Dec 20 11:39:06.096: INFO: Created: latency-svc-m4dfh
Dec 20 11:39:06.123: INFO: Got endpoints: latency-svc-znr6h [758.231683ms]
Dec 20 11:39:06.159: INFO: Created: latency-svc-kwpcf
Dec 20 11:39:06.194: INFO: Got endpoints: latency-svc-w9wss [777.568327ms]
Dec 20 11:39:06.225: INFO: Got endpoints: latency-svc-brnzg [753.754199ms]
Dec 20 11:39:06.224: INFO: Created: latency-svc-2m7w6
Dec 20 11:39:06.264: INFO: Got endpoints: latency-svc-9sg9r [740.878157ms]
Dec 20 11:39:06.265: INFO: Created: latency-svc-q4dvr
Dec 20 11:39:06.314: INFO: Created: latency-svc-95fxk
Dec 20 11:39:06.329: INFO: Got endpoints: latency-svc-vf7nv [759.111681ms]
Dec 20 11:39:06.353: INFO: Created: latency-svc-8s6rk
Dec 20 11:39:06.377: INFO: Got endpoints: latency-svc-k5n6g [761.175346ms]
Dec 20 11:39:06.401: INFO: Created: latency-svc-mgvl8
Dec 20 11:39:06.413: INFO: Got endpoints: latency-svc-2c9vx [748.301381ms]
Dec 20 11:39:06.438: INFO: Created: latency-svc-28vjp
Dec 20 11:39:06.473: INFO: Got endpoints: latency-svc-7t2tl [754.647478ms]
Dec 20 11:39:06.515: INFO: Created: latency-svc-bbltf
Dec 20 11:39:06.515: INFO: Got endpoints: latency-svc-dbr4c [735.34108ms]
Dec 20 11:39:06.538: INFO: Created: latency-svc-7dk8m
Dec 20 11:39:06.567: INFO: Got endpoints: latency-svc-rfpd8 [742.009938ms]
Dec 20 11:39:06.601: INFO: Created: latency-svc-znzq9
Dec 20 11:39:06.616: INFO: Got endpoints: latency-svc-ggwb5 [734.310153ms]
Dec 20 11:39:06.647: INFO: Created: latency-svc-6mtfw
Dec 20 11:39:06.687: INFO: Got endpoints: latency-svc-q758n [763.376253ms]
Dec 20 11:39:06.711: INFO: Created: latency-svc-mkbgc
Dec 20 11:39:06.730: INFO: Got endpoints: latency-svc-qvr75 [765.311458ms]
Dec 20 11:39:06.754: INFO: Created: latency-svc-46hnp
Dec 20 11:39:06.768: INFO: Got endpoints: latency-svc-wswvb [739.691514ms]
Dec 20 11:39:06.789: INFO: Created: latency-svc-wcjtd
Dec 20 11:39:06.823: INFO: Got endpoints: latency-svc-m4dfh [755.723311ms]
Dec 20 11:39:06.854: INFO: Created: latency-svc-tlnd7
Dec 20 11:39:06.865: INFO: Got endpoints: latency-svc-kwpcf [741.418031ms]
Dec 20 11:39:06.896: INFO: Created: latency-svc-lpqt6
Dec 20 11:39:06.924: INFO: Got endpoints: latency-svc-2m7w6 [729.888293ms]
Dec 20 11:39:06.948: INFO: Created: latency-svc-nxw27
Dec 20 11:39:06.966: INFO: Got endpoints: latency-svc-q4dvr [741.050746ms]
Dec 20 11:39:06.989: INFO: Created: latency-svc-bzk6l
Dec 20 11:39:07.024: INFO: Got endpoints: latency-svc-95fxk [759.631779ms]
Dec 20 11:39:07.060: INFO: Created: latency-svc-rpksj
Dec 20 11:39:07.065: INFO: Got endpoints: latency-svc-8s6rk [736.127193ms]
Dec 20 11:39:07.085: INFO: Created: latency-svc-t2dqw
Dec 20 11:39:07.120: INFO: Got endpoints: latency-svc-mgvl8 [742.75471ms]
Dec 20 11:39:07.150: INFO: Created: latency-svc-s5ssd
Dec 20 11:39:07.169: INFO: Got endpoints: latency-svc-28vjp [755.189474ms]
Dec 20 11:39:07.194: INFO: Created: latency-svc-j5v52
Dec 20 11:39:07.234: INFO: Got endpoints: latency-svc-bbltf [759.933514ms]
Dec 20 11:39:07.302: INFO: Created: latency-svc-zjzcn
Dec 20 11:39:07.303: INFO: Got endpoints: latency-svc-7dk8m [787.747056ms]
Dec 20 11:39:07.319: INFO: Got endpoints: latency-svc-znzq9 [751.642231ms]
Dec 20 11:39:07.362: INFO: Created: latency-svc-7kgk9
Dec 20 11:39:07.376: INFO: Got endpoints: latency-svc-6mtfw [760.114808ms]
Dec 20 11:39:07.392: INFO: Created: latency-svc-fdt7b
Dec 20 11:39:07.420: INFO: Created: latency-svc-5lkk6
Dec 20 11:39:07.421: INFO: Got endpoints: latency-svc-mkbgc [733.408629ms]
Dec 20 11:39:07.446: INFO: Created: latency-svc-lfbpq
Dec 20 11:39:07.473: INFO: Got endpoints: latency-svc-46hnp [742.192733ms]
Dec 20 11:39:07.504: INFO: Created: latency-svc-qdqx4
Dec 20 11:39:07.528: INFO: Got endpoints: latency-svc-wcjtd [759.546418ms]
Dec 20 11:39:07.578: INFO: Created: latency-svc-5lm6w
Dec 20 11:39:07.583: INFO: Got endpoints: latency-svc-tlnd7 [759.86716ms]
Dec 20 11:39:07.607: INFO: Created: latency-svc-jxsfs
Dec 20 11:39:07.620: INFO: Got endpoints: latency-svc-lpqt6 [755.868515ms]
Dec 20 11:39:07.673: INFO: Got endpoints: latency-svc-nxw27 [749.06367ms]
Dec 20 11:39:07.679: INFO: Created: latency-svc-t8f9v
Dec 20 11:39:07.705: INFO: Created: latency-svc-ssvbv
Dec 20 11:39:07.715: INFO: Got endpoints: latency-svc-bzk6l [748.585545ms]
Dec 20 11:39:07.738: INFO: Created: latency-svc-cdchg
Dec 20 11:39:07.774: INFO: Got endpoints: latency-svc-rpksj [750.460148ms]
Dec 20 11:39:07.794: INFO: Created: latency-svc-dfvqs
Dec 20 11:39:07.813: INFO: Got endpoints: latency-svc-t2dqw [748.047322ms]
Dec 20 11:39:07.833: INFO: Created: latency-svc-7hvt4
Dec 20 11:39:07.870: INFO: Got endpoints: latency-svc-s5ssd [749.861248ms]
Dec 20 11:39:07.896: INFO: Created: latency-svc-cx976
Dec 20 11:39:07.922: INFO: Got endpoints: latency-svc-j5v52 [753.075538ms]
Dec 20 11:39:07.947: INFO: Created: latency-svc-8h4h4
Dec 20 11:39:07.973: INFO: Got endpoints: latency-svc-zjzcn [739.817625ms]
Dec 20 11:39:07.995: INFO: Created: latency-svc-whrsf
Dec 20 11:39:08.036: INFO: Got endpoints: latency-svc-7kgk9 [733.362206ms]
Dec 20 11:39:08.073: INFO: Created: latency-svc-kbhbc
Dec 20 11:39:08.076: INFO: Got endpoints: latency-svc-fdt7b [756.42254ms]
Dec 20 11:39:08.113: INFO: Created: latency-svc-c6jpf
Dec 20 11:39:08.120: INFO: Got endpoints: latency-svc-5lkk6 [743.694505ms]
Dec 20 11:39:08.153: INFO: Created: latency-svc-kpzjq
Dec 20 11:39:08.182: INFO: Got endpoints: latency-svc-lfbpq [760.825623ms]
Dec 20 11:39:08.213: INFO: Created: latency-svc-r9d2p
Dec 20 11:39:08.220: INFO: Got endpoints: latency-svc-qdqx4 [747.235285ms]
Dec 20 11:39:08.250: INFO: Created: latency-svc-qbmk8
Dec 20 11:39:08.265: INFO: Got endpoints: latency-svc-5lm6w [737.377384ms]
Dec 20 11:39:08.301: INFO: Created: latency-svc-sxczh
Dec 20 11:39:08.314: INFO: Got endpoints: latency-svc-jxsfs [730.800268ms]
Dec 20 11:39:08.335: INFO: Created: latency-svc-hjwtc
Dec 20 11:39:08.366: INFO: Got endpoints: latency-svc-t8f9v [745.858216ms]
Dec 20 11:39:08.398: INFO: Created: latency-svc-4vqh9
Dec 20 11:39:08.419: INFO: Got endpoints: latency-svc-ssvbv [745.68925ms]
Dec 20 11:39:08.444: INFO: Created: latency-svc-ksg8n
Dec 20 11:39:08.479: INFO: Got endpoints: latency-svc-cdchg [764.045749ms]
Dec 20 11:39:08.499: INFO: Created: latency-svc-422mm
Dec 20 11:39:08.513: INFO: Got endpoints: latency-svc-dfvqs [738.848971ms]
Dec 20 11:39:08.546: INFO: Created: latency-svc-9tf47
Dec 20 11:39:08.570: INFO: Got endpoints: latency-svc-7hvt4 [756.868393ms]
Dec 20 11:39:08.604: INFO: Created: latency-svc-ghd59
Dec 20 11:39:08.616: INFO: Got endpoints: latency-svc-cx976 [745.273305ms]
Dec 20 11:39:08.635: INFO: Created: latency-svc-dxqqd
Dec 20 11:39:08.667: INFO: Got endpoints: latency-svc-8h4h4 [744.949387ms]
Dec 20 11:39:08.687: INFO: Created: latency-svc-mhd74
Dec 20 11:39:08.716: INFO: Got endpoints: latency-svc-whrsf [742.173878ms]
Dec 20 11:39:08.740: INFO: Created: latency-svc-67lvz
Dec 20 11:39:08.769: INFO: Got endpoints: latency-svc-kbhbc [732.303686ms]
Dec 20 11:39:08.796: INFO: Created: latency-svc-9gsj9
Dec 20 11:39:08.814: INFO: Got endpoints: latency-svc-c6jpf [737.84802ms]
Dec 20 11:39:08.838: INFO: Created: latency-svc-dsh4z
Dec 20 11:39:08.867: INFO: Got endpoints: latency-svc-kpzjq [746.876913ms]
Dec 20 11:39:08.889: INFO: Created: latency-svc-f89tv
Dec 20 11:39:08.916: INFO: Got endpoints: latency-svc-r9d2p [733.659449ms]
Dec 20 11:39:08.943: INFO: Created: latency-svc-9pgnk
Dec 20 11:39:08.968: INFO: Got endpoints: latency-svc-qbmk8 [747.696374ms]
Dec 20 11:39:08.989: INFO: Created: latency-svc-9tdg4
Dec 20 11:39:09.015: INFO: Got endpoints: latency-svc-sxczh [749.456103ms]
Dec 20 11:39:09.050: INFO: Created: latency-svc-h4qmq
Dec 20 11:39:09.071: INFO: Got endpoints: latency-svc-hjwtc [756.551426ms]
Dec 20 11:39:09.120: INFO: Created: latency-svc-j528j
Dec 20 11:39:09.120: INFO: Got endpoints: latency-svc-4vqh9 [753.640956ms]
Dec 20 11:39:09.162: INFO: Created: latency-svc-l255v
Dec 20 11:39:09.172: INFO: Got endpoints: latency-svc-ksg8n [752.356914ms]
Dec 20 11:39:09.193: INFO: Created: latency-svc-8v794
Dec 20 11:39:09.228: INFO: Got endpoints: latency-svc-422mm [748.775033ms]
Dec 20 11:39:09.249: INFO: Created: latency-svc-cf8xc
Dec 20 11:39:09.267: INFO: Got endpoints: latency-svc-9tf47 [754.396044ms]
Dec 20 11:39:09.306: INFO: Created: latency-svc-j9hkt
Dec 20 11:39:09.317: INFO: Got endpoints: latency-svc-ghd59 [746.079642ms]
Dec 20 11:39:09.344: INFO: Created: latency-svc-sn2wg
Dec 20 11:39:09.364: INFO: Got endpoints: latency-svc-dxqqd [748.602646ms]
Dec 20 11:39:09.391: INFO: Created: latency-svc-z8vft
Dec 20 11:39:09.420: INFO: Got endpoints: latency-svc-mhd74 [752.54854ms]
Dec 20 11:39:09.446: INFO: Created: latency-svc-gfqx4
Dec 20 11:39:09.479: INFO: Got endpoints: latency-svc-67lvz [762.926609ms]
Dec 20 11:39:09.514: INFO: Created: latency-svc-5dnbq
Dec 20 11:39:09.523: INFO: Got endpoints: latency-svc-9gsj9 [753.697842ms]
Dec 20 11:39:09.555: INFO: Created: latency-svc-vb5vr
Dec 20 11:39:09.564: INFO: Got endpoints: latency-svc-dsh4z [750.216352ms]
Dec 20 11:39:09.624: INFO: Got endpoints: latency-svc-f89tv [756.348099ms]
Dec 20 11:39:09.629: INFO: Created: latency-svc-x4ztt
Dec 20 11:39:09.648: INFO: Created: latency-svc-d66k2
Dec 20 11:39:09.669: INFO: Got endpoints: latency-svc-9pgnk [752.749343ms]
Dec 20 11:39:09.689: INFO: Created: latency-svc-nrs6j
Dec 20 11:39:09.714: INFO: Got endpoints: latency-svc-9tdg4 [745.504174ms]
Dec 20 11:39:09.737: INFO: Created: latency-svc-5szd7
Dec 20 11:39:09.765: INFO: Got endpoints: latency-svc-h4qmq [749.378884ms]
Dec 20 11:39:09.806: INFO: Created: latency-svc-9bfd2
Dec 20 11:39:09.815: INFO: Got endpoints: latency-svc-j528j [743.994722ms]
Dec 20 11:39:09.844: INFO: Created: latency-svc-wtchh
Dec 20 11:39:09.927: INFO: Got endpoints: latency-svc-l255v [806.847273ms]
Dec 20 11:39:09.930: INFO: Got endpoints: latency-svc-8v794 [757.963435ms]
Dec 20 11:39:09.952: INFO: Created: latency-svc-7nd85
Dec 20 11:39:09.973: INFO: Created: latency-svc-22wpp
Dec 20 11:39:09.981: INFO: Got endpoints: latency-svc-cf8xc [752.509788ms]
Dec 20 11:39:10.002: INFO: Created: latency-svc-9vk72
Dec 20 11:39:10.013: INFO: Got endpoints: latency-svc-j9hkt [745.105653ms]
Dec 20 11:39:10.050: INFO: Created: latency-svc-vb629
Dec 20 11:39:10.075: INFO: Got endpoints: latency-svc-sn2wg [757.750423ms]
Dec 20 11:39:10.105: INFO: Created: latency-svc-5k7lr
Dec 20 11:39:10.120: INFO: Got endpoints: latency-svc-z8vft [755.759401ms]
Dec 20 11:39:10.154: INFO: Created: latency-svc-lpkdt
Dec 20 11:39:10.164: INFO: Got endpoints: latency-svc-gfqx4 [744.735335ms]
Dec 20 11:39:10.188: INFO: Created: latency-svc-mp7mf
Dec 20 11:39:10.219: INFO: Got endpoints: latency-svc-5dnbq [740.043289ms]
Dec 20 11:39:10.275: INFO: Got endpoints: latency-svc-vb5vr [751.616097ms]
Dec 20 11:39:10.281: INFO: Created: latency-svc-gcb75
Dec 20 11:39:10.301: INFO: Created: latency-svc-dnf25
Dec 20 11:39:10.321: INFO: Got endpoints: latency-svc-x4ztt [756.975811ms]
Dec 20 11:39:10.383: INFO: Got endpoints: latency-svc-d66k2 [759.044988ms]
Dec 20 11:39:10.425: INFO: Got endpoints: latency-svc-nrs6j [755.502973ms]
Dec 20 11:39:10.464: INFO: Got endpoints: latency-svc-5szd7 [750.049725ms]
Dec 20 11:39:10.530: INFO: Got endpoints: latency-svc-9bfd2 [764.857013ms]
Dec 20 11:39:10.541: INFO: Created: latency-svc-w86sp
Dec 20 11:39:10.554: INFO: Created: latency-svc-hsglb
Dec 20 11:39:10.568: INFO: Created: latency-svc-npcbh
Dec 20 11:39:10.615: INFO: Got endpoints: latency-svc-7nd85 [685.139423ms]
Dec 20 11:39:10.673: INFO: Got endpoints: latency-svc-22wpp [745.274638ms]
Dec 20 11:39:10.689: INFO: Got endpoints: latency-svc-wtchh [872.956583ms]
Dec 20 11:39:10.767: INFO: Got endpoints: latency-svc-vb629 [754.148396ms]
Dec 20 11:39:10.814: INFO: Got endpoints: latency-svc-5k7lr [739.091291ms]
Dec 20 11:39:10.816: INFO: Got endpoints: latency-svc-9vk72 [835.042654ms]
Dec 20 11:39:10.919: INFO: Got endpoints: latency-svc-mp7mf [754.048994ms]
Dec 20 11:39:10.945: INFO: Got endpoints: latency-svc-lpkdt [824.651849ms]
Dec 20 11:39:10.969: INFO: Got endpoints: latency-svc-gcb75 [749.452565ms]
Dec 20 11:39:11.064: INFO: Got endpoints: latency-svc-w86sp [743.16775ms]
Dec 20 11:39:11.073: INFO: Created: latency-svc-d8vps
Dec 20 11:39:11.088: INFO: Created: latency-svc-fsxxq
Dec 20 11:39:11.114: INFO: Got endpoints: latency-svc-hsglb [730.231352ms]
Dec 20 11:39:11.172: INFO: Got endpoints: latency-svc-dnf25 [897.172454ms]
Dec 20 11:39:11.218: INFO: Got endpoints: latency-svc-d8vps [753.319316ms]
Dec 20 11:39:11.246: INFO: Got endpoints: latency-svc-npcbh [821.117077ms]
Dec 20 11:39:11.267: INFO: Got endpoints: latency-svc-fsxxq [736.302141ms]
Dec 20 11:39:11.267: INFO: Latencies: [49.680144ms 75.011988ms 126.386825ms 163.48442ms 166.319372ms 190.364885ms 214.419102ms 230.268746ms 241.747306ms 288.645282ms 298.081679ms 305.88107ms 313.505693ms 315.303938ms 317.246737ms 319.854441ms 320.997372ms 321.769422ms 323.021617ms 338.656194ms 342.010178ms 349.241402ms 350.107376ms 351.325212ms 352.658164ms 353.423966ms 354.518569ms 366.05534ms 367.524348ms 368.682919ms 374.29314ms 379.463018ms 382.205075ms 385.934029ms 389.329394ms 393.97454ms 397.587721ms 397.92221ms 401.720943ms 403.260734ms 409.370105ms 410.734349ms 413.056438ms 425.425632ms 428.675921ms 432.987108ms 433.05134ms 433.351822ms 440.016635ms 440.039069ms 440.640393ms 442.477642ms 444.585308ms 455.369576ms 476.104632ms 479.64173ms 480.924381ms 487.561981ms 490.094802ms 492.133718ms 493.416842ms 494.824377ms 504.668783ms 507.593404ms 508.399741ms 511.938125ms 522.104046ms 523.138133ms 528.88093ms 533.604942ms 561.906525ms 579.711737ms 615.555327ms 631.466784ms 637.895276ms 647.747083ms 675.997725ms 685.139423ms 686.414851ms 689.750742ms 716.683043ms 718.425321ms 729.888293ms 730.231352ms 730.800268ms 732.303686ms 732.846906ms 733.362206ms 733.408629ms 733.659449ms 734.310153ms 735.34108ms 736.122064ms 736.127193ms 736.302141ms 737.377384ms 737.84802ms 738.848971ms 738.936592ms 739.091291ms 739.691514ms 739.817625ms 740.043289ms 740.878157ms 741.050746ms 741.418031ms 742.009938ms 742.173878ms 742.192733ms 742.75471ms 742.952702ms 743.16775ms 743.212069ms 743.694505ms 743.994722ms 744.735335ms 744.913177ms 744.949387ms 745.105653ms 745.143428ms 745.273305ms 745.274638ms 745.504174ms 745.68925ms 745.858216ms 746.079642ms 746.876913ms 747.082201ms 747.235285ms 747.696374ms 748.047322ms 748.301381ms 748.585545ms 748.602646ms 748.775033ms 749.06367ms 749.140975ms 749.35083ms 749.378884ms 749.452565ms 749.456103ms 749.861248ms 750.049725ms 750.216352ms 750.460148ms 751.616097ms 751.642231ms 752.356914ms 752.509788ms 752.54854ms 752.749343ms 753.075538ms 753.319316ms 753.454529ms 753.640956ms 753.697842ms 753.754199ms 754.048994ms 754.148396ms 754.396044ms 754.647478ms 755.189474ms 755.502973ms 755.723311ms 755.759401ms 755.868515ms 756.348099ms 756.42254ms 756.551426ms 756.83992ms 756.868393ms 756.975811ms 757.750423ms 757.963435ms 758.231683ms 759.044988ms 759.111681ms 759.546418ms 759.631779ms 759.69596ms 759.86716ms 759.933514ms 760.114808ms 760.825623ms 761.175346ms 762.926609ms 763.376253ms 764.045749ms 764.857013ms 765.311458ms 777.088338ms 777.568327ms 787.747056ms 799.346455ms 806.847273ms 821.117077ms 824.651849ms 835.042654ms 872.956583ms 897.172454ms]
Dec 20 11:39:11.267: INFO: 50 %ile: 739.691514ms
Dec 20 11:39:11.267: INFO: 90 %ile: 759.86716ms
Dec 20 11:39:11.267: INFO: 99 %ile: 872.956583ms
Dec 20 11:39:11.267: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:39:11.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-760" for this suite.
Dec 20 11:39:39.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:39:39.765: INFO: namespace svc-latency-760 deletion completed in 28.488477603s

• [SLOW TEST:43.486 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:39:39.768: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1619
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:39:44.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1619" for this suite.
Dec 20 11:39:58.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:39:58.340: INFO: namespace containers-1619 deletion completed in 14.290592669s

• [SLOW TEST:18.573 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:39:58.343: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-410
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1220 11:40:08.833849      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 20 11:40:08.834: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:40:08.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-410" for this suite.
Dec 20 11:40:23.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:40:23.806: INFO: namespace gc-410 deletion completed in 14.96357847s

• [SLOW TEST:25.464 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:40:23.807: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8843
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-4ffb4853-9491-400f-8484-3c6c6e21c81d
Dec 20 11:40:24.022: INFO: Pod name my-hostname-basic-4ffb4853-9491-400f-8484-3c6c6e21c81d: Found 0 pods out of 1
Dec 20 11:40:29.031: INFO: Pod name my-hostname-basic-4ffb4853-9491-400f-8484-3c6c6e21c81d: Found 1 pods out of 1
Dec 20 11:40:29.031: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-4ffb4853-9491-400f-8484-3c6c6e21c81d" are running
Dec 20 11:40:35.045: INFO: Pod "my-hostname-basic-4ffb4853-9491-400f-8484-3c6c6e21c81d-nx2lk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-20 11:40:24 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-20 11:40:24 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-4ffb4853-9491-400f-8484-3c6c6e21c81d]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-20 11:40:24 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-4ffb4853-9491-400f-8484-3c6c6e21c81d]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-20 11:40:24 +0000 UTC Reason: Message:}])
Dec 20 11:40:35.046: INFO: Trying to dial the pod
Dec 20 11:40:40.467: INFO: Controller my-hostname-basic-4ffb4853-9491-400f-8484-3c6c6e21c81d: Got expected result from replica 1 [my-hostname-basic-4ffb4853-9491-400f-8484-3c6c6e21c81d-nx2lk]: "my-hostname-basic-4ffb4853-9491-400f-8484-3c6c6e21c81d-nx2lk", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:40:40.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8843" for this suite.
Dec 20 11:40:50.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:40:51.222: INFO: namespace replication-controller-8843 deletion completed in 10.746657457s

• [SLOW TEST:27.415 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:40:51.223: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-3492
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 20 11:40:52.521: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Dec 20 11:40:54.542: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438852, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438852, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438852, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712438852, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 20 11:40:57.573: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 11:40:57.581: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:40:59.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3492" for this suite.
Dec 20 11:41:07.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:41:07.672: INFO: namespace crd-webhook-3492 deletion completed in 8.31550055s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:16.482 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:41:07.705: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4935
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 20 11:41:07.891: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 20 11:41:07.915: INFO: Waiting for terminating namespaces to be deleted...
Dec 20 11:41:07.921: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-457fd-578f899757-428w5 before test
Dec 20 11:41:07.945: INFO: node-local-dns-kj7kj from kube-system started at 2019-12-20 10:26:58 +0000 UTC (1 container statuses recorded)
Dec 20 11:41:07.945: INFO: 	Container node-cache ready: true, restart count 0
Dec 20 11:41:07.945: INFO: node-exporter-6fmjz from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 11:41:07.945: INFO: 	Container node-exporter ready: true, restart count 0
Dec 20 11:41:07.945: INFO: canal-64h7p from kube-system started at 2019-12-20 10:27:04 +0000 UTC (2 container statuses recorded)
Dec 20 11:41:07.945: INFO: 	Container calico-node ready: true, restart count 0
Dec 20 11:41:07.945: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 20 11:41:07.945: INFO: sonobuoy-systemd-logs-daemon-set-73919c9b23754985-gvq9z from sonobuoy started at 2019-12-20 10:42:00 +0000 UTC (2 container statuses recorded)
Dec 20 11:41:07.945: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 20 11:41:07.945: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 20 11:41:07.945: INFO: kube-proxy-nfqd6 from kube-system started at 2019-12-20 10:26:58 +0000 UTC (1 container statuses recorded)
Dec 20 11:41:07.945: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 20 11:41:07.945: INFO: sonobuoy from sonobuoy started at 2019-12-20 10:41:51 +0000 UTC (1 container statuses recorded)
Dec 20 11:41:07.946: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 20 11:41:07.946: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-457fd-578f899757-dsqmn before test
Dec 20 11:41:08.005: INFO: tiller-deploy-78f78bb476-6t2mk from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 11:41:08.005: INFO: 	Container tiller ready: true, restart count 0
Dec 20 11:41:08.005: INFO: kube-proxy-wq825 from kube-system started at 2019-12-20 10:26:26 +0000 UTC (1 container statuses recorded)
Dec 20 11:41:08.005: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 20 11:41:08.005: INFO: canal-gcrnc from kube-system started at 2019-12-20 10:26:27 +0000 UTC (2 container statuses recorded)
Dec 20 11:41:08.005: INFO: 	Container calico-node ready: true, restart count 0
Dec 20 11:41:08.005: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 20 11:41:08.005: INFO: openvpn-client-64df8b95c9-wr7dw from kube-system started at 2019-12-20 10:27:01 +0000 UTC (2 container statuses recorded)
Dec 20 11:41:08.006: INFO: 	Container dnat-controller ready: true, restart count 0
Dec 20 11:41:08.006: INFO: 	Container openvpn-client ready: true, restart count 0
Dec 20 11:41:08.006: INFO: coredns-547f89d7d5-5w7wq from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 11:41:08.006: INFO: 	Container coredns ready: true, restart count 0
Dec 20 11:41:08.007: INFO: node-local-dns-7tvth from kube-system started at 2019-12-20 10:26:26 +0000 UTC (1 container statuses recorded)
Dec 20 11:41:08.007: INFO: 	Container node-cache ready: true, restart count 0
Dec 20 11:41:08.007: INFO: node-exporter-qnl6p from kube-system started at 2019-12-20 10:26:26 +0000 UTC (1 container statuses recorded)
Dec 20 11:41:08.007: INFO: 	Container node-exporter ready: true, restart count 0
Dec 20 11:41:08.007: INFO: coredns-547f89d7d5-s26jq from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 11:41:08.007: INFO: 	Container coredns ready: true, restart count 0
Dec 20 11:41:08.008: INFO: cluster-autoscaler-8c65c7d54-9cqhv from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 11:41:08.008: INFO: 	Container cluster-autoscaler ready: true, restart count 0
Dec 20 11:41:08.008: INFO: sonobuoy-systemd-logs-daemon-set-73919c9b23754985-dp62g from sonobuoy started at 2019-12-20 10:42:00 +0000 UTC (2 container statuses recorded)
Dec 20 11:41:08.008: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 20 11:41:08.008: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 20 11:41:08.008: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-457fd-578f899757-zpxc2 before test
Dec 20 11:41:08.063: INFO: node-exporter-wxf2r from kube-system started at 2019-12-20 10:27:10 +0000 UTC (1 container statuses recorded)
Dec 20 11:41:08.063: INFO: 	Container node-exporter ready: true, restart count 0
Dec 20 11:41:08.063: INFO: sonobuoy-e2e-job-1e71c3c7847b472c from sonobuoy started at 2019-12-20 10:42:00 +0000 UTC (2 container statuses recorded)
Dec 20 11:41:08.063: INFO: 	Container e2e ready: true, restart count 0
Dec 20 11:41:08.063: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 20 11:41:08.063: INFO: node-local-dns-4wkt5 from kube-system started at 2019-12-20 10:27:10 +0000 UTC (1 container statuses recorded)
Dec 20 11:41:08.063: INFO: 	Container node-cache ready: true, restart count 0
Dec 20 11:41:08.063: INFO: canal-w56q4 from kube-system started at 2019-12-20 10:27:10 +0000 UTC (2 container statuses recorded)
Dec 20 11:41:08.063: INFO: 	Container calico-node ready: true, restart count 0
Dec 20 11:41:08.063: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 20 11:41:08.063: INFO: sonobuoy-systemd-logs-daemon-set-73919c9b23754985-gwhsz from sonobuoy started at 2019-12-20 10:42:00 +0000 UTC (2 container statuses recorded)
Dec 20 11:41:08.063: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 20 11:41:08.063: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 20 11:41:08.063: INFO: kube-proxy-pml2l from kube-system started at 2019-12-20 10:27:10 +0000 UTC (1 container statuses recorded)
Dec 20 11:41:08.063: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-df425525-fcff-4346-a8ae-53375a88ff4b 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-df425525-fcff-4346-a8ae-53375a88ff4b off the node metakube-worker-457fd-578f899757-zpxc2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-df425525-fcff-4346-a8ae-53375a88ff4b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:41:32.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4935" for this suite.
Dec 20 11:41:54.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:41:54.647: INFO: namespace sched-pred-4935 deletion completed in 22.293309092s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:46.944 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:41:54.656: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3762
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-3762
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 20 11:41:54.874: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 20 11:42:23.212: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.0.27 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3762 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 20 11:42:23.213: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 11:42:27.309: INFO: Found all expected endpoints: [netserver-0]
Dec 20 11:42:27.318: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.1.113 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3762 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 20 11:42:27.318: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 11:42:29.815: INFO: Found all expected endpoints: [netserver-1]
Dec 20 11:42:29.931: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.2.77 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3762 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 20 11:42:29.931: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 11:42:31.464: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:42:31.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3762" for this suite.
Dec 20 11:42:43.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:42:43.758: INFO: namespace pod-network-test-3762 deletion completed in 12.285177099s

• [SLOW TEST:49.103 seconds]
[sig-network] Networking
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:42:43.763: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7003
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 20 11:42:48.037: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:42:48.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7003" for this suite.
Dec 20 11:42:56.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:42:56.569: INFO: namespace container-runtime-7003 deletion completed in 8.479131057s

• [SLOW TEST:12.806 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:42:56.569: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1350
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-1350
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Dec 20 11:42:57.070: INFO: Found 0 stateful pods, waiting for 3
Dec 20 11:43:07.078: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 20 11:43:07.078: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 20 11:43:07.078: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec 20 11:43:07.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-1350 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 20 11:43:07.810: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 20 11:43:07.810: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 20 11:43:07.810: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Dec 20 11:43:17.881: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec 20 11:43:27.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-1350 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 11:43:30.369: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 20 11:43:30.370: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 20 11:43:30.370: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 20 11:43:40.411: INFO: Waiting for StatefulSet statefulset-1350/ss2 to complete update
Dec 20 11:43:40.411: INFO: Waiting for Pod statefulset-1350/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 20 11:43:40.411: INFO: Waiting for Pod statefulset-1350/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 20 11:43:50.429: INFO: Waiting for StatefulSet statefulset-1350/ss2 to complete update
Dec 20 11:43:50.429: INFO: Waiting for Pod statefulset-1350/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 20 11:44:00.425: INFO: Waiting for StatefulSet statefulset-1350/ss2 to complete update
Dec 20 11:44:00.425: INFO: Waiting for Pod statefulset-1350/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 20 11:44:10.428: INFO: Waiting for StatefulSet statefulset-1350/ss2 to complete update
Dec 20 11:44:20.427: INFO: Waiting for StatefulSet statefulset-1350/ss2 to complete update
STEP: Rolling back to a previous revision
Dec 20 11:44:30.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-1350 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 20 11:44:31.084: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 20 11:44:31.084: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 20 11:44:31.084: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 20 11:44:41.151: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec 20 11:44:51.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-1350 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 11:44:51.938: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 20 11:44:51.938: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 20 11:44:51.938: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 20 11:45:11.990: INFO: Waiting for StatefulSet statefulset-1350/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 20 11:45:22.024: INFO: Deleting all statefulset in ns statefulset-1350
Dec 20 11:45:22.153: INFO: Scaling statefulset ss2 to 0
Dec 20 11:45:42.199: INFO: Waiting for statefulset status.replicas updated to 0
Dec 20 11:45:42.208: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:45:42.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1350" for this suite.
Dec 20 11:45:50.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:45:50.618: INFO: namespace statefulset-1350 deletion completed in 8.358990849s

• [SLOW TEST:174.049 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:45:50.625: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4993
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 11:45:50.840: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 20 11:45:54.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 --namespace=crd-publish-openapi-4993 create -f -'
Dec 20 11:46:01.617: INFO: stderr: ""
Dec 20 11:46:01.617: INFO: stdout: "e2e-test-crd-publish-openapi-6619-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 20 11:46:01.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 --namespace=crd-publish-openapi-4993 delete e2e-test-crd-publish-openapi-6619-crds test-cr'
Dec 20 11:46:01.859: INFO: stderr: ""
Dec 20 11:46:01.859: INFO: stdout: "e2e-test-crd-publish-openapi-6619-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Dec 20 11:46:01.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 --namespace=crd-publish-openapi-4993 apply -f -'
Dec 20 11:46:02.331: INFO: stderr: ""
Dec 20 11:46:02.331: INFO: stdout: "e2e-test-crd-publish-openapi-6619-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 20 11:46:02.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 --namespace=crd-publish-openapi-4993 delete e2e-test-crd-publish-openapi-6619-crds test-cr'
Dec 20 11:46:02.669: INFO: stderr: ""
Dec 20 11:46:02.669: INFO: stdout: "e2e-test-crd-publish-openapi-6619-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 20 11:46:02.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 explain e2e-test-crd-publish-openapi-6619-crds'
Dec 20 11:46:03.107: INFO: stderr: ""
Dec 20 11:46:03.107: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6619-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:46:06.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4993" for this suite.
Dec 20 11:46:12.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:46:13.243: INFO: namespace crd-publish-openapi-4993 deletion completed in 6.311165207s

• [SLOW TEST:22.619 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:46:13.245: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-359
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 20 11:46:14.241: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 20 11:46:16.271: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712439174, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712439174, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712439174, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712439174, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 20 11:46:19.356: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 11:46:21.786: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6234-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:46:24.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-359" for this suite.
Dec 20 11:46:30.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:46:31.023: INFO: namespace webhook-359 deletion completed in 6.27917801s
STEP: Destroying namespace "webhook-359-markers" for this suite.
Dec 20 11:46:37.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:46:37.312: INFO: namespace webhook-359-markers deletion completed in 6.288259005s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:24.104 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:46:37.351: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2913
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 11:46:37.595: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:46:44.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2913" for this suite.
Dec 20 11:46:52.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:46:52.734: INFO: namespace custom-resource-definition-2913 deletion completed in 8.291958207s

• [SLOW TEST:15.383 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:46:52.738: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7190
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7190.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7190.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 20 11:47:02.068: INFO: DNS probes using dns-7190/dns-test-063a5e5a-7769-4d4d-8b05-ea2f432a0c49 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:47:02.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7190" for this suite.
Dec 20 11:47:09.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:47:09.314: INFO: namespace dns-7190 deletion completed in 6.33180978s

• [SLOW TEST:16.576 seconds]
[sig-network] DNS
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:47:09.316: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-901
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-901.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-901.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-901.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-901.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-901.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-901.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-901.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-901.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-901.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-901.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-901.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-901.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-901.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 61.20.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.20.61_udp@PTR;check="$$(dig +tcp +noall +answer +search 61.20.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.20.61_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-901.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-901.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-901.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-901.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-901.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-901.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-901.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-901.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-901.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-901.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-901.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-901.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-901.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 61.20.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.20.61_udp@PTR;check="$$(dig +tcp +noall +answer +search 61.20.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.20.61_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 20 11:47:19.747: INFO: Unable to read wheezy_udp@dns-test-service.dns-901.svc.cluster.local from pod dns-901/dns-test-274d02a1-b276-4339-b517-a14eb0b31032: the server could not find the requested resource (get pods dns-test-274d02a1-b276-4339-b517-a14eb0b31032)
Dec 20 11:47:19.793: INFO: Unable to read wheezy_tcp@dns-test-service.dns-901.svc.cluster.local from pod dns-901/dns-test-274d02a1-b276-4339-b517-a14eb0b31032: the server could not find the requested resource (get pods dns-test-274d02a1-b276-4339-b517-a14eb0b31032)
Dec 20 11:47:19.806: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-901.svc.cluster.local from pod dns-901/dns-test-274d02a1-b276-4339-b517-a14eb0b31032: the server could not find the requested resource (get pods dns-test-274d02a1-b276-4339-b517-a14eb0b31032)
Dec 20 11:47:19.819: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-901.svc.cluster.local from pod dns-901/dns-test-274d02a1-b276-4339-b517-a14eb0b31032: the server could not find the requested resource (get pods dns-test-274d02a1-b276-4339-b517-a14eb0b31032)
Dec 20 11:47:20.257: INFO: Unable to read jessie_udp@dns-test-service.dns-901.svc.cluster.local from pod dns-901/dns-test-274d02a1-b276-4339-b517-a14eb0b31032: the server could not find the requested resource (get pods dns-test-274d02a1-b276-4339-b517-a14eb0b31032)
Dec 20 11:47:20.269: INFO: Unable to read jessie_tcp@dns-test-service.dns-901.svc.cluster.local from pod dns-901/dns-test-274d02a1-b276-4339-b517-a14eb0b31032: the server could not find the requested resource (get pods dns-test-274d02a1-b276-4339-b517-a14eb0b31032)
Dec 20 11:47:20.281: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-901.svc.cluster.local from pod dns-901/dns-test-274d02a1-b276-4339-b517-a14eb0b31032: the server could not find the requested resource (get pods dns-test-274d02a1-b276-4339-b517-a14eb0b31032)
Dec 20 11:47:20.294: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-901.svc.cluster.local from pod dns-901/dns-test-274d02a1-b276-4339-b517-a14eb0b31032: the server could not find the requested resource (get pods dns-test-274d02a1-b276-4339-b517-a14eb0b31032)
Dec 20 11:47:20.683: INFO: Lookups using dns-901/dns-test-274d02a1-b276-4339-b517-a14eb0b31032 failed for: [wheezy_udp@dns-test-service.dns-901.svc.cluster.local wheezy_tcp@dns-test-service.dns-901.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-901.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-901.svc.cluster.local jessie_udp@dns-test-service.dns-901.svc.cluster.local jessie_tcp@dns-test-service.dns-901.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-901.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-901.svc.cluster.local]

Dec 20 11:47:27.629: INFO: DNS probes using dns-901/dns-test-274d02a1-b276-4339-b517-a14eb0b31032 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:47:29.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-901" for this suite.
Dec 20 11:47:35.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:47:35.993: INFO: namespace dns-901 deletion completed in 6.432443713s

• [SLOW TEST:26.678 seconds]
[sig-network] DNS
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:47:35.999: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2644
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Dec 20 11:47:36.261: INFO: namespace kubectl-2644
Dec 20 11:47:36.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 create -f - --namespace=kubectl-2644'
Dec 20 11:47:36.627: INFO: stderr: ""
Dec 20 11:47:36.627: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 20 11:47:37.635: INFO: Selector matched 1 pods for map[app:redis]
Dec 20 11:47:37.635: INFO: Found 0 / 1
Dec 20 11:47:38.635: INFO: Selector matched 1 pods for map[app:redis]
Dec 20 11:47:38.635: INFO: Found 0 / 1
Dec 20 11:47:39.635: INFO: Selector matched 1 pods for map[app:redis]
Dec 20 11:47:39.635: INFO: Found 0 / 1
Dec 20 11:47:40.635: INFO: Selector matched 1 pods for map[app:redis]
Dec 20 11:47:40.635: INFO: Found 0 / 1
Dec 20 11:47:41.635: INFO: Selector matched 1 pods for map[app:redis]
Dec 20 11:47:41.635: INFO: Found 1 / 1
Dec 20 11:47:41.635: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 20 11:47:41.640: INFO: Selector matched 1 pods for map[app:redis]
Dec 20 11:47:41.641: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 20 11:47:41.641: INFO: wait on redis-master startup in kubectl-2644 
Dec 20 11:47:41.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 logs redis-master-764xc redis-master --namespace=kubectl-2644'
Dec 20 11:47:41.907: INFO: stderr: ""
Dec 20 11:47:41.907: INFO: stdout: "1:C 20 Dec 2019 11:47:41.307 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 20 Dec 2019 11:47:41.307 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 20 Dec 2019 11:47:41.307 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 20 Dec 2019 11:47:41.309 * Running mode=standalone, port=6379.\n1:M 20 Dec 2019 11:47:41.309 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 20 Dec 2019 11:47:41.309 # Server initialized\n1:M 20 Dec 2019 11:47:41.309 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 20 Dec 2019 11:47:41.309 * Ready to accept connections\n"
STEP: exposing RC
Dec 20 11:47:41.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-2644'
Dec 20 11:47:42.112: INFO: stderr: ""
Dec 20 11:47:42.112: INFO: stdout: "service/rm2 exposed\n"
Dec 20 11:47:42.120: INFO: Service rm2 in namespace kubectl-2644 found.
STEP: exposing service
Dec 20 11:47:44.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-2644'
Dec 20 11:47:44.287: INFO: stderr: ""
Dec 20 11:47:44.287: INFO: stdout: "service/rm3 exposed\n"
Dec 20 11:47:44.293: INFO: Service rm3 in namespace kubectl-2644 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:47:46.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2644" for this suite.
Dec 20 11:47:58.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:47:58.584: INFO: namespace kubectl-2644 deletion completed in 12.270069893s

• [SLOW TEST:22.586 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:47:58.591: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9682
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9682.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9682.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9682.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9682.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9682.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9682.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 20 11:48:05.533: INFO: DNS probes using dns-9682/dns-test-fb18bb6f-a843-4622-9079-01c8b8bc557e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:48:05.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9682" for this suite.
Dec 20 11:48:11.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:48:11.891: INFO: namespace dns-9682 deletion completed in 6.280591386s

• [SLOW TEST:13.301 seconds]
[sig-network] DNS
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:48:11.892: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6439
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 11:48:12.282: INFO: Waiting up to 5m0s for pod "busybox-user-65534-56e496eb-02c7-4437-819f-71b424a257dc" in namespace "security-context-test-6439" to be "success or failure"
Dec 20 11:48:12.291: INFO: Pod "busybox-user-65534-56e496eb-02c7-4437-819f-71b424a257dc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.28277ms
Dec 20 11:48:14.301: INFO: Pod "busybox-user-65534-56e496eb-02c7-4437-819f-71b424a257dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019119963s
Dec 20 11:48:16.310: INFO: Pod "busybox-user-65534-56e496eb-02c7-4437-819f-71b424a257dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027969581s
Dec 20 11:48:16.310: INFO: Pod "busybox-user-65534-56e496eb-02c7-4437-819f-71b424a257dc" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:48:16.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6439" for this suite.
Dec 20 11:48:24.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:48:24.595: INFO: namespace security-context-test-6439 deletion completed in 8.273740182s

• [SLOW TEST:12.703 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:48:24.597: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4156
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:49:24.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4156" for this suite.
Dec 20 11:49:36.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:49:37.304: INFO: namespace container-probe-4156 deletion completed in 12.446805466s

• [SLOW TEST:72.708 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:49:37.308: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1326
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 11:49:37.818: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-89b59f97-9f40-4202-952b-aac218cabe2f" in namespace "security-context-test-1326" to be "success or failure"
Dec 20 11:49:37.853: INFO: Pod "busybox-readonly-false-89b59f97-9f40-4202-952b-aac218cabe2f": Phase="Pending", Reason="", readiness=false. Elapsed: 34.601488ms
Dec 20 11:49:39.862: INFO: Pod "busybox-readonly-false-89b59f97-9f40-4202-952b-aac218cabe2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043250199s
Dec 20 11:49:41.868: INFO: Pod "busybox-readonly-false-89b59f97-9f40-4202-952b-aac218cabe2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050095288s
Dec 20 11:49:41.869: INFO: Pod "busybox-readonly-false-89b59f97-9f40-4202-952b-aac218cabe2f" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:49:41.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1326" for this suite.
Dec 20 11:49:47.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:49:48.172: INFO: namespace security-context-test-1326 deletion completed in 6.293816789s

• [SLOW TEST:10.864 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:49:48.177: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9527
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec 20 11:49:48.407: INFO: Pod name pod-release: Found 0 pods out of 1
Dec 20 11:49:53.414: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:49:54.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9527" for this suite.
Dec 20 11:50:00.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:50:00.798: INFO: namespace replication-controller-9527 deletion completed in 6.318565956s

• [SLOW TEST:12.622 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:50:00.800: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4067
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 20 11:50:01.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-4067'
Dec 20 11:50:01.165: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 20 11:50:01.165: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Dec 20 11:50:01.178: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Dec 20 11:50:01.191: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Dec 20 11:50:01.215: INFO: scanned /root for discovery docs: <nil>
Dec 20 11:50:01.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-4067'
Dec 20 11:50:17.220: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec 20 11:50:17.220: INFO: stdout: "Created e2e-test-httpd-rc-06917d622dad46e6cd7f844bbf8853b9\nScaling up e2e-test-httpd-rc-06917d622dad46e6cd7f844bbf8853b9 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-06917d622dad46e6cd7f844bbf8853b9 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-06917d622dad46e6cd7f844bbf8853b9 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Dec 20 11:50:17.220: INFO: stdout: "Created e2e-test-httpd-rc-06917d622dad46e6cd7f844bbf8853b9\nScaling up e2e-test-httpd-rc-06917d622dad46e6cd7f844bbf8853b9 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-06917d622dad46e6cd7f844bbf8853b9 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-06917d622dad46e6cd7f844bbf8853b9 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Dec 20 11:50:17.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-4067'
Dec 20 11:50:17.357: INFO: stderr: ""
Dec 20 11:50:17.357: INFO: stdout: "e2e-test-httpd-rc-06917d622dad46e6cd7f844bbf8853b9-45jkc "
Dec 20 11:50:17.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods e2e-test-httpd-rc-06917d622dad46e6cd7f844bbf8853b9-45jkc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4067'
Dec 20 11:50:17.476: INFO: stderr: ""
Dec 20 11:50:17.476: INFO: stdout: "true"
Dec 20 11:50:17.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods e2e-test-httpd-rc-06917d622dad46e6cd7f844bbf8853b9-45jkc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4067'
Dec 20 11:50:17.590: INFO: stderr: ""
Dec 20 11:50:17.590: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Dec 20 11:50:17.590: INFO: e2e-test-httpd-rc-06917d622dad46e6cd7f844bbf8853b9-45jkc is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Dec 20 11:50:17.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 delete rc e2e-test-httpd-rc --namespace=kubectl-4067'
Dec 20 11:50:17.716: INFO: stderr: ""
Dec 20 11:50:17.716: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:50:17.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4067" for this suite.
Dec 20 11:50:32.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:50:33.184: INFO: namespace kubectl-4067 deletion completed in 15.434866553s

• [SLOW TEST:32.383 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:50:33.187: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8522
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 11:50:33.412: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:50:34.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8522" for this suite.
Dec 20 11:50:40.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:50:41.082: INFO: namespace custom-resource-definition-8522 deletion completed in 6.496530958s

• [SLOW TEST:7.895 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:50:41.082: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2713
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-f543460e-f134-4271-9e1c-68613cb1b48d
STEP: Creating a pod to test consume secrets
Dec 20 11:50:41.597: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e83c868e-aa5c-4bbe-904a-6b84f8ccde4c" in namespace "projected-2713" to be "success or failure"
Dec 20 11:50:41.603: INFO: Pod "pod-projected-secrets-e83c868e-aa5c-4bbe-904a-6b84f8ccde4c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.617551ms
Dec 20 11:50:43.613: INFO: Pod "pod-projected-secrets-e83c868e-aa5c-4bbe-904a-6b84f8ccde4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016157389s
Dec 20 11:50:45.622: INFO: Pod "pod-projected-secrets-e83c868e-aa5c-4bbe-904a-6b84f8ccde4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024893738s
STEP: Saw pod success
Dec 20 11:50:45.622: INFO: Pod "pod-projected-secrets-e83c868e-aa5c-4bbe-904a-6b84f8ccde4c" satisfied condition "success or failure"
Dec 20 11:50:45.628: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-projected-secrets-e83c868e-aa5c-4bbe-904a-6b84f8ccde4c container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 20 11:50:45.838: INFO: Waiting for pod pod-projected-secrets-e83c868e-aa5c-4bbe-904a-6b84f8ccde4c to disappear
Dec 20 11:50:45.843: INFO: Pod pod-projected-secrets-e83c868e-aa5c-4bbe-904a-6b84f8ccde4c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:50:45.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2713" for this suite.
Dec 20 11:50:55.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:50:56.141: INFO: namespace projected-2713 deletion completed in 10.288751529s

• [SLOW TEST:15.059 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:50:56.142: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4068
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-4068
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 20 11:50:56.358: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 20 11:51:22.613: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.2.86:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4068 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 20 11:51:22.614: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 11:51:23.144: INFO: Found all expected endpoints: [netserver-0]
Dec 20 11:51:23.150: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.0.31:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4068 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 20 11:51:23.150: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 11:51:23.712: INFO: Found all expected endpoints: [netserver-1]
Dec 20 11:51:23.720: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.1.127:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4068 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 20 11:51:23.720: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 11:51:24.196: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:51:24.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4068" for this suite.
Dec 20 11:51:42.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:51:42.505: INFO: namespace pod-network-test-4068 deletion completed in 18.293367077s

• [SLOW TEST:46.363 seconds]
[sig-network] Networking
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:51:42.507: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4937
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-999e4dbe-7f82-4fa5-b4c4-25251f8e8dcb
STEP: Creating a pod to test consume secrets
Dec 20 11:51:42.777: INFO: Waiting up to 5m0s for pod "pod-secrets-fe71e8d5-c818-44b5-8135-c7daf04a6892" in namespace "secrets-4937" to be "success or failure"
Dec 20 11:51:42.786: INFO: Pod "pod-secrets-fe71e8d5-c818-44b5-8135-c7daf04a6892": Phase="Pending", Reason="", readiness=false. Elapsed: 8.070839ms
Dec 20 11:51:44.799: INFO: Pod "pod-secrets-fe71e8d5-c818-44b5-8135-c7daf04a6892": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021667774s
Dec 20 11:51:46.807: INFO: Pod "pod-secrets-fe71e8d5-c818-44b5-8135-c7daf04a6892": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029154874s
STEP: Saw pod success
Dec 20 11:51:46.807: INFO: Pod "pod-secrets-fe71e8d5-c818-44b5-8135-c7daf04a6892" satisfied condition "success or failure"
Dec 20 11:51:46.814: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-secrets-fe71e8d5-c818-44b5-8135-c7daf04a6892 container secret-volume-test: <nil>
STEP: delete the pod
Dec 20 11:51:47.010: INFO: Waiting for pod pod-secrets-fe71e8d5-c818-44b5-8135-c7daf04a6892 to disappear
Dec 20 11:51:47.016: INFO: Pod pod-secrets-fe71e8d5-c818-44b5-8135-c7daf04a6892 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:51:47.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4937" for this suite.
Dec 20 11:51:57.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:51:57.553: INFO: namespace secrets-4937 deletion completed in 10.527907279s

• [SLOW TEST:15.047 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:51:57.554: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4366
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-c814b6b9-2f56-4462-85e8-6ee0a9d9700b
STEP: Creating a pod to test consume secrets
Dec 20 11:51:59.739: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f5b22144-44d6-4b6f-ae58-dbb8dda0a61e" in namespace "projected-4366" to be "success or failure"
Dec 20 11:51:59.917: INFO: Pod "pod-projected-secrets-f5b22144-44d6-4b6f-ae58-dbb8dda0a61e": Phase="Pending", Reason="", readiness=false. Elapsed: 177.711698ms
Dec 20 11:52:01.930: INFO: Pod "pod-projected-secrets-f5b22144-44d6-4b6f-ae58-dbb8dda0a61e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.190515649s
Dec 20 11:52:04.327: INFO: Pod "pod-projected-secrets-f5b22144-44d6-4b6f-ae58-dbb8dda0a61e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.587565211s
Dec 20 11:52:06.334: INFO: Pod "pod-projected-secrets-f5b22144-44d6-4b6f-ae58-dbb8dda0a61e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.594904964s
STEP: Saw pod success
Dec 20 11:52:06.334: INFO: Pod "pod-projected-secrets-f5b22144-44d6-4b6f-ae58-dbb8dda0a61e" satisfied condition "success or failure"
Dec 20 11:52:06.628: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-projected-secrets-f5b22144-44d6-4b6f-ae58-dbb8dda0a61e container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 20 11:52:07.062: INFO: Waiting for pod pod-projected-secrets-f5b22144-44d6-4b6f-ae58-dbb8dda0a61e to disappear
Dec 20 11:52:07.260: INFO: Pod pod-projected-secrets-f5b22144-44d6-4b6f-ae58-dbb8dda0a61e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:52:07.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4366" for this suite.
Dec 20 11:52:13.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:52:13.604: INFO: namespace projected-4366 deletion completed in 6.326426097s

• [SLOW TEST:16.051 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:52:13.612: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2828
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-2828
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-2828
Dec 20 11:52:13.895: INFO: Found 0 stateful pods, waiting for 1
Dec 20 11:52:23.906: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 20 11:52:23.955: INFO: Deleting all statefulset in ns statefulset-2828
Dec 20 11:52:23.961: INFO: Scaling statefulset ss to 0
Dec 20 11:52:44.066: INFO: Waiting for statefulset status.replicas updated to 0
Dec 20 11:52:44.076: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:52:44.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2828" for this suite.
Dec 20 11:52:50.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:52:50.426: INFO: namespace statefulset-2828 deletion completed in 6.289699457s

• [SLOW TEST:36.815 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:52:50.430: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4341
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4341
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4341
STEP: creating replication controller externalsvc in namespace services-4341
I1220 11:52:50.780529      20 runners.go:184] Created replication controller with name: externalsvc, namespace: services-4341, replica count: 2
I1220 11:52:53.831100      20 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Dec 20 11:52:53.899: INFO: Creating new exec pod
Dec 20 11:52:57.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=services-4341 execpod7vtc2 -- /bin/sh -x -c nslookup nodeport-service'
Dec 20 11:52:58.640: INFO: stderr: "+ nslookup nodeport-service\n"
Dec 20 11:52:58.640: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nnodeport-service.services-4341.svc.cluster.local\tcanonical name = externalsvc.services-4341.svc.cluster.local.\nName:\texternalsvc.services-4341.svc.cluster.local\nAddress: 10.240.18.16\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4341, will wait for the garbage collector to delete the pods
Dec 20 11:52:58.725: INFO: Deleting ReplicationController externalsvc took: 25.361642ms
Dec 20 11:52:58.825: INFO: Terminating ReplicationController externalsvc pods took: 100.329488ms
Dec 20 11:53:03.988: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:53:04.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4341" for this suite.
Dec 20 11:53:10.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:53:10.305: INFO: namespace services-4341 deletion completed in 6.249342697s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:19.875 seconds]
[sig-network] Services
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:53:10.306: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8621
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 20 11:53:10.523: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 20 11:53:10.556: INFO: Waiting for terminating namespaces to be deleted...
Dec 20 11:53:10.565: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-457fd-578f899757-428w5 before test
Dec 20 11:53:10.593: INFO: node-exporter-6fmjz from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 11:53:10.593: INFO: 	Container node-exporter ready: true, restart count 0
Dec 20 11:53:10.593: INFO: canal-64h7p from kube-system started at 2019-12-20 10:27:04 +0000 UTC (2 container statuses recorded)
Dec 20 11:53:10.593: INFO: 	Container calico-node ready: true, restart count 0
Dec 20 11:53:10.594: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 20 11:53:10.594: INFO: sonobuoy-systemd-logs-daemon-set-73919c9b23754985-gvq9z from sonobuoy started at 2019-12-20 10:42:00 +0000 UTC (2 container statuses recorded)
Dec 20 11:53:10.594: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 20 11:53:10.594: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 20 11:53:10.594: INFO: kube-proxy-nfqd6 from kube-system started at 2019-12-20 10:26:58 +0000 UTC (1 container statuses recorded)
Dec 20 11:53:10.594: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 20 11:53:10.595: INFO: sonobuoy from sonobuoy started at 2019-12-20 10:41:51 +0000 UTC (1 container statuses recorded)
Dec 20 11:53:10.595: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 20 11:53:10.595: INFO: node-local-dns-kj7kj from kube-system started at 2019-12-20 10:26:58 +0000 UTC (1 container statuses recorded)
Dec 20 11:53:10.595: INFO: 	Container node-cache ready: true, restart count 0
Dec 20 11:53:10.595: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-457fd-578f899757-dsqmn before test
Dec 20 11:53:10.752: INFO: tiller-deploy-78f78bb476-6t2mk from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 11:53:10.752: INFO: 	Container tiller ready: true, restart count 0
Dec 20 11:53:10.752: INFO: kube-proxy-wq825 from kube-system started at 2019-12-20 10:26:26 +0000 UTC (1 container statuses recorded)
Dec 20 11:53:10.752: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 20 11:53:10.752: INFO: canal-gcrnc from kube-system started at 2019-12-20 10:26:27 +0000 UTC (2 container statuses recorded)
Dec 20 11:53:10.752: INFO: 	Container calico-node ready: true, restart count 0
Dec 20 11:53:10.752: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 20 11:53:10.752: INFO: openvpn-client-64df8b95c9-wr7dw from kube-system started at 2019-12-20 10:27:01 +0000 UTC (2 container statuses recorded)
Dec 20 11:53:10.753: INFO: 	Container dnat-controller ready: true, restart count 0
Dec 20 11:53:10.753: INFO: 	Container openvpn-client ready: true, restart count 0
Dec 20 11:53:10.753: INFO: coredns-547f89d7d5-5w7wq from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 11:53:10.753: INFO: 	Container coredns ready: true, restart count 0
Dec 20 11:53:10.753: INFO: node-local-dns-7tvth from kube-system started at 2019-12-20 10:26:26 +0000 UTC (1 container statuses recorded)
Dec 20 11:53:10.753: INFO: 	Container node-cache ready: true, restart count 0
Dec 20 11:53:10.753: INFO: node-exporter-qnl6p from kube-system started at 2019-12-20 10:26:26 +0000 UTC (1 container statuses recorded)
Dec 20 11:53:10.753: INFO: 	Container node-exporter ready: true, restart count 0
Dec 20 11:53:10.753: INFO: coredns-547f89d7d5-s26jq from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 11:53:10.753: INFO: 	Container coredns ready: true, restart count 0
Dec 20 11:53:10.753: INFO: cluster-autoscaler-8c65c7d54-9cqhv from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 11:53:10.753: INFO: 	Container cluster-autoscaler ready: true, restart count 0
Dec 20 11:53:10.753: INFO: sonobuoy-systemd-logs-daemon-set-73919c9b23754985-dp62g from sonobuoy started at 2019-12-20 10:42:00 +0000 UTC (2 container statuses recorded)
Dec 20 11:53:10.753: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 20 11:53:10.753: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 20 11:53:10.753: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-457fd-578f899757-zpxc2 before test
Dec 20 11:53:10.812: INFO: node-exporter-wxf2r from kube-system started at 2019-12-20 10:27:10 +0000 UTC (1 container statuses recorded)
Dec 20 11:53:10.812: INFO: 	Container node-exporter ready: true, restart count 0
Dec 20 11:53:10.812: INFO: sonobuoy-e2e-job-1e71c3c7847b472c from sonobuoy started at 2019-12-20 10:42:00 +0000 UTC (2 container statuses recorded)
Dec 20 11:53:10.812: INFO: 	Container e2e ready: true, restart count 0
Dec 20 11:53:10.812: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 20 11:53:10.812: INFO: node-local-dns-4wkt5 from kube-system started at 2019-12-20 10:27:10 +0000 UTC (1 container statuses recorded)
Dec 20 11:53:10.812: INFO: 	Container node-cache ready: true, restart count 0
Dec 20 11:53:10.812: INFO: canal-w56q4 from kube-system started at 2019-12-20 10:27:10 +0000 UTC (2 container statuses recorded)
Dec 20 11:53:10.812: INFO: 	Container calico-node ready: true, restart count 0
Dec 20 11:53:10.812: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 20 11:53:10.812: INFO: sonobuoy-systemd-logs-daemon-set-73919c9b23754985-gwhsz from sonobuoy started at 2019-12-20 10:42:00 +0000 UTC (2 container statuses recorded)
Dec 20 11:53:10.812: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 20 11:53:10.812: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 20 11:53:10.812: INFO: kube-proxy-pml2l from kube-system started at 2019-12-20 10:27:10 +0000 UTC (1 container statuses recorded)
Dec 20 11:53:10.812: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node metakube-worker-457fd-578f899757-428w5
STEP: verifying the node has the label node metakube-worker-457fd-578f899757-dsqmn
STEP: verifying the node has the label node metakube-worker-457fd-578f899757-zpxc2
Dec 20 11:53:10.929: INFO: Pod canal-64h7p requesting resource cpu=350m on Node metakube-worker-457fd-578f899757-428w5
Dec 20 11:53:10.930: INFO: Pod canal-gcrnc requesting resource cpu=350m on Node metakube-worker-457fd-578f899757-dsqmn
Dec 20 11:53:10.930: INFO: Pod canal-w56q4 requesting resource cpu=350m on Node metakube-worker-457fd-578f899757-zpxc2
Dec 20 11:53:10.930: INFO: Pod cluster-autoscaler-8c65c7d54-9cqhv requesting resource cpu=10m on Node metakube-worker-457fd-578f899757-dsqmn
Dec 20 11:53:10.930: INFO: Pod coredns-547f89d7d5-5w7wq requesting resource cpu=100m on Node metakube-worker-457fd-578f899757-dsqmn
Dec 20 11:53:10.930: INFO: Pod coredns-547f89d7d5-s26jq requesting resource cpu=100m on Node metakube-worker-457fd-578f899757-dsqmn
Dec 20 11:53:10.930: INFO: Pod kube-proxy-nfqd6 requesting resource cpu=75m on Node metakube-worker-457fd-578f899757-428w5
Dec 20 11:53:10.930: INFO: Pod kube-proxy-pml2l requesting resource cpu=75m on Node metakube-worker-457fd-578f899757-zpxc2
Dec 20 11:53:10.930: INFO: Pod kube-proxy-wq825 requesting resource cpu=75m on Node metakube-worker-457fd-578f899757-dsqmn
Dec 20 11:53:10.930: INFO: Pod node-exporter-6fmjz requesting resource cpu=3m on Node metakube-worker-457fd-578f899757-428w5
Dec 20 11:53:10.930: INFO: Pod node-exporter-qnl6p requesting resource cpu=3m on Node metakube-worker-457fd-578f899757-dsqmn
Dec 20 11:53:10.930: INFO: Pod node-exporter-wxf2r requesting resource cpu=3m on Node metakube-worker-457fd-578f899757-zpxc2
Dec 20 11:53:10.930: INFO: Pod node-local-dns-4wkt5 requesting resource cpu=25m on Node metakube-worker-457fd-578f899757-zpxc2
Dec 20 11:53:10.930: INFO: Pod node-local-dns-7tvth requesting resource cpu=25m on Node metakube-worker-457fd-578f899757-dsqmn
Dec 20 11:53:10.931: INFO: Pod node-local-dns-kj7kj requesting resource cpu=25m on Node metakube-worker-457fd-578f899757-428w5
Dec 20 11:53:10.931: INFO: Pod openvpn-client-64df8b95c9-wr7dw requesting resource cpu=30m on Node metakube-worker-457fd-578f899757-dsqmn
Dec 20 11:53:10.931: INFO: Pod tiller-deploy-78f78bb476-6t2mk requesting resource cpu=0m on Node metakube-worker-457fd-578f899757-dsqmn
Dec 20 11:53:10.931: INFO: Pod sonobuoy requesting resource cpu=0m on Node metakube-worker-457fd-578f899757-428w5
Dec 20 11:53:10.931: INFO: Pod sonobuoy-e2e-job-1e71c3c7847b472c requesting resource cpu=0m on Node metakube-worker-457fd-578f899757-zpxc2
Dec 20 11:53:10.931: INFO: Pod sonobuoy-systemd-logs-daemon-set-73919c9b23754985-dp62g requesting resource cpu=0m on Node metakube-worker-457fd-578f899757-dsqmn
Dec 20 11:53:10.932: INFO: Pod sonobuoy-systemd-logs-daemon-set-73919c9b23754985-gvq9z requesting resource cpu=0m on Node metakube-worker-457fd-578f899757-428w5
Dec 20 11:53:10.932: INFO: Pod sonobuoy-systemd-logs-daemon-set-73919c9b23754985-gwhsz requesting resource cpu=0m on Node metakube-worker-457fd-578f899757-zpxc2
STEP: Starting Pods to consume most of the cluster CPU.
Dec 20 11:53:10.932: INFO: Creating a pod which consumes cpu=942m on Node metakube-worker-457fd-578f899757-zpxc2
Dec 20 11:53:10.951: INFO: Creating a pod which consumes cpu=942m on Node metakube-worker-457fd-578f899757-428w5
Dec 20 11:53:10.995: INFO: Creating a pod which consumes cpu=774m on Node metakube-worker-457fd-578f899757-dsqmn
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6784ca8d-a4ed-4d0a-8477-7dff93818f65.15e21229fc555df7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8621/filler-pod-6784ca8d-a4ed-4d0a-8477-7dff93818f65 to metakube-worker-457fd-578f899757-dsqmn]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6784ca8d-a4ed-4d0a-8477-7dff93818f65.15e2122a6bc89ab9], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6784ca8d-a4ed-4d0a-8477-7dff93818f65.15e2122a791d5747], Reason = [Created], Message = [Created container filler-pod-6784ca8d-a4ed-4d0a-8477-7dff93818f65]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6784ca8d-a4ed-4d0a-8477-7dff93818f65.15e2122a8b05a66c], Reason = [Started], Message = [Started container filler-pod-6784ca8d-a4ed-4d0a-8477-7dff93818f65]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-aacd01c4-943f-408c-962d-c1764af6a098.15e21229f9e587b0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8621/filler-pod-aacd01c4-943f-408c-962d-c1764af6a098 to metakube-worker-457fd-578f899757-428w5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-aacd01c4-943f-408c-962d-c1764af6a098.15e2122a59e9adaa], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-aacd01c4-943f-408c-962d-c1764af6a098.15e2122a694607e9], Reason = [Created], Message = [Created container filler-pod-aacd01c4-943f-408c-962d-c1764af6a098]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-aacd01c4-943f-408c-962d-c1764af6a098.15e2122a7a00abc1], Reason = [Started], Message = [Started container filler-pod-aacd01c4-943f-408c-962d-c1764af6a098]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f4972a0b-8c31-4d20-b7bb-d6d91252d9e5.15e21229f8205c44], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8621/filler-pod-f4972a0b-8c31-4d20-b7bb-d6d91252d9e5 to metakube-worker-457fd-578f899757-zpxc2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f4972a0b-8c31-4d20-b7bb-d6d91252d9e5.15e2122a61e565a7], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f4972a0b-8c31-4d20-b7bb-d6d91252d9e5.15e2122a6f71827b], Reason = [Created], Message = [Created container filler-pod-f4972a0b-8c31-4d20-b7bb-d6d91252d9e5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f4972a0b-8c31-4d20-b7bb-d6d91252d9e5.15e2122a80ad2dc2], Reason = [Started], Message = [Started container filler-pod-f4972a0b-8c31-4d20-b7bb-d6d91252d9e5]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15e2122aed7783eb], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node metakube-worker-457fd-578f899757-428w5
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node metakube-worker-457fd-578f899757-dsqmn
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node metakube-worker-457fd-578f899757-zpxc2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:53:16.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8621" for this suite.
Dec 20 11:53:22.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:53:22.539: INFO: namespace sched-pred-8621 deletion completed in 6.302361556s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:12.234 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:53:22.545: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-263
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Dec 20 11:53:27.339: INFO: Successfully updated pod "annotationupdatee93e85b3-2dad-4f17-abe8-170e6218c209"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:53:29.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-263" for this suite.
Dec 20 11:53:41.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:53:41.682: INFO: namespace downward-api-263 deletion completed in 12.289487235s

• [SLOW TEST:19.138 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:53:41.686: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6166
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-7e3974ef-d1f9-4ce2-82ab-04a35625cc5f
STEP: Creating a pod to test consume secrets
Dec 20 11:53:41.953: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6d2cc35c-6b28-412a-9fd8-9b9444a5cd65" in namespace "projected-6166" to be "success or failure"
Dec 20 11:53:41.960: INFO: Pod "pod-projected-secrets-6d2cc35c-6b28-412a-9fd8-9b9444a5cd65": Phase="Pending", Reason="", readiness=false. Elapsed: 7.253908ms
Dec 20 11:53:43.968: INFO: Pod "pod-projected-secrets-6d2cc35c-6b28-412a-9fd8-9b9444a5cd65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014671181s
Dec 20 11:53:45.976: INFO: Pod "pod-projected-secrets-6d2cc35c-6b28-412a-9fd8-9b9444a5cd65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022364059s
STEP: Saw pod success
Dec 20 11:53:45.976: INFO: Pod "pod-projected-secrets-6d2cc35c-6b28-412a-9fd8-9b9444a5cd65" satisfied condition "success or failure"
Dec 20 11:53:45.981: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-projected-secrets-6d2cc35c-6b28-412a-9fd8-9b9444a5cd65 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 20 11:53:46.044: INFO: Waiting for pod pod-projected-secrets-6d2cc35c-6b28-412a-9fd8-9b9444a5cd65 to disappear
Dec 20 11:53:46.052: INFO: Pod pod-projected-secrets-6d2cc35c-6b28-412a-9fd8-9b9444a5cd65 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:53:46.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6166" for this suite.
Dec 20 11:53:52.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:53:52.328: INFO: namespace projected-6166 deletion completed in 6.265670051s

• [SLOW TEST:10.643 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:53:52.335: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1095
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 11:53:56.658: INFO: Waiting up to 5m0s for pod "client-envvars-da9180cf-e827-4a28-80bb-313c38e1b765" in namespace "pods-1095" to be "success or failure"
Dec 20 11:53:56.679: INFO: Pod "client-envvars-da9180cf-e827-4a28-80bb-313c38e1b765": Phase="Pending", Reason="", readiness=false. Elapsed: 20.611138ms
Dec 20 11:53:58.687: INFO: Pod "client-envvars-da9180cf-e827-4a28-80bb-313c38e1b765": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028610491s
Dec 20 11:54:00.694: INFO: Pod "client-envvars-da9180cf-e827-4a28-80bb-313c38e1b765": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035455817s
STEP: Saw pod success
Dec 20 11:54:00.694: INFO: Pod "client-envvars-da9180cf-e827-4a28-80bb-313c38e1b765" satisfied condition "success or failure"
Dec 20 11:54:00.700: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod client-envvars-da9180cf-e827-4a28-80bb-313c38e1b765 container env3cont: <nil>
STEP: delete the pod
Dec 20 11:54:00.775: INFO: Waiting for pod client-envvars-da9180cf-e827-4a28-80bb-313c38e1b765 to disappear
Dec 20 11:54:00.781: INFO: Pod client-envvars-da9180cf-e827-4a28-80bb-313c38e1b765 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:54:00.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1095" for this suite.
Dec 20 11:54:12.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:54:13.068: INFO: namespace pods-1095 deletion completed in 12.278850183s

• [SLOW TEST:20.734 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:54:13.071: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8940
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 20 11:54:14.188: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 20 11:54:16.273: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712439654, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712439654, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712439654, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712439654, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 20 11:54:19.323: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:54:22.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8940" for this suite.
Dec 20 11:54:28.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:54:28.480: INFO: namespace webhook-8940 deletion completed in 6.278697486s
STEP: Destroying namespace "webhook-8940-markers" for this suite.
Dec 20 11:54:34.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:54:34.758: INFO: namespace webhook-8940-markers deletion completed in 6.277743549s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.736 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:54:34.808: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7147
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7147
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Dec 20 11:54:35.093: INFO: Found 0 stateful pods, waiting for 3
Dec 20 11:54:45.102: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 20 11:54:45.102: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 20 11:54:45.102: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Dec 20 11:54:45.158: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec 20 11:54:55.232: INFO: Updating stateful set ss2
Dec 20 11:54:55.249: INFO: Waiting for Pod statefulset-7147/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Dec 20 11:55:05.476: INFO: Found 2 stateful pods, waiting for 3
Dec 20 11:55:15.484: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 20 11:55:15.485: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 20 11:55:15.485: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec 20 11:55:15.788: INFO: Updating stateful set ss2
Dec 20 11:55:15.807: INFO: Waiting for Pod statefulset-7147/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 20 11:55:25.857: INFO: Updating stateful set ss2
Dec 20 11:55:25.876: INFO: Waiting for StatefulSet statefulset-7147/ss2 to complete update
Dec 20 11:55:25.876: INFO: Waiting for Pod statefulset-7147/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 20 11:55:35.897: INFO: Waiting for StatefulSet statefulset-7147/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 20 11:55:45.891: INFO: Deleting all statefulset in ns statefulset-7147
Dec 20 11:55:45.897: INFO: Scaling statefulset ss2 to 0
Dec 20 11:55:55.937: INFO: Waiting for statefulset status.replicas updated to 0
Dec 20 11:55:55.943: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:55:55.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7147" for this suite.
Dec 20 11:56:04.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:56:04.322: INFO: namespace statefulset-7147 deletion completed in 8.341143657s

• [SLOW TEST:89.514 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:56:04.323: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2323
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 20 11:56:04.614: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b23525c7-3507-410a-ba5e-b01880c8a5ac" in namespace "projected-2323" to be "success or failure"
Dec 20 11:56:04.624: INFO: Pod "downwardapi-volume-b23525c7-3507-410a-ba5e-b01880c8a5ac": Phase="Pending", Reason="", readiness=false. Elapsed: 9.177387ms
Dec 20 11:56:06.634: INFO: Pod "downwardapi-volume-b23525c7-3507-410a-ba5e-b01880c8a5ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019973363s
Dec 20 11:56:08.641: INFO: Pod "downwardapi-volume-b23525c7-3507-410a-ba5e-b01880c8a5ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026880096s
STEP: Saw pod success
Dec 20 11:56:08.642: INFO: Pod "downwardapi-volume-b23525c7-3507-410a-ba5e-b01880c8a5ac" satisfied condition "success or failure"
Dec 20 11:56:08.650: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod downwardapi-volume-b23525c7-3507-410a-ba5e-b01880c8a5ac container client-container: <nil>
STEP: delete the pod
Dec 20 11:56:08.852: INFO: Waiting for pod downwardapi-volume-b23525c7-3507-410a-ba5e-b01880c8a5ac to disappear
Dec 20 11:56:08.857: INFO: Pod downwardapi-volume-b23525c7-3507-410a-ba5e-b01880c8a5ac no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:56:08.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2323" for this suite.
Dec 20 11:56:14.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:56:15.139: INFO: namespace projected-2323 deletion completed in 6.272298685s

• [SLOW TEST:10.821 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:56:15.145: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9673
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:56:31.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9673" for this suite.
Dec 20 11:56:51.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:56:52.290: INFO: namespace resourcequota-9673 deletion completed in 20.635231184s

• [SLOW TEST:37.145 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:56:52.293: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9221
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Dec 20 11:56:55.503: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:57:06.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9221" for this suite.
Dec 20 11:57:12.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:57:13.270: INFO: namespace init-container-9221 deletion completed in 6.302944026s

• [SLOW TEST:20.977 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:57:13.270: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8787
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-649af826-1c61-4127-b45d-584e1169e26d in namespace container-probe-8787
Dec 20 11:57:17.593: INFO: Started pod liveness-649af826-1c61-4127-b45d-584e1169e26d in namespace container-probe-8787
STEP: checking the pod's current state and verifying that restartCount is present
Dec 20 11:57:17.599: INFO: Initial restart count of pod liveness-649af826-1c61-4127-b45d-584e1169e26d is 0
Dec 20 11:57:37.861: INFO: Restart count of pod container-probe-8787/liveness-649af826-1c61-4127-b45d-584e1169e26d is now 1 (20.262041884s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:57:37.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8787" for this suite.
Dec 20 11:57:43.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:57:44.245: INFO: namespace container-probe-8787 deletion completed in 6.306278739s

• [SLOW TEST:30.975 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:57:44.251: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1152
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 11:57:44.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 create -f - --namespace=kubectl-1152'
Dec 20 11:57:46.450: INFO: stderr: ""
Dec 20 11:57:46.450: INFO: stdout: "replicationcontroller/redis-master created\n"
Dec 20 11:57:46.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 create -f - --namespace=kubectl-1152'
Dec 20 11:57:46.871: INFO: stderr: ""
Dec 20 11:57:46.871: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 20 11:57:47.880: INFO: Selector matched 1 pods for map[app:redis]
Dec 20 11:57:47.880: INFO: Found 0 / 1
Dec 20 11:57:48.880: INFO: Selector matched 1 pods for map[app:redis]
Dec 20 11:57:48.880: INFO: Found 0 / 1
Dec 20 11:57:49.879: INFO: Selector matched 1 pods for map[app:redis]
Dec 20 11:57:49.879: INFO: Found 1 / 1
Dec 20 11:57:49.879: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 20 11:57:49.887: INFO: Selector matched 1 pods for map[app:redis]
Dec 20 11:57:49.887: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 20 11:57:49.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 describe pod redis-master-6bmgf --namespace=kubectl-1152'
Dec 20 11:57:50.040: INFO: stderr: ""
Dec 20 11:57:50.040: INFO: stdout: "Name:         redis-master-6bmgf\nNamespace:    kubectl-1152\nPriority:     0\nNode:         metakube-worker-457fd-578f899757-428w5/192.168.1.3\nStart Time:   Fri, 20 Dec 2019 11:57:46 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 172.25.1.141/32\nStatus:       Running\nIP:           172.25.1.141\nIPs:\n  IP:           172.25.1.141\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://12526e5ddbb71ed0fc3c352e1d7a2b3b3e524a4bbfd3971e01fa45ddf90231a5\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 20 Dec 2019 11:57:48 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-tflt6 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-tflt6:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-tflt6\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                                             Message\n  ----    ------     ----       ----                                             -------\n  Normal  Scheduled  <unknown>  default-scheduler                                Successfully assigned kubectl-1152/redis-master-6bmgf to metakube-worker-457fd-578f899757-428w5\n  Normal  Pulled     2s         kubelet, metakube-worker-457fd-578f899757-428w5  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    2s         kubelet, metakube-worker-457fd-578f899757-428w5  Created container redis-master\n  Normal  Started    2s         kubelet, metakube-worker-457fd-578f899757-428w5  Started container redis-master\n"
Dec 20 11:57:50.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 describe rc redis-master --namespace=kubectl-1152'
Dec 20 11:57:50.211: INFO: stderr: ""
Dec 20 11:57:50.211: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-1152\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-6bmgf\n"
Dec 20 11:57:50.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 describe service redis-master --namespace=kubectl-1152'
Dec 20 11:57:50.372: INFO: stderr: ""
Dec 20 11:57:50.372: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-1152\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.240.27.86\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.25.1.141:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec 20 11:57:50.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 describe node metakube-worker-457fd-578f899757-428w5'
Dec 20 11:57:50.571: INFO: stderr: ""
Dec 20 11:57:50.571: INFO: stdout: "Name:               metakube-worker-457fd-578f899757-428w5\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=216cc975-806e-44dd-93a1-c0b27513a975\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=dbl\n                    failure-domain.beta.kubernetes.io/zone=dbl1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=metakube-worker-457fd-578f899757-428w5\n                    kubernetes.io/os=linux\n                    machine-controller/host-id=d333bf13560fc9efa841f2ae85af198e90eec8fc4350d51eb837128e\n                    machine-controller/owned-by=fd7be2ec-bb69-489a-8f2a-221a4f1b6cf3\n                    system/cluster=455hqrx2nz\n                    system/project=cdfjnlgv6b\nAnnotations:        cluster.k8s.io/machine: kube-system/metakube-worker-457fd-578f899757-428w5\n                    flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"36:b6:fa:9e:70:4f\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.1.3\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.25.1.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 20 Dec 2019 10:26:57 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Fri, 20 Dec 2019 11:57:30 +0000   Fri, 20 Dec 2019 10:26:57 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 20 Dec 2019 11:57:30 +0000   Fri, 20 Dec 2019 10:26:57 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 20 Dec 2019 11:57:30 +0000   Fri, 20 Dec 2019 10:26:57 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 20 Dec 2019 11:57:30 +0000   Fri, 20 Dec 2019 10:27:27 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.1.3\n  Hostname:    metakube-worker-457fd-578f899757-428w5\nCapacity:\n attachable-volumes-cinder:  25\n cpu:                        2\n ephemeral-storage:          50633164Ki\n hugepages-2Mi:              0\n memory:                     8168248Ki\n pods:                       110\nAllocatable:\n attachable-volumes-cinder:  25\n cpu:                        1800m\n ephemeral-storage:          44516040218\n hugepages-2Mi:              0\n memory:                     7861048Ki\n pods:                       110\nSystem Info:\n Machine ID:                 bdbc45f907c94c61b182869befbf0f39\n System UUID:                BDBC45F9-07C9-4C61-B182-869BEFBF0F39\n Boot ID:                    06dd9295-d1ab-41e1-8e2a-4d06317c39c6\n Kernel Version:             4.15.0-72-generic\n OS Image:                   Ubuntu 18.04.3 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.2\n Kubelet Version:            v1.16.3-sys11-2\n Kube-Proxy Version:         v1.16.3-sys11-2\nPodCIDR:                     172.25.1.0/24\nPodCIDRs:                    172.25.1.0/24\nProviderID:                  openstack:///bdbc45f9-07c9-4c61-b182-869befbf0f39\nNon-terminated Pods:         (7 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                canal-64h7p                                                350m (19%)    200m (11%)  50Mi (0%)        256Mi (3%)     90m\n  kube-system                kube-proxy-nfqd6                                           75m (4%)      250m (13%)  50Mi (0%)        250Mi (3%)     90m\n  kube-system                node-exporter-6fmjz                                        3m (0%)       200m (11%)  16Mi (0%)        50Mi (0%)      90m\n  kube-system                node-local-dns-kj7kj                                       25m (1%)      0 (0%)      5Mi (0%)         30Mi (0%)      90m\n  kubectl-1152               redis-master-6bmgf                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  sonobuoy                   sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         75m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-73919c9b23754985-gvq9z    0 (0%)        0 (0%)      0 (0%)           0 (0%)         75m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                   Requests    Limits\n  --------                   --------    ------\n  cpu                        453m (25%)  650m (36%)\n  memory                     121Mi (1%)  586Mi (7%)\n  ephemeral-storage          0 (0%)      0 (0%)\n  attachable-volumes-cinder  0           0\nEvents:                      <none>\n"
Dec 20 11:57:50.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 describe namespace kubectl-1152'
Dec 20 11:57:50.742: INFO: stderr: ""
Dec 20 11:57:50.742: INFO: stdout: "Name:         kubectl-1152\nLabels:       e2e-framework=kubectl\n              e2e-run=da69ec06-09a7-47a8-b943-b2e026b5a8ee\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:57:50.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1152" for this suite.
Dec 20 11:58:18.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:58:19.040: INFO: namespace kubectl-1152 deletion completed in 28.289422368s

• [SLOW TEST:34.790 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:58:19.047: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3375
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Dec 20 11:58:19.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 cluster-info'
Dec 20 11:58:19.401: INFO: stderr: ""
Dec 20 11:58:19.401: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.240.16.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.240.16.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:58:19.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3375" for this suite.
Dec 20 11:58:25.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:58:25.713: INFO: namespace kubectl-3375 deletion completed in 6.303086353s

• [SLOW TEST:6.667 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:58:25.716: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1321
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Dec 20 11:58:25.931: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:58:32.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1321" for this suite.
Dec 20 11:58:38.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:58:38.530: INFO: namespace init-container-1321 deletion completed in 6.271035613s

• [SLOW TEST:12.814 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:58:38.537: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2247
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-ebc1e27d-b054-4dd8-9bd9-ddae952700eb
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:58:39.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2247" for this suite.
Dec 20 11:58:47.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:58:47.418: INFO: namespace configmap-2247 deletion completed in 8.295781432s

• [SLOW TEST:8.881 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:58:47.423: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3964
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 11:58:47.658: INFO: Creating deployment "test-recreate-deployment"
Dec 20 11:58:47.678: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec 20 11:58:47.692: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec 20 11:58:49.706: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec 20 11:58:49.713: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712439927, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712439927, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712439927, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712439927, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 11:58:51.723: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec 20 11:58:51.748: INFO: Updating deployment test-recreate-deployment
Dec 20 11:58:51.748: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 20 11:58:52.014: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3964 /apis/apps/v1/namespaces/deployment-3964/deployments/test-recreate-deployment 3b92c211-6266-4111-9546-f9188196869a 28960 2 2019-12-20 11:58:47 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00510e518 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2019-12-20 11:58:51 +0000 UTC,LastTransitionTime:2019-12-20 11:58:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2019-12-20 11:58:51 +0000 UTC,LastTransitionTime:2019-12-20 11:58:47 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Dec 20 11:58:52.027: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-3964 /apis/apps/v1/namespaces/deployment-3964/replicasets/test-recreate-deployment-5f94c574ff 1fd790d8-6885-4cd8-b0a4-de9d459cfe27 28959 1 2019-12-20 11:58:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 3b92c211-6266-4111-9546-f9188196869a 0xc00510e917 0xc00510e918}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00510e978 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 20 11:58:52.027: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec 20 11:58:52.027: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-3964 /apis/apps/v1/namespaces/deployment-3964/replicasets/test-recreate-deployment-68fc85c7bb 16694d2c-b391-4917-97ef-2923d3853807 28945 2 2019-12-20 11:58:47 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 3b92c211-6266-4111-9546-f9188196869a 0xc00510e9e7 0xc00510e9e8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00510ea48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 20 11:58:52.036: INFO: Pod "test-recreate-deployment-5f94c574ff-cbnx6" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-cbnx6 test-recreate-deployment-5f94c574ff- deployment-3964 /api/v1/namespaces/deployment-3964/pods/test-recreate-deployment-5f94c574ff-cbnx6 8e34dd65-34d6-4f48-8e28-cd136d503d67 28961 0 2019-12-20 11:58:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 1fd790d8-6885-4cd8-b0a4-de9d459cfe27 0xc00510eec7 0xc00510eec8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-n5rnk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-n5rnk,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-n5rnk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-zpxc2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:58:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:58:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 11:58:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.11,PodIP:,StartTime:2019-12-20 11:58:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:58:52.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3964" for this suite.
Dec 20 11:58:58.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:58:58.344: INFO: namespace deployment-3964 deletion completed in 6.29980433s

• [SLOW TEST:10.921 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:58:58.346: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-126
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 11:58:58.572: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 20 11:59:02.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 --namespace=crd-publish-openapi-126 create -f -'
Dec 20 11:59:04.980: INFO: stderr: ""
Dec 20 11:59:04.980: INFO: stdout: "e2e-test-crd-publish-openapi-8188-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 20 11:59:04.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 --namespace=crd-publish-openapi-126 delete e2e-test-crd-publish-openapi-8188-crds test-cr'
Dec 20 11:59:05.166: INFO: stderr: ""
Dec 20 11:59:05.166: INFO: stdout: "e2e-test-crd-publish-openapi-8188-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Dec 20 11:59:05.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 --namespace=crd-publish-openapi-126 apply -f -'
Dec 20 11:59:05.637: INFO: stderr: ""
Dec 20 11:59:05.637: INFO: stdout: "e2e-test-crd-publish-openapi-8188-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 20 11:59:05.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 --namespace=crd-publish-openapi-126 delete e2e-test-crd-publish-openapi-8188-crds test-cr'
Dec 20 11:59:05.804: INFO: stderr: ""
Dec 20 11:59:05.804: INFO: stdout: "e2e-test-crd-publish-openapi-8188-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Dec 20 11:59:05.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 explain e2e-test-crd-publish-openapi-8188-crds'
Dec 20 11:59:06.246: INFO: stderr: ""
Dec 20 11:59:06.247: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8188-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:59:10.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-126" for this suite.
Dec 20 11:59:16.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:59:16.502: INFO: namespace crd-publish-openapi-126 deletion completed in 6.385027928s

• [SLOW TEST:18.157 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:59:16.506: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6712
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 20 11:59:16.788: INFO: Waiting up to 5m0s for pod "downward-api-8484a371-9fd0-413f-9a57-be37be57377a" in namespace "downward-api-6712" to be "success or failure"
Dec 20 11:59:16.796: INFO: Pod "downward-api-8484a371-9fd0-413f-9a57-be37be57377a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.205694ms
Dec 20 11:59:19.292: INFO: Pod "downward-api-8484a371-9fd0-413f-9a57-be37be57377a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.503717436s
Dec 20 11:59:21.299: INFO: Pod "downward-api-8484a371-9fd0-413f-9a57-be37be57377a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.510571841s
STEP: Saw pod success
Dec 20 11:59:21.299: INFO: Pod "downward-api-8484a371-9fd0-413f-9a57-be37be57377a" satisfied condition "success or failure"
Dec 20 11:59:21.306: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod downward-api-8484a371-9fd0-413f-9a57-be37be57377a container dapi-container: <nil>
STEP: delete the pod
Dec 20 11:59:21.371: INFO: Waiting for pod downward-api-8484a371-9fd0-413f-9a57-be37be57377a to disappear
Dec 20 11:59:21.377: INFO: Pod downward-api-8484a371-9fd0-413f-9a57-be37be57377a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:59:21.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6712" for this suite.
Dec 20 11:59:27.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:59:27.744: INFO: namespace downward-api-6712 deletion completed in 6.358979421s

• [SLOW TEST:11.239 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:59:27.746: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7213
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Dec 20 11:59:27.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 create -f - --namespace=kubectl-7213'
Dec 20 11:59:28.351: INFO: stderr: ""
Dec 20 11:59:28.351: INFO: stdout: "pod/pause created\n"
Dec 20 11:59:28.351: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec 20 11:59:28.351: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7213" to be "running and ready"
Dec 20 11:59:28.363: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 10.999891ms
Dec 20 11:59:30.372: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020298045s
Dec 20 11:59:32.393: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.04146884s
Dec 20 11:59:32.393: INFO: Pod "pause" satisfied condition "running and ready"
Dec 20 11:59:32.393: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Dec 20 11:59:32.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 label pods pause testing-label=testing-label-value --namespace=kubectl-7213'
Dec 20 11:59:32.729: INFO: stderr: ""
Dec 20 11:59:32.729: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec 20 11:59:32.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pod pause -L testing-label --namespace=kubectl-7213'
Dec 20 11:59:32.849: INFO: stderr: ""
Dec 20 11:59:32.849: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec 20 11:59:32.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 label pods pause testing-label- --namespace=kubectl-7213'
Dec 20 11:59:33.127: INFO: stderr: ""
Dec 20 11:59:33.127: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec 20 11:59:33.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pod pause -L testing-label --namespace=kubectl-7213'
Dec 20 11:59:33.260: INFO: stderr: ""
Dec 20 11:59:33.260: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Dec 20 11:59:33.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 delete --grace-period=0 --force -f - --namespace=kubectl-7213'
Dec 20 11:59:33.434: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 20 11:59:33.434: INFO: stdout: "pod \"pause\" force deleted\n"
Dec 20 11:59:33.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get rc,svc -l name=pause --no-headers --namespace=kubectl-7213'
Dec 20 11:59:33.594: INFO: stderr: "No resources found in kubectl-7213 namespace.\n"
Dec 20 11:59:33.594: INFO: stdout: ""
Dec 20 11:59:33.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods -l name=pause --namespace=kubectl-7213 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 20 11:59:33.702: INFO: stderr: ""
Dec 20 11:59:33.702: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:59:33.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7213" for this suite.
Dec 20 11:59:41.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:59:42.235: INFO: namespace kubectl-7213 deletion completed in 8.371914548s

• [SLOW TEST:14.489 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:59:42.241: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1083
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-d36ee589-43bb-4ef4-b31c-220033455b71
STEP: Creating a pod to test consume configMaps
Dec 20 11:59:43.018: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2e9b7b47-e967-4ae3-ad1d-11726ef45920" in namespace "projected-1083" to be "success or failure"
Dec 20 11:59:43.030: INFO: Pod "pod-projected-configmaps-2e9b7b47-e967-4ae3-ad1d-11726ef45920": Phase="Pending", Reason="", readiness=false. Elapsed: 11.082837ms
Dec 20 11:59:45.037: INFO: Pod "pod-projected-configmaps-2e9b7b47-e967-4ae3-ad1d-11726ef45920": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018517882s
Dec 20 11:59:47.045: INFO: Pod "pod-projected-configmaps-2e9b7b47-e967-4ae3-ad1d-11726ef45920": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026601718s
STEP: Saw pod success
Dec 20 11:59:47.045: INFO: Pod "pod-projected-configmaps-2e9b7b47-e967-4ae3-ad1d-11726ef45920" satisfied condition "success or failure"
Dec 20 11:59:47.050: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-projected-configmaps-2e9b7b47-e967-4ae3-ad1d-11726ef45920 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 20 11:59:47.129: INFO: Waiting for pod pod-projected-configmaps-2e9b7b47-e967-4ae3-ad1d-11726ef45920 to disappear
Dec 20 11:59:47.136: INFO: Pod pod-projected-configmaps-2e9b7b47-e967-4ae3-ad1d-11726ef45920 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 11:59:47.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1083" for this suite.
Dec 20 11:59:53.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 11:59:53.428: INFO: namespace projected-1083 deletion completed in 6.28112314s

• [SLOW TEST:11.188 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 11:59:53.436: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7486
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 20 11:59:58.367: INFO: Successfully updated pod "pod-update-activedeadlineseconds-7f0b10ea-adfa-4553-ae35-5eca07965176"
Dec 20 11:59:58.368: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-7f0b10ea-adfa-4553-ae35-5eca07965176" in namespace "pods-7486" to be "terminated due to deadline exceeded"
Dec 20 11:59:58.380: INFO: Pod "pod-update-activedeadlineseconds-7f0b10ea-adfa-4553-ae35-5eca07965176": Phase="Running", Reason="", readiness=true. Elapsed: 12.064497ms
Dec 20 12:00:00.386: INFO: Pod "pod-update-activedeadlineseconds-7f0b10ea-adfa-4553-ae35-5eca07965176": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.017916081s
Dec 20 12:00:00.386: INFO: Pod "pod-update-activedeadlineseconds-7f0b10ea-adfa-4553-ae35-5eca07965176" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:00:00.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7486" for this suite.
Dec 20 12:00:06.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:00:06.654: INFO: namespace pods-7486 deletion completed in 6.2604508s

• [SLOW TEST:13.219 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:00:06.655: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9602
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-78893e94-0cca-4400-a104-6484cc056c2e
STEP: Creating a pod to test consume secrets
Dec 20 12:00:06.935: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1cacdf63-4786-4caa-8245-dac21ad9a259" in namespace "projected-9602" to be "success or failure"
Dec 20 12:00:06.942: INFO: Pod "pod-projected-secrets-1cacdf63-4786-4caa-8245-dac21ad9a259": Phase="Pending", Reason="", readiness=false. Elapsed: 7.229445ms
Dec 20 12:00:08.949: INFO: Pod "pod-projected-secrets-1cacdf63-4786-4caa-8245-dac21ad9a259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014147745s
Dec 20 12:00:10.957: INFO: Pod "pod-projected-secrets-1cacdf63-4786-4caa-8245-dac21ad9a259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02269887s
STEP: Saw pod success
Dec 20 12:00:10.957: INFO: Pod "pod-projected-secrets-1cacdf63-4786-4caa-8245-dac21ad9a259" satisfied condition "success or failure"
Dec 20 12:00:10.965: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-projected-secrets-1cacdf63-4786-4caa-8245-dac21ad9a259 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 20 12:00:11.036: INFO: Waiting for pod pod-projected-secrets-1cacdf63-4786-4caa-8245-dac21ad9a259 to disappear
Dec 20 12:00:11.042: INFO: Pod pod-projected-secrets-1cacdf63-4786-4caa-8245-dac21ad9a259 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:00:11.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9602" for this suite.
Dec 20 12:00:17.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:00:18.068: INFO: namespace projected-9602 deletion completed in 7.018456596s

• [SLOW TEST:11.413 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:00:18.075: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6779
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:00:34.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6779" for this suite.
Dec 20 12:00:40.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:00:41.092: INFO: namespace resourcequota-6779 deletion completed in 6.300481149s

• [SLOW TEST:23.018 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:00:41.095: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7119
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1220 12:01:11.969168      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 20 12:01:11.969: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:01:11.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7119" for this suite.
Dec 20 12:01:20.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:01:20.264: INFO: namespace gc-7119 deletion completed in 8.286848346s

• [SLOW TEST:39.170 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:01:20.268: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8903
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-5adc36c7-956c-4465-a6bc-3772fdcc26e5
STEP: Creating configMap with name cm-test-opt-upd-d71d1c8e-d0e1-4ffb-9ceb-922dba0ca7f4
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-5adc36c7-956c-4465-a6bc-3772fdcc26e5
STEP: Updating configmap cm-test-opt-upd-d71d1c8e-d0e1-4ffb-9ceb-922dba0ca7f4
STEP: Creating configMap with name cm-test-opt-create-c63abe7d-9cd7-404a-b3b0-78d71a6c5ae9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:01:29.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8903" for this suite.
Dec 20 12:01:41.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:01:41.427: INFO: namespace configmap-8903 deletion completed in 12.279978096s

• [SLOW TEST:21.160 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:01:41.427: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7620
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7620
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7620
STEP: Creating statefulset with conflicting port in namespace statefulset-7620
STEP: Waiting until pod test-pod will start running in namespace statefulset-7620
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7620
Dec 20 12:01:45.757: INFO: Observed stateful pod in namespace: statefulset-7620, name: ss-0, uid: 4d4b3c89-71f8-48a6-854f-bcad3ed59e5e, status phase: Pending. Waiting for statefulset controller to delete.
Dec 20 12:01:45.952: INFO: Observed stateful pod in namespace: statefulset-7620, name: ss-0, uid: 4d4b3c89-71f8-48a6-854f-bcad3ed59e5e, status phase: Failed. Waiting for statefulset controller to delete.
Dec 20 12:01:45.973: INFO: Observed stateful pod in namespace: statefulset-7620, name: ss-0, uid: 4d4b3c89-71f8-48a6-854f-bcad3ed59e5e, status phase: Failed. Waiting for statefulset controller to delete.
Dec 20 12:01:45.992: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7620
STEP: Removing pod with conflicting port in namespace statefulset-7620
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7620 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 20 12:01:50.103: INFO: Deleting all statefulset in ns statefulset-7620
Dec 20 12:01:50.108: INFO: Scaling statefulset ss to 0
Dec 20 12:02:00.147: INFO: Waiting for statefulset status.replicas updated to 0
Dec 20 12:02:00.154: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:02:00.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7620" for this suite.
Dec 20 12:02:06.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:02:06.472: INFO: namespace statefulset-7620 deletion completed in 6.267714208s

• [SLOW TEST:25.045 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:02:06.474: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-8860
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Dec 20 12:02:06.692: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Dec 20 12:02:07.143: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Dec 20 12:02:09.295: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440127, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440127, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440127, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440127, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 12:02:11.303: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440127, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440127, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440127, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440127, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 12:02:13.302: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440127, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440127, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440127, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440127, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 12:02:15.304: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440127, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440127, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440127, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440127, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 12:02:17.303: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440127, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440127, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440127, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440127, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 12:02:20.703: INFO: Waited 1.379099704s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:02:22.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8860" for this suite.
Dec 20 12:02:28.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:02:28.969: INFO: namespace aggregator-8860 deletion completed in 6.375978624s

• [SLOW TEST:22.495 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:02:28.974: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7432
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 20 12:02:29.293: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5fd6715a-ca27-460a-a3b4-879a0a26ef50" in namespace "projected-7432" to be "success or failure"
Dec 20 12:02:29.310: INFO: Pod "downwardapi-volume-5fd6715a-ca27-460a-a3b4-879a0a26ef50": Phase="Pending", Reason="", readiness=false. Elapsed: 17.119777ms
Dec 20 12:02:31.318: INFO: Pod "downwardapi-volume-5fd6715a-ca27-460a-a3b4-879a0a26ef50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02459608s
Dec 20 12:02:33.325: INFO: Pod "downwardapi-volume-5fd6715a-ca27-460a-a3b4-879a0a26ef50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031514515s
STEP: Saw pod success
Dec 20 12:02:33.325: INFO: Pod "downwardapi-volume-5fd6715a-ca27-460a-a3b4-879a0a26ef50" satisfied condition "success or failure"
Dec 20 12:02:33.331: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod downwardapi-volume-5fd6715a-ca27-460a-a3b4-879a0a26ef50 container client-container: <nil>
STEP: delete the pod
Dec 20 12:02:33.421: INFO: Waiting for pod downwardapi-volume-5fd6715a-ca27-460a-a3b4-879a0a26ef50 to disappear
Dec 20 12:02:33.428: INFO: Pod downwardapi-volume-5fd6715a-ca27-460a-a3b4-879a0a26ef50 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:02:33.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7432" for this suite.
Dec 20 12:02:39.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:02:39.733: INFO: namespace projected-7432 deletion completed in 6.29743813s

• [SLOW TEST:10.760 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:02:39.734: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3420
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 20 12:02:39.983: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6bb30283-3587-4056-961c-6b081314adb8" in namespace "projected-3420" to be "success or failure"
Dec 20 12:02:40.019: INFO: Pod "downwardapi-volume-6bb30283-3587-4056-961c-6b081314adb8": Phase="Pending", Reason="", readiness=false. Elapsed: 35.889639ms
Dec 20 12:02:42.027: INFO: Pod "downwardapi-volume-6bb30283-3587-4056-961c-6b081314adb8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043234952s
Dec 20 12:02:44.035: INFO: Pod "downwardapi-volume-6bb30283-3587-4056-961c-6b081314adb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05169589s
STEP: Saw pod success
Dec 20 12:02:44.035: INFO: Pod "downwardapi-volume-6bb30283-3587-4056-961c-6b081314adb8" satisfied condition "success or failure"
Dec 20 12:02:44.040: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod downwardapi-volume-6bb30283-3587-4056-961c-6b081314adb8 container client-container: <nil>
STEP: delete the pod
Dec 20 12:02:44.108: INFO: Waiting for pod downwardapi-volume-6bb30283-3587-4056-961c-6b081314adb8 to disappear
Dec 20 12:02:44.114: INFO: Pod downwardapi-volume-6bb30283-3587-4056-961c-6b081314adb8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:02:44.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3420" for this suite.
Dec 20 12:02:50.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:02:50.410: INFO: namespace projected-3420 deletion completed in 6.285936485s

• [SLOW TEST:10.677 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:02:50.411: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1446
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-1446
STEP: creating replication controller nodeport-test in namespace services-1446
I1220 12:02:51.428444      20 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-1446, replica count: 2
I1220 12:02:54.480040      20 runners.go:184] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1220 12:02:57.480527      20 runners.go:184] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 20 12:03:00.481: INFO: Creating new exec pod
I1220 12:03:00.481100      20 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 20 12:03:06.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=services-1446 execpodk45hf -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Dec 20 12:03:07.119: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec 20 12:03:07.119: INFO: stdout: ""
Dec 20 12:03:07.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=services-1446 execpodk45hf -- /bin/sh -x -c nc -zv -t -w 2 10.240.23.236 80'
Dec 20 12:03:07.789: INFO: stderr: "+ nc -zv -t -w 2 10.240.23.236 80\nConnection to 10.240.23.236 80 port [tcp/http] succeeded!\n"
Dec 20 12:03:07.789: INFO: stdout: ""
Dec 20 12:03:07.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=services-1446 execpodk45hf -- /bin/sh -x -c nc -zv -t -w 2 192.168.1.3 31517'
Dec 20 12:03:08.451: INFO: stderr: "+ nc -zv -t -w 2 192.168.1.3 31517\nConnection to 192.168.1.3 31517 port [tcp/31517] succeeded!\n"
Dec 20 12:03:08.451: INFO: stdout: ""
Dec 20 12:03:08.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=services-1446 execpodk45hf -- /bin/sh -x -c nc -zv -t -w 2 192.168.1.6 31517'
Dec 20 12:03:09.150: INFO: stderr: "+ nc -zv -t -w 2 192.168.1.6 31517\nConnection to 192.168.1.6 31517 port [tcp/31517] succeeded!\n"
Dec 20 12:03:09.150: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:03:09.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1446" for this suite.
Dec 20 12:03:17.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:03:17.431: INFO: namespace services-1446 deletion completed in 8.270064846s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:27.020 seconds]
[sig-network] Services
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:03:17.434: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2176
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1220 12:03:27.735445      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 20 12:03:27.736: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:03:27.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2176" for this suite.
Dec 20 12:03:33.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:03:34.066: INFO: namespace gc-2176 deletion completed in 6.321999678s

• [SLOW TEST:16.632 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:03:34.068: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5968
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 20 12:03:34.887: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 20 12:03:36.909: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440214, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440214, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440214, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440214, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 12:03:38.931: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440214, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440214, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440214, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440214, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 12:03:40.918: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440214, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440214, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440214, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440214, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 12:03:42.917: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440214, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440214, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440214, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440214, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 20 12:03:45.963: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Dec 20 12:03:50.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 attach --namespace=webhook-5968 to-be-attached-pod -i -c=container1'
Dec 20 12:03:50.437: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:03:50.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5968" for this suite.
Dec 20 12:04:04.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:04:04.754: INFO: namespace webhook-5968 deletion completed in 14.291521487s
STEP: Destroying namespace "webhook-5968-markers" for this suite.
Dec 20 12:04:10.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:04:11.032: INFO: namespace webhook-5968-markers deletion completed in 6.277708426s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:37.006 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:04:11.076: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7307
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Dec 20 12:04:15.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec pod-sharedvolume-7ef26a38-6368-4fae-b8f5-da5dce0ebea2 -c busybox-main-container --namespace=emptydir-7307 -- cat /usr/share/volumeshare/shareddata.txt'
Dec 20 12:04:16.101: INFO: stderr: ""
Dec 20 12:04:16.101: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:04:16.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7307" for this suite.
Dec 20 12:04:26.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:04:26.425: INFO: namespace emptydir-7307 deletion completed in 10.315012072s

• [SLOW TEST:15.349 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:04:26.429: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8908
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 20 12:04:26.661: INFO: Waiting up to 5m0s for pod "downward-api-4f68e60a-948a-452a-81d0-22c268a7fa9d" in namespace "downward-api-8908" to be "success or failure"
Dec 20 12:04:26.668: INFO: Pod "downward-api-4f68e60a-948a-452a-81d0-22c268a7fa9d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.384941ms
Dec 20 12:04:28.678: INFO: Pod "downward-api-4f68e60a-948a-452a-81d0-22c268a7fa9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016624534s
Dec 20 12:04:30.686: INFO: Pod "downward-api-4f68e60a-948a-452a-81d0-22c268a7fa9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024626282s
STEP: Saw pod success
Dec 20 12:04:30.686: INFO: Pod "downward-api-4f68e60a-948a-452a-81d0-22c268a7fa9d" satisfied condition "success or failure"
Dec 20 12:04:30.692: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod downward-api-4f68e60a-948a-452a-81d0-22c268a7fa9d container dapi-container: <nil>
STEP: delete the pod
Dec 20 12:04:30.809: INFO: Waiting for pod downward-api-4f68e60a-948a-452a-81d0-22c268a7fa9d to disappear
Dec 20 12:04:30.816: INFO: Pod downward-api-4f68e60a-948a-452a-81d0-22c268a7fa9d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:04:30.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8908" for this suite.
Dec 20 12:04:36.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:04:37.118: INFO: namespace downward-api-8908 deletion completed in 6.288797068s

• [SLOW TEST:10.689 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:04:37.122: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-755
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 20 12:04:37.351: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d45ff052-59fc-4383-80e1-a815076f249b" in namespace "downward-api-755" to be "success or failure"
Dec 20 12:04:37.359: INFO: Pod "downwardapi-volume-d45ff052-59fc-4383-80e1-a815076f249b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.469079ms
Dec 20 12:04:39.366: INFO: Pod "downwardapi-volume-d45ff052-59fc-4383-80e1-a815076f249b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015318271s
Dec 20 12:04:41.374: INFO: Pod "downwardapi-volume-d45ff052-59fc-4383-80e1-a815076f249b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022905096s
STEP: Saw pod success
Dec 20 12:04:41.374: INFO: Pod "downwardapi-volume-d45ff052-59fc-4383-80e1-a815076f249b" satisfied condition "success or failure"
Dec 20 12:04:41.381: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod downwardapi-volume-d45ff052-59fc-4383-80e1-a815076f249b container client-container: <nil>
STEP: delete the pod
Dec 20 12:04:41.596: INFO: Waiting for pod downwardapi-volume-d45ff052-59fc-4383-80e1-a815076f249b to disappear
Dec 20 12:04:41.606: INFO: Pod downwardapi-volume-d45ff052-59fc-4383-80e1-a815076f249b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:04:41.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-755" for this suite.
Dec 20 12:04:47.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:04:47.925: INFO: namespace downward-api-755 deletion completed in 6.308100619s

• [SLOW TEST:10.803 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:04:47.930: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8358
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Dec 20 12:04:52.764: INFO: Successfully updated pod "annotationupdate3543981f-559e-4a6b-80fe-7ba152fb6d59"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:04:56.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8358" for this suite.
Dec 20 12:05:24.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:05:25.177: INFO: namespace projected-8358 deletion completed in 28.288545092s

• [SLOW TEST:37.248 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:05:25.178: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4625
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 20 12:05:25.414: INFO: Waiting up to 5m0s for pod "downward-api-1ac90925-739a-4d44-8c74-44497ef4234c" in namespace "downward-api-4625" to be "success or failure"
Dec 20 12:05:25.436: INFO: Pod "downward-api-1ac90925-739a-4d44-8c74-44497ef4234c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.531646ms
Dec 20 12:05:27.445: INFO: Pod "downward-api-1ac90925-739a-4d44-8c74-44497ef4234c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030649253s
Dec 20 12:05:29.453: INFO: Pod "downward-api-1ac90925-739a-4d44-8c74-44497ef4234c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038611872s
STEP: Saw pod success
Dec 20 12:05:29.453: INFO: Pod "downward-api-1ac90925-739a-4d44-8c74-44497ef4234c" satisfied condition "success or failure"
Dec 20 12:05:29.463: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod downward-api-1ac90925-739a-4d44-8c74-44497ef4234c container dapi-container: <nil>
STEP: delete the pod
Dec 20 12:05:29.551: INFO: Waiting for pod downward-api-1ac90925-739a-4d44-8c74-44497ef4234c to disappear
Dec 20 12:05:29.562: INFO: Pod downward-api-1ac90925-739a-4d44-8c74-44497ef4234c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:05:29.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4625" for this suite.
Dec 20 12:05:35.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:05:35.972: INFO: namespace downward-api-4625 deletion completed in 6.40153613s

• [SLOW TEST:10.794 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:05:35.974: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-625
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 12:06:00.305: INFO: Container started at 2019-12-20 12:05:38 +0000 UTC, pod became ready at 2019-12-20 12:05:58 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:06:00.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-625" for this suite.
Dec 20 12:06:28.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:06:28.572: INFO: namespace container-probe-625 deletion completed in 28.256339977s

• [SLOW TEST:52.598 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:06:28.575: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5192
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 20 12:06:28.803: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6ab7fa20-9a92-4009-9275-ad48ed755a6c" in namespace "downward-api-5192" to be "success or failure"
Dec 20 12:06:28.819: INFO: Pod "downwardapi-volume-6ab7fa20-9a92-4009-9275-ad48ed755a6c": Phase="Pending", Reason="", readiness=false. Elapsed: 15.914029ms
Dec 20 12:06:30.826: INFO: Pod "downwardapi-volume-6ab7fa20-9a92-4009-9275-ad48ed755a6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022722884s
Dec 20 12:06:32.834: INFO: Pod "downwardapi-volume-6ab7fa20-9a92-4009-9275-ad48ed755a6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030717941s
STEP: Saw pod success
Dec 20 12:06:32.834: INFO: Pod "downwardapi-volume-6ab7fa20-9a92-4009-9275-ad48ed755a6c" satisfied condition "success or failure"
Dec 20 12:06:32.841: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod downwardapi-volume-6ab7fa20-9a92-4009-9275-ad48ed755a6c container client-container: <nil>
STEP: delete the pod
Dec 20 12:06:33.036: INFO: Waiting for pod downwardapi-volume-6ab7fa20-9a92-4009-9275-ad48ed755a6c to disappear
Dec 20 12:06:33.047: INFO: Pod downwardapi-volume-6ab7fa20-9a92-4009-9275-ad48ed755a6c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:06:33.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5192" for this suite.
Dec 20 12:06:41.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:06:41.614: INFO: namespace downward-api-5192 deletion completed in 8.55665106s

• [SLOW TEST:13.040 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:06:41.616: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3850
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 20 12:06:52.251: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 20 12:06:52.268: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 20 12:06:54.268: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 20 12:06:54.278: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 20 12:06:56.268: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 20 12:06:56.276: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 20 12:06:58.268: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 20 12:06:58.278: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 20 12:07:00.268: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 20 12:07:00.292: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:07:00.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3850" for this suite.
Dec 20 12:07:12.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:07:12.633: INFO: namespace container-lifecycle-hook-3850 deletion completed in 12.33195523s

• [SLOW TEST:31.017 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:07:12.634: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-4073
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5773
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5554
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:07:52.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4073" for this suite.
Dec 20 12:07:58.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:07:58.689: INFO: namespace namespaces-4073 deletion completed in 6.298414223s
STEP: Destroying namespace "nsdeletetest-5773" for this suite.
Dec 20 12:07:58.695: INFO: Namespace nsdeletetest-5773 was already deleted
STEP: Destroying namespace "nsdeletetest-5554" for this suite.
Dec 20 12:08:08.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:08:09.277: INFO: namespace nsdeletetest-5554 deletion completed in 10.582070148s

• [SLOW TEST:56.643 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:08:09.280: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1998
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 20 12:08:11.568: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 20 12:08:14.386: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440491, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440491, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440491, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440491, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 12:08:16.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440491, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440491, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440491, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440491, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 20 12:08:19.801: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 12:08:20.022: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:08:23.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1998" for this suite.
Dec 20 12:08:29.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:08:29.359: INFO: namespace webhook-1998 deletion completed in 6.347536872s
STEP: Destroying namespace "webhook-1998-markers" for this suite.
Dec 20 12:08:35.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:08:35.653: INFO: namespace webhook-1998-markers deletion completed in 6.29328399s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.403 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:08:35.684: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8413
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec 20 12:08:36.056: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8413 /api/v1/namespaces/watch-8413/configmaps/e2e-watch-test-watch-closed 1c04e22c-a99d-4d2f-89f6-e2f00157549e 31925 0 2019-12-20 12:08:36 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 20 12:08:36.062: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8413 /api/v1/namespaces/watch-8413/configmaps/e2e-watch-test-watch-closed 1c04e22c-a99d-4d2f-89f6-e2f00157549e 31926 0 2019-12-20 12:08:36 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec 20 12:08:36.102: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8413 /api/v1/namespaces/watch-8413/configmaps/e2e-watch-test-watch-closed 1c04e22c-a99d-4d2f-89f6-e2f00157549e 31927 0 2019-12-20 12:08:36 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 20 12:08:36.102: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8413 /api/v1/namespaces/watch-8413/configmaps/e2e-watch-test-watch-closed 1c04e22c-a99d-4d2f-89f6-e2f00157549e 31928 0 2019-12-20 12:08:36 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:08:36.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8413" for this suite.
Dec 20 12:08:42.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:08:42.471: INFO: namespace watch-8413 deletion completed in 6.348743343s

• [SLOW TEST:6.787 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:08:42.476: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6020
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Dec 20 12:08:47.267: INFO: Successfully updated pod "labelsupdatebffe95b3-3275-46f0-aaab-d1683b77e3ce"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:08:49.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6020" for this suite.
Dec 20 12:09:01.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:09:01.614: INFO: namespace downward-api-6020 deletion completed in 12.29519171s

• [SLOW TEST:19.138 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:09:01.619: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6316
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-fc3698c8-3512-4af0-9f21-ee45608a6476
STEP: Creating a pod to test consume configMaps
Dec 20 12:09:01.894: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f1b66060-cbea-46fc-abb5-d123df83cd36" in namespace "projected-6316" to be "success or failure"
Dec 20 12:09:01.907: INFO: Pod "pod-projected-configmaps-f1b66060-cbea-46fc-abb5-d123df83cd36": Phase="Pending", Reason="", readiness=false. Elapsed: 13.131566ms
Dec 20 12:09:03.914: INFO: Pod "pod-projected-configmaps-f1b66060-cbea-46fc-abb5-d123df83cd36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020025735s
Dec 20 12:09:05.934: INFO: Pod "pod-projected-configmaps-f1b66060-cbea-46fc-abb5-d123df83cd36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03960126s
STEP: Saw pod success
Dec 20 12:09:05.934: INFO: Pod "pod-projected-configmaps-f1b66060-cbea-46fc-abb5-d123df83cd36" satisfied condition "success or failure"
Dec 20 12:09:05.940: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-projected-configmaps-f1b66060-cbea-46fc-abb5-d123df83cd36 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 20 12:09:06.011: INFO: Waiting for pod pod-projected-configmaps-f1b66060-cbea-46fc-abb5-d123df83cd36 to disappear
Dec 20 12:09:06.020: INFO: Pod pod-projected-configmaps-f1b66060-cbea-46fc-abb5-d123df83cd36 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:09:06.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6316" for this suite.
Dec 20 12:09:12.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:09:12.348: INFO: namespace projected-6316 deletion completed in 6.319949707s

• [SLOW TEST:10.729 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:09:12.350: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2565
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 12:09:12.550: INFO: Creating ReplicaSet my-hostname-basic-2438dbfb-2528-4d0a-8f06-c003ccc4d7cc
Dec 20 12:09:12.570: INFO: Pod name my-hostname-basic-2438dbfb-2528-4d0a-8f06-c003ccc4d7cc: Found 0 pods out of 1
Dec 20 12:09:17.577: INFO: Pod name my-hostname-basic-2438dbfb-2528-4d0a-8f06-c003ccc4d7cc: Found 1 pods out of 1
Dec 20 12:09:17.578: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-2438dbfb-2528-4d0a-8f06-c003ccc4d7cc" is running
Dec 20 12:09:17.583: INFO: Pod "my-hostname-basic-2438dbfb-2528-4d0a-8f06-c003ccc4d7cc-lzf7c" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-20 12:09:12 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-20 12:09:15 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-20 12:09:15 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-20 12:09:12 +0000 UTC Reason: Message:}])
Dec 20 12:09:17.584: INFO: Trying to dial the pod
Dec 20 12:09:22.700: INFO: Controller my-hostname-basic-2438dbfb-2528-4d0a-8f06-c003ccc4d7cc: Got expected result from replica 1 [my-hostname-basic-2438dbfb-2528-4d0a-8f06-c003ccc4d7cc-lzf7c]: "my-hostname-basic-2438dbfb-2528-4d0a-8f06-c003ccc4d7cc-lzf7c", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:09:22.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2565" for this suite.
Dec 20 12:09:28.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:09:28.991: INFO: namespace replicaset-2565 deletion completed in 6.278201088s

• [SLOW TEST:16.641 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:09:28.992: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-332
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2497
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1334
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:09:35.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-332" for this suite.
Dec 20 12:09:41.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:09:42.053: INFO: namespace namespaces-332 deletion completed in 6.303810961s
STEP: Destroying namespace "nsdeletetest-2497" for this suite.
Dec 20 12:09:42.070: INFO: Namespace nsdeletetest-2497 was already deleted
STEP: Destroying namespace "nsdeletetest-1334" for this suite.
Dec 20 12:09:48.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:09:48.376: INFO: namespace nsdeletetest-1334 deletion completed in 6.306028351s

• [SLOW TEST:19.384 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:09:48.387: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1073
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 20 12:09:48.615: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b7a29b6d-0e1c-4b64-a4cf-b77eec8f5be5" in namespace "projected-1073" to be "success or failure"
Dec 20 12:09:48.626: INFO: Pod "downwardapi-volume-b7a29b6d-0e1c-4b64-a4cf-b77eec8f5be5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.641721ms
Dec 20 12:09:50.633: INFO: Pod "downwardapi-volume-b7a29b6d-0e1c-4b64-a4cf-b77eec8f5be5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017424981s
Dec 20 12:09:52.640: INFO: Pod "downwardapi-volume-b7a29b6d-0e1c-4b64-a4cf-b77eec8f5be5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025092466s
Dec 20 12:09:54.647: INFO: Pod "downwardapi-volume-b7a29b6d-0e1c-4b64-a4cf-b77eec8f5be5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031744989s
STEP: Saw pod success
Dec 20 12:09:54.647: INFO: Pod "downwardapi-volume-b7a29b6d-0e1c-4b64-a4cf-b77eec8f5be5" satisfied condition "success or failure"
Dec 20 12:09:54.659: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod downwardapi-volume-b7a29b6d-0e1c-4b64-a4cf-b77eec8f5be5 container client-container: <nil>
STEP: delete the pod
Dec 20 12:09:54.730: INFO: Waiting for pod downwardapi-volume-b7a29b6d-0e1c-4b64-a4cf-b77eec8f5be5 to disappear
Dec 20 12:09:54.738: INFO: Pod downwardapi-volume-b7a29b6d-0e1c-4b64-a4cf-b77eec8f5be5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:09:54.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1073" for this suite.
Dec 20 12:10:00.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:10:01.053: INFO: namespace projected-1073 deletion completed in 6.307698892s

• [SLOW TEST:12.667 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:10:01.062: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5757
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:10:30.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5757" for this suite.
Dec 20 12:10:37.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:10:37.272: INFO: namespace container-runtime-5757 deletion completed in 6.283251603s

• [SLOW TEST:36.211 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:10:37.277: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7435
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Dec 20 12:10:37.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 create -f - --namespace=kubectl-7435'
Dec 20 12:10:39.278: INFO: stderr: ""
Dec 20 12:10:39.278: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 20 12:10:39.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7435'
Dec 20 12:10:39.428: INFO: stderr: ""
Dec 20 12:10:39.428: INFO: stdout: "update-demo-nautilus-6jz2v update-demo-nautilus-9tqnz "
Dec 20 12:10:39.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-6jz2v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7435'
Dec 20 12:10:39.546: INFO: stderr: ""
Dec 20 12:10:39.546: INFO: stdout: ""
Dec 20 12:10:39.546: INFO: update-demo-nautilus-6jz2v is created but not running
Dec 20 12:10:44.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7435'
Dec 20 12:10:44.680: INFO: stderr: ""
Dec 20 12:10:44.680: INFO: stdout: "update-demo-nautilus-6jz2v update-demo-nautilus-9tqnz "
Dec 20 12:10:44.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-6jz2v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7435'
Dec 20 12:10:44.791: INFO: stderr: ""
Dec 20 12:10:44.791: INFO: stdout: "true"
Dec 20 12:10:44.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-6jz2v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7435'
Dec 20 12:10:44.909: INFO: stderr: ""
Dec 20 12:10:44.909: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 20 12:10:44.909: INFO: validating pod update-demo-nautilus-6jz2v
Dec 20 12:10:45.020: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 20 12:10:45.020: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 20 12:10:45.020: INFO: update-demo-nautilus-6jz2v is verified up and running
Dec 20 12:10:45.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-9tqnz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7435'
Dec 20 12:10:45.160: INFO: stderr: ""
Dec 20 12:10:45.160: INFO: stdout: "true"
Dec 20 12:10:45.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-9tqnz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7435'
Dec 20 12:10:45.286: INFO: stderr: ""
Dec 20 12:10:45.286: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 20 12:10:45.286: INFO: validating pod update-demo-nautilus-9tqnz
Dec 20 12:10:45.384: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 20 12:10:45.384: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 20 12:10:45.384: INFO: update-demo-nautilus-9tqnz is verified up and running
STEP: using delete to clean up resources
Dec 20 12:10:45.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 delete --grace-period=0 --force -f - --namespace=kubectl-7435'
Dec 20 12:10:45.522: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 20 12:10:45.522: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 20 12:10:45.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7435'
Dec 20 12:10:45.656: INFO: stderr: "No resources found in kubectl-7435 namespace.\n"
Dec 20 12:10:45.656: INFO: stdout: ""
Dec 20 12:10:45.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods -l name=update-demo --namespace=kubectl-7435 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 20 12:10:45.777: INFO: stderr: ""
Dec 20 12:10:45.778: INFO: stdout: "update-demo-nautilus-6jz2v\nupdate-demo-nautilus-9tqnz\n"
Dec 20 12:10:46.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7435'
Dec 20 12:10:46.424: INFO: stderr: "No resources found in kubectl-7435 namespace.\n"
Dec 20 12:10:46.424: INFO: stdout: ""
Dec 20 12:10:46.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods -l name=update-demo --namespace=kubectl-7435 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 20 12:10:46.565: INFO: stderr: ""
Dec 20 12:10:46.565: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:10:46.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7435" for this suite.
Dec 20 12:11:14.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:11:15.321: INFO: namespace kubectl-7435 deletion completed in 28.747872285s

• [SLOW TEST:38.044 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:11:15.322: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7471
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 20 12:11:16.676: INFO: Waiting up to 5m0s for pod "downward-api-7347bbde-e70b-4d3c-a448-7bd5276b2349" in namespace "downward-api-7471" to be "success or failure"
Dec 20 12:11:16.681: INFO: Pod "downward-api-7347bbde-e70b-4d3c-a448-7bd5276b2349": Phase="Pending", Reason="", readiness=false. Elapsed: 4.959517ms
Dec 20 12:11:18.687: INFO: Pod "downward-api-7347bbde-e70b-4d3c-a448-7bd5276b2349": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011382121s
Dec 20 12:11:20.787: INFO: Pod "downward-api-7347bbde-e70b-4d3c-a448-7bd5276b2349": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.110813246s
STEP: Saw pod success
Dec 20 12:11:20.787: INFO: Pod "downward-api-7347bbde-e70b-4d3c-a448-7bd5276b2349" satisfied condition "success or failure"
Dec 20 12:11:20.793: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod downward-api-7347bbde-e70b-4d3c-a448-7bd5276b2349 container dapi-container: <nil>
STEP: delete the pod
Dec 20 12:11:20.852: INFO: Waiting for pod downward-api-7347bbde-e70b-4d3c-a448-7bd5276b2349 to disappear
Dec 20 12:11:20.859: INFO: Pod downward-api-7347bbde-e70b-4d3c-a448-7bd5276b2349 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:11:20.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7471" for this suite.
Dec 20 12:11:26.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:11:27.144: INFO: namespace downward-api-7471 deletion completed in 6.271464948s

• [SLOW TEST:11.822 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:11:27.144: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3847
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-3847/configmap-test-9fda9777-f7e5-432f-b361-55e328561d79
STEP: Creating a pod to test consume configMaps
Dec 20 12:11:27.375: INFO: Waiting up to 5m0s for pod "pod-configmaps-a34b59c7-9a06-4182-8946-1a4a736dd3a5" in namespace "configmap-3847" to be "success or failure"
Dec 20 12:11:27.380: INFO: Pod "pod-configmaps-a34b59c7-9a06-4182-8946-1a4a736dd3a5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.578935ms
Dec 20 12:11:29.387: INFO: Pod "pod-configmaps-a34b59c7-9a06-4182-8946-1a4a736dd3a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011939683s
Dec 20 12:11:31.404: INFO: Pod "pod-configmaps-a34b59c7-9a06-4182-8946-1a4a736dd3a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028930321s
STEP: Saw pod success
Dec 20 12:11:31.404: INFO: Pod "pod-configmaps-a34b59c7-9a06-4182-8946-1a4a736dd3a5" satisfied condition "success or failure"
Dec 20 12:11:31.411: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-configmaps-a34b59c7-9a06-4182-8946-1a4a736dd3a5 container env-test: <nil>
STEP: delete the pod
Dec 20 12:11:31.511: INFO: Waiting for pod pod-configmaps-a34b59c7-9a06-4182-8946-1a4a736dd3a5 to disappear
Dec 20 12:11:31.526: INFO: Pod pod-configmaps-a34b59c7-9a06-4182-8946-1a4a736dd3a5 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:11:31.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3847" for this suite.
Dec 20 12:11:37.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:11:37.798: INFO: namespace configmap-3847 deletion completed in 6.254785196s

• [SLOW TEST:10.655 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:11:37.799: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9833
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 20 12:11:38.029: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e4929270-f797-45cc-a7bd-6317dbc1a723" in namespace "downward-api-9833" to be "success or failure"
Dec 20 12:11:38.034: INFO: Pod "downwardapi-volume-e4929270-f797-45cc-a7bd-6317dbc1a723": Phase="Pending", Reason="", readiness=false. Elapsed: 4.868395ms
Dec 20 12:11:40.041: INFO: Pod "downwardapi-volume-e4929270-f797-45cc-a7bd-6317dbc1a723": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011319991s
Dec 20 12:11:42.058: INFO: Pod "downwardapi-volume-e4929270-f797-45cc-a7bd-6317dbc1a723": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028896719s
STEP: Saw pod success
Dec 20 12:11:42.058: INFO: Pod "downwardapi-volume-e4929270-f797-45cc-a7bd-6317dbc1a723" satisfied condition "success or failure"
Dec 20 12:11:42.070: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod downwardapi-volume-e4929270-f797-45cc-a7bd-6317dbc1a723 container client-container: <nil>
STEP: delete the pod
Dec 20 12:11:42.126: INFO: Waiting for pod downwardapi-volume-e4929270-f797-45cc-a7bd-6317dbc1a723 to disappear
Dec 20 12:11:42.137: INFO: Pod downwardapi-volume-e4929270-f797-45cc-a7bd-6317dbc1a723 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:11:42.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9833" for this suite.
Dec 20 12:11:48.186: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:11:48.422: INFO: namespace downward-api-9833 deletion completed in 6.274899248s

• [SLOW TEST:10.623 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:11:48.422: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4533
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 20 12:11:49.256: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 20 12:11:51.289: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440709, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440709, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440709, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712440709, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 20 12:11:54.317: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 12:11:54.325: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6071-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:11:55.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4533" for this suite.
Dec 20 12:12:01.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:12:02.258: INFO: namespace webhook-4533 deletion completed in 6.316309426s
STEP: Destroying namespace "webhook-4533-markers" for this suite.
Dec 20 12:12:08.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:12:08.622: INFO: namespace webhook-4533-markers deletion completed in 6.362737355s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:20.265 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:12:08.687: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-524
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 20 12:12:17.013: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 20 12:12:17.020: INFO: Pod pod-with-prestop-http-hook still exists
Dec 20 12:12:19.021: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 20 12:12:19.028: INFO: Pod pod-with-prestop-http-hook still exists
Dec 20 12:12:21.021: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 20 12:12:21.029: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:12:21.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-524" for this suite.
Dec 20 12:12:38.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:12:38.475: INFO: namespace container-lifecycle-hook-524 deletion completed in 17.374423272s

• [SLOW TEST:29.788 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:12:38.479: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-191
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 20 12:12:47.763: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:12:47.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-191" for this suite.
Dec 20 12:12:57.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:12:58.133: INFO: namespace container-runtime-191 deletion completed in 10.29603338s

• [SLOW TEST:19.654 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:12:58.138: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1040
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-1f5d128e-f2ff-470a-bec4-303b592d2e5e in namespace container-probe-1040
Dec 20 12:13:02.480: INFO: Started pod liveness-1f5d128e-f2ff-470a-bec4-303b592d2e5e in namespace container-probe-1040
STEP: checking the pod's current state and verifying that restartCount is present
Dec 20 12:13:02.486: INFO: Initial restart count of pod liveness-1f5d128e-f2ff-470a-bec4-303b592d2e5e is 0
Dec 20 12:13:14.731: INFO: Restart count of pod container-probe-1040/liveness-1f5d128e-f2ff-470a-bec4-303b592d2e5e is now 1 (12.244711005s elapsed)
Dec 20 12:13:34.824: INFO: Restart count of pod container-probe-1040/liveness-1f5d128e-f2ff-470a-bec4-303b592d2e5e is now 2 (32.336979093s elapsed)
Dec 20 12:13:56.910: INFO: Restart count of pod container-probe-1040/liveness-1f5d128e-f2ff-470a-bec4-303b592d2e5e is now 3 (54.423389804s elapsed)
Dec 20 12:14:15.008: INFO: Restart count of pod container-probe-1040/liveness-1f5d128e-f2ff-470a-bec4-303b592d2e5e is now 4 (1m12.52097137s elapsed)
Dec 20 12:15:25.780: INFO: Restart count of pod container-probe-1040/liveness-1f5d128e-f2ff-470a-bec4-303b592d2e5e is now 5 (2m23.293866844s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:15:25.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1040" for this suite.
Dec 20 12:15:31.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:15:32.129: INFO: namespace container-probe-1040 deletion completed in 6.297700215s

• [SLOW TEST:153.992 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:15:32.129: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2011
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-2011
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-2011
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2011
Dec 20 12:15:32.419: INFO: Found 0 stateful pods, waiting for 1
Dec 20 12:15:42.429: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec 20 12:15:42.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 20 12:15:43.285: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 20 12:15:43.285: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 20 12:15:43.285: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 20 12:15:43.292: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 20 12:15:53.300: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 20 12:15:53.300: INFO: Waiting for statefulset status.replicas updated to 0
Dec 20 12:15:53.330: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Dec 20 12:15:53.331: INFO: ss-0  metakube-worker-457fd-578f899757-zpxc2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:32 +0000 UTC  }]
Dec 20 12:15:53.331: INFO: 
Dec 20 12:15:53.331: INFO: StatefulSet ss has not reached scale 3, at 1
Dec 20 12:15:54.339: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992677343s
Dec 20 12:15:55.349: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.984351773s
Dec 20 12:15:56.358: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.974029369s
Dec 20 12:15:57.371: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.964910059s
Dec 20 12:15:58.379: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.952706746s
Dec 20 12:15:59.387: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.944235252s
Dec 20 12:16:00.395: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.936056132s
Dec 20 12:16:01.403: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.927910204s
Dec 20 12:16:02.414: INFO: Verifying statefulset ss doesn't scale past 3 for another 920.638164ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2011
Dec 20 12:16:03.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:16:04.134: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 20 12:16:04.134: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 20 12:16:04.134: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 20 12:16:04.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:16:04.756: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 20 12:16:04.756: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 20 12:16:04.756: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 20 12:16:04.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:16:05.387: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 20 12:16:05.387: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 20 12:16:05.387: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 20 12:16:05.395: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 20 12:16:05.395: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 20 12:16:05.395: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec 20 12:16:05.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 20 12:16:06.119: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 20 12:16:06.119: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 20 12:16:06.119: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 20 12:16:06.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 20 12:16:06.879: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 20 12:16:06.880: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 20 12:16:06.881: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 20 12:16:06.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 20 12:16:07.635: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 20 12:16:07.635: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 20 12:16:07.635: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 20 12:16:07.635: INFO: Waiting for statefulset status.replicas updated to 0
Dec 20 12:16:07.643: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec 20 12:16:17.664: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 20 12:16:17.664: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 20 12:16:17.664: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 20 12:16:18.202: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Dec 20 12:16:18.202: INFO: ss-0  metakube-worker-457fd-578f899757-zpxc2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:32 +0000 UTC  }]
Dec 20 12:16:18.202: INFO: ss-1  metakube-worker-457fd-578f899757-428w5  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  }]
Dec 20 12:16:18.202: INFO: ss-2  metakube-worker-457fd-578f899757-dsqmn  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  }]
Dec 20 12:16:18.202: INFO: 
Dec 20 12:16:18.202: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 20 12:16:19.336: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Dec 20 12:16:19.336: INFO: ss-0  metakube-worker-457fd-578f899757-zpxc2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:32 +0000 UTC  }]
Dec 20 12:16:19.336: INFO: ss-1  metakube-worker-457fd-578f899757-428w5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  }]
Dec 20 12:16:19.337: INFO: ss-2  metakube-worker-457fd-578f899757-dsqmn  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  }]
Dec 20 12:16:19.337: INFO: 
Dec 20 12:16:19.337: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 20 12:16:20.344: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Dec 20 12:16:20.345: INFO: ss-0  metakube-worker-457fd-578f899757-zpxc2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:32 +0000 UTC  }]
Dec 20 12:16:20.345: INFO: ss-1  metakube-worker-457fd-578f899757-428w5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  }]
Dec 20 12:16:20.345: INFO: ss-2  metakube-worker-457fd-578f899757-dsqmn  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  }]
Dec 20 12:16:20.346: INFO: 
Dec 20 12:16:20.346: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 20 12:16:21.352: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Dec 20 12:16:21.352: INFO: ss-0  metakube-worker-457fd-578f899757-zpxc2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:32 +0000 UTC  }]
Dec 20 12:16:21.352: INFO: ss-1  metakube-worker-457fd-578f899757-428w5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  }]
Dec 20 12:16:21.352: INFO: ss-2  metakube-worker-457fd-578f899757-dsqmn  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  }]
Dec 20 12:16:21.353: INFO: 
Dec 20 12:16:21.353: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 20 12:16:22.361: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Dec 20 12:16:22.361: INFO: ss-1  metakube-worker-457fd-578f899757-428w5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  }]
Dec 20 12:16:22.361: INFO: ss-2  metakube-worker-457fd-578f899757-dsqmn  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  }]
Dec 20 12:16:22.361: INFO: 
Dec 20 12:16:22.361: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 20 12:16:23.369: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Dec 20 12:16:23.369: INFO: ss-1  metakube-worker-457fd-578f899757-428w5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  }]
Dec 20 12:16:23.369: INFO: ss-2  metakube-worker-457fd-578f899757-dsqmn  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  }]
Dec 20 12:16:23.369: INFO: 
Dec 20 12:16:23.369: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 20 12:16:24.378: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Dec 20 12:16:24.378: INFO: ss-1  metakube-worker-457fd-578f899757-428w5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  }]
Dec 20 12:16:24.378: INFO: ss-2  metakube-worker-457fd-578f899757-dsqmn  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  }]
Dec 20 12:16:24.378: INFO: 
Dec 20 12:16:24.378: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 20 12:16:25.386: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Dec 20 12:16:25.387: INFO: ss-2  metakube-worker-457fd-578f899757-dsqmn  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  }]
Dec 20 12:16:25.387: INFO: 
Dec 20 12:16:25.388: INFO: StatefulSet ss has not reached scale 0, at 1
Dec 20 12:16:26.398: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Dec 20 12:16:26.398: INFO: ss-2  metakube-worker-457fd-578f899757-dsqmn  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  }]
Dec 20 12:16:26.398: INFO: 
Dec 20 12:16:26.398: INFO: StatefulSet ss has not reached scale 0, at 1
Dec 20 12:16:27.406: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Dec 20 12:16:27.407: INFO: ss-2  metakube-worker-457fd-578f899757-dsqmn  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:16:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-20 12:15:53 +0000 UTC  }]
Dec 20 12:16:27.407: INFO: 
Dec 20 12:16:27.407: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2011
Dec 20 12:16:28.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:16:28.701: INFO: rc: 1
Dec 20 12:16:28.701: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  error: unable to upgrade connection: container not found ("webserver")
 [] <nil> 0xc004b7bdd0 exit status 1 <nil> <nil> true [0xc00191fb28 0xc00191fb40 0xc00191fb58] [0xc00191fb28 0xc00191fb40 0xc00191fb58] [0xc00191fb38 0xc00191fb50] [0x10efe30 0x10efe30] 0xc007017aa0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Dec 20 12:16:38.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:16:38.831: INFO: rc: 1
Dec 20 12:16:38.831: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0034e0180 exit status 1 <nil> <nil> true [0xc00191fb60 0xc00191fb78 0xc00191fb90] [0xc00191fb60 0xc00191fb78 0xc00191fb90] [0xc00191fb70 0xc00191fb88] [0x10efe30 0x10efe30] 0xc007017f80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:16:48.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:16:48.930: INFO: rc: 1
Dec 20 12:16:48.930: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00634da40 exit status 1 <nil> <nil> true [0xc000d67b68 0xc000d67b80 0xc000d67b98] [0xc000d67b68 0xc000d67b80 0xc000d67b98] [0xc000d67b78 0xc000d67b90] [0x10efe30 0x10efe30] 0xc0027151a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:16:58.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:16:59.132: INFO: rc: 1
Dec 20 12:16:59.132: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0034e0540 exit status 1 <nil> <nil> true [0xc00191fb98 0xc00191fbb0 0xc00191fbc8] [0xc00191fb98 0xc00191fbb0 0xc00191fbc8] [0xc00191fba8 0xc00191fbc0] [0x10efe30 0x10efe30] 0xc00344a660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:17:09.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:17:09.277: INFO: rc: 1
Dec 20 12:17:09.277: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0034e0900 exit status 1 <nil> <nil> true [0xc00191fbd0 0xc00191fbe8 0xc00191fc00] [0xc00191fbd0 0xc00191fbe8 0xc00191fc00] [0xc00191fbe0 0xc00191fbf8] [0x10efe30 0x10efe30] 0xc00344ac60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:17:19.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:17:19.406: INFO: rc: 1
Dec 20 12:17:19.406: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032de390 exit status 1 <nil> <nil> true [0xc00191e008 0xc00191e020 0xc00191e038] [0xc00191e008 0xc00191e020 0xc00191e038] [0xc00191e018 0xc00191e030] [0x10efe30 0x10efe30] 0xc0087d22a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:17:29.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:17:29.556: INFO: rc: 1
Dec 20 12:17:29.556: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032de750 exit status 1 <nil> <nil> true [0xc00191e040 0xc00191e058 0xc00191e070] [0xc00191e040 0xc00191e058 0xc00191e070] [0xc00191e050 0xc00191e068] [0x10efe30 0x10efe30] 0xc0087d2660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:17:39.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:17:39.707: INFO: rc: 1
Dec 20 12:17:39.707: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc007816570 exit status 1 <nil> <nil> true [0xc000d66018 0xc000d66070 0xc000d660d0] [0xc000d66018 0xc000d66070 0xc000d660d0] [0xc000d66060 0xc000d660b8] [0x10efe30 0x10efe30] 0xc002b202a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:17:49.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:17:49.816: INFO: rc: 1
Dec 20 12:17:49.816: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc007816900 exit status 1 <nil> <nil> true [0xc000d660e0 0xc000d66148 0xc000d661d8] [0xc000d660e0 0xc000d66148 0xc000d661d8] [0xc000d66130 0xc000d661b8] [0x10efe30 0x10efe30] 0xc002b20600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:17:59.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:17:59.945: INFO: rc: 1
Dec 20 12:17:59.945: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032deab0 exit status 1 <nil> <nil> true [0xc00191e078 0xc00191e090 0xc00191e0a8] [0xc00191e078 0xc00191e090 0xc00191e0a8] [0xc00191e088 0xc00191e0a0] [0x10efe30 0x10efe30] 0xc0087d2a20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:18:09.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:18:10.066: INFO: rc: 1
Dec 20 12:18:10.066: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032df050 exit status 1 <nil> <nil> true [0xc00191e0b0 0xc00191e0c8 0xc00191e0e0] [0xc00191e0b0 0xc00191e0c8 0xc00191e0e0] [0xc00191e0c0 0xc00191e0d8] [0x10efe30 0x10efe30] 0xc0087d2de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:18:20.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:18:20.204: INFO: rc: 1
Dec 20 12:18:20.204: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032df3e0 exit status 1 <nil> <nil> true [0xc00191e0e8 0xc00191e100 0xc00191e118] [0xc00191e0e8 0xc00191e100 0xc00191e118] [0xc00191e0f8 0xc00191e110] [0x10efe30 0x10efe30] 0xc0087d3200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:18:30.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:18:30.333: INFO: rc: 1
Dec 20 12:18:30.333: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032df770 exit status 1 <nil> <nil> true [0xc00191e120 0xc00191e138 0xc00191e150] [0xc00191e120 0xc00191e138 0xc00191e150] [0xc00191e130 0xc00191e148] [0x10efe30 0x10efe30] 0xc0087d3620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:18:40.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:18:40.457: INFO: rc: 1
Dec 20 12:18:40.457: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc007816cc0 exit status 1 <nil> <nil> true [0xc000d66238 0xc000d66290 0xc000d66320] [0xc000d66238 0xc000d66290 0xc000d66320] [0xc000d66280 0xc000d662e8] [0x10efe30 0x10efe30] 0xc002b209c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:18:50.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:18:50.576: INFO: rc: 1
Dec 20 12:18:50.576: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc007817050 exit status 1 <nil> <nil> true [0xc000d66368 0xc000d66398 0xc000d663e8] [0xc000d66368 0xc000d66398 0xc000d663e8] [0xc000d66388 0xc000d663d8] [0x10efe30 0x10efe30] 0xc002b20e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:19:00.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:19:00.698: INFO: rc: 1
Dec 20 12:19:00.698: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032dfad0 exit status 1 <nil> <nil> true [0xc00191e158 0xc00191e170 0xc00191e188] [0xc00191e158 0xc00191e170 0xc00191e188] [0xc00191e168 0xc00191e180] [0x10efe30 0x10efe30] 0xc0087d3aa0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:19:10.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:19:10.819: INFO: rc: 1
Dec 20 12:19:10.819: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0078173e0 exit status 1 <nil> <nil> true [0xc000d66408 0xc000d66448 0xc000d664b0] [0xc000d66408 0xc000d66448 0xc000d664b0] [0xc000d66438 0xc000d664a0] [0x10efe30 0x10efe30] 0xc002b211a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:19:20.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:19:20.942: INFO: rc: 1
Dec 20 12:19:20.942: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc007816540 exit status 1 <nil> <nil> true [0xc000d66028 0xc000d66098 0xc000d660e0] [0xc000d66028 0xc000d66098 0xc000d660e0] [0xc000d66070 0xc000d660d0] [0x10efe30 0x10efe30] 0xc002b202a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:19:30.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:19:31.059: INFO: rc: 1
Dec 20 12:19:31.059: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032de360 exit status 1 <nil> <nil> true [0xc00191e000 0xc00191e018 0xc00191e030] [0xc00191e000 0xc00191e018 0xc00191e030] [0xc00191e010 0xc00191e028] [0x10efe30 0x10efe30] 0xc0087d22a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:19:41.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:19:41.191: INFO: rc: 1
Dec 20 12:19:41.191: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc007816930 exit status 1 <nil> <nil> true [0xc000d660f8 0xc000d66180 0xc000d66238] [0xc000d660f8 0xc000d66180 0xc000d66238] [0xc000d66148 0xc000d661d8] [0x10efe30 0x10efe30] 0xc002b20600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:19:51.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:19:51.311: INFO: rc: 1
Dec 20 12:19:51.311: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032de720 exit status 1 <nil> <nil> true [0xc00191e038 0xc00191e050 0xc00191e068] [0xc00191e038 0xc00191e050 0xc00191e068] [0xc00191e048 0xc00191e060] [0x10efe30 0x10efe30] 0xc0087d2660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:20:01.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:20:01.437: INFO: rc: 1
Dec 20 12:20:01.437: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032deb10 exit status 1 <nil> <nil> true [0xc00191e070 0xc00191e088 0xc00191e0a0] [0xc00191e070 0xc00191e088 0xc00191e0a0] [0xc00191e080 0xc00191e098] [0x10efe30 0x10efe30] 0xc0087d2a20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:20:11.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:20:11.580: INFO: rc: 1
Dec 20 12:20:11.580: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc007816cf0 exit status 1 <nil> <nil> true [0xc000d66248 0xc000d662b8 0xc000d66368] [0xc000d66248 0xc000d662b8 0xc000d66368] [0xc000d66290 0xc000d66320] [0x10efe30 0x10efe30] 0xc002b209c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:20:21.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:20:22.128: INFO: rc: 1
Dec 20 12:20:22.128: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032df0e0 exit status 1 <nil> <nil> true [0xc00191e0a8 0xc00191e0c0 0xc00191e0d8] [0xc00191e0a8 0xc00191e0c0 0xc00191e0d8] [0xc00191e0b8 0xc00191e0d0] [0x10efe30 0x10efe30] 0xc0087d2de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:20:32.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:20:32.262: INFO: rc: 1
Dec 20 12:20:32.263: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0078170b0 exit status 1 <nil> <nil> true [0xc000d66378 0xc000d663b8 0xc000d66408] [0xc000d66378 0xc000d663b8 0xc000d66408] [0xc000d66398 0xc000d663e8] [0x10efe30 0x10efe30] 0xc002b20e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:20:42.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:20:43.584: INFO: rc: 1
Dec 20 12:20:43.584: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032df4a0 exit status 1 <nil> <nil> true [0xc00191e0e0 0xc00191e0f8 0xc00191e110] [0xc00191e0e0 0xc00191e0f8 0xc00191e110] [0xc00191e0f0 0xc00191e108] [0x10efe30 0x10efe30] 0xc0087d3200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:20:53.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:20:53.715: INFO: rc: 1
Dec 20 12:20:53.715: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032df860 exit status 1 <nil> <nil> true [0xc00191e118 0xc00191e130 0xc00191e148] [0xc00191e118 0xc00191e130 0xc00191e148] [0xc00191e128 0xc00191e140] [0x10efe30 0x10efe30] 0xc0087d3620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:21:03.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:21:03.837: INFO: rc: 1
Dec 20 12:21:03.837: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc007817470 exit status 1 <nil> <nil> true [0xc000d66428 0xc000d66458 0xc000d664d0] [0xc000d66428 0xc000d66458 0xc000d664d0] [0xc000d66448 0xc000d664b0] [0x10efe30 0x10efe30] 0xc002b211a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:21:13.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:21:13.958: INFO: rc: 1
Dec 20 12:21:13.958: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032dfc20 exit status 1 <nil> <nil> true [0xc00191e150 0xc00191e168 0xc00191e180] [0xc00191e150 0xc00191e168 0xc00191e180] [0xc00191e160 0xc00191e178] [0x10efe30 0x10efe30] 0xc0087d3aa0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:21:23.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:21:24.075: INFO: rc: 1
Dec 20 12:21:24.075: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc007816570 exit status 1 <nil> <nil> true [0xc000d66028 0xc000d66098 0xc000d660e0] [0xc000d66028 0xc000d66098 0xc000d660e0] [0xc000d66070 0xc000d660d0] [0x10efe30 0x10efe30] 0xc002b202a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 20 12:21:34.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=statefulset-2011 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 20 12:21:34.197: INFO: rc: 1
Dec 20 12:21:34.197: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Dec 20 12:21:34.197: INFO: Scaling statefulset ss to 0
Dec 20 12:21:34.225: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 20 12:21:34.232: INFO: Deleting all statefulset in ns statefulset-2011
Dec 20 12:21:34.239: INFO: Scaling statefulset ss to 0
Dec 20 12:21:34.259: INFO: Waiting for statefulset status.replicas updated to 0
Dec 20 12:21:34.265: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:21:34.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2011" for this suite.
Dec 20 12:21:46.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:21:46.655: INFO: namespace statefulset-2011 deletion completed in 12.340770174s

• [SLOW TEST:374.526 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:21:46.667: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3211
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-c32b9991-1915-4bc0-bfd1-e1e6d5dca837
STEP: Creating a pod to test consume secrets
Dec 20 12:21:46.900: INFO: Waiting up to 5m0s for pod "pod-secrets-5a04f60c-4797-4385-b738-91e532741919" in namespace "secrets-3211" to be "success or failure"
Dec 20 12:21:46.920: INFO: Pod "pod-secrets-5a04f60c-4797-4385-b738-91e532741919": Phase="Pending", Reason="", readiness=false. Elapsed: 19.924489ms
Dec 20 12:21:48.927: INFO: Pod "pod-secrets-5a04f60c-4797-4385-b738-91e532741919": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027178661s
Dec 20 12:21:50.934: INFO: Pod "pod-secrets-5a04f60c-4797-4385-b738-91e532741919": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034683982s
Dec 20 12:21:52.942: INFO: Pod "pod-secrets-5a04f60c-4797-4385-b738-91e532741919": Phase="Pending", Reason="", readiness=false. Elapsed: 6.042655073s
Dec 20 12:21:54.951: INFO: Pod "pod-secrets-5a04f60c-4797-4385-b738-91e532741919": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.051610237s
STEP: Saw pod success
Dec 20 12:21:54.952: INFO: Pod "pod-secrets-5a04f60c-4797-4385-b738-91e532741919" satisfied condition "success or failure"
Dec 20 12:21:54.958: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-secrets-5a04f60c-4797-4385-b738-91e532741919 container secret-volume-test: <nil>
STEP: delete the pod
Dec 20 12:21:55.006: INFO: Waiting for pod pod-secrets-5a04f60c-4797-4385-b738-91e532741919 to disappear
Dec 20 12:21:55.012: INFO: Pod pod-secrets-5a04f60c-4797-4385-b738-91e532741919 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:21:55.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3211" for this suite.
Dec 20 12:22:01.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:22:01.272: INFO: namespace secrets-3211 deletion completed in 6.249548031s

• [SLOW TEST:14.605 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:22:01.273: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1688
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 12:22:01.487: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:22:02.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1688" for this suite.
Dec 20 12:22:08.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:22:08.513: INFO: namespace custom-resource-definition-1688 deletion completed in 6.313867879s

• [SLOW TEST:7.241 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:22:08.519: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-8410
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 12:22:08.747: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6b5d9321-b0a5-4199-9a65-c7b7b9b30455" in namespace "security-context-test-8410" to be "success or failure"
Dec 20 12:22:08.755: INFO: Pod "busybox-privileged-false-6b5d9321-b0a5-4199-9a65-c7b7b9b30455": Phase="Pending", Reason="", readiness=false. Elapsed: 8.3085ms
Dec 20 12:22:10.763: INFO: Pod "busybox-privileged-false-6b5d9321-b0a5-4199-9a65-c7b7b9b30455": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016368981s
Dec 20 12:22:12.770: INFO: Pod "busybox-privileged-false-6b5d9321-b0a5-4199-9a65-c7b7b9b30455": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023343502s
Dec 20 12:22:14.776: INFO: Pod "busybox-privileged-false-6b5d9321-b0a5-4199-9a65-c7b7b9b30455": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029623932s
Dec 20 12:22:16.783: INFO: Pod "busybox-privileged-false-6b5d9321-b0a5-4199-9a65-c7b7b9b30455": Phase="Pending", Reason="", readiness=false. Elapsed: 8.036151493s
Dec 20 12:22:18.929: INFO: Pod "busybox-privileged-false-6b5d9321-b0a5-4199-9a65-c7b7b9b30455": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.18218493s
Dec 20 12:22:18.929: INFO: Pod "busybox-privileged-false-6b5d9321-b0a5-4199-9a65-c7b7b9b30455" satisfied condition "success or failure"
Dec 20 12:22:18.946: INFO: Got logs for pod "busybox-privileged-false-6b5d9321-b0a5-4199-9a65-c7b7b9b30455": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:22:18.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8410" for this suite.
Dec 20 12:22:30.979: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:22:31.230: INFO: namespace security-context-test-8410 deletion completed in 12.275853258s

• [SLOW TEST:22.711 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:22:31.230: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5081
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 20 12:22:32.386: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712441352, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712441352, loc:(*time.Location)(0x84bfb00)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-86d95b659d\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712441352, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712441352, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Dec 20 12:22:34.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712441352, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712441352, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712441352, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712441352, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 12:22:36.692: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712441352, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712441352, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712441352, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712441352, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 20 12:22:39.421: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Dec 20 12:22:39.654: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:22:39.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5081" for this suite.
Dec 20 12:22:47.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:22:48.050: INFO: namespace webhook-5081 deletion completed in 8.315763241s
STEP: Destroying namespace "webhook-5081-markers" for this suite.
Dec 20 12:22:54.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:22:54.306: INFO: namespace webhook-5081-markers deletion completed in 6.254812284s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:23.114 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:22:54.347: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9362
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:23:12.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9362" for this suite.
Dec 20 12:23:18.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:23:18.689: INFO: namespace resourcequota-9362 deletion completed in 6.276819644s

• [SLOW TEST:24.343 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:23:18.693: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6218
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec 20 12:23:24.027: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:23:24.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6218" for this suite.
Dec 20 12:23:56.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:23:56.388: INFO: namespace replicaset-6218 deletion completed in 32.312719157s

• [SLOW TEST:37.695 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:23:56.389: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2338
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 20 12:23:56.968: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 20 12:23:58.996: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712441437, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712441437, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712441437, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712441436, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 20 12:24:02.027: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:24:02.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2338" for this suite.
Dec 20 12:24:08.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:24:08.992: INFO: namespace webhook-2338 deletion completed in 6.281434658s
STEP: Destroying namespace "webhook-2338-markers" for this suite.
Dec 20 12:24:21.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:24:21.256: INFO: namespace webhook-2338-markers deletion completed in 12.263585464s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:24.903 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:24:21.292: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-3872
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:24:33.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3872" for this suite.
Dec 20 12:24:41.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:24:41.822: INFO: namespace job-3872 deletion completed in 8.302245327s

• [SLOW TEST:20.530 seconds]
[sig-apps] Job
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:24:41.823: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7080
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 20 12:24:42.076: INFO: Waiting up to 5m0s for pod "downwardapi-volume-936861ca-155f-4705-a404-7d3e40f6acb5" in namespace "downward-api-7080" to be "success or failure"
Dec 20 12:24:42.093: INFO: Pod "downwardapi-volume-936861ca-155f-4705-a404-7d3e40f6acb5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.889539ms
Dec 20 12:24:44.100: INFO: Pod "downwardapi-volume-936861ca-155f-4705-a404-7d3e40f6acb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023962706s
Dec 20 12:24:46.112: INFO: Pod "downwardapi-volume-936861ca-155f-4705-a404-7d3e40f6acb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035910226s
STEP: Saw pod success
Dec 20 12:24:46.112: INFO: Pod "downwardapi-volume-936861ca-155f-4705-a404-7d3e40f6acb5" satisfied condition "success or failure"
Dec 20 12:24:46.122: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod downwardapi-volume-936861ca-155f-4705-a404-7d3e40f6acb5 container client-container: <nil>
STEP: delete the pod
Dec 20 12:24:46.252: INFO: Waiting for pod downwardapi-volume-936861ca-155f-4705-a404-7d3e40f6acb5 to disappear
Dec 20 12:24:46.266: INFO: Pod downwardapi-volume-936861ca-155f-4705-a404-7d3e40f6acb5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:24:46.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7080" for this suite.
Dec 20 12:24:52.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:24:52.576: INFO: namespace downward-api-7080 deletion completed in 6.299822911s

• [SLOW TEST:10.754 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:24:52.578: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7133
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 20 12:24:52.810: INFO: Waiting up to 5m0s for pod "pod-f69c0707-61a0-47d0-9c25-24b161791754" in namespace "emptydir-7133" to be "success or failure"
Dec 20 12:24:52.820: INFO: Pod "pod-f69c0707-61a0-47d0-9c25-24b161791754": Phase="Pending", Reason="", readiness=false. Elapsed: 10.228033ms
Dec 20 12:24:54.827: INFO: Pod "pod-f69c0707-61a0-47d0-9c25-24b161791754": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01688097s
Dec 20 12:24:56.834: INFO: Pod "pod-f69c0707-61a0-47d0-9c25-24b161791754": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024345911s
STEP: Saw pod success
Dec 20 12:24:56.834: INFO: Pod "pod-f69c0707-61a0-47d0-9c25-24b161791754" satisfied condition "success or failure"
Dec 20 12:24:56.839: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-f69c0707-61a0-47d0-9c25-24b161791754 container test-container: <nil>
STEP: delete the pod
Dec 20 12:24:56.900: INFO: Waiting for pod pod-f69c0707-61a0-47d0-9c25-24b161791754 to disappear
Dec 20 12:24:56.906: INFO: Pod pod-f69c0707-61a0-47d0-9c25-24b161791754 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:24:56.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7133" for this suite.
Dec 20 12:25:02.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:25:03.192: INFO: namespace emptydir-7133 deletion completed in 6.277223536s

• [SLOW TEST:10.614 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:25:03.196: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1178
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:25:20.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1178" for this suite.
Dec 20 12:25:27.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:25:27.569: INFO: namespace kubelet-test-1178 deletion completed in 7.351319178s

• [SLOW TEST:24.373 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:25:27.575: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1760
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1220 12:26:07.867677      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 20 12:26:07.867: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:26:07.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1760" for this suite.
Dec 20 12:26:15.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:26:16.162: INFO: namespace gc-1760 deletion completed in 8.281382036s

• [SLOW TEST:48.588 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:26:16.164: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9297
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-fs4s
STEP: Creating a pod to test atomic-volume-subpath
Dec 20 12:26:16.471: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-fs4s" in namespace "subpath-9297" to be "success or failure"
Dec 20 12:26:16.477: INFO: Pod "pod-subpath-test-configmap-fs4s": Phase="Pending", Reason="", readiness=false. Elapsed: 5.652224ms
Dec 20 12:26:18.491: INFO: Pod "pod-subpath-test-configmap-fs4s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019509137s
Dec 20 12:26:20.498: INFO: Pod "pod-subpath-test-configmap-fs4s": Phase="Running", Reason="", readiness=true. Elapsed: 4.026688351s
Dec 20 12:26:22.518: INFO: Pod "pod-subpath-test-configmap-fs4s": Phase="Running", Reason="", readiness=true. Elapsed: 6.047118368s
Dec 20 12:26:24.527: INFO: Pod "pod-subpath-test-configmap-fs4s": Phase="Running", Reason="", readiness=true. Elapsed: 8.056003006s
Dec 20 12:26:26.538: INFO: Pod "pod-subpath-test-configmap-fs4s": Phase="Running", Reason="", readiness=true. Elapsed: 10.06709204s
Dec 20 12:26:28.547: INFO: Pod "pod-subpath-test-configmap-fs4s": Phase="Running", Reason="", readiness=true. Elapsed: 12.075578129s
Dec 20 12:26:30.556: INFO: Pod "pod-subpath-test-configmap-fs4s": Phase="Running", Reason="", readiness=true. Elapsed: 14.08459489s
Dec 20 12:26:32.564: INFO: Pod "pod-subpath-test-configmap-fs4s": Phase="Running", Reason="", readiness=true. Elapsed: 16.092557387s
Dec 20 12:26:34.571: INFO: Pod "pod-subpath-test-configmap-fs4s": Phase="Running", Reason="", readiness=true. Elapsed: 18.099427808s
Dec 20 12:26:36.577: INFO: Pod "pod-subpath-test-configmap-fs4s": Phase="Running", Reason="", readiness=true. Elapsed: 20.105507587s
Dec 20 12:26:38.589: INFO: Pod "pod-subpath-test-configmap-fs4s": Phase="Running", Reason="", readiness=true. Elapsed: 22.11746044s
Dec 20 12:26:40.815: INFO: Pod "pod-subpath-test-configmap-fs4s": Phase="Running", Reason="", readiness=true. Elapsed: 24.344246106s
Dec 20 12:26:43.978: INFO: Pod "pod-subpath-test-configmap-fs4s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 27.506808635s
STEP: Saw pod success
Dec 20 12:26:43.979: INFO: Pod "pod-subpath-test-configmap-fs4s" satisfied condition "success or failure"
Dec 20 12:26:44.476: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-subpath-test-configmap-fs4s container test-container-subpath-configmap-fs4s: <nil>
STEP: delete the pod
Dec 20 12:26:44.956: INFO: Waiting for pod pod-subpath-test-configmap-fs4s to disappear
Dec 20 12:26:44.988: INFO: Pod pod-subpath-test-configmap-fs4s no longer exists
STEP: Deleting pod pod-subpath-test-configmap-fs4s
Dec 20 12:26:44.988: INFO: Deleting pod "pod-subpath-test-configmap-fs4s" in namespace "subpath-9297"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:26:44.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9297" for this suite.
Dec 20 12:26:51.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:26:51.410: INFO: namespace subpath-9297 deletion completed in 6.405245679s

• [SLOW TEST:35.246 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:26:51.411: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1245
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 20 12:26:51.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-1245'
Dec 20 12:26:51.746: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 20 12:26:51.746: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Dec 20 12:26:51.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 delete jobs e2e-test-httpd-job --namespace=kubectl-1245'
Dec 20 12:26:51.912: INFO: stderr: ""
Dec 20 12:26:51.912: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:26:51.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1245" for this suite.
Dec 20 12:26:57.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:26:58.237: INFO: namespace kubectl-1245 deletion completed in 6.301967017s

• [SLOW TEST:6.827 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:26:58.241: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5548
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-240f14a1-96cd-4784-8649-fab96d036b39
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-240f14a1-96cd-4784-8649-fab96d036b39
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:28:30.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5548" for this suite.
Dec 20 12:28:58.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:28:58.523: INFO: namespace configmap-5548 deletion completed in 28.294074431s

• [SLOW TEST:120.282 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:28:58.525: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6608
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 12:28:58.788: INFO: Create a RollingUpdate DaemonSet
Dec 20 12:28:58.799: INFO: Check that daemon pods launch on every node of the cluster
Dec 20 12:28:58.816: INFO: Number of nodes with available pods: 0
Dec 20 12:28:58.817: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 12:28:59.832: INFO: Number of nodes with available pods: 0
Dec 20 12:28:59.832: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 12:29:00.831: INFO: Number of nodes with available pods: 0
Dec 20 12:29:00.831: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 12:29:01.832: INFO: Number of nodes with available pods: 1
Dec 20 12:29:01.832: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 12:29:03.266: INFO: Number of nodes with available pods: 1
Dec 20 12:29:03.266: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 12:29:03.837: INFO: Number of nodes with available pods: 1
Dec 20 12:29:03.837: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 12:29:04.836: INFO: Number of nodes with available pods: 1
Dec 20 12:29:04.836: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 12:29:05.831: INFO: Number of nodes with available pods: 3
Dec 20 12:29:05.831: INFO: Number of running nodes: 3, number of available pods: 3
Dec 20 12:29:05.831: INFO: Update the DaemonSet to trigger a rollout
Dec 20 12:29:05.924: INFO: Updating DaemonSet daemon-set
Dec 20 12:29:10.967: INFO: Roll back the DaemonSet before rollout is complete
Dec 20 12:29:10.995: INFO: Updating DaemonSet daemon-set
Dec 20 12:29:10.995: INFO: Make sure DaemonSet rollback is complete
Dec 20 12:29:11.006: INFO: Wrong image for pod: daemon-set-6m5p5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 20 12:29:11.006: INFO: Pod daemon-set-6m5p5 is not available
Dec 20 12:29:12.020: INFO: Pod daemon-set-fsbk2 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6608, will wait for the garbage collector to delete the pods
Dec 20 12:29:12.130: INFO: Deleting DaemonSet.extensions daemon-set took: 23.841684ms
Dec 20 12:29:12.631: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.657516ms
Dec 20 12:29:19.238: INFO: Number of nodes with available pods: 0
Dec 20 12:29:19.238: INFO: Number of running nodes: 0, number of available pods: 0
Dec 20 12:29:19.244: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6608/daemonsets","resourceVersion":"37349"},"items":null}

Dec 20 12:29:19.249: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6608/pods","resourceVersion":"37349"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:29:19.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6608" for this suite.
Dec 20 12:29:25.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:29:25.589: INFO: namespace daemonsets-6608 deletion completed in 6.301435839s

• [SLOW TEST:27.064 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:29:25.590: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9432
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 20 12:29:25.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-9432'
Dec 20 12:29:25.996: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 20 12:29:25.996: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Dec 20 12:29:28.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 delete deployment e2e-test-httpd-deployment --namespace=kubectl-9432'
Dec 20 12:29:28.140: INFO: stderr: ""
Dec 20 12:29:28.140: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:29:28.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9432" for this suite.
Dec 20 12:30:48.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:30:48.459: INFO: namespace kubectl-9432 deletion completed in 1m20.308747916s

• [SLOW TEST:82.870 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:30:48.464: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9544
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec 20 12:30:55.272: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9544 pod-service-account-1103351c-aba2-4c63-86fd-4bb997e71617 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec 20 12:31:16.786: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9544 pod-service-account-1103351c-aba2-4c63-86fd-4bb997e71617 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec 20 12:31:17.522: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9544 pod-service-account-1103351c-aba2-4c63-86fd-4bb997e71617 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:31:18.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9544" for this suite.
Dec 20 12:31:24.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:31:24.545: INFO: namespace svcaccounts-9544 deletion completed in 6.296036429s

• [SLOW TEST:36.082 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:31:24.547: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8394
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-fab933f1-a361-4aa9-84f0-e3307ef865c4 in namespace container-probe-8394
Dec 20 12:31:34.891: INFO: Started pod test-webserver-fab933f1-a361-4aa9-84f0-e3307ef865c4 in namespace container-probe-8394
STEP: checking the pod's current state and verifying that restartCount is present
Dec 20 12:31:34.898: INFO: Initial restart count of pod test-webserver-fab933f1-a361-4aa9-84f0-e3307ef865c4 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:35:36.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8394" for this suite.
Dec 20 12:35:44.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:35:45.139: INFO: namespace container-probe-8394 deletion completed in 8.296347637s

• [SLOW TEST:260.592 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:35:45.139: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4239
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Dec 20 12:35:52.213: INFO: Pod pod-hostip-cf71bb7f-da31-4c0c-b674-fe1ec2df3e9f has hostIP: 192.168.1.3
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:35:52.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4239" for this suite.
Dec 20 12:36:14.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:36:15.907: INFO: namespace pods-4239 deletion completed in 23.684703098s

• [SLOW TEST:30.768 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:36:15.909: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4562
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-b51df3ef-c17f-4985-9785-f485240b8783
STEP: Creating a pod to test consume configMaps
Dec 20 12:36:16.286: INFO: Waiting up to 5m0s for pod "pod-configmaps-b1308478-7697-4705-85df-283d668c8744" in namespace "configmap-4562" to be "success or failure"
Dec 20 12:36:16.854: INFO: Pod "pod-configmaps-b1308478-7697-4705-85df-283d668c8744": Phase="Pending", Reason="", readiness=false. Elapsed: 568.776236ms
Dec 20 12:36:18.867: INFO: Pod "pod-configmaps-b1308478-7697-4705-85df-283d668c8744": Phase="Pending", Reason="", readiness=false. Elapsed: 2.581296305s
Dec 20 12:36:20.891: INFO: Pod "pod-configmaps-b1308478-7697-4705-85df-283d668c8744": Phase="Pending", Reason="", readiness=false. Elapsed: 4.605061271s
Dec 20 12:36:22.897: INFO: Pod "pod-configmaps-b1308478-7697-4705-85df-283d668c8744": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.611514264s
STEP: Saw pod success
Dec 20 12:36:22.897: INFO: Pod "pod-configmaps-b1308478-7697-4705-85df-283d668c8744" satisfied condition "success or failure"
Dec 20 12:36:22.903: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-configmaps-b1308478-7697-4705-85df-283d668c8744 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 20 12:36:22.963: INFO: Waiting for pod pod-configmaps-b1308478-7697-4705-85df-283d668c8744 to disappear
Dec 20 12:36:22.968: INFO: Pod pod-configmaps-b1308478-7697-4705-85df-283d668c8744 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:36:22.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4562" for this suite.
Dec 20 12:36:35.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:36:36.033: INFO: namespace configmap-4562 deletion completed in 13.057777359s

• [SLOW TEST:20.124 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:36:36.034: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-165
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 12:36:36.380: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec 20 12:36:36.405: INFO: Number of nodes with available pods: 0
Dec 20 12:36:36.405: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 12:36:37.426: INFO: Number of nodes with available pods: 0
Dec 20 12:36:37.426: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 12:36:38.422: INFO: Number of nodes with available pods: 0
Dec 20 12:36:38.422: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 12:36:39.421: INFO: Number of nodes with available pods: 0
Dec 20 12:36:39.421: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 12:36:40.425: INFO: Number of nodes with available pods: 0
Dec 20 12:36:40.425: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 12:36:41.425: INFO: Number of nodes with available pods: 2
Dec 20 12:36:41.425: INFO: Node metakube-worker-457fd-578f899757-zpxc2 is running more than one daemon pod
Dec 20 12:36:42.422: INFO: Number of nodes with available pods: 2
Dec 20 12:36:42.422: INFO: Node metakube-worker-457fd-578f899757-zpxc2 is running more than one daemon pod
Dec 20 12:36:43.422: INFO: Number of nodes with available pods: 2
Dec 20 12:36:43.422: INFO: Node metakube-worker-457fd-578f899757-zpxc2 is running more than one daemon pod
Dec 20 12:36:44.421: INFO: Number of nodes with available pods: 2
Dec 20 12:36:44.421: INFO: Node metakube-worker-457fd-578f899757-zpxc2 is running more than one daemon pod
Dec 20 12:36:45.422: INFO: Number of nodes with available pods: 3
Dec 20 12:36:45.422: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec 20 12:36:45.490: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:45.491: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:45.491: INFO: Wrong image for pod: daemon-set-ltvf7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:46.550: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:46.550: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:46.550: INFO: Wrong image for pod: daemon-set-ltvf7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:47.566: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:47.566: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:47.566: INFO: Wrong image for pod: daemon-set-ltvf7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:48.552: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:48.552: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:48.552: INFO: Wrong image for pod: daemon-set-ltvf7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:49.549: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:49.550: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:49.550: INFO: Wrong image for pod: daemon-set-ltvf7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:49.550: INFO: Pod daemon-set-ltvf7 is not available
Dec 20 12:36:50.550: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:50.550: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:50.550: INFO: Wrong image for pod: daemon-set-ltvf7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:50.550: INFO: Pod daemon-set-ltvf7 is not available
Dec 20 12:36:51.552: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:51.552: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:51.552: INFO: Wrong image for pod: daemon-set-ltvf7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:51.552: INFO: Pod daemon-set-ltvf7 is not available
Dec 20 12:36:52.550: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:52.550: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:52.550: INFO: Wrong image for pod: daemon-set-ltvf7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:52.550: INFO: Pod daemon-set-ltvf7 is not available
Dec 20 12:36:53.699: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:53.699: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:53.699: INFO: Wrong image for pod: daemon-set-ltvf7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:53.699: INFO: Pod daemon-set-ltvf7 is not available
Dec 20 12:36:54.551: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:54.551: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:54.551: INFO: Wrong image for pod: daemon-set-ltvf7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:54.551: INFO: Pod daemon-set-ltvf7 is not available
Dec 20 12:36:55.551: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:55.551: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:55.551: INFO: Pod daemon-set-xf9vj is not available
Dec 20 12:36:56.550: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:56.550: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:56.550: INFO: Pod daemon-set-xf9vj is not available
Dec 20 12:36:57.557: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:57.557: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:57.558: INFO: Pod daemon-set-xf9vj is not available
Dec 20 12:36:58.551: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:58.552: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:58.552: INFO: Pod daemon-set-xf9vj is not available
Dec 20 12:36:59.558: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:59.559: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:36:59.559: INFO: Pod daemon-set-xf9vj is not available
Dec 20 12:37:00.551: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:00.551: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:00.551: INFO: Pod daemon-set-xf9vj is not available
Dec 20 12:37:01.556: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:01.556: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:01.556: INFO: Pod daemon-set-xf9vj is not available
Dec 20 12:37:02.550: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:02.550: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:02.550: INFO: Pod daemon-set-xf9vj is not available
Dec 20 12:37:03.582: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:03.582: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:03.582: INFO: Pod daemon-set-xf9vj is not available
Dec 20 12:37:04.550: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:04.550: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:05.549: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:05.549: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:06.549: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:06.550: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:07.550: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:07.550: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:08.557: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:08.557: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:09.550: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:09.550: INFO: Pod daemon-set-78nkc is not available
Dec 20 12:37:09.550: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:10.555: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:10.555: INFO: Pod daemon-set-78nkc is not available
Dec 20 12:37:10.555: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:11.554: INFO: Wrong image for pod: daemon-set-78nkc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:11.554: INFO: Pod daemon-set-78nkc is not available
Dec 20 12:37:11.554: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:12.560: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:13.559: INFO: Pod daemon-set-8wrcc is not available
Dec 20 12:37:13.559: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:14.550: INFO: Pod daemon-set-8wrcc is not available
Dec 20 12:37:14.551: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:15.552: INFO: Pod daemon-set-8wrcc is not available
Dec 20 12:37:15.552: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:16.548: INFO: Pod daemon-set-8wrcc is not available
Dec 20 12:37:16.548: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:17.551: INFO: Pod daemon-set-8wrcc is not available
Dec 20 12:37:17.551: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:18.560: INFO: Pod daemon-set-8wrcc is not available
Dec 20 12:37:18.560: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:19.551: INFO: Pod daemon-set-8wrcc is not available
Dec 20 12:37:19.551: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:20.550: INFO: Pod daemon-set-8wrcc is not available
Dec 20 12:37:20.551: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:21.549: INFO: Pod daemon-set-8wrcc is not available
Dec 20 12:37:21.549: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:22.552: INFO: Pod daemon-set-8wrcc is not available
Dec 20 12:37:22.553: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:23.550: INFO: Pod daemon-set-8wrcc is not available
Dec 20 12:37:23.550: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:24.550: INFO: Pod daemon-set-8wrcc is not available
Dec 20 12:37:24.550: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:25.551: INFO: Pod daemon-set-8wrcc is not available
Dec 20 12:37:25.551: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:26.563: INFO: Pod daemon-set-8wrcc is not available
Dec 20 12:37:26.563: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:27.551: INFO: Pod daemon-set-8wrcc is not available
Dec 20 12:37:27.551: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:28.555: INFO: Pod daemon-set-8wrcc is not available
Dec 20 12:37:28.555: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:29.789: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:30.549: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:31.550: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:32.619: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:32.619: INFO: Pod daemon-set-dhmzl is not available
Dec 20 12:37:33.703: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:33.703: INFO: Pod daemon-set-dhmzl is not available
Dec 20 12:37:39.102: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:39.102: INFO: Pod daemon-set-dhmzl is not available
Dec 20 12:37:41.434: INFO: Wrong image for pod: daemon-set-dhmzl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 20 12:37:41.434: INFO: Pod daemon-set-dhmzl is not available
Dec 20 12:37:41.837: INFO: Pod daemon-set-rt69r is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Dec 20 12:37:41.869: INFO: Number of nodes with available pods: 2
Dec 20 12:37:41.870: INFO: Node metakube-worker-457fd-578f899757-zpxc2 is running more than one daemon pod
Dec 20 12:37:42.890: INFO: Number of nodes with available pods: 2
Dec 20 12:37:42.890: INFO: Node metakube-worker-457fd-578f899757-zpxc2 is running more than one daemon pod
Dec 20 12:37:43.888: INFO: Number of nodes with available pods: 2
Dec 20 12:37:43.888: INFO: Node metakube-worker-457fd-578f899757-zpxc2 is running more than one daemon pod
Dec 20 12:37:45.066: INFO: Number of nodes with available pods: 2
Dec 20 12:37:45.066: INFO: Node metakube-worker-457fd-578f899757-zpxc2 is running more than one daemon pod
Dec 20 12:37:46.013: INFO: Number of nodes with available pods: 2
Dec 20 12:37:46.013: INFO: Node metakube-worker-457fd-578f899757-zpxc2 is running more than one daemon pod
Dec 20 12:37:46.888: INFO: Number of nodes with available pods: 2
Dec 20 12:37:46.888: INFO: Node metakube-worker-457fd-578f899757-zpxc2 is running more than one daemon pod
Dec 20 12:37:51.064: INFO: Number of nodes with available pods: 2
Dec 20 12:37:51.064: INFO: Node metakube-worker-457fd-578f899757-zpxc2 is running more than one daemon pod
Dec 20 12:37:51.886: INFO: Number of nodes with available pods: 2
Dec 20 12:37:51.887: INFO: Node metakube-worker-457fd-578f899757-zpxc2 is running more than one daemon pod
Dec 20 12:37:53.754: INFO: Number of nodes with available pods: 2
Dec 20 12:37:53.754: INFO: Node metakube-worker-457fd-578f899757-zpxc2 is running more than one daemon pod
Dec 20 12:37:53.906: INFO: Number of nodes with available pods: 2
Dec 20 12:37:53.906: INFO: Node metakube-worker-457fd-578f899757-zpxc2 is running more than one daemon pod
Dec 20 12:37:54.921: INFO: Number of nodes with available pods: 2
Dec 20 12:37:54.921: INFO: Node metakube-worker-457fd-578f899757-zpxc2 is running more than one daemon pod
Dec 20 12:37:55.887: INFO: Number of nodes with available pods: 2
Dec 20 12:37:55.888: INFO: Node metakube-worker-457fd-578f899757-zpxc2 is running more than one daemon pod
Dec 20 12:37:56.885: INFO: Number of nodes with available pods: 2
Dec 20 12:37:56.885: INFO: Node metakube-worker-457fd-578f899757-zpxc2 is running more than one daemon pod
Dec 20 12:37:57.896: INFO: Number of nodes with available pods: 3
Dec 20 12:37:57.896: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-165, will wait for the garbage collector to delete the pods
Dec 20 12:37:58.015: INFO: Deleting DaemonSet.extensions daemon-set took: 19.495831ms
Dec 20 12:37:58.516: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.39878ms
Dec 20 12:38:12.624: INFO: Number of nodes with available pods: 0
Dec 20 12:38:12.624: INFO: Number of running nodes: 0, number of available pods: 0
Dec 20 12:38:12.629: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-165/daemonsets","resourceVersion":"39202"},"items":null}

Dec 20 12:38:12.685: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-165/pods","resourceVersion":"39202"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:38:12.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-165" for this suite.
Dec 20 12:38:20.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:38:21.294: INFO: namespace daemonsets-165 deletion completed in 8.529770247s

• [SLOW TEST:105.261 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:38:21.295: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-907
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:38:34.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-907" for this suite.
Dec 20 12:38:40.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:38:41.028: INFO: namespace resourcequota-907 deletion completed in 6.354864221s

• [SLOW TEST:19.733 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:38:41.029: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1553
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:38:52.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1553" for this suite.
Dec 20 12:39:02.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:39:02.317: INFO: namespace resourcequota-1553 deletion completed in 9.817625851s

• [SLOW TEST:21.288 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:39:02.318: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5442
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-5442
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 20 12:39:02.781: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 20 12:39:45.492: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.2.139:8080/dial?request=hostName&protocol=http&host=172.25.1.193&port=8080&tries=1'] Namespace:pod-network-test-5442 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 20 12:39:45.492: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 12:39:46.034: INFO: Waiting for endpoints: map[]
Dec 20 12:39:46.046: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.2.139:8080/dial?request=hostName&protocol=http&host=172.25.2.138&port=8080&tries=1'] Namespace:pod-network-test-5442 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 20 12:39:46.047: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 12:39:46.589: INFO: Waiting for endpoints: map[]
Dec 20 12:39:46.596: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.2.139:8080/dial?request=hostName&protocol=http&host=172.25.0.42&port=8080&tries=1'] Namespace:pod-network-test-5442 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 20 12:39:46.596: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 12:39:47.161: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:39:47.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5442" for this suite.
Dec 20 12:40:06.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:40:07.970: INFO: namespace pod-network-test-5442 deletion completed in 20.80151443s

• [SLOW TEST:65.652 seconds]
[sig-network] Networking
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:40:07.971: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5442
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 20 12:40:08.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-5442'
Dec 20 12:40:09.110: INFO: stderr: ""
Dec 20 12:40:09.110: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Dec 20 12:40:19.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pod e2e-test-httpd-pod --namespace=kubectl-5442 -o json'
Dec 20 12:40:19.284: INFO: stderr: ""
Dec 20 12:40:19.284: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.25.2.140/32\"\n        },\n        \"creationTimestamp\": \"2019-12-20T12:40:09Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5442\",\n        \"resourceVersion\": \"39743\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5442/pods/e2e-test-httpd-pod\",\n        \"uid\": \"850ad799-b79a-40f4-9ca5-f14e35aeef47\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-jb9nm\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"metakube-worker-457fd-578f899757-zpxc2\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-jb9nm\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-jb9nm\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-20T12:40:09Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-20T12:40:16Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-20T12:40:16Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-20T12:40:09Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://b008c181680c0e6c12e969ad3edd67df4d822b3fd967806299bb0b8ece16f9e7\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-12-20T12:40:15Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.1.11\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.25.2.140\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.25.2.140\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-12-20T12:40:09Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec 20 12:40:19.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 replace -f - --namespace=kubectl-5442'
Dec 20 12:40:19.891: INFO: stderr: ""
Dec 20 12:40:19.891: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Dec 20 12:40:19.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 delete pods e2e-test-httpd-pod --namespace=kubectl-5442'
Dec 20 12:40:26.657: INFO: stderr: ""
Dec 20 12:40:26.657: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:40:26.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5442" for this suite.
Dec 20 12:40:35.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:40:36.594: INFO: namespace kubectl-5442 deletion completed in 9.927104803s

• [SLOW TEST:28.623 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:40:36.595: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8943
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-fj9r
STEP: Creating a pod to test atomic-volume-subpath
Dec 20 12:40:37.456: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-fj9r" in namespace "subpath-8943" to be "success or failure"
Dec 20 12:40:37.462: INFO: Pod "pod-subpath-test-configmap-fj9r": Phase="Pending", Reason="", readiness=false. Elapsed: 5.05162ms
Dec 20 12:40:39.469: INFO: Pod "pod-subpath-test-configmap-fj9r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011938853s
Dec 20 12:40:41.687: INFO: Pod "pod-subpath-test-configmap-fj9r": Phase="Pending", Reason="", readiness=false. Elapsed: 4.230355674s
Dec 20 12:40:43.938: INFO: Pod "pod-subpath-test-configmap-fj9r": Phase="Running", Reason="", readiness=true. Elapsed: 6.481228299s
Dec 20 12:40:45.948: INFO: Pod "pod-subpath-test-configmap-fj9r": Phase="Running", Reason="", readiness=true. Elapsed: 8.491324901s
Dec 20 12:40:47.956: INFO: Pod "pod-subpath-test-configmap-fj9r": Phase="Running", Reason="", readiness=true. Elapsed: 10.499235615s
Dec 20 12:40:49.964: INFO: Pod "pod-subpath-test-configmap-fj9r": Phase="Running", Reason="", readiness=true. Elapsed: 12.507509718s
Dec 20 12:40:51.971: INFO: Pod "pod-subpath-test-configmap-fj9r": Phase="Running", Reason="", readiness=true. Elapsed: 14.513985859s
Dec 20 12:40:53.977: INFO: Pod "pod-subpath-test-configmap-fj9r": Phase="Running", Reason="", readiness=true. Elapsed: 16.520459022s
Dec 20 12:40:55.984: INFO: Pod "pod-subpath-test-configmap-fj9r": Phase="Running", Reason="", readiness=true. Elapsed: 18.527490793s
Dec 20 12:40:57.992: INFO: Pod "pod-subpath-test-configmap-fj9r": Phase="Running", Reason="", readiness=true. Elapsed: 20.535272534s
Dec 20 12:40:59.999: INFO: Pod "pod-subpath-test-configmap-fj9r": Phase="Running", Reason="", readiness=true. Elapsed: 22.542405933s
Dec 20 12:41:02.032: INFO: Pod "pod-subpath-test-configmap-fj9r": Phase="Running", Reason="", readiness=true. Elapsed: 24.575693021s
Dec 20 12:41:04.046: INFO: Pod "pod-subpath-test-configmap-fj9r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.589060022s
STEP: Saw pod success
Dec 20 12:41:04.046: INFO: Pod "pod-subpath-test-configmap-fj9r" satisfied condition "success or failure"
Dec 20 12:41:04.056: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-subpath-test-configmap-fj9r container test-container-subpath-configmap-fj9r: <nil>
STEP: delete the pod
Dec 20 12:41:04.129: INFO: Waiting for pod pod-subpath-test-configmap-fj9r to disappear
Dec 20 12:41:04.136: INFO: Pod pod-subpath-test-configmap-fj9r no longer exists
STEP: Deleting pod pod-subpath-test-configmap-fj9r
Dec 20 12:41:04.136: INFO: Deleting pod "pod-subpath-test-configmap-fj9r" in namespace "subpath-8943"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:41:04.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8943" for this suite.
Dec 20 12:41:20.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:41:21.166: INFO: namespace subpath-8943 deletion completed in 17.016062533s

• [SLOW TEST:44.571 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:41:21.167: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8669
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Dec 20 12:41:21.551: INFO: Waiting up to 5m0s for pod "var-expansion-da74c41c-f825-451c-81db-c0afae6b26e5" in namespace "var-expansion-8669" to be "success or failure"
Dec 20 12:41:21.560: INFO: Pod "var-expansion-da74c41c-f825-451c-81db-c0afae6b26e5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.613589ms
Dec 20 12:41:23.575: INFO: Pod "var-expansion-da74c41c-f825-451c-81db-c0afae6b26e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024106447s
Dec 20 12:41:25.583: INFO: Pod "var-expansion-da74c41c-f825-451c-81db-c0afae6b26e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031564975s
Dec 20 12:41:27.593: INFO: Pod "var-expansion-da74c41c-f825-451c-81db-c0afae6b26e5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041628043s
Dec 20 12:41:29.601: INFO: Pod "var-expansion-da74c41c-f825-451c-81db-c0afae6b26e5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.049686767s
Dec 20 12:41:31.609: INFO: Pod "var-expansion-da74c41c-f825-451c-81db-c0afae6b26e5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.057939629s
Dec 20 12:41:33.920: INFO: Pod "var-expansion-da74c41c-f825-451c-81db-c0afae6b26e5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.368536984s
Dec 20 12:41:35.934: INFO: Pod "var-expansion-da74c41c-f825-451c-81db-c0afae6b26e5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.382683802s
Dec 20 12:41:37.944: INFO: Pod "var-expansion-da74c41c-f825-451c-81db-c0afae6b26e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.392158412s
STEP: Saw pod success
Dec 20 12:41:37.944: INFO: Pod "var-expansion-da74c41c-f825-451c-81db-c0afae6b26e5" satisfied condition "success or failure"
Dec 20 12:41:37.951: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod var-expansion-da74c41c-f825-451c-81db-c0afae6b26e5 container dapi-container: <nil>
STEP: delete the pod
Dec 20 12:41:38.074: INFO: Waiting for pod var-expansion-da74c41c-f825-451c-81db-c0afae6b26e5 to disappear
Dec 20 12:41:38.083: INFO: Pod var-expansion-da74c41c-f825-451c-81db-c0afae6b26e5 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:41:38.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8669" for this suite.
Dec 20 12:41:44.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:41:44.485: INFO: namespace var-expansion-8669 deletion completed in 6.384515927s

• [SLOW TEST:23.319 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:41:44.486: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-696
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec 20 12:41:44.785: INFO: Waiting up to 5m0s for pod "pod-e4fb0a2b-6a33-48c4-9869-6ff187c97be1" in namespace "emptydir-696" to be "success or failure"
Dec 20 12:41:44.809: INFO: Pod "pod-e4fb0a2b-6a33-48c4-9869-6ff187c97be1": Phase="Pending", Reason="", readiness=false. Elapsed: 23.766778ms
Dec 20 12:41:46.816: INFO: Pod "pod-e4fb0a2b-6a33-48c4-9869-6ff187c97be1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030545427s
Dec 20 12:41:48.826: INFO: Pod "pod-e4fb0a2b-6a33-48c4-9869-6ff187c97be1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040646745s
Dec 20 12:41:50.833: INFO: Pod "pod-e4fb0a2b-6a33-48c4-9869-6ff187c97be1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047459622s
STEP: Saw pod success
Dec 20 12:41:50.833: INFO: Pod "pod-e4fb0a2b-6a33-48c4-9869-6ff187c97be1" satisfied condition "success or failure"
Dec 20 12:41:50.840: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-e4fb0a2b-6a33-48c4-9869-6ff187c97be1 container test-container: <nil>
STEP: delete the pod
Dec 20 12:41:51.000: INFO: Waiting for pod pod-e4fb0a2b-6a33-48c4-9869-6ff187c97be1 to disappear
Dec 20 12:41:51.006: INFO: Pod pod-e4fb0a2b-6a33-48c4-9869-6ff187c97be1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:41:51.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-696" for this suite.
Dec 20 12:41:57.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:41:57.435: INFO: namespace emptydir-696 deletion completed in 6.419188283s

• [SLOW TEST:12.948 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:41:57.435: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9074
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Dec 20 12:41:57.767: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 12:42:02.818: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:42:24.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9074" for this suite.
Dec 20 12:42:32.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:42:33.304: INFO: namespace crd-publish-openapi-9074 deletion completed in 7.198014621s

• [SLOW TEST:35.869 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:42:33.306: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-7286
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Dec 20 12:42:42.265: INFO: Successfully updated pod "adopt-release-v2zqk"
STEP: Checking that the Job readopts the Pod
Dec 20 12:42:42.265: INFO: Waiting up to 15m0s for pod "adopt-release-v2zqk" in namespace "job-7286" to be "adopted"
Dec 20 12:42:42.270: INFO: Pod "adopt-release-v2zqk": Phase="Running", Reason="", readiness=true. Elapsed: 5.138678ms
Dec 20 12:42:44.324: INFO: Pod "adopt-release-v2zqk": Phase="Running", Reason="", readiness=true. Elapsed: 2.058759452s
Dec 20 12:42:44.324: INFO: Pod "adopt-release-v2zqk" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Dec 20 12:42:44.882: INFO: Successfully updated pod "adopt-release-v2zqk"
STEP: Checking that the Job releases the Pod
Dec 20 12:42:44.883: INFO: Waiting up to 15m0s for pod "adopt-release-v2zqk" in namespace "job-7286" to be "released"
Dec 20 12:42:45.566: INFO: Pod "adopt-release-v2zqk": Phase="Running", Reason="", readiness=true. Elapsed: 683.398657ms
Dec 20 12:42:45.566: INFO: Pod "adopt-release-v2zqk" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:42:45.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7286" for this suite.
Dec 20 12:43:41.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:43:44.834: INFO: namespace job-7286 deletion completed in 59.224945623s

• [SLOW TEST:71.528 seconds]
[sig-apps] Job
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:43:44.835: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3930
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 20 12:43:45.504: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 20 12:43:45.527: INFO: Waiting for terminating namespaces to be deleted...
Dec 20 12:43:45.533: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-457fd-578f899757-428w5 before test
Dec 20 12:43:45.602: INFO: sonobuoy-systemd-logs-daemon-set-73919c9b23754985-gvq9z from sonobuoy started at 2019-12-20 10:42:00 +0000 UTC (2 container statuses recorded)
Dec 20 12:43:45.602: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Dec 20 12:43:45.602: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 20 12:43:45.602: INFO: node-exporter-6fmjz from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 12:43:45.603: INFO: 	Container node-exporter ready: true, restart count 0
Dec 20 12:43:45.603: INFO: canal-64h7p from kube-system started at 2019-12-20 10:27:04 +0000 UTC (2 container statuses recorded)
Dec 20 12:43:45.603: INFO: 	Container calico-node ready: true, restart count 0
Dec 20 12:43:45.603: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 20 12:43:45.603: INFO: kube-proxy-nfqd6 from kube-system started at 2019-12-20 10:26:58 +0000 UTC (1 container statuses recorded)
Dec 20 12:43:45.603: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 20 12:43:45.603: INFO: sonobuoy from sonobuoy started at 2019-12-20 10:41:51 +0000 UTC (1 container statuses recorded)
Dec 20 12:43:45.604: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 20 12:43:45.604: INFO: node-local-dns-kj7kj from kube-system started at 2019-12-20 10:26:58 +0000 UTC (1 container statuses recorded)
Dec 20 12:43:45.604: INFO: 	Container node-cache ready: true, restart count 0
Dec 20 12:43:45.604: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-457fd-578f899757-dsqmn before test
Dec 20 12:43:45.809: INFO: coredns-547f89d7d5-s26jq from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 12:43:45.809: INFO: 	Container coredns ready: true, restart count 0
Dec 20 12:43:45.809: INFO: cluster-autoscaler-8c65c7d54-9cqhv from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 12:43:45.810: INFO: 	Container cluster-autoscaler ready: true, restart count 0
Dec 20 12:43:45.810: INFO: node-local-dns-7tvth from kube-system started at 2019-12-20 10:26:26 +0000 UTC (1 container statuses recorded)
Dec 20 12:43:45.810: INFO: 	Container node-cache ready: true, restart count 0
Dec 20 12:43:45.810: INFO: node-exporter-qnl6p from kube-system started at 2019-12-20 10:26:26 +0000 UTC (1 container statuses recorded)
Dec 20 12:43:45.810: INFO: 	Container node-exporter ready: true, restart count 0
Dec 20 12:43:45.810: INFO: sonobuoy-systemd-logs-daemon-set-73919c9b23754985-dp62g from sonobuoy started at 2019-12-20 10:42:00 +0000 UTC (2 container statuses recorded)
Dec 20 12:43:45.811: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Dec 20 12:43:45.811: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 20 12:43:45.811: INFO: tiller-deploy-78f78bb476-6t2mk from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 12:43:45.811: INFO: 	Container tiller ready: true, restart count 0
Dec 20 12:43:45.811: INFO: openvpn-client-64df8b95c9-wr7dw from kube-system started at 2019-12-20 10:27:01 +0000 UTC (2 container statuses recorded)
Dec 20 12:43:45.812: INFO: 	Container dnat-controller ready: true, restart count 0
Dec 20 12:43:45.812: INFO: 	Container openvpn-client ready: true, restart count 0
Dec 20 12:43:45.812: INFO: coredns-547f89d7d5-5w7wq from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 12:43:45.812: INFO: 	Container coredns ready: true, restart count 0
Dec 20 12:43:45.812: INFO: kube-proxy-wq825 from kube-system started at 2019-12-20 10:26:26 +0000 UTC (1 container statuses recorded)
Dec 20 12:43:45.812: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 20 12:43:45.813: INFO: canal-gcrnc from kube-system started at 2019-12-20 10:26:27 +0000 UTC (2 container statuses recorded)
Dec 20 12:43:45.813: INFO: 	Container calico-node ready: true, restart count 0
Dec 20 12:43:45.813: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 20 12:43:45.813: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-457fd-578f899757-zpxc2 before test
Dec 20 12:43:45.872: INFO: node-exporter-wxf2r from kube-system started at 2019-12-20 10:27:10 +0000 UTC (1 container statuses recorded)
Dec 20 12:43:45.872: INFO: 	Container node-exporter ready: true, restart count 0
Dec 20 12:43:45.872: INFO: sonobuoy-e2e-job-1e71c3c7847b472c from sonobuoy started at 2019-12-20 10:42:00 +0000 UTC (2 container statuses recorded)
Dec 20 12:43:45.872: INFO: 	Container e2e ready: true, restart count 0
Dec 20 12:43:45.872: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 20 12:43:45.873: INFO: node-local-dns-4wkt5 from kube-system started at 2019-12-20 10:27:10 +0000 UTC (1 container statuses recorded)
Dec 20 12:43:45.873: INFO: 	Container node-cache ready: true, restart count 0
Dec 20 12:43:45.873: INFO: canal-w56q4 from kube-system started at 2019-12-20 10:27:10 +0000 UTC (2 container statuses recorded)
Dec 20 12:43:45.873: INFO: 	Container calico-node ready: true, restart count 0
Dec 20 12:43:45.873: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 20 12:43:45.873: INFO: sonobuoy-systemd-logs-daemon-set-73919c9b23754985-gwhsz from sonobuoy started at 2019-12-20 10:42:00 +0000 UTC (2 container statuses recorded)
Dec 20 12:43:45.873: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Dec 20 12:43:45.873: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 20 12:43:45.878: INFO: kube-proxy-pml2l from kube-system started at 2019-12-20 10:27:10 +0000 UTC (1 container statuses recorded)
Dec 20 12:43:45.878: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15e214ec9c783e40], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:43:46.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3930" for this suite.
Dec 20 12:43:52.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:43:53.339: INFO: namespace sched-pred-3930 deletion completed in 6.373240318s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:8.504 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:43:53.341: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4813
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:44:07.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4813" for this suite.
Dec 20 12:44:16.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:44:16.318: INFO: namespace resourcequota-4813 deletion completed in 9.047743592s

• [SLOW TEST:22.977 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:44:16.319: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8429
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-h4ck
STEP: Creating a pod to test atomic-volume-subpath
Dec 20 12:44:16.687: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-h4ck" in namespace "subpath-8429" to be "success or failure"
Dec 20 12:44:16.700: INFO: Pod "pod-subpath-test-secret-h4ck": Phase="Pending", Reason="", readiness=false. Elapsed: 12.832663ms
Dec 20 12:44:18.706: INFO: Pod "pod-subpath-test-secret-h4ck": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019184553s
Dec 20 12:44:20.714: INFO: Pod "pod-subpath-test-secret-h4ck": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026919172s
Dec 20 12:44:22.732: INFO: Pod "pod-subpath-test-secret-h4ck": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044777493s
Dec 20 12:44:24.742: INFO: Pod "pod-subpath-test-secret-h4ck": Phase="Pending", Reason="", readiness=false. Elapsed: 8.054929858s
Dec 20 12:44:26.767: INFO: Pod "pod-subpath-test-secret-h4ck": Phase="Running", Reason="", readiness=true. Elapsed: 10.08015017s
Dec 20 12:44:28.774: INFO: Pod "pod-subpath-test-secret-h4ck": Phase="Running", Reason="", readiness=true. Elapsed: 12.087127099s
Dec 20 12:44:30.781: INFO: Pod "pod-subpath-test-secret-h4ck": Phase="Running", Reason="", readiness=true. Elapsed: 14.093958079s
Dec 20 12:44:32.794: INFO: Pod "pod-subpath-test-secret-h4ck": Phase="Running", Reason="", readiness=true. Elapsed: 16.107230984s
Dec 20 12:44:34.801: INFO: Pod "pod-subpath-test-secret-h4ck": Phase="Running", Reason="", readiness=true. Elapsed: 18.114063492s
Dec 20 12:44:36.808: INFO: Pod "pod-subpath-test-secret-h4ck": Phase="Running", Reason="", readiness=true. Elapsed: 20.120948708s
Dec 20 12:44:38.817: INFO: Pod "pod-subpath-test-secret-h4ck": Phase="Running", Reason="", readiness=true. Elapsed: 22.129754414s
Dec 20 12:44:40.825: INFO: Pod "pod-subpath-test-secret-h4ck": Phase="Running", Reason="", readiness=true. Elapsed: 24.138537039s
Dec 20 12:44:42.835: INFO: Pod "pod-subpath-test-secret-h4ck": Phase="Running", Reason="", readiness=true. Elapsed: 26.148149469s
Dec 20 12:44:44.842: INFO: Pod "pod-subpath-test-secret-h4ck": Phase="Running", Reason="", readiness=true. Elapsed: 28.155556456s
Dec 20 12:44:46.850: INFO: Pod "pod-subpath-test-secret-h4ck": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.163198442s
STEP: Saw pod success
Dec 20 12:44:46.850: INFO: Pod "pod-subpath-test-secret-h4ck" satisfied condition "success or failure"
Dec 20 12:44:46.856: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-subpath-test-secret-h4ck container test-container-subpath-secret-h4ck: <nil>
STEP: delete the pod
Dec 20 12:44:46.996: INFO: Waiting for pod pod-subpath-test-secret-h4ck to disappear
Dec 20 12:44:47.003: INFO: Pod pod-subpath-test-secret-h4ck no longer exists
STEP: Deleting pod pod-subpath-test-secret-h4ck
Dec 20 12:44:47.003: INFO: Deleting pod "pod-subpath-test-secret-h4ck" in namespace "subpath-8429"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:44:47.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8429" for this suite.
Dec 20 12:44:53.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:44:53.432: INFO: namespace subpath-8429 deletion completed in 6.409776886s

• [SLOW TEST:37.113 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:44:53.433: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4344
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 20 12:44:53.876: INFO: Waiting up to 5m0s for pod "downwardapi-volume-42579ed6-fd1c-493c-aaf9-571b374987f4" in namespace "downward-api-4344" to be "success or failure"
Dec 20 12:44:53.883: INFO: Pod "downwardapi-volume-42579ed6-fd1c-493c-aaf9-571b374987f4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.516613ms
Dec 20 12:44:55.891: INFO: Pod "downwardapi-volume-42579ed6-fd1c-493c-aaf9-571b374987f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014839416s
Dec 20 12:44:57.901: INFO: Pod "downwardapi-volume-42579ed6-fd1c-493c-aaf9-571b374987f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025004527s
Dec 20 12:45:00.232: INFO: Pod "downwardapi-volume-42579ed6-fd1c-493c-aaf9-571b374987f4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.356457202s
Dec 20 12:45:02.392: INFO: Pod "downwardapi-volume-42579ed6-fd1c-493c-aaf9-571b374987f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.516185193s
STEP: Saw pod success
Dec 20 12:45:02.392: INFO: Pod "downwardapi-volume-42579ed6-fd1c-493c-aaf9-571b374987f4" satisfied condition "success or failure"
Dec 20 12:45:02.399: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod downwardapi-volume-42579ed6-fd1c-493c-aaf9-571b374987f4 container client-container: <nil>
STEP: delete the pod
Dec 20 12:45:02.583: INFO: Waiting for pod downwardapi-volume-42579ed6-fd1c-493c-aaf9-571b374987f4 to disappear
Dec 20 12:45:02.592: INFO: Pod downwardapi-volume-42579ed6-fd1c-493c-aaf9-571b374987f4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:45:02.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4344" for this suite.
Dec 20 12:45:08.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:45:09.009: INFO: namespace downward-api-4344 deletion completed in 6.404460173s

• [SLOW TEST:15.576 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:45:09.010: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1287
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 20 12:45:09.325: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 20 12:45:09.353: INFO: Waiting for terminating namespaces to be deleted...
Dec 20 12:45:09.361: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-457fd-578f899757-428w5 before test
Dec 20 12:45:09.501: INFO: kube-proxy-nfqd6 from kube-system started at 2019-12-20 10:26:58 +0000 UTC (1 container statuses recorded)
Dec 20 12:45:09.501: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 20 12:45:09.501: INFO: sonobuoy from sonobuoy started at 2019-12-20 10:41:51 +0000 UTC (1 container statuses recorded)
Dec 20 12:45:09.501: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 20 12:45:09.501: INFO: node-local-dns-kj7kj from kube-system started at 2019-12-20 10:26:58 +0000 UTC (1 container statuses recorded)
Dec 20 12:45:09.501: INFO: 	Container node-cache ready: true, restart count 0
Dec 20 12:45:09.501: INFO: node-exporter-6fmjz from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 12:45:09.501: INFO: 	Container node-exporter ready: true, restart count 0
Dec 20 12:45:09.501: INFO: canal-64h7p from kube-system started at 2019-12-20 10:27:04 +0000 UTC (2 container statuses recorded)
Dec 20 12:45:09.501: INFO: 	Container calico-node ready: true, restart count 0
Dec 20 12:45:09.501: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 20 12:45:09.501: INFO: sonobuoy-systemd-logs-daemon-set-73919c9b23754985-gvq9z from sonobuoy started at 2019-12-20 10:42:00 +0000 UTC (2 container statuses recorded)
Dec 20 12:45:09.501: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Dec 20 12:45:09.501: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 20 12:45:09.501: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-457fd-578f899757-dsqmn before test
Dec 20 12:45:09.719: INFO: node-local-dns-7tvth from kube-system started at 2019-12-20 10:26:26 +0000 UTC (1 container statuses recorded)
Dec 20 12:45:09.719: INFO: 	Container node-cache ready: true, restart count 0
Dec 20 12:45:09.720: INFO: node-exporter-qnl6p from kube-system started at 2019-12-20 10:26:26 +0000 UTC (1 container statuses recorded)
Dec 20 12:45:09.720: INFO: 	Container node-exporter ready: true, restart count 0
Dec 20 12:45:09.720: INFO: coredns-547f89d7d5-s26jq from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 12:45:09.720: INFO: 	Container coredns ready: true, restart count 0
Dec 20 12:45:09.721: INFO: cluster-autoscaler-8c65c7d54-9cqhv from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 12:45:09.721: INFO: 	Container cluster-autoscaler ready: true, restart count 0
Dec 20 12:45:09.721: INFO: sonobuoy-systemd-logs-daemon-set-73919c9b23754985-dp62g from sonobuoy started at 2019-12-20 10:42:00 +0000 UTC (2 container statuses recorded)
Dec 20 12:45:09.721: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Dec 20 12:45:09.728: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 20 12:45:09.729: INFO: tiller-deploy-78f78bb476-6t2mk from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 12:45:09.729: INFO: 	Container tiller ready: true, restart count 0
Dec 20 12:45:09.729: INFO: kube-proxy-wq825 from kube-system started at 2019-12-20 10:26:26 +0000 UTC (1 container statuses recorded)
Dec 20 12:45:09.729: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 20 12:45:09.729: INFO: canal-gcrnc from kube-system started at 2019-12-20 10:26:27 +0000 UTC (2 container statuses recorded)
Dec 20 12:45:09.730: INFO: 	Container calico-node ready: true, restart count 0
Dec 20 12:45:09.730: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 20 12:45:09.730: INFO: openvpn-client-64df8b95c9-wr7dw from kube-system started at 2019-12-20 10:27:01 +0000 UTC (2 container statuses recorded)
Dec 20 12:45:09.730: INFO: 	Container dnat-controller ready: true, restart count 0
Dec 20 12:45:09.730: INFO: 	Container openvpn-client ready: true, restart count 0
Dec 20 12:45:09.730: INFO: coredns-547f89d7d5-5w7wq from kube-system started at 2019-12-20 10:27:01 +0000 UTC (1 container statuses recorded)
Dec 20 12:45:09.730: INFO: 	Container coredns ready: true, restart count 0
Dec 20 12:45:09.730: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-457fd-578f899757-zpxc2 before test
Dec 20 12:45:10.192: INFO: node-local-dns-4wkt5 from kube-system started at 2019-12-20 10:27:10 +0000 UTC (1 container statuses recorded)
Dec 20 12:45:10.192: INFO: 	Container node-cache ready: true, restart count 0
Dec 20 12:45:10.192: INFO: canal-w56q4 from kube-system started at 2019-12-20 10:27:10 +0000 UTC (2 container statuses recorded)
Dec 20 12:45:10.192: INFO: 	Container calico-node ready: true, restart count 0
Dec 20 12:45:10.192: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 20 12:45:10.192: INFO: sonobuoy-systemd-logs-daemon-set-73919c9b23754985-gwhsz from sonobuoy started at 2019-12-20 10:42:00 +0000 UTC (2 container statuses recorded)
Dec 20 12:45:10.193: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Dec 20 12:45:10.193: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 20 12:45:10.193: INFO: kube-proxy-pml2l from kube-system started at 2019-12-20 10:27:10 +0000 UTC (1 container statuses recorded)
Dec 20 12:45:10.193: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 20 12:45:10.193: INFO: node-exporter-wxf2r from kube-system started at 2019-12-20 10:27:10 +0000 UTC (1 container statuses recorded)
Dec 20 12:45:10.193: INFO: 	Container node-exporter ready: true, restart count 0
Dec 20 12:45:10.193: INFO: sonobuoy-e2e-job-1e71c3c7847b472c from sonobuoy started at 2019-12-20 10:42:00 +0000 UTC (2 container statuses recorded)
Dec 20 12:45:10.193: INFO: 	Container e2e ready: true, restart count 0
Dec 20 12:45:10.193: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-51aa31e4-7934-40b4-8dc2-a52fe89ce323 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-51aa31e4-7934-40b4-8dc2-a52fe89ce323 off the node metakube-worker-457fd-578f899757-428w5
STEP: verifying the node doesn't have the label kubernetes.io/e2e-51aa31e4-7934-40b4-8dc2-a52fe89ce323
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:45:28.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1287" for this suite.
Dec 20 12:45:49.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:45:49.445: INFO: namespace sched-pred-1287 deletion completed in 20.593570821s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:40.435 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:45:49.446: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5812
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Dec 20 12:45:49.881: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:46:14.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5812" for this suite.
Dec 20 12:46:22.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:46:22.548: INFO: namespace crd-publish-openapi-5812 deletion completed in 8.322573817s

• [SLOW TEST:33.103 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:46:22.553: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7036
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Dec 20 12:46:23.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-7036 -- logs-generator --log-lines-total 100 --run-duration 20s'
Dec 20 12:46:33.202: INFO: stderr: ""
Dec 20 12:46:33.202: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Dec 20 12:46:33.202: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Dec 20 12:46:33.202: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7036" to be "running and ready, or succeeded"
Dec 20 12:46:33.228: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 25.2892ms
Dec 20 12:46:35.234: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031827152s
Dec 20 12:46:37.250: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047317889s
Dec 20 12:46:39.280: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.077940828s
Dec 20 12:46:41.288: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 8.085549409s
Dec 20 12:46:41.288: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Dec 20 12:46:41.288: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Dec 20 12:46:41.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 logs logs-generator logs-generator --namespace=kubectl-7036'
Dec 20 12:46:41.788: INFO: stderr: ""
Dec 20 12:46:41.788: INFO: stdout: "I1220 12:46:38.539173       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/s7t4 573\nI1220 12:46:38.739375       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/q57 285\nI1220 12:46:38.939386       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/45bz 580\nI1220 12:46:39.140015       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/s5f 528\nI1220 12:46:39.339398       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/zmm 235\nI1220 12:46:39.539334       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/zhs 374\nI1220 12:46:39.742027       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/p2n 540\nI1220 12:46:39.939456       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/bhz4 232\nI1220 12:46:40.139357       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/s8v7 544\nI1220 12:46:40.339434       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/xjn5 317\nI1220 12:46:40.539360       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/5dh 581\nI1220 12:46:40.739400       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/g8nv 281\nI1220 12:46:40.939379       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/k8md 522\nI1220 12:46:41.140565       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/xnz 473\nI1220 12:46:41.339342       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/chll 202\nI1220 12:46:41.542217       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/8cl8 266\n"
STEP: limiting log lines
Dec 20 12:46:41.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 logs logs-generator logs-generator --namespace=kubectl-7036 --tail=1'
Dec 20 12:46:42.100: INFO: stderr: ""
Dec 20 12:46:42.101: INFO: stdout: "I1220 12:46:41.942130       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/grb 591\n"
STEP: limiting log bytes
Dec 20 12:46:42.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 logs logs-generator logs-generator --namespace=kubectl-7036 --limit-bytes=1'
Dec 20 12:46:42.417: INFO: stderr: ""
Dec 20 12:46:42.417: INFO: stdout: "I"
STEP: exposing timestamps
Dec 20 12:46:42.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 logs logs-generator logs-generator --namespace=kubectl-7036 --tail=1 --timestamps'
Dec 20 12:46:42.727: INFO: stderr: ""
Dec 20 12:46:42.727: INFO: stdout: "2019-12-20T12:46:42.53954286Z I1220 12:46:42.539296       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/lpl4 481\n"
STEP: restricting to a time range
Dec 20 12:46:45.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 logs logs-generator logs-generator --namespace=kubectl-7036 --since=1s'
Dec 20 12:46:45.514: INFO: stderr: ""
Dec 20 12:46:45.514: INFO: stdout: "I1220 12:46:44.539717       1 logs_generator.go:76] 30 POST /api/v1/namespaces/kube-system/pods/n6vn 429\nI1220 12:46:44.739420       1 logs_generator.go:76] 31 PUT /api/v1/namespaces/kube-system/pods/nn8 208\nI1220 12:46:44.939400       1 logs_generator.go:76] 32 PUT /api/v1/namespaces/default/pods/c9cn 548\nI1220 12:46:45.139361       1 logs_generator.go:76] 33 GET /api/v1/namespaces/ns/pods/s9wb 423\nI1220 12:46:45.342131       1 logs_generator.go:76] 34 PUT /api/v1/namespaces/default/pods/tvhv 549\n"
Dec 20 12:46:45.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 logs logs-generator logs-generator --namespace=kubectl-7036 --since=24h'
Dec 20 12:46:45.749: INFO: stderr: ""
Dec 20 12:46:45.749: INFO: stdout: "I1220 12:46:38.539173       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/s7t4 573\nI1220 12:46:38.739375       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/q57 285\nI1220 12:46:38.939386       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/45bz 580\nI1220 12:46:39.140015       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/s5f 528\nI1220 12:46:39.339398       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/zmm 235\nI1220 12:46:39.539334       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/zhs 374\nI1220 12:46:39.742027       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/p2n 540\nI1220 12:46:39.939456       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/bhz4 232\nI1220 12:46:40.139357       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/s8v7 544\nI1220 12:46:40.339434       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/xjn5 317\nI1220 12:46:40.539360       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/5dh 581\nI1220 12:46:40.739400       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/g8nv 281\nI1220 12:46:40.939379       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/k8md 522\nI1220 12:46:41.140565       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/xnz 473\nI1220 12:46:41.339342       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/chll 202\nI1220 12:46:41.542217       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/8cl8 266\nI1220 12:46:41.739339       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/45jw 217\nI1220 12:46:41.942130       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/grb 591\nI1220 12:46:42.141570       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/xsr 538\nI1220 12:46:42.339461       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/ch6 228\nI1220 12:46:42.539296       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/lpl4 481\nI1220 12:46:42.739353       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/9kzl 231\nI1220 12:46:42.939405       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/default/pods/w62f 217\nI1220 12:46:43.139299       1 logs_generator.go:76] 23 GET /api/v1/namespaces/default/pods/w9x 384\nI1220 12:46:43.339598       1 logs_generator.go:76] 24 GET /api/v1/namespaces/ns/pods/gq5 516\nI1220 12:46:43.539360       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/kube-system/pods/5cls 572\nI1220 12:46:43.739324       1 logs_generator.go:76] 26 GET /api/v1/namespaces/default/pods/tslr 449\nI1220 12:46:43.939494       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/m6dw 411\nI1220 12:46:44.139394       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/default/pods/8sl 341\nI1220 12:46:44.339417       1 logs_generator.go:76] 29 POST /api/v1/namespaces/kube-system/pods/lbj9 548\nI1220 12:46:44.539717       1 logs_generator.go:76] 30 POST /api/v1/namespaces/kube-system/pods/n6vn 429\nI1220 12:46:44.739420       1 logs_generator.go:76] 31 PUT /api/v1/namespaces/kube-system/pods/nn8 208\nI1220 12:46:44.939400       1 logs_generator.go:76] 32 PUT /api/v1/namespaces/default/pods/c9cn 548\nI1220 12:46:45.139361       1 logs_generator.go:76] 33 GET /api/v1/namespaces/ns/pods/s9wb 423\nI1220 12:46:45.342131       1 logs_generator.go:76] 34 PUT /api/v1/namespaces/default/pods/tvhv 549\nI1220 12:46:45.539351       1 logs_generator.go:76] 35 PUT /api/v1/namespaces/default/pods/8j5k 246\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Dec 20 12:46:45.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 delete pod logs-generator --namespace=kubectl-7036'
Dec 20 12:46:59.268: INFO: stderr: ""
Dec 20 12:46:59.268: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:46:59.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7036" for this suite.
Dec 20 12:47:05.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:47:05.688: INFO: namespace kubectl-7036 deletion completed in 6.410306465s

• [SLOW TEST:43.136 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:47:05.689: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-7563
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:47:05.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7563" for this suite.
Dec 20 12:47:17.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:47:18.318: INFO: namespace tables-7563 deletion completed in 12.36492975s

• [SLOW TEST:12.629 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:47:18.329: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6076
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6076
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-6076
I1220 12:47:19.129488      20 runners.go:184] Created replication controller with name: externalname-service, namespace: services-6076, replica count: 2
I1220 12:47:22.180019      20 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1220 12:47:25.180297      20 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1220 12:47:28.180548      20 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 20 12:47:28.180: INFO: Creating new exec pod
Dec 20 12:47:37.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=services-6076 execpodb4mbm -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec 20 12:47:38.448: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 20 12:47:38.448: INFO: stdout: ""
Dec 20 12:47:38.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=services-6076 execpodb4mbm -- /bin/sh -x -c nc -zv -t -w 2 10.240.16.126 80'
Dec 20 12:47:39.632: INFO: stderr: "+ nc -zv -t -w 2 10.240.16.126 80\nConnection to 10.240.16.126 80 port [tcp/http] succeeded!\n"
Dec 20 12:47:39.632: INFO: stdout: ""
Dec 20 12:47:39.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=services-6076 execpodb4mbm -- /bin/sh -x -c nc -zv -t -w 2 192.168.1.3 32399'
Dec 20 12:47:40.612: INFO: stderr: "+ nc -zv -t -w 2 192.168.1.3 32399\nConnection to 192.168.1.3 32399 port [tcp/32399] succeeded!\n"
Dec 20 12:47:40.612: INFO: stdout: ""
Dec 20 12:47:40.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 exec --namespace=services-6076 execpodb4mbm -- /bin/sh -x -c nc -zv -t -w 2 192.168.1.6 32399'
Dec 20 12:47:41.648: INFO: stderr: "+ nc -zv -t -w 2 192.168.1.6 32399\nConnection to 192.168.1.6 32399 port [tcp/32399] succeeded!\n"
Dec 20 12:47:41.648: INFO: stdout: ""
Dec 20 12:47:41.648: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:47:41.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6076" for this suite.
Dec 20 12:47:49.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:47:50.355: INFO: namespace services-6076 deletion completed in 8.387747681s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:32.027 seconds]
[sig-network] Services
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:47:50.356: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1568
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-1568
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1568 to expose endpoints map[]
Dec 20 12:47:50.724: INFO: successfully validated that service multi-endpoint-test in namespace services-1568 exposes endpoints map[] (36.837088ms elapsed)
STEP: Creating pod pod1 in namespace services-1568
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1568 to expose endpoints map[pod1:[100]]
Dec 20 12:47:54.878: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (4.117515328s elapsed, will retry)
Dec 20 12:47:56.915: INFO: successfully validated that service multi-endpoint-test in namespace services-1568 exposes endpoints map[pod1:[100]] (6.155295769s elapsed)
STEP: Creating pod pod2 in namespace services-1568
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1568 to expose endpoints map[pod1:[100] pod2:[101]]
Dec 20 12:48:01.159: INFO: Unexpected endpoints: found map[4339843f-4cef-44e4-bae9-d2527783b724:[100]], expected map[pod1:[100] pod2:[101]] (4.214893081s elapsed, will retry)
Dec 20 12:48:03.197: INFO: successfully validated that service multi-endpoint-test in namespace services-1568 exposes endpoints map[pod1:[100] pod2:[101]] (6.252974216s elapsed)
STEP: Deleting pod pod1 in namespace services-1568
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1568 to expose endpoints map[pod2:[101]]
Dec 20 12:48:03.295: INFO: successfully validated that service multi-endpoint-test in namespace services-1568 exposes endpoints map[pod2:[101]] (68.033989ms elapsed)
STEP: Deleting pod pod2 in namespace services-1568
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1568 to expose endpoints map[]
Dec 20 12:48:03.402: INFO: successfully validated that service multi-endpoint-test in namespace services-1568 exposes endpoints map[] (11.596888ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:48:03.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1568" for this suite.
Dec 20 12:48:17.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:48:17.896: INFO: namespace services-1568 deletion completed in 14.39481251s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:27.541 seconds]
[sig-network] Services
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:48:17.900: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7971
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 20 12:48:18.781: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 20 12:48:20.800: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712442898, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712442898, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712442898, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712442898, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 12:48:22.809: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712442898, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712442898, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712442898, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712442898, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 12:48:24.813: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712442898, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712442898, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712442898, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712442898, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 20 12:48:27.886: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 12:48:27.894: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1036-crds.webhook.example.com via the AdmissionRegistration API
Dec 20 12:48:28.569: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:48:29.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7971" for this suite.
Dec 20 12:48:38.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:48:38.352: INFO: namespace webhook-7971 deletion completed in 8.65066495s
STEP: Destroying namespace "webhook-7971-markers" for this suite.
Dec 20 12:48:44.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:48:44.650: INFO: namespace webhook-7971-markers deletion completed in 6.297548185s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.837 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:48:44.740: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1174
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 20 12:48:54.191: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:48:54.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1174" for this suite.
Dec 20 12:49:00.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:49:00.569: INFO: namespace container-runtime-1174 deletion completed in 6.334926921s

• [SLOW TEST:15.829 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:49:00.573: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5281
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 pods, got 1 pods
STEP: Gathering metrics
Dec 20 12:49:02.575: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:49:02.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1220 12:49:02.575038      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-5281" for this suite.
Dec 20 12:49:08.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:49:08.893: INFO: namespace gc-5281 deletion completed in 6.309820769s

• [SLOW TEST:8.320 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:49:08.898: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5479
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-d770b594-54f9-4522-887d-0a7006941608
STEP: Creating a pod to test consume configMaps
Dec 20 12:49:09.372: INFO: Waiting up to 5m0s for pod "pod-configmaps-eeb66c27-f9e7-49ab-897c-bd1b5615b22a" in namespace "configmap-5479" to be "success or failure"
Dec 20 12:49:09.387: INFO: Pod "pod-configmaps-eeb66c27-f9e7-49ab-897c-bd1b5615b22a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.518198ms
Dec 20 12:49:11.394: INFO: Pod "pod-configmaps-eeb66c27-f9e7-49ab-897c-bd1b5615b22a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0222005s
Dec 20 12:49:13.400: INFO: Pod "pod-configmaps-eeb66c27-f9e7-49ab-897c-bd1b5615b22a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028387209s
Dec 20 12:49:15.794: INFO: Pod "pod-configmaps-eeb66c27-f9e7-49ab-897c-bd1b5615b22a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.421674613s
Dec 20 12:49:18.044: INFO: Pod "pod-configmaps-eeb66c27-f9e7-49ab-897c-bd1b5615b22a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.671924402s
STEP: Saw pod success
Dec 20 12:49:18.044: INFO: Pod "pod-configmaps-eeb66c27-f9e7-49ab-897c-bd1b5615b22a" satisfied condition "success or failure"
Dec 20 12:49:18.774: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-configmaps-eeb66c27-f9e7-49ab-897c-bd1b5615b22a container configmap-volume-test: <nil>
STEP: delete the pod
Dec 20 12:49:20.388: INFO: Waiting for pod pod-configmaps-eeb66c27-f9e7-49ab-897c-bd1b5615b22a to disappear
Dec 20 12:49:20.533: INFO: Pod pod-configmaps-eeb66c27-f9e7-49ab-897c-bd1b5615b22a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:49:20.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5479" for this suite.
Dec 20 12:49:29.674: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:49:29.934: INFO: namespace configmap-5479 deletion completed in 9.029010452s

• [SLOW TEST:21.036 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:49:29.936: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7026
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-24c2e673-0894-49f7-90bd-33e1e1c4494f
STEP: Creating secret with name s-test-opt-upd-276dd86b-1fed-4203-b8aa-b07a4ffb8a9e
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-24c2e673-0894-49f7-90bd-33e1e1c4494f
STEP: Updating secret s-test-opt-upd-276dd86b-1fed-4203-b8aa-b07a4ffb8a9e
STEP: Creating secret with name s-test-opt-create-e4301392-37d7-4f2f-b121-c6ded11a0686
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:51:11.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7026" for this suite.
Dec 20 12:51:39.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:51:39.619: INFO: namespace secrets-7026 deletion completed in 28.446941556s

• [SLOW TEST:129.683 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:51:39.619: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3587
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-ht62
STEP: Creating a pod to test atomic-volume-subpath
Dec 20 12:51:40.162: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-ht62" in namespace "subpath-3587" to be "success or failure"
Dec 20 12:51:40.207: INFO: Pod "pod-subpath-test-downwardapi-ht62": Phase="Pending", Reason="", readiness=false. Elapsed: 45.259272ms
Dec 20 12:51:42.214: INFO: Pod "pod-subpath-test-downwardapi-ht62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052530753s
Dec 20 12:51:44.228: INFO: Pod "pod-subpath-test-downwardapi-ht62": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065764141s
Dec 20 12:51:46.236: INFO: Pod "pod-subpath-test-downwardapi-ht62": Phase="Pending", Reason="", readiness=false. Elapsed: 6.07388827s
Dec 20 12:51:48.243: INFO: Pod "pod-subpath-test-downwardapi-ht62": Phase="Running", Reason="", readiness=true. Elapsed: 8.080609898s
Dec 20 12:51:50.252: INFO: Pod "pod-subpath-test-downwardapi-ht62": Phase="Running", Reason="", readiness=true. Elapsed: 10.089634228s
Dec 20 12:51:52.259: INFO: Pod "pod-subpath-test-downwardapi-ht62": Phase="Running", Reason="", readiness=true. Elapsed: 12.096900481s
Dec 20 12:51:54.268: INFO: Pod "pod-subpath-test-downwardapi-ht62": Phase="Running", Reason="", readiness=true. Elapsed: 14.105716473s
Dec 20 12:51:56.275: INFO: Pod "pod-subpath-test-downwardapi-ht62": Phase="Running", Reason="", readiness=true. Elapsed: 16.11314452s
Dec 20 12:51:58.282: INFO: Pod "pod-subpath-test-downwardapi-ht62": Phase="Running", Reason="", readiness=true. Elapsed: 18.12049082s
Dec 20 12:52:00.289: INFO: Pod "pod-subpath-test-downwardapi-ht62": Phase="Running", Reason="", readiness=true. Elapsed: 20.126803731s
Dec 20 12:52:02.296: INFO: Pod "pod-subpath-test-downwardapi-ht62": Phase="Running", Reason="", readiness=true. Elapsed: 22.133645255s
Dec 20 12:52:04.305: INFO: Pod "pod-subpath-test-downwardapi-ht62": Phase="Running", Reason="", readiness=true. Elapsed: 24.142878049s
Dec 20 12:52:06.312: INFO: Pod "pod-subpath-test-downwardapi-ht62": Phase="Running", Reason="", readiness=true. Elapsed: 26.149593489s
Dec 20 12:52:08.321: INFO: Pod "pod-subpath-test-downwardapi-ht62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.159243431s
STEP: Saw pod success
Dec 20 12:52:08.321: INFO: Pod "pod-subpath-test-downwardapi-ht62" satisfied condition "success or failure"
Dec 20 12:52:08.328: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-subpath-test-downwardapi-ht62 container test-container-subpath-downwardapi-ht62: <nil>
STEP: delete the pod
Dec 20 12:52:08.766: INFO: Waiting for pod pod-subpath-test-downwardapi-ht62 to disappear
Dec 20 12:52:08.789: INFO: Pod pod-subpath-test-downwardapi-ht62 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-ht62
Dec 20 12:52:08.789: INFO: Deleting pod "pod-subpath-test-downwardapi-ht62" in namespace "subpath-3587"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:52:08.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3587" for this suite.
Dec 20 12:52:14.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:52:15.167: INFO: namespace subpath-3587 deletion completed in 6.363797989s

• [SLOW TEST:35.548 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:52:15.168: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5128
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:52:21.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5128" for this suite.
Dec 20 12:53:07.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:53:07.828: INFO: namespace kubelet-test-5128 deletion completed in 46.278742764s

• [SLOW TEST:52.660 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:53:07.831: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1593
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 20 12:53:08.782: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 20 12:53:10.807: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443188, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443188, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443188, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443188, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 12:53:12.819: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443188, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443188, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443188, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443188, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 12:53:14.816: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443188, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443188, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443188, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443188, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 20 12:53:17.839: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
Dec 20 12:53:17.905: INFO: Waiting for webhook configuration to be ready...
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:53:19.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1593" for this suite.
Dec 20 12:53:27.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:53:27.413: INFO: namespace webhook-1593 deletion completed in 8.309640833s
STEP: Destroying namespace "webhook-1593-markers" for this suite.
Dec 20 12:53:43.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:53:44.992: INFO: namespace webhook-1593-markers deletion completed in 17.578877447s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:37.266 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:53:45.098: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9986
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Dec 20 12:53:45.431: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-773140870 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:53:45.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9986" for this suite.
Dec 20 12:53:51.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:53:51.882: INFO: namespace kubectl-9986 deletion completed in 6.335917995s

• [SLOW TEST:6.785 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:53:51.883: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6719
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 20 12:53:52.286: INFO: Waiting up to 5m0s for pod "pod-e40bfcf6-4d61-4ef0-9261-ab0dd6b5d6d9" in namespace "emptydir-6719" to be "success or failure"
Dec 20 12:53:52.292: INFO: Pod "pod-e40bfcf6-4d61-4ef0-9261-ab0dd6b5d6d9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.122431ms
Dec 20 12:53:54.300: INFO: Pod "pod-e40bfcf6-4d61-4ef0-9261-ab0dd6b5d6d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013913729s
Dec 20 12:53:56.307: INFO: Pod "pod-e40bfcf6-4d61-4ef0-9261-ab0dd6b5d6d9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021308745s
Dec 20 12:53:58.315: INFO: Pod "pod-e40bfcf6-4d61-4ef0-9261-ab0dd6b5d6d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028846276s
STEP: Saw pod success
Dec 20 12:53:58.315: INFO: Pod "pod-e40bfcf6-4d61-4ef0-9261-ab0dd6b5d6d9" satisfied condition "success or failure"
Dec 20 12:53:58.322: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-e40bfcf6-4d61-4ef0-9261-ab0dd6b5d6d9 container test-container: <nil>
STEP: delete the pod
Dec 20 12:53:58.753: INFO: Waiting for pod pod-e40bfcf6-4d61-4ef0-9261-ab0dd6b5d6d9 to disappear
Dec 20 12:53:58.763: INFO: Pod pod-e40bfcf6-4d61-4ef0-9261-ab0dd6b5d6d9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:53:58.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6719" for this suite.
Dec 20 12:54:04.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:54:05.099: INFO: namespace emptydir-6719 deletion completed in 6.321312214s

• [SLOW TEST:13.216 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:54:05.100: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1158
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-57ed5746-f133-440b-b289-4d751297ea17
STEP: Creating a pod to test consume configMaps
Dec 20 12:54:05.467: INFO: Waiting up to 5m0s for pod "pod-configmaps-149b426e-46b8-481b-a57e-3d3c1be3de16" in namespace "configmap-1158" to be "success or failure"
Dec 20 12:54:05.503: INFO: Pod "pod-configmaps-149b426e-46b8-481b-a57e-3d3c1be3de16": Phase="Pending", Reason="", readiness=false. Elapsed: 35.66446ms
Dec 20 12:54:07.511: INFO: Pod "pod-configmaps-149b426e-46b8-481b-a57e-3d3c1be3de16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043371938s
Dec 20 12:54:09.517: INFO: Pod "pod-configmaps-149b426e-46b8-481b-a57e-3d3c1be3de16": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04963666s
Dec 20 12:54:11.525: INFO: Pod "pod-configmaps-149b426e-46b8-481b-a57e-3d3c1be3de16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0577422s
STEP: Saw pod success
Dec 20 12:54:11.525: INFO: Pod "pod-configmaps-149b426e-46b8-481b-a57e-3d3c1be3de16" satisfied condition "success or failure"
Dec 20 12:54:11.622: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-configmaps-149b426e-46b8-481b-a57e-3d3c1be3de16 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 20 12:54:12.302: INFO: Waiting for pod pod-configmaps-149b426e-46b8-481b-a57e-3d3c1be3de16 to disappear
Dec 20 12:54:12.308: INFO: Pod pod-configmaps-149b426e-46b8-481b-a57e-3d3c1be3de16 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:54:12.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1158" for this suite.
Dec 20 12:54:18.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:54:19.070: INFO: namespace configmap-1158 deletion completed in 6.746420675s

• [SLOW TEST:13.970 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:54:19.071: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9646
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-hzjdd in namespace proxy-9646
I1220 12:54:19.437487      20 runners.go:184] Created replication controller with name: proxy-service-hzjdd, namespace: proxy-9646, replica count: 1
I1220 12:54:20.488053      20 runners.go:184] proxy-service-hzjdd Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1220 12:54:21.488349      20 runners.go:184] proxy-service-hzjdd Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1220 12:54:22.488642      20 runners.go:184] proxy-service-hzjdd Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1220 12:54:23.488938      20 runners.go:184] proxy-service-hzjdd Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1220 12:54:24.489346      20 runners.go:184] proxy-service-hzjdd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1220 12:54:25.489609      20 runners.go:184] proxy-service-hzjdd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1220 12:54:26.490024      20 runners.go:184] proxy-service-hzjdd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1220 12:54:27.490344      20 runners.go:184] proxy-service-hzjdd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1220 12:54:28.490547      20 runners.go:184] proxy-service-hzjdd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1220 12:54:29.491030      20 runners.go:184] proxy-service-hzjdd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1220 12:54:30.491281      20 runners.go:184] proxy-service-hzjdd Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 20 12:54:30.501: INFO: setup took 11.159552086s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec 20 12:54:30.629: INFO: (0) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 125.587988ms)
Dec 20 12:54:30.629: INFO: (0) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 127.012197ms)
Dec 20 12:54:30.629: INFO: (0) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 127.71073ms)
Dec 20 12:54:30.629: INFO: (0) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 125.547642ms)
Dec 20 12:54:30.629: INFO: (0) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname1/proxy/: foo (200; 125.546021ms)
Dec 20 12:54:30.629: INFO: (0) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname2/proxy/: bar (200; 126.946563ms)
Dec 20 12:54:30.629: INFO: (0) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname2/proxy/: bar (200; 126.999148ms)
Dec 20 12:54:30.632: INFO: (0) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/rewriteme">test</a> (200; 131.207049ms)
Dec 20 12:54:30.632: INFO: (0) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname1/proxy/: foo (200; 128.846914ms)
Dec 20 12:54:30.632: INFO: (0) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">test<... (200; 129.313745ms)
Dec 20 12:54:30.654: INFO: (0) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">... (200; 151.191516ms)
Dec 20 12:54:30.766: INFO: (0) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/tlsrewritem... (200; 264.004461ms)
Dec 20 12:54:30.766: INFO: (0) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:462/proxy/: tls qux (200; 263.85421ms)
Dec 20 12:54:30.774: INFO: (0) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname1/proxy/: tls baz (200; 270.026979ms)
Dec 20 12:54:30.826: INFO: (0) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:460/proxy/: tls baz (200; 323.718593ms)
Dec 20 12:54:30.826: INFO: (0) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname2/proxy/: tls qux (200; 325.102434ms)
Dec 20 12:54:30.899: INFO: (1) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:460/proxy/: tls baz (200; 70.881725ms)
Dec 20 12:54:30.899: INFO: (1) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname1/proxy/: tls baz (200; 72.3965ms)
Dec 20 12:54:30.903: INFO: (1) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">... (200; 75.840809ms)
Dec 20 12:54:30.903: INFO: (1) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 75.440828ms)
Dec 20 12:54:30.904: INFO: (1) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:462/proxy/: tls qux (200; 75.187823ms)
Dec 20 12:54:30.904: INFO: (1) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname2/proxy/: tls qux (200; 75.643008ms)
Dec 20 12:54:30.918: INFO: (1) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/rewriteme">test</a> (200; 90.737423ms)
Dec 20 12:54:30.918: INFO: (1) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 89.90533ms)
Dec 20 12:54:30.919: INFO: (1) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/tlsrewritem... (200; 90.48045ms)
Dec 20 12:54:30.919: INFO: (1) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 90.847018ms)
Dec 20 12:54:30.919: INFO: (1) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 90.420179ms)
Dec 20 12:54:30.945: INFO: (1) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">test<... (200; 116.929946ms)
Dec 20 12:54:31.274: INFO: (1) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname1/proxy/: foo (200; 446.195496ms)
Dec 20 12:54:31.274: INFO: (1) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname2/proxy/: bar (200; 445.708371ms)
Dec 20 12:54:31.274: INFO: (1) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname2/proxy/: bar (200; 445.939016ms)
Dec 20 12:54:31.274: INFO: (1) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname1/proxy/: foo (200; 446.463912ms)
Dec 20 12:54:31.327: INFO: (2) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">test<... (200; 52.221983ms)
Dec 20 12:54:31.327: INFO: (2) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/rewriteme">test</a> (200; 52.050451ms)
Dec 20 12:54:31.327: INFO: (2) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 51.887972ms)
Dec 20 12:54:31.327: INFO: (2) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 52.428798ms)
Dec 20 12:54:31.327: INFO: (2) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">... (200; 52.119055ms)
Dec 20 12:54:31.327: INFO: (2) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 52.408652ms)
Dec 20 12:54:31.327: INFO: (2) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 52.876439ms)
Dec 20 12:54:31.327: INFO: (2) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:462/proxy/: tls qux (200; 52.585339ms)
Dec 20 12:54:31.328: INFO: (2) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:460/proxy/: tls baz (200; 52.962035ms)
Dec 20 12:54:31.328: INFO: (2) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/tlsrewritem... (200; 53.233947ms)
Dec 20 12:54:31.331: INFO: (2) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname1/proxy/: tls baz (200; 56.584271ms)
Dec 20 12:54:31.331: INFO: (2) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname2/proxy/: tls qux (200; 56.623018ms)
Dec 20 12:54:31.346: INFO: (2) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname1/proxy/: foo (200; 71.657816ms)
Dec 20 12:54:31.362: INFO: (2) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname2/proxy/: bar (200; 87.086956ms)
Dec 20 12:54:31.362: INFO: (2) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname2/proxy/: bar (200; 86.978429ms)
Dec 20 12:54:31.379: INFO: (2) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname1/proxy/: foo (200; 103.99471ms)
Dec 20 12:54:31.435: INFO: (3) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 55.599436ms)
Dec 20 12:54:31.435: INFO: (3) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">... (200; 55.419902ms)
Dec 20 12:54:31.435: INFO: (3) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:462/proxy/: tls qux (200; 55.630832ms)
Dec 20 12:54:31.435: INFO: (3) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 55.607651ms)
Dec 20 12:54:31.435: INFO: (3) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/tlsrewritem... (200; 56.262002ms)
Dec 20 12:54:31.436: INFO: (3) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 57.341999ms)
Dec 20 12:54:31.437: INFO: (3) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 57.816667ms)
Dec 20 12:54:31.437: INFO: (3) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">test<... (200; 57.707645ms)
Dec 20 12:54:31.437: INFO: (3) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:460/proxy/: tls baz (200; 57.811173ms)
Dec 20 12:54:31.438: INFO: (3) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname1/proxy/: tls baz (200; 58.769578ms)
Dec 20 12:54:31.438: INFO: (3) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname2/proxy/: tls qux (200; 59.001799ms)
Dec 20 12:54:31.438: INFO: (3) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/rewriteme">test</a> (200; 59.4427ms)
Dec 20 12:54:31.484: INFO: (3) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname2/proxy/: bar (200; 105.164173ms)
Dec 20 12:54:31.490: INFO: (3) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname1/proxy/: foo (200; 111.138341ms)
Dec 20 12:54:31.490: INFO: (3) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname2/proxy/: bar (200; 111.001164ms)
Dec 20 12:54:31.514: INFO: (3) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname1/proxy/: foo (200; 135.397418ms)
Dec 20 12:54:31.570: INFO: (4) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:462/proxy/: tls qux (200; 54.663215ms)
Dec 20 12:54:31.570: INFO: (4) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 55.210903ms)
Dec 20 12:54:31.578: INFO: (4) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname1/proxy/: tls baz (200; 62.573118ms)
Dec 20 12:54:31.578: INFO: (4) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">... (200; 63.030882ms)
Dec 20 12:54:31.578: INFO: (4) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/tlsrewritem... (200; 63.008863ms)
Dec 20 12:54:31.579: INFO: (4) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/rewriteme">test</a> (200; 62.652361ms)
Dec 20 12:54:31.579: INFO: (4) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 62.749148ms)
Dec 20 12:54:31.597: INFO: (4) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:460/proxy/: tls baz (200; 81.203488ms)
Dec 20 12:54:31.597: INFO: (4) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname2/proxy/: bar (200; 81.963817ms)
Dec 20 12:54:31.597: INFO: (4) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname2/proxy/: tls qux (200; 82.337349ms)
Dec 20 12:54:31.598: INFO: (4) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">test<... (200; 82.147808ms)
Dec 20 12:54:31.598: INFO: (4) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname1/proxy/: foo (200; 82.276071ms)
Dec 20 12:54:31.598: INFO: (4) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 82.149281ms)
Dec 20 12:54:31.600: INFO: (4) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 85.705716ms)
Dec 20 12:54:31.601: INFO: (4) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname2/proxy/: bar (200; 85.857938ms)
Dec 20 12:54:31.702: INFO: (4) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname1/proxy/: foo (200; 186.648531ms)
Dec 20 12:54:31.811: INFO: (5) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 108.35499ms)
Dec 20 12:54:31.811: INFO: (5) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:460/proxy/: tls baz (200; 108.190853ms)
Dec 20 12:54:31.811: INFO: (5) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">... (200; 108.324831ms)
Dec 20 12:54:31.812: INFO: (5) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">test<... (200; 108.51413ms)
Dec 20 12:54:31.812: INFO: (5) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/rewriteme">test</a> (200; 108.47205ms)
Dec 20 12:54:31.812: INFO: (5) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname2/proxy/: bar (200; 109.2502ms)
Dec 20 12:54:31.812: INFO: (5) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 108.776424ms)
Dec 20 12:54:31.812: INFO: (5) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 109.231204ms)
Dec 20 12:54:31.812: INFO: (5) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/tlsrewritem... (200; 109.359426ms)
Dec 20 12:54:31.812: INFO: (5) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:462/proxy/: tls qux (200; 109.835209ms)
Dec 20 12:54:31.822: INFO: (5) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname2/proxy/: tls qux (200; 119.220885ms)
Dec 20 12:54:31.822: INFO: (5) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname1/proxy/: tls baz (200; 118.896281ms)
Dec 20 12:54:32.126: INFO: (5) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname1/proxy/: foo (200; 423.258418ms)
Dec 20 12:54:32.127: INFO: (5) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname1/proxy/: foo (200; 423.36676ms)
Dec 20 12:54:32.127: INFO: (5) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname2/proxy/: bar (200; 423.854153ms)
Dec 20 12:54:32.127: INFO: (5) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 424.142656ms)
Dec 20 12:54:32.161: INFO: (6) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 33.14622ms)
Dec 20 12:54:32.161: INFO: (6) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 33.209323ms)
Dec 20 12:54:32.161: INFO: (6) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">test<... (200; 33.301879ms)
Dec 20 12:54:32.161: INFO: (6) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname1/proxy/: tls baz (200; 33.293117ms)
Dec 20 12:54:32.161: INFO: (6) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/tlsrewritem... (200; 33.624185ms)
Dec 20 12:54:32.161: INFO: (6) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 34.019829ms)
Dec 20 12:54:32.161: INFO: (6) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/rewriteme">test</a> (200; 33.477215ms)
Dec 20 12:54:32.161: INFO: (6) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:462/proxy/: tls qux (200; 33.765256ms)
Dec 20 12:54:32.161: INFO: (6) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">... (200; 34.132438ms)
Dec 20 12:54:32.161: INFO: (6) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 34.066428ms)
Dec 20 12:54:32.162: INFO: (6) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:460/proxy/: tls baz (200; 34.191743ms)
Dec 20 12:54:32.170: INFO: (6) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname2/proxy/: tls qux (200; 42.44107ms)
Dec 20 12:54:32.194: INFO: (6) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname2/proxy/: bar (200; 67.655539ms)
Dec 20 12:54:32.195: INFO: (6) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname1/proxy/: foo (200; 67.334918ms)
Dec 20 12:54:32.196: INFO: (6) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname1/proxy/: foo (200; 68.20749ms)
Dec 20 12:54:32.196: INFO: (6) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname2/proxy/: bar (200; 68.323788ms)
Dec 20 12:54:32.238: INFO: (7) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">... (200; 41.569848ms)
Dec 20 12:54:32.238: INFO: (7) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 41.457879ms)
Dec 20 12:54:32.254: INFO: (7) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:460/proxy/: tls baz (200; 56.092738ms)
Dec 20 12:54:32.254: INFO: (7) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname1/proxy/: tls baz (200; 58.093045ms)
Dec 20 12:54:32.254: INFO: (7) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 56.12622ms)
Dec 20 12:54:32.254: INFO: (7) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:462/proxy/: tls qux (200; 57.378067ms)
Dec 20 12:54:32.255: INFO: (7) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 57.336757ms)
Dec 20 12:54:32.255: INFO: (7) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/tlsrewritem... (200; 58.267766ms)
Dec 20 12:54:32.382: INFO: (7) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">test<... (200; 184.257129ms)
Dec 20 12:54:32.391: INFO: (7) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname2/proxy/: tls qux (200; 194.936066ms)
Dec 20 12:54:32.393: INFO: (7) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/rewriteme">test</a> (200; 195.26629ms)
Dec 20 12:54:32.487: INFO: (7) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname2/proxy/: bar (200; 290.357739ms)
Dec 20 12:54:32.487: INFO: (7) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname1/proxy/: foo (200; 288.583635ms)
Dec 20 12:54:32.530: INFO: (7) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname2/proxy/: bar (200; 333.146371ms)
Dec 20 12:54:32.530: INFO: (7) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 332.22013ms)
Dec 20 12:54:32.827: INFO: (7) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname1/proxy/: foo (200; 630.167702ms)
Dec 20 12:54:33.078: INFO: (8) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:460/proxy/: tls baz (200; 249.764028ms)
Dec 20 12:54:33.078: INFO: (8) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 250.110273ms)
Dec 20 12:54:33.078: INFO: (8) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">... (200; 250.446914ms)
Dec 20 12:54:33.079: INFO: (8) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 250.569687ms)
Dec 20 12:54:33.079: INFO: (8) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">test<... (200; 251.311911ms)
Dec 20 12:54:33.080: INFO: (8) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 252.325665ms)
Dec 20 12:54:33.080: INFO: (8) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname2/proxy/: tls qux (200; 252.435988ms)
Dec 20 12:54:33.080: INFO: (8) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 252.225878ms)
Dec 20 12:54:33.080: INFO: (8) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/tlsrewritem... (200; 252.789231ms)
Dec 20 12:54:33.080: INFO: (8) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:462/proxy/: tls qux (200; 252.550398ms)
Dec 20 12:54:33.082: INFO: (8) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname1/proxy/: tls baz (200; 253.956787ms)
Dec 20 12:54:33.082: INFO: (8) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/rewriteme">test</a> (200; 254.170831ms)
Dec 20 12:54:33.183: INFO: (8) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname1/proxy/: foo (200; 355.003299ms)
Dec 20 12:54:33.183: INFO: (8) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname2/proxy/: bar (200; 355.402965ms)
Dec 20 12:54:33.183: INFO: (8) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname1/proxy/: foo (200; 355.340303ms)
Dec 20 12:54:33.183: INFO: (8) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname2/proxy/: bar (200; 355.584394ms)
Dec 20 12:54:33.282: INFO: (9) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/rewriteme">test</a> (200; 98.17516ms)
Dec 20 12:54:33.282: INFO: (9) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 97.990227ms)
Dec 20 12:54:33.282: INFO: (9) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:460/proxy/: tls baz (200; 98.332288ms)
Dec 20 12:54:33.283: INFO: (9) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 97.979368ms)
Dec 20 12:54:33.283: INFO: (9) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">... (200; 98.098202ms)
Dec 20 12:54:33.283: INFO: (9) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">test<... (200; 99.476137ms)
Dec 20 12:54:33.294: INFO: (9) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/tlsrewritem... (200; 109.260596ms)
Dec 20 12:54:33.294: INFO: (9) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 109.955374ms)
Dec 20 12:54:33.294: INFO: (9) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:462/proxy/: tls qux (200; 109.280307ms)
Dec 20 12:54:33.294: INFO: (9) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 110.023242ms)
Dec 20 12:54:33.314: INFO: (9) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname1/proxy/: tls baz (200; 130.160643ms)
Dec 20 12:54:33.324: INFO: (9) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname2/proxy/: tls qux (200; 139.263373ms)
Dec 20 12:54:33.370: INFO: (9) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname1/proxy/: foo (200; 186.026707ms)
Dec 20 12:54:33.370: INFO: (9) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname1/proxy/: foo (200; 186.128469ms)
Dec 20 12:54:33.370: INFO: (9) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname2/proxy/: bar (200; 185.607568ms)
Dec 20 12:54:33.371: INFO: (9) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname2/proxy/: bar (200; 187.018605ms)
Dec 20 12:54:33.435: INFO: (10) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 63.972352ms)
Dec 20 12:54:33.435: INFO: (10) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/rewriteme">test</a> (200; 63.635986ms)
Dec 20 12:54:33.435: INFO: (10) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">test<... (200; 63.701049ms)
Dec 20 12:54:33.442: INFO: (10) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 71.18008ms)
Dec 20 12:54:33.442: INFO: (10) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:462/proxy/: tls qux (200; 71.221908ms)
Dec 20 12:54:33.442: INFO: (10) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/tlsrewritem... (200; 71.348941ms)
Dec 20 12:54:33.442: INFO: (10) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">... (200; 71.5491ms)
Dec 20 12:54:33.458: INFO: (10) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname2/proxy/: tls qux (200; 86.896198ms)
Dec 20 12:54:33.459: INFO: (10) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 87.259568ms)
Dec 20 12:54:33.459: INFO: (10) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:460/proxy/: tls baz (200; 87.52377ms)
Dec 20 12:54:33.459: INFO: (10) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname1/proxy/: tls baz (200; 87.423869ms)
Dec 20 12:54:33.459: INFO: (10) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname1/proxy/: foo (200; 87.70639ms)
Dec 20 12:54:33.478: INFO: (10) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 106.864421ms)
Dec 20 12:54:33.487: INFO: (10) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname2/proxy/: bar (200; 116.577604ms)
Dec 20 12:54:33.488: INFO: (10) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname2/proxy/: bar (200; 117.275418ms)
Dec 20 12:54:33.489: INFO: (10) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname1/proxy/: foo (200; 117.910764ms)
Dec 20 12:54:33.610: INFO: (11) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 120.263029ms)
Dec 20 12:54:33.611: INFO: (11) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/rewriteme">test</a> (200; 120.913836ms)
Dec 20 12:54:33.611: INFO: (11) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/tlsrewritem... (200; 120.418802ms)
Dec 20 12:54:33.635: INFO: (11) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:462/proxy/: tls qux (200; 144.808155ms)
Dec 20 12:54:33.639: INFO: (11) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 149.435645ms)
Dec 20 12:54:33.640: INFO: (11) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 149.553498ms)
Dec 20 12:54:33.678: INFO: (11) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname2/proxy/: tls qux (200; 188.370131ms)
Dec 20 12:54:33.678: INFO: (11) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">test<... (200; 188.566943ms)
Dec 20 12:54:33.678: INFO: (11) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">... (200; 188.109394ms)
Dec 20 12:54:33.679: INFO: (11) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:460/proxy/: tls baz (200; 188.48964ms)
Dec 20 12:54:33.679: INFO: (11) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname1/proxy/: tls baz (200; 188.990786ms)
Dec 20 12:54:33.691: INFO: (11) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname1/proxy/: foo (200; 201.379081ms)
Dec 20 12:54:33.691: INFO: (11) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname2/proxy/: bar (200; 201.063553ms)
Dec 20 12:54:33.706: INFO: (11) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 216.528197ms)
Dec 20 12:54:33.779: INFO: (11) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname2/proxy/: bar (200; 289.837615ms)
Dec 20 12:54:33.779: INFO: (11) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname1/proxy/: foo (200; 289.293587ms)
Dec 20 12:54:33.971: INFO: (12) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 189.969178ms)
Dec 20 12:54:33.971: INFO: (12) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname1/proxy/: tls baz (200; 191.155143ms)
Dec 20 12:54:33.971: INFO: (12) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/rewriteme">test</a> (200; 191.150068ms)
Dec 20 12:54:33.971: INFO: (12) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">test<... (200; 191.144933ms)
Dec 20 12:54:33.971: INFO: (12) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">... (200; 191.747689ms)
Dec 20 12:54:33.972: INFO: (12) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:462/proxy/: tls qux (200; 191.304204ms)
Dec 20 12:54:33.972: INFO: (12) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 191.647614ms)
Dec 20 12:54:33.972: INFO: (12) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname2/proxy/: bar (200; 191.648881ms)
Dec 20 12:54:33.972: INFO: (12) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 192.145495ms)
Dec 20 12:54:33.972: INFO: (12) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/tlsrewritem... (200; 191.710851ms)
Dec 20 12:54:33.972: INFO: (12) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:460/proxy/: tls baz (200; 191.869161ms)
Dec 20 12:54:33.979: INFO: (12) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname2/proxy/: tls qux (200; 198.179854ms)
Dec 20 12:54:33.996: INFO: (12) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 215.914569ms)
Dec 20 12:54:33.998: INFO: (12) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname1/proxy/: foo (200; 217.319171ms)
Dec 20 12:54:33.998: INFO: (12) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname2/proxy/: bar (200; 217.194821ms)
Dec 20 12:54:34.001: INFO: (12) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname1/proxy/: foo (200; 220.09922ms)
Dec 20 12:54:34.034: INFO: (13) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">... (200; 32.624556ms)
Dec 20 12:54:34.035: INFO: (13) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:462/proxy/: tls qux (200; 32.875433ms)
Dec 20 12:54:34.035: INFO: (13) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 32.939182ms)
Dec 20 12:54:34.035: INFO: (13) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">test<... (200; 33.189943ms)
Dec 20 12:54:34.042: INFO: (13) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname1/proxy/: foo (200; 40.678359ms)
Dec 20 12:54:34.043: INFO: (13) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:460/proxy/: tls baz (200; 40.907704ms)
Dec 20 12:54:34.043: INFO: (13) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/rewriteme">test</a> (200; 41.334256ms)
Dec 20 12:54:34.043: INFO: (13) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 41.653369ms)
Dec 20 12:54:34.044: INFO: (13) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 42.110557ms)
Dec 20 12:54:34.044: INFO: (13) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/tlsrewritem... (200; 42.362945ms)
Dec 20 12:54:34.054: INFO: (13) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname1/proxy/: tls baz (200; 52.048801ms)
Dec 20 12:54:34.054: INFO: (13) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname2/proxy/: tls qux (200; 52.654374ms)
Dec 20 12:54:34.072: INFO: (13) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 70.419646ms)
Dec 20 12:54:34.087: INFO: (13) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname1/proxy/: foo (200; 85.269368ms)
Dec 20 12:54:34.087: INFO: (13) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname2/proxy/: bar (200; 85.490176ms)
Dec 20 12:54:34.095: INFO: (13) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname2/proxy/: bar (200; 93.490705ms)
Dec 20 12:54:34.174: INFO: (14) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">test<... (200; 79.253281ms)
Dec 20 12:54:34.175: INFO: (14) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/rewriteme">test</a> (200; 79.355352ms)
Dec 20 12:54:34.175: INFO: (14) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 79.302368ms)
Dec 20 12:54:34.175: INFO: (14) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 79.493855ms)
Dec 20 12:54:34.178: INFO: (14) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname2/proxy/: tls qux (200; 82.621347ms)
Dec 20 12:54:34.178: INFO: (14) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:462/proxy/: tls qux (200; 82.398579ms)
Dec 20 12:54:34.178: INFO: (14) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:460/proxy/: tls baz (200; 82.266085ms)
Dec 20 12:54:34.178: INFO: (14) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 82.874015ms)
Dec 20 12:54:34.178: INFO: (14) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">... (200; 83.293893ms)
Dec 20 12:54:34.179: INFO: (14) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/tlsrewritem... (200; 83.154905ms)
Dec 20 12:54:34.258: INFO: (14) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname1/proxy/: tls baz (200; 163.126499ms)
Dec 20 12:54:34.258: INFO: (14) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname1/proxy/: foo (200; 162.335794ms)
Dec 20 12:54:34.258: INFO: (14) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname1/proxy/: foo (200; 162.444089ms)
Dec 20 12:54:34.258: INFO: (14) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname2/proxy/: bar (200; 162.544958ms)
Dec 20 12:54:34.258: INFO: (14) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname2/proxy/: bar (200; 162.69375ms)
Dec 20 12:54:34.357: INFO: (14) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 261.635214ms)
Dec 20 12:54:34.390: INFO: (15) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 31.692185ms)
Dec 20 12:54:34.406: INFO: (15) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 48.584348ms)
Dec 20 12:54:34.422: INFO: (15) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:460/proxy/: tls baz (200; 63.77502ms)
Dec 20 12:54:34.422: INFO: (15) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname2/proxy/: tls qux (200; 64.661229ms)
Dec 20 12:54:34.423: INFO: (15) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname1/proxy/: foo (200; 64.331055ms)
Dec 20 12:54:34.423: INFO: (15) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:462/proxy/: tls qux (200; 64.814519ms)
Dec 20 12:54:34.423: INFO: (15) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">... (200; 64.626455ms)
Dec 20 12:54:34.423: INFO: (15) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 64.749915ms)
Dec 20 12:54:34.432: INFO: (15) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 74.251012ms)
Dec 20 12:54:34.432: INFO: (15) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/rewriteme">test</a> (200; 74.333247ms)
Dec 20 12:54:34.442: INFO: (15) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/tlsrewritem... (200; 84.230606ms)
Dec 20 12:54:34.442: INFO: (15) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname1/proxy/: tls baz (200; 84.398538ms)
Dec 20 12:54:34.443: INFO: (15) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">test<... (200; 85.265334ms)
Dec 20 12:54:34.455: INFO: (15) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname2/proxy/: bar (200; 96.568405ms)
Dec 20 12:54:34.455: INFO: (15) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname2/proxy/: bar (200; 96.560555ms)
Dec 20 12:54:34.455: INFO: (15) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname1/proxy/: foo (200; 97.227761ms)
Dec 20 12:54:34.504: INFO: (16) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:462/proxy/: tls qux (200; 48.403445ms)
Dec 20 12:54:34.504: INFO: (16) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 48.587797ms)
Dec 20 12:54:34.504: INFO: (16) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 48.673711ms)
Dec 20 12:54:34.504: INFO: (16) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 48.634135ms)
Dec 20 12:54:34.505: INFO: (16) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/rewriteme">test</a> (200; 50.419308ms)
Dec 20 12:54:34.514: INFO: (16) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 58.978685ms)
Dec 20 12:54:34.515: INFO: (16) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:460/proxy/: tls baz (200; 59.125294ms)
Dec 20 12:54:34.515: INFO: (16) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/tlsrewritem... (200; 59.372377ms)
Dec 20 12:54:34.515: INFO: (16) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">test<... (200; 59.447493ms)
Dec 20 12:54:34.515: INFO: (16) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">... (200; 59.841439ms)
Dec 20 12:54:34.534: INFO: (16) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname1/proxy/: tls baz (200; 78.776708ms)
Dec 20 12:54:34.535: INFO: (16) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname2/proxy/: tls qux (200; 79.468006ms)
Dec 20 12:54:34.543: INFO: (16) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname1/proxy/: foo (200; 87.035543ms)
Dec 20 12:54:34.546: INFO: (16) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname2/proxy/: bar (200; 90.563232ms)
Dec 20 12:54:34.546: INFO: (16) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname2/proxy/: bar (200; 90.697167ms)
Dec 20 12:54:34.554: INFO: (16) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname1/proxy/: foo (200; 98.539661ms)
Dec 20 12:54:34.603: INFO: (17) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 47.960357ms)
Dec 20 12:54:34.603: INFO: (17) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 48.664721ms)
Dec 20 12:54:34.603: INFO: (17) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 48.619216ms)
Dec 20 12:54:34.604: INFO: (17) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname1/proxy/: foo (200; 49.214553ms)
Dec 20 12:54:34.618: INFO: (17) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/tlsrewritem... (200; 63.313804ms)
Dec 20 12:54:34.618: INFO: (17) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:462/proxy/: tls qux (200; 63.298458ms)
Dec 20 12:54:34.618: INFO: (17) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:460/proxy/: tls baz (200; 63.690403ms)
Dec 20 12:54:34.618: INFO: (17) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname1/proxy/: tls baz (200; 63.756365ms)
Dec 20 12:54:34.621: INFO: (17) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">... (200; 66.363172ms)
Dec 20 12:54:34.621: INFO: (17) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/rewriteme">test</a> (200; 66.746366ms)
Dec 20 12:54:34.621: INFO: (17) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">test<... (200; 66.580231ms)
Dec 20 12:54:34.629: INFO: (17) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname2/proxy/: tls qux (200; 74.362756ms)
Dec 20 12:54:34.652: INFO: (17) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname2/proxy/: bar (200; 97.377729ms)
Dec 20 12:54:34.652: INFO: (17) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname2/proxy/: bar (200; 97.637108ms)
Dec 20 12:54:34.653: INFO: (17) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 97.756794ms)
Dec 20 12:54:34.653: INFO: (17) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname1/proxy/: foo (200; 97.997956ms)
Dec 20 12:54:34.699: INFO: (18) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 46.197465ms)
Dec 20 12:54:34.699: INFO: (18) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 45.198211ms)
Dec 20 12:54:34.700: INFO: (18) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 45.002534ms)
Dec 20 12:54:34.703: INFO: (18) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">test<... (200; 48.134151ms)
Dec 20 12:54:34.703: INFO: (18) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:462/proxy/: tls qux (200; 48.975902ms)
Dec 20 12:54:34.702: INFO: (18) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/tlsrewritem... (200; 49.007664ms)
Dec 20 12:54:34.751: INFO: (18) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname1/proxy/: tls baz (200; 95.77472ms)
Dec 20 12:54:34.752: INFO: (18) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:460/proxy/: tls baz (200; 96.625138ms)
Dec 20 12:54:34.752: INFO: (18) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 96.987362ms)
Dec 20 12:54:34.752: INFO: (18) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">... (200; 97.851323ms)
Dec 20 12:54:34.752: INFO: (18) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname2/proxy/: tls qux (200; 98.965335ms)
Dec 20 12:54:34.767: INFO: (18) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/rewriteme">test</a> (200; 113.909894ms)
Dec 20 12:54:34.778: INFO: (18) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname2/proxy/: bar (200; 124.533116ms)
Dec 20 12:54:34.790: INFO: (18) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname1/proxy/: foo (200; 135.318154ms)
Dec 20 12:54:34.814: INFO: (18) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname1/proxy/: foo (200; 159.00807ms)
Dec 20 12:54:34.817: INFO: (18) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname2/proxy/: bar (200; 163.675972ms)
Dec 20 12:54:34.850: INFO: (19) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 31.946369ms)
Dec 20 12:54:34.850: INFO: (19) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 32.027772ms)
Dec 20 12:54:34.851: INFO: (19) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:160/proxy/: foo (200; 32.568082ms)
Dec 20 12:54:34.851: INFO: (19) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:462/proxy/: tls qux (200; 32.807505ms)
Dec 20 12:54:34.851: INFO: (19) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">... (200; 32.759ms)
Dec 20 12:54:34.851: INFO: (19) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm:1080/proxy/rewriteme">test<... (200; 33.369165ms)
Dec 20 12:54:34.851: INFO: (19) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:443/proxy/tlsrewritem... (200; 33.33883ms)
Dec 20 12:54:34.860: INFO: (19) /api/v1/namespaces/proxy-9646/pods/https:proxy-service-hzjdd-6ttmm:460/proxy/: tls baz (200; 42.108519ms)
Dec 20 12:54:34.860: INFO: (19) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname1/proxy/: tls baz (200; 42.014932ms)
Dec 20 12:54:34.861: INFO: (19) /api/v1/namespaces/proxy-9646/pods/http:proxy-service-hzjdd-6ttmm:162/proxy/: bar (200; 42.34363ms)
Dec 20 12:54:34.861: INFO: (19) /api/v1/namespaces/proxy-9646/services/https:proxy-service-hzjdd:tlsportname2/proxy/: tls qux (200; 43.221245ms)
Dec 20 12:54:34.862: INFO: (19) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname2/proxy/: bar (200; 43.687706ms)
Dec 20 12:54:34.862: INFO: (19) /api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/: <a href="/api/v1/namespaces/proxy-9646/pods/proxy-service-hzjdd-6ttmm/proxy/rewriteme">test</a> (200; 43.462964ms)
Dec 20 12:54:34.862: INFO: (19) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname1/proxy/: foo (200; 43.891631ms)
Dec 20 12:54:34.862: INFO: (19) /api/v1/namespaces/proxy-9646/services/proxy-service-hzjdd:portname1/proxy/: foo (200; 44.131177ms)
Dec 20 12:54:34.951: INFO: (19) /api/v1/namespaces/proxy-9646/services/http:proxy-service-hzjdd:portname2/proxy/: bar (200; 133.256367ms)
STEP: deleting ReplicationController proxy-service-hzjdd in namespace proxy-9646, will wait for the garbage collector to delete the pods
Dec 20 12:54:35.045: INFO: Deleting ReplicationController proxy-service-hzjdd took: 36.354335ms
Dec 20 12:54:35.546: INFO: Terminating ReplicationController proxy-service-hzjdd pods took: 500.712957ms
[AfterEach] version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:54:39.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9646" for this suite.
Dec 20 12:54:50.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:54:50.378: INFO: namespace proxy-9646 deletion completed in 10.422406079s

• [SLOW TEST:31.307 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:54:50.383: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-1491
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Dec 20 12:54:50.742: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-1491" to be "success or failure"
Dec 20 12:54:50.751: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 9.191467ms
Dec 20 12:54:52.759: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017048132s
Dec 20 12:54:54.765: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023014708s
Dec 20 12:54:56.774: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031963585s
Dec 20 12:54:58.781: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.038968474s
STEP: Saw pod success
Dec 20 12:54:58.781: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Dec 20 12:54:58.789: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Dec 20 12:54:59.144: INFO: Waiting for pod pod-host-path-test to disappear
Dec 20 12:54:59.150: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:54:59.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-1491" for this suite.
Dec 20 12:55:05.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:55:05.461: INFO: namespace hostpath-1491 deletion completed in 6.293909562s

• [SLOW TEST:15.079 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:55:05.462: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2734
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:55:15.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2734" for this suite.
Dec 20 12:55:21.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:55:22.369: INFO: namespace kubelet-test-2734 deletion completed in 6.997030263s

• [SLOW TEST:16.907 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:55:22.372: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5341
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Dec 20 12:55:22.710: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:55:34.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5341" for this suite.
Dec 20 12:55:46.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:55:46.993: INFO: namespace init-container-5341 deletion completed in 12.559794912s

• [SLOW TEST:24.621 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:55:46.994: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7459
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 12:55:47.395: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec 20 12:55:52.408: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 20 12:55:52.408: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 20 12:55:58.546: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7459 /apis/apps/v1/namespaces/deployment-7459/deployments/test-cleanup-deployment c7187a5e-6d05-4b65-a979-46b0206d9313 43739 1 2019-12-20 12:55:52 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0003276a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-12-20 12:55:52 +0000 UTC,LastTransitionTime:2019-12-20 12:55:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-65db99849b" has successfully progressed.,LastUpdateTime:2019-12-20 12:55:57 +0000 UTC,LastTransitionTime:2019-12-20 12:55:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 20 12:55:58.555: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-7459 /apis/apps/v1/namespaces/deployment-7459/replicasets/test-cleanup-deployment-65db99849b 2469be87-b010-41f5-94dc-5a2cc541aef2 43727 1 2019-12-20 12:55:52 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment c7187a5e-6d05-4b65-a979-46b0206d9313 0xc0000539d7 0xc0000539d8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc000053cc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 20 12:55:58.570: INFO: Pod "test-cleanup-deployment-65db99849b-x86cs" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-65db99849b-x86cs test-cleanup-deployment-65db99849b- deployment-7459 /api/v1/namespaces/deployment-7459/pods/test-cleanup-deployment-65db99849b-x86cs 66de1c44-e80f-442a-ad51-c5bd56fa2831 43726 0 2019-12-20 12:55:52 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[cni.projectcalico.org/podIP:172.25.2.153/32] [{apps/v1 ReplicaSet test-cleanup-deployment-65db99849b 2469be87-b010-41f5-94dc-5a2cc541aef2 0xc000482ba7 0xc000482ba8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-n7khh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-n7khh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-n7khh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-zpxc2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 12:55:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 12:55:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 12:55:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 12:55:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.11,PodIP:172.25.2.153,StartTime:2019-12-20 12:55:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-20 12:55:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://b3ca6103e07f8f3e227c792efbc9507a66cc19d210ccf81481a6ee9c075efd83,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:55:58.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7459" for this suite.
Dec 20 12:56:06.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:56:06.992: INFO: namespace deployment-7459 deletion completed in 8.413406491s

• [SLOW TEST:19.999 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:56:06.995: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-420
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 20 12:56:08.126: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 20 12:56:10.153: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443368, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443368, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443368, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443368, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 12:56:12.177: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443368, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443368, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443368, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443368, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 12:56:14.228: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443368, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443368, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443368, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443368, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 20 12:56:17.198: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:56:19.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-420" for this suite.
Dec 20 12:56:27.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:56:27.575: INFO: namespace webhook-420 deletion completed in 8.31482436s
STEP: Destroying namespace "webhook-420-markers" for this suite.
Dec 20 12:56:33.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:56:34.197: INFO: namespace webhook-420-markers deletion completed in 6.621483861s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.241 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:56:34.242: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4317
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec 20 12:56:34.565: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4317 /api/v1/namespaces/watch-4317/configmaps/e2e-watch-test-configmap-a 41d4a37e-0104-4b94-932f-9a61a0ac9af7 43975 0 2019-12-20 12:56:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 20 12:56:34.566: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4317 /api/v1/namespaces/watch-4317/configmaps/e2e-watch-test-configmap-a 41d4a37e-0104-4b94-932f-9a61a0ac9af7 43975 0 2019-12-20 12:56:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec 20 12:56:44.624: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4317 /api/v1/namespaces/watch-4317/configmaps/e2e-watch-test-configmap-a 41d4a37e-0104-4b94-932f-9a61a0ac9af7 44008 0 2019-12-20 12:56:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec 20 12:56:44.624: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4317 /api/v1/namespaces/watch-4317/configmaps/e2e-watch-test-configmap-a 41d4a37e-0104-4b94-932f-9a61a0ac9af7 44008 0 2019-12-20 12:56:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec 20 12:56:54.657: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4317 /api/v1/namespaces/watch-4317/configmaps/e2e-watch-test-configmap-a 41d4a37e-0104-4b94-932f-9a61a0ac9af7 44041 0 2019-12-20 12:56:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 20 12:56:54.657: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4317 /api/v1/namespaces/watch-4317/configmaps/e2e-watch-test-configmap-a 41d4a37e-0104-4b94-932f-9a61a0ac9af7 44041 0 2019-12-20 12:56:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec 20 12:57:04.744: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4317 /api/v1/namespaces/watch-4317/configmaps/e2e-watch-test-configmap-a 41d4a37e-0104-4b94-932f-9a61a0ac9af7 44075 0 2019-12-20 12:56:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 20 12:57:04.744: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4317 /api/v1/namespaces/watch-4317/configmaps/e2e-watch-test-configmap-a 41d4a37e-0104-4b94-932f-9a61a0ac9af7 44075 0 2019-12-20 12:56:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec 20 12:57:14.773: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4317 /api/v1/namespaces/watch-4317/configmaps/e2e-watch-test-configmap-b ee9e6065-e9f4-4a0b-9bf7-5d54602c459c 44104 0 2019-12-20 12:57:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 20 12:57:14.774: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4317 /api/v1/namespaces/watch-4317/configmaps/e2e-watch-test-configmap-b ee9e6065-e9f4-4a0b-9bf7-5d54602c459c 44104 0 2019-12-20 12:57:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec 20 12:57:24.806: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4317 /api/v1/namespaces/watch-4317/configmaps/e2e-watch-test-configmap-b ee9e6065-e9f4-4a0b-9bf7-5d54602c459c 44136 0 2019-12-20 12:57:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 20 12:57:24.806: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4317 /api/v1/namespaces/watch-4317/configmaps/e2e-watch-test-configmap-b ee9e6065-e9f4-4a0b-9bf7-5d54602c459c 44136 0 2019-12-20 12:57:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:57:34.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4317" for this suite.
Dec 20 12:57:52.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:57:53.181: INFO: namespace watch-4317 deletion completed in 18.360249295s

• [SLOW TEST:78.939 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:57:53.182: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5451
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Dec 20 12:57:53.639: INFO: Waiting up to 5m0s for pod "var-expansion-ac659bce-fc58-4608-a825-d3226788f3f0" in namespace "var-expansion-5451" to be "success or failure"
Dec 20 12:57:53.820: INFO: Pod "var-expansion-ac659bce-fc58-4608-a825-d3226788f3f0": Phase="Pending", Reason="", readiness=false. Elapsed: 180.781471ms
Dec 20 12:57:55.832: INFO: Pod "var-expansion-ac659bce-fc58-4608-a825-d3226788f3f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.193597071s
Dec 20 12:57:57.843: INFO: Pod "var-expansion-ac659bce-fc58-4608-a825-d3226788f3f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.204116727s
Dec 20 12:57:59.850: INFO: Pod "var-expansion-ac659bce-fc58-4608-a825-d3226788f3f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.210803921s
STEP: Saw pod success
Dec 20 12:57:59.850: INFO: Pod "var-expansion-ac659bce-fc58-4608-a825-d3226788f3f0" satisfied condition "success or failure"
Dec 20 12:57:59.857: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod var-expansion-ac659bce-fc58-4608-a825-d3226788f3f0 container dapi-container: <nil>
STEP: delete the pod
Dec 20 12:58:00.117: INFO: Waiting for pod var-expansion-ac659bce-fc58-4608-a825-d3226788f3f0 to disappear
Dec 20 12:58:00.125: INFO: Pod var-expansion-ac659bce-fc58-4608-a825-d3226788f3f0 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:58:00.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5451" for this suite.
Dec 20 12:58:06.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:58:06.544: INFO: namespace var-expansion-5451 deletion completed in 6.409751376s

• [SLOW TEST:13.363 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:58:06.552: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5009
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:58:16.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5009" for this suite.
Dec 20 12:59:07.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:59:07.623: INFO: namespace kubelet-test-5009 deletion completed in 50.76349666s

• [SLOW TEST:61.072 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:59:07.628: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1720
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 12:59:07.872: INFO: (0) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 17.060786ms)
Dec 20 12:59:07.920: INFO: (1) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 47.651309ms)
Dec 20 12:59:07.931: INFO: (2) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.654431ms)
Dec 20 12:59:07.943: INFO: (3) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.631687ms)
Dec 20 12:59:07.958: INFO: (4) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 14.92521ms)
Dec 20 12:59:07.969: INFO: (5) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.032869ms)
Dec 20 12:59:07.987: INFO: (6) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 18.049576ms)
Dec 20 12:59:07.999: INFO: (7) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.143788ms)
Dec 20 12:59:08.009: INFO: (8) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.977934ms)
Dec 20 12:59:08.020: INFO: (9) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.597764ms)
Dec 20 12:59:08.032: INFO: (10) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.716573ms)
Dec 20 12:59:08.044: INFO: (11) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.285982ms)
Dec 20 12:59:08.055: INFO: (12) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.758178ms)
Dec 20 12:59:08.067: INFO: (13) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.454543ms)
Dec 20 12:59:08.080: INFO: (14) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.594375ms)
Dec 20 12:59:08.093: INFO: (15) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 13.686167ms)
Dec 20 12:59:08.109: INFO: (16) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 15.589713ms)
Dec 20 12:59:08.120: INFO: (17) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.729956ms)
Dec 20 12:59:08.134: INFO: (18) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 13.189651ms)
Dec 20 12:59:08.145: INFO: (19) /api/v1/nodes/metakube-worker-457fd-578f899757-428w5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.742196ms)
[AfterEach] version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:59:08.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1720" for this suite.
Dec 20 12:59:14.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 12:59:14.438: INFO: namespace proxy-1720 deletion completed in 6.279114764s

• [SLOW TEST:6.811 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 12:59:14.443: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-530
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Dec 20 12:59:14.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 create -f - --namespace=kubectl-530'
Dec 20 12:59:17.117: INFO: stderr: ""
Dec 20 12:59:17.117: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 20 12:59:17.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-530'
Dec 20 12:59:17.468: INFO: stderr: ""
Dec 20 12:59:17.468: INFO: stdout: "update-demo-nautilus-b6kkr update-demo-nautilus-hgg4c "
Dec 20 12:59:17.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-b6kkr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-530'
Dec 20 12:59:17.609: INFO: stderr: ""
Dec 20 12:59:17.609: INFO: stdout: ""
Dec 20 12:59:17.609: INFO: update-demo-nautilus-b6kkr is created but not running
Dec 20 12:59:22.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-530'
Dec 20 12:59:22.737: INFO: stderr: ""
Dec 20 12:59:22.738: INFO: stdout: "update-demo-nautilus-b6kkr update-demo-nautilus-hgg4c "
Dec 20 12:59:22.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-b6kkr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-530'
Dec 20 12:59:22.857: INFO: stderr: ""
Dec 20 12:59:22.857: INFO: stdout: "true"
Dec 20 12:59:22.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-b6kkr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-530'
Dec 20 12:59:22.986: INFO: stderr: ""
Dec 20 12:59:22.986: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 20 12:59:22.987: INFO: validating pod update-demo-nautilus-b6kkr
Dec 20 12:59:23.093: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 20 12:59:23.093: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 20 12:59:23.093: INFO: update-demo-nautilus-b6kkr is verified up and running
Dec 20 12:59:23.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-hgg4c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-530'
Dec 20 12:59:23.221: INFO: stderr: ""
Dec 20 12:59:23.221: INFO: stdout: "true"
Dec 20 12:59:23.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-hgg4c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-530'
Dec 20 12:59:23.343: INFO: stderr: ""
Dec 20 12:59:23.343: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 20 12:59:23.343: INFO: validating pod update-demo-nautilus-hgg4c
Dec 20 12:59:23.449: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 20 12:59:23.449: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 20 12:59:23.449: INFO: update-demo-nautilus-hgg4c is verified up and running
STEP: scaling down the replication controller
Dec 20 12:59:23.465: INFO: scanned /root for discovery docs: <nil>
Dec 20 12:59:23.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-530'
Dec 20 12:59:24.890: INFO: stderr: ""
Dec 20 12:59:24.891: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 20 12:59:24.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-530'
Dec 20 12:59:25.019: INFO: stderr: ""
Dec 20 12:59:25.019: INFO: stdout: "update-demo-nautilus-b6kkr update-demo-nautilus-hgg4c "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 20 12:59:30.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-530'
Dec 20 12:59:30.140: INFO: stderr: ""
Dec 20 12:59:30.140: INFO: stdout: "update-demo-nautilus-hgg4c "
Dec 20 12:59:30.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-hgg4c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-530'
Dec 20 12:59:30.268: INFO: stderr: ""
Dec 20 12:59:30.268: INFO: stdout: "true"
Dec 20 12:59:30.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-hgg4c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-530'
Dec 20 12:59:30.402: INFO: stderr: ""
Dec 20 12:59:30.402: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 20 12:59:30.402: INFO: validating pod update-demo-nautilus-hgg4c
Dec 20 12:59:30.425: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 20 12:59:30.425: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 20 12:59:30.425: INFO: update-demo-nautilus-hgg4c is verified up and running
STEP: scaling up the replication controller
Dec 20 12:59:30.428: INFO: scanned /root for discovery docs: <nil>
Dec 20 12:59:30.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-530'
Dec 20 12:59:31.582: INFO: stderr: ""
Dec 20 12:59:31.582: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 20 12:59:31.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-530'
Dec 20 12:59:31.707: INFO: stderr: ""
Dec 20 12:59:31.707: INFO: stdout: "update-demo-nautilus-ctl24 update-demo-nautilus-hgg4c "
Dec 20 12:59:31.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-ctl24 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-530'
Dec 20 12:59:31.819: INFO: stderr: ""
Dec 20 12:59:31.819: INFO: stdout: ""
Dec 20 12:59:31.819: INFO: update-demo-nautilus-ctl24 is created but not running
Dec 20 12:59:36.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-530'
Dec 20 12:59:36.938: INFO: stderr: ""
Dec 20 12:59:36.938: INFO: stdout: "update-demo-nautilus-ctl24 update-demo-nautilus-hgg4c "
Dec 20 12:59:36.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-ctl24 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-530'
Dec 20 12:59:37.055: INFO: stderr: ""
Dec 20 12:59:37.055: INFO: stdout: "true"
Dec 20 12:59:37.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-ctl24 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-530'
Dec 20 12:59:37.565: INFO: stderr: ""
Dec 20 12:59:37.565: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 20 12:59:37.565: INFO: validating pod update-demo-nautilus-ctl24
Dec 20 12:59:38.601: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 20 12:59:38.601: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 20 12:59:38.601: INFO: update-demo-nautilus-ctl24 is verified up and running
Dec 20 12:59:38.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-hgg4c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-530'
Dec 20 12:59:40.908: INFO: stderr: ""
Dec 20 12:59:40.908: INFO: stdout: "true"
Dec 20 12:59:40.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods update-demo-nautilus-hgg4c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-530'
Dec 20 12:59:41.037: INFO: stderr: ""
Dec 20 12:59:41.037: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 20 12:59:41.037: INFO: validating pod update-demo-nautilus-hgg4c
Dec 20 12:59:41.049: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 20 12:59:41.049: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 20 12:59:41.049: INFO: update-demo-nautilus-hgg4c is verified up and running
STEP: using delete to clean up resources
Dec 20 12:59:41.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 delete --grace-period=0 --force -f - --namespace=kubectl-530'
Dec 20 12:59:41.921: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 20 12:59:41.921: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 20 12:59:41.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-530'
Dec 20 12:59:42.094: INFO: stderr: "No resources found in kubectl-530 namespace.\n"
Dec 20 12:59:42.094: INFO: stdout: ""
Dec 20 12:59:42.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods -l name=update-demo --namespace=kubectl-530 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 20 12:59:42.212: INFO: stderr: ""
Dec 20 12:59:42.212: INFO: stdout: "update-demo-nautilus-ctl24\nupdate-demo-nautilus-hgg4c\n"
Dec 20 12:59:42.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-530'
Dec 20 12:59:42.849: INFO: stderr: "No resources found in kubectl-530 namespace.\n"
Dec 20 12:59:42.849: INFO: stdout: ""
Dec 20 12:59:42.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 get pods -l name=update-demo --namespace=kubectl-530 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 20 12:59:42.974: INFO: stderr: ""
Dec 20 12:59:42.974: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 12:59:42.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-530" for this suite.
Dec 20 13:00:11.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 13:00:11.808: INFO: namespace kubectl-530 deletion completed in 28.826527729s

• [SLOW TEST:57.366 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 13:00:11.815: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-196
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-71b8ef62-443f-4656-ba87-f42c4d6c6c9d
STEP: Creating a pod to test consume configMaps
Dec 20 13:00:12.141: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-474eff19-dec5-4958-9f3d-2877de16aff5" in namespace "projected-196" to be "success or failure"
Dec 20 13:00:12.156: INFO: Pod "pod-projected-configmaps-474eff19-dec5-4958-9f3d-2877de16aff5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.873148ms
Dec 20 13:00:14.165: INFO: Pod "pod-projected-configmaps-474eff19-dec5-4958-9f3d-2877de16aff5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023020679s
Dec 20 13:00:16.173: INFO: Pod "pod-projected-configmaps-474eff19-dec5-4958-9f3d-2877de16aff5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031517868s
Dec 20 13:00:18.185: INFO: Pod "pod-projected-configmaps-474eff19-dec5-4958-9f3d-2877de16aff5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043924763s
STEP: Saw pod success
Dec 20 13:00:18.186: INFO: Pod "pod-projected-configmaps-474eff19-dec5-4958-9f3d-2877de16aff5" satisfied condition "success or failure"
Dec 20 13:00:18.195: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-projected-configmaps-474eff19-dec5-4958-9f3d-2877de16aff5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 20 13:00:18.402: INFO: Waiting for pod pod-projected-configmaps-474eff19-dec5-4958-9f3d-2877de16aff5 to disappear
Dec 20 13:00:18.408: INFO: Pod pod-projected-configmaps-474eff19-dec5-4958-9f3d-2877de16aff5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 13:00:18.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-196" for this suite.
Dec 20 13:00:24.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 13:00:24.705: INFO: namespace projected-196 deletion completed in 6.286955178s

• [SLOW TEST:12.891 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 13:00:24.712: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3647
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 13:00:24.906: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 13:00:31.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3647" for this suite.
Dec 20 13:01:17.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 13:01:17.602: INFO: namespace pods-3647 deletion completed in 46.325367603s

• [SLOW TEST:52.891 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 13:01:17.608: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2161
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 20 13:01:17.956: INFO: Number of nodes with available pods: 0
Dec 20 13:01:17.957: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 13:01:18.971: INFO: Number of nodes with available pods: 0
Dec 20 13:01:18.971: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 13:01:19.976: INFO: Number of nodes with available pods: 0
Dec 20 13:01:19.976: INFO: Node metakube-worker-457fd-578f899757-428w5 is running more than one daemon pod
Dec 20 13:01:20.973: INFO: Number of nodes with available pods: 3
Dec 20 13:01:20.973: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec 20 13:01:21.029: INFO: Number of nodes with available pods: 2
Dec 20 13:01:21.030: INFO: Node metakube-worker-457fd-578f899757-dsqmn is running more than one daemon pod
Dec 20 13:01:22.048: INFO: Number of nodes with available pods: 2
Dec 20 13:01:22.048: INFO: Node metakube-worker-457fd-578f899757-dsqmn is running more than one daemon pod
Dec 20 13:01:23.054: INFO: Number of nodes with available pods: 2
Dec 20 13:01:23.054: INFO: Node metakube-worker-457fd-578f899757-dsqmn is running more than one daemon pod
Dec 20 13:01:24.047: INFO: Number of nodes with available pods: 2
Dec 20 13:01:24.047: INFO: Node metakube-worker-457fd-578f899757-dsqmn is running more than one daemon pod
Dec 20 13:01:25.045: INFO: Number of nodes with available pods: 2
Dec 20 13:01:25.045: INFO: Node metakube-worker-457fd-578f899757-dsqmn is running more than one daemon pod
Dec 20 13:01:26.047: INFO: Number of nodes with available pods: 2
Dec 20 13:01:26.047: INFO: Node metakube-worker-457fd-578f899757-dsqmn is running more than one daemon pod
Dec 20 13:01:27.047: INFO: Number of nodes with available pods: 2
Dec 20 13:01:27.047: INFO: Node metakube-worker-457fd-578f899757-dsqmn is running more than one daemon pod
Dec 20 13:01:28.047: INFO: Number of nodes with available pods: 3
Dec 20 13:01:28.047: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2161, will wait for the garbage collector to delete the pods
Dec 20 13:01:28.134: INFO: Deleting DaemonSet.extensions daemon-set took: 22.390176ms
Dec 20 13:01:28.634: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.342608ms
Dec 20 13:01:42.546: INFO: Number of nodes with available pods: 0
Dec 20 13:01:42.546: INFO: Number of running nodes: 0, number of available pods: 0
Dec 20 13:01:42.555: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2161/daemonsets","resourceVersion":"45195"},"items":null}

Dec 20 13:01:42.562: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2161/pods","resourceVersion":"45195"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 13:01:42.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2161" for this suite.
Dec 20 13:01:48.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 13:01:48.936: INFO: namespace daemonsets-2161 deletion completed in 6.298688648s

• [SLOW TEST:31.329 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 13:01:48.939: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2125
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-6de83151-250b-4aec-a4fd-09c971cb091d
STEP: Creating a pod to test consume configMaps
Dec 20 13:01:49.176: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bec9cc10-75e2-4bb9-a290-268e77ec1224" in namespace "projected-2125" to be "success or failure"
Dec 20 13:01:49.186: INFO: Pod "pod-projected-configmaps-bec9cc10-75e2-4bb9-a290-268e77ec1224": Phase="Pending", Reason="", readiness=false. Elapsed: 9.52997ms
Dec 20 13:01:51.196: INFO: Pod "pod-projected-configmaps-bec9cc10-75e2-4bb9-a290-268e77ec1224": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019825196s
Dec 20 13:01:53.242: INFO: Pod "pod-projected-configmaps-bec9cc10-75e2-4bb9-a290-268e77ec1224": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06553574s
STEP: Saw pod success
Dec 20 13:01:53.242: INFO: Pod "pod-projected-configmaps-bec9cc10-75e2-4bb9-a290-268e77ec1224" satisfied condition "success or failure"
Dec 20 13:01:53.254: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-projected-configmaps-bec9cc10-75e2-4bb9-a290-268e77ec1224 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 20 13:01:53.485: INFO: Waiting for pod pod-projected-configmaps-bec9cc10-75e2-4bb9-a290-268e77ec1224 to disappear
Dec 20 13:01:53.493: INFO: Pod pod-projected-configmaps-bec9cc10-75e2-4bb9-a290-268e77ec1224 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 13:01:53.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2125" for this suite.
Dec 20 13:01:59.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 13:01:59.814: INFO: namespace projected-2125 deletion completed in 6.308166343s

• [SLOW TEST:10.875 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 13:01:59.814: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7750
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 20 13:02:00.468: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 20 13:02:02.544: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443720, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443720, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443720, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443720, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 20 13:02:04.551: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443720, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443720, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443720, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712443720, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 20 13:02:07.569: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 13:02:07.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7750" for this suite.
Dec 20 13:02:13.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 13:02:13.916: INFO: namespace webhook-7750 deletion completed in 6.319962849s
STEP: Destroying namespace "webhook-7750-markers" for this suite.
Dec 20 13:02:19.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 13:02:20.241: INFO: namespace webhook-7750-markers deletion completed in 6.324163848s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:20.625 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 13:02:20.443: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8807
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 20 13:02:20.928: INFO: Waiting up to 5m0s for pod "pod-a80450c8-caa8-4814-a220-2327f3891787" in namespace "emptydir-8807" to be "success or failure"
Dec 20 13:02:20.935: INFO: Pod "pod-a80450c8-caa8-4814-a220-2327f3891787": Phase="Pending", Reason="", readiness=false. Elapsed: 6.798713ms
Dec 20 13:02:22.944: INFO: Pod "pod-a80450c8-caa8-4814-a220-2327f3891787": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016038515s
Dec 20 13:02:24.956: INFO: Pod "pod-a80450c8-caa8-4814-a220-2327f3891787": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028088018s
STEP: Saw pod success
Dec 20 13:02:24.956: INFO: Pod "pod-a80450c8-caa8-4814-a220-2327f3891787" satisfied condition "success or failure"
Dec 20 13:02:24.963: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod pod-a80450c8-caa8-4814-a220-2327f3891787 container test-container: <nil>
STEP: delete the pod
Dec 20 13:02:25.045: INFO: Waiting for pod pod-a80450c8-caa8-4814-a220-2327f3891787 to disappear
Dec 20 13:02:25.083: INFO: Pod pod-a80450c8-caa8-4814-a220-2327f3891787 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 13:02:25.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8807" for this suite.
Dec 20 13:02:31.127: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 13:02:31.401: INFO: namespace emptydir-8807 deletion completed in 6.306599464s

• [SLOW TEST:10.959 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 13:02:31.403: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6670
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 20 13:02:31.642: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Dec 20 13:02:36.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 --namespace=crd-publish-openapi-6670 create -f -'
Dec 20 13:02:42.391: INFO: stderr: ""
Dec 20 13:02:42.391: INFO: stdout: "e2e-test-crd-publish-openapi-5022-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 20 13:02:42.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 --namespace=crd-publish-openapi-6670 delete e2e-test-crd-publish-openapi-5022-crds test-foo'
Dec 20 13:02:42.513: INFO: stderr: ""
Dec 20 13:02:42.513: INFO: stdout: "e2e-test-crd-publish-openapi-5022-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Dec 20 13:02:42.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 --namespace=crd-publish-openapi-6670 apply -f -'
Dec 20 13:02:43.564: INFO: stderr: ""
Dec 20 13:02:43.564: INFO: stdout: "e2e-test-crd-publish-openapi-5022-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 20 13:02:43.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 --namespace=crd-publish-openapi-6670 delete e2e-test-crd-publish-openapi-5022-crds test-foo'
Dec 20 13:02:43.727: INFO: stderr: ""
Dec 20 13:02:43.727: INFO: stdout: "e2e-test-crd-publish-openapi-5022-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Dec 20 13:02:43.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 --namespace=crd-publish-openapi-6670 create -f -'
Dec 20 13:02:44.352: INFO: rc: 1
Dec 20 13:02:44.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 --namespace=crd-publish-openapi-6670 apply -f -'
Dec 20 13:02:44.659: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Dec 20 13:02:44.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 --namespace=crd-publish-openapi-6670 create -f -'
Dec 20 13:02:45.013: INFO: rc: 1
Dec 20 13:02:45.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 --namespace=crd-publish-openapi-6670 apply -f -'
Dec 20 13:02:45.306: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Dec 20 13:02:45.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 explain e2e-test-crd-publish-openapi-5022-crds'
Dec 20 13:02:45.717: INFO: stderr: ""
Dec 20 13:02:45.717: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5022-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Dec 20 13:02:45.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 explain e2e-test-crd-publish-openapi-5022-crds.metadata'
Dec 20 13:02:46.097: INFO: stderr: ""
Dec 20 13:02:46.097: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5022-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Dec 20 13:02:46.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 explain e2e-test-crd-publish-openapi-5022-crds.spec'
Dec 20 13:02:46.402: INFO: stderr: ""
Dec 20 13:02:46.402: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5022-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Dec 20 13:02:46.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 explain e2e-test-crd-publish-openapi-5022-crds.spec.bars'
Dec 20 13:02:46.670: INFO: stderr: ""
Dec 20 13:02:46.670: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5022-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Dec 20 13:02:46.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 explain e2e-test-crd-publish-openapi-5022-crds.spec.bars2'
Dec 20 13:02:46.983: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 13:02:51.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6670" for this suite.
Dec 20 13:02:57.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 13:02:57.901: INFO: namespace crd-publish-openapi-6670 deletion completed in 6.28109372s

• [SLOW TEST:26.499 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 13:02:57.905: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7275
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Dec 20 13:02:58.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 create -f - --namespace=kubectl-7275'
Dec 20 13:03:00.324: INFO: stderr: ""
Dec 20 13:03:00.324: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 20 13:03:01.334: INFO: Selector matched 1 pods for map[app:redis]
Dec 20 13:03:01.334: INFO: Found 0 / 1
Dec 20 13:03:02.331: INFO: Selector matched 1 pods for map[app:redis]
Dec 20 13:03:02.331: INFO: Found 0 / 1
Dec 20 13:03:03.332: INFO: Selector matched 1 pods for map[app:redis]
Dec 20 13:03:03.332: INFO: Found 0 / 1
Dec 20 13:03:04.331: INFO: Selector matched 1 pods for map[app:redis]
Dec 20 13:03:04.331: INFO: Found 0 / 1
Dec 20 13:03:05.332: INFO: Selector matched 1 pods for map[app:redis]
Dec 20 13:03:05.332: INFO: Found 0 / 1
Dec 20 13:03:06.786: INFO: Selector matched 1 pods for map[app:redis]
Dec 20 13:03:06.786: INFO: Found 1 / 1
Dec 20 13:03:06.786: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec 20 13:03:06.793: INFO: Selector matched 1 pods for map[app:redis]
Dec 20 13:03:06.793: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 20 13:03:06.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 patch pod redis-master-jcbtx --namespace=kubectl-7275 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec 20 13:03:07.772: INFO: stderr: ""
Dec 20 13:03:07.772: INFO: stdout: "pod/redis-master-jcbtx patched\n"
STEP: checking annotations
Dec 20 13:03:11.984: INFO: Selector matched 1 pods for map[app:redis]
Dec 20 13:03:11.984: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 13:03:11.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7275" for this suite.
Dec 20 13:03:31.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 13:03:31.386: INFO: namespace kubectl-7275 deletion completed in 18.346696087s

• [SLOW TEST:33.482 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 13:03:31.390: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8299
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-d6ddc711-40ad-4a83-8652-8337c0c3ebcc
STEP: Creating configMap with name cm-test-opt-upd-48409b4f-0207-4b64-a4d9-db258f048dcb
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-d6ddc711-40ad-4a83-8652-8337c0c3ebcc
STEP: Updating configmap cm-test-opt-upd-48409b4f-0207-4b64-a4d9-db258f048dcb
STEP: Creating configMap with name cm-test-opt-create-0cbb8251-270b-42a6-a847-778a6b4431bc
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 13:04:54.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8299" for this suite.
Dec 20 13:05:06.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 13:05:06.689: INFO: namespace projected-8299 deletion completed in 12.29068385s

• [SLOW TEST:95.300 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 13:05:06.691: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5551
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 13:05:07.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5551" for this suite.
Dec 20 13:05:33.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 13:05:33.365: INFO: namespace pods-5551 deletion completed in 26.339623822s

• [SLOW TEST:26.675 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 13:05:33.366: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2335
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Dec 20 13:05:33.622: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-773140870 proxy --unix-socket=/tmp/kubectl-proxy-unix320301821/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 13:05:33.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2335" for this suite.
Dec 20 13:05:39.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 13:05:40.131: INFO: namespace kubectl-2335 deletion completed in 6.381622526s

• [SLOW TEST:6.765 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 13:05:40.131: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9212
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 20 13:05:53.863: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 20 13:05:53.877: INFO: Pod pod-with-poststart-http-hook still exists
Dec 20 13:05:55.878: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 20 13:05:55.884: INFO: Pod pod-with-poststart-http-hook still exists
Dec 20 13:05:57.878: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 20 13:05:57.887: INFO: Pod pod-with-poststart-http-hook still exists
Dec 20 13:05:59.878: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 20 13:05:59.888: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 13:05:59.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9212" for this suite.
Dec 20 13:06:29.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 13:06:30.320: INFO: namespace container-lifecycle-hook-9212 deletion completed in 30.37536348s

• [SLOW TEST:50.189 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 13:06:30.324: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4075
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 13:06:30.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4075" for this suite.
Dec 20 13:06:36.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 13:06:37.133: INFO: namespace resourcequota-4075 deletion completed in 6.435884272s

• [SLOW TEST:6.810 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 13:06:37.140: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6148
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 20 13:06:37.411: INFO: Waiting up to 5m0s for pod "downwardapi-volume-decd8686-c8c1-4a18-b3a3-18ecdf999a9e" in namespace "downward-api-6148" to be "success or failure"
Dec 20 13:06:37.418: INFO: Pod "downwardapi-volume-decd8686-c8c1-4a18-b3a3-18ecdf999a9e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.896721ms
Dec 20 13:06:39.426: INFO: Pod "downwardapi-volume-decd8686-c8c1-4a18-b3a3-18ecdf999a9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015673667s
Dec 20 13:06:41.434: INFO: Pod "downwardapi-volume-decd8686-c8c1-4a18-b3a3-18ecdf999a9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023452664s
Dec 20 13:06:43.441: INFO: Pod "downwardapi-volume-decd8686-c8c1-4a18-b3a3-18ecdf999a9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030245893s
STEP: Saw pod success
Dec 20 13:06:43.441: INFO: Pod "downwardapi-volume-decd8686-c8c1-4a18-b3a3-18ecdf999a9e" satisfied condition "success or failure"
Dec 20 13:06:43.447: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-zpxc2 pod downwardapi-volume-decd8686-c8c1-4a18-b3a3-18ecdf999a9e container client-container: <nil>
STEP: delete the pod
Dec 20 13:06:43.999: INFO: Waiting for pod downwardapi-volume-decd8686-c8c1-4a18-b3a3-18ecdf999a9e to disappear
Dec 20 13:06:44.006: INFO: Pod downwardapi-volume-decd8686-c8c1-4a18-b3a3-18ecdf999a9e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 13:06:44.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6148" for this suite.
Dec 20 13:06:50.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 13:06:50.388: INFO: namespace downward-api-6148 deletion completed in 6.336688285s

• [SLOW TEST:13.248 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 13:06:50.398: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-7903
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec 20 13:06:59.428: INFO: &Pod{ObjectMeta:{send-events-c3ce46c1-6c8d-45e5-87f1-ec566fe38efe  events-7903 /api/v1/namespaces/events-7903/pods/send-events-c3ce46c1-6c8d-45e5-87f1-ec566fe38efe af139b9f-4924-4ef6-af25-356d3122c880 46529 0 2019-12-20 13:06:50 +0000 UTC <nil> <nil> map[name:foo time:674421485] map[cni.projectcalico.org/podIP:172.25.1.218/32] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kvn2j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kvn2j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kvn2j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-457fd-578f899757-428w5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 13:06:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 13:06:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 13:06:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-20 13:06:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.3,PodIP:172.25.1.218,StartTime:2019-12-20 13:06:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-20 13:06:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:docker://89b5099f448a33ea7b4f2fb0db1b13b5314b5af6c1eb5733b64fed32744429fe,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.218,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Dec 20 13:07:01.441: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec 20 13:07:03.452: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 13:07:03.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7903" for this suite.
Dec 20 13:07:49.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 13:07:49.847: INFO: namespace events-7903 deletion completed in 46.344159635s

• [SLOW TEST:59.450 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 13:07:49.849: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-312
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-007d08cc-2886-45b4-a694-3ba6caf5c8ac
STEP: Creating a pod to test consume configMaps
Dec 20 13:07:50.361: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a19ea033-0566-4fb8-9358-f8b4f64116a6" in namespace "projected-312" to be "success or failure"
Dec 20 13:07:50.383: INFO: Pod "pod-projected-configmaps-a19ea033-0566-4fb8-9358-f8b4f64116a6": Phase="Pending", Reason="", readiness=false. Elapsed: 22.083155ms
Dec 20 13:07:52.392: INFO: Pod "pod-projected-configmaps-a19ea033-0566-4fb8-9358-f8b4f64116a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031380116s
Dec 20 13:07:55.058: INFO: Pod "pod-projected-configmaps-a19ea033-0566-4fb8-9358-f8b4f64116a6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.696935354s
Dec 20 13:07:57.066: INFO: Pod "pod-projected-configmaps-a19ea033-0566-4fb8-9358-f8b4f64116a6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.705081984s
Dec 20 13:07:59.074: INFO: Pod "pod-projected-configmaps-a19ea033-0566-4fb8-9358-f8b4f64116a6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.712959709s
Dec 20 13:08:01.681: INFO: Pod "pod-projected-configmaps-a19ea033-0566-4fb8-9358-f8b4f64116a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.319897562s
STEP: Saw pod success
Dec 20 13:08:01.681: INFO: Pod "pod-projected-configmaps-a19ea033-0566-4fb8-9358-f8b4f64116a6" satisfied condition "success or failure"
Dec 20 13:08:01.770: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-projected-configmaps-a19ea033-0566-4fb8-9358-f8b4f64116a6 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 20 13:08:02.810: INFO: Waiting for pod pod-projected-configmaps-a19ea033-0566-4fb8-9358-f8b4f64116a6 to disappear
Dec 20 13:08:02.819: INFO: Pod pod-projected-configmaps-a19ea033-0566-4fb8-9358-f8b4f64116a6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 13:08:02.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-312" for this suite.
Dec 20 13:08:14.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 13:08:15.304: INFO: namespace projected-312 deletion completed in 12.412029221s

• [SLOW TEST:25.455 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 13:08:15.304: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4739
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 20 13:08:15.848: INFO: Waiting up to 5m0s for pod "downwardapi-volume-88d44420-d742-47de-8c41-f6c1cebf7ab1" in namespace "projected-4739" to be "success or failure"
Dec 20 13:08:15.859: INFO: Pod "downwardapi-volume-88d44420-d742-47de-8c41-f6c1cebf7ab1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.968257ms
Dec 20 13:08:17.871: INFO: Pod "downwardapi-volume-88d44420-d742-47de-8c41-f6c1cebf7ab1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023194709s
Dec 20 13:08:19.894: INFO: Pod "downwardapi-volume-88d44420-d742-47de-8c41-f6c1cebf7ab1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046006184s
Dec 20 13:08:21.936: INFO: Pod "downwardapi-volume-88d44420-d742-47de-8c41-f6c1cebf7ab1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.088247575s
Dec 20 13:08:23.942: INFO: Pod "downwardapi-volume-88d44420-d742-47de-8c41-f6c1cebf7ab1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.093940255s
STEP: Saw pod success
Dec 20 13:08:23.942: INFO: Pod "downwardapi-volume-88d44420-d742-47de-8c41-f6c1cebf7ab1" satisfied condition "success or failure"
Dec 20 13:08:23.961: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod downwardapi-volume-88d44420-d742-47de-8c41-f6c1cebf7ab1 container client-container: <nil>
STEP: delete the pod
Dec 20 13:08:24.398: INFO: Waiting for pod downwardapi-volume-88d44420-d742-47de-8c41-f6c1cebf7ab1 to disappear
Dec 20 13:08:24.407: INFO: Pod downwardapi-volume-88d44420-d742-47de-8c41-f6c1cebf7ab1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 13:08:24.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4739" for this suite.
Dec 20 13:08:30.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 13:08:30.785: INFO: namespace projected-4739 deletion completed in 6.357014953s

• [SLOW TEST:15.481 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 13:08:30.787: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-756
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-756
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 20 13:08:31.044: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 20 13:09:05.855: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.222:8080/dial?request=hostName&protocol=udp&host=172.25.0.45&port=8081&tries=1'] Namespace:pod-network-test-756 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 20 13:09:05.855: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 13:09:06.777: INFO: Waiting for endpoints: map[]
Dec 20 13:09:06.783: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.222:8080/dial?request=hostName&protocol=udp&host=172.25.2.166&port=8081&tries=1'] Namespace:pod-network-test-756 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 20 13:09:06.783: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 13:09:07.897: INFO: Waiting for endpoints: map[]
Dec 20 13:09:07.903: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.222:8080/dial?request=hostName&protocol=udp&host=172.25.1.221&port=8081&tries=1'] Namespace:pod-network-test-756 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 20 13:09:07.903: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
Dec 20 13:09:08.712: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 13:09:08.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-756" for this suite.
Dec 20 13:09:22.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 13:09:23.065: INFO: namespace pod-network-test-756 deletion completed in 14.342921572s

• [SLOW TEST:52.278 seconds]
[sig-network] Networking
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 13:09:23.068: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3285
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-sr9c
STEP: Creating a pod to test atomic-volume-subpath
Dec 20 13:09:23.507: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-sr9c" in namespace "subpath-3285" to be "success or failure"
Dec 20 13:09:23.550: INFO: Pod "pod-subpath-test-projected-sr9c": Phase="Pending", Reason="", readiness=false. Elapsed: 42.891558ms
Dec 20 13:09:25.584: INFO: Pod "pod-subpath-test-projected-sr9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076793736s
Dec 20 13:09:27.608: INFO: Pod "pod-subpath-test-projected-sr9c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.100417041s
Dec 20 13:09:29.625: INFO: Pod "pod-subpath-test-projected-sr9c": Phase="Running", Reason="", readiness=true. Elapsed: 6.117887031s
Dec 20 13:09:31.634: INFO: Pod "pod-subpath-test-projected-sr9c": Phase="Running", Reason="", readiness=true. Elapsed: 8.126610965s
Dec 20 13:09:33.654: INFO: Pod "pod-subpath-test-projected-sr9c": Phase="Running", Reason="", readiness=true. Elapsed: 10.147006636s
Dec 20 13:09:35.671: INFO: Pod "pod-subpath-test-projected-sr9c": Phase="Running", Reason="", readiness=true. Elapsed: 12.1631089s
Dec 20 13:09:37.676: INFO: Pod "pod-subpath-test-projected-sr9c": Phase="Running", Reason="", readiness=true. Elapsed: 14.16886073s
Dec 20 13:09:39.683: INFO: Pod "pod-subpath-test-projected-sr9c": Phase="Running", Reason="", readiness=true. Elapsed: 16.175497912s
Dec 20 13:09:41.692: INFO: Pod "pod-subpath-test-projected-sr9c": Phase="Running", Reason="", readiness=true. Elapsed: 18.184303817s
Dec 20 13:09:43.710: INFO: Pod "pod-subpath-test-projected-sr9c": Phase="Running", Reason="", readiness=true. Elapsed: 20.202876348s
Dec 20 13:09:45.723: INFO: Pod "pod-subpath-test-projected-sr9c": Phase="Running", Reason="", readiness=true. Elapsed: 22.215202755s
Dec 20 13:09:47.741: INFO: Pod "pod-subpath-test-projected-sr9c": Phase="Running", Reason="", readiness=true. Elapsed: 24.23341034s
Dec 20 13:09:49.751: INFO: Pod "pod-subpath-test-projected-sr9c": Phase="Running", Reason="", readiness=true. Elapsed: 26.243336162s
Dec 20 13:09:51.759: INFO: Pod "pod-subpath-test-projected-sr9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.251332218s
STEP: Saw pod success
Dec 20 13:09:51.759: INFO: Pod "pod-subpath-test-projected-sr9c" satisfied condition "success or failure"
Dec 20 13:09:51.766: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod pod-subpath-test-projected-sr9c container test-container-subpath-projected-sr9c: <nil>
STEP: delete the pod
Dec 20 13:09:52.095: INFO: Waiting for pod pod-subpath-test-projected-sr9c to disappear
Dec 20 13:09:52.099: INFO: Pod pod-subpath-test-projected-sr9c no longer exists
STEP: Deleting pod pod-subpath-test-projected-sr9c
Dec 20 13:09:52.099: INFO: Deleting pod "pod-subpath-test-projected-sr9c" in namespace "subpath-3285"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 13:09:52.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3285" for this suite.
Dec 20 13:10:00.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 13:10:00.972: INFO: namespace subpath-3285 deletion completed in 8.827835194s

• [SLOW TEST:37.904 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 13:10:00.974: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1754
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Dec 20 13:10:01.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-773140870 api-versions'
Dec 20 13:10:02.171: INFO: stderr: ""
Dec 20 13:10:02.171: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetakube.syseleven.de/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 13:10:02.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1754" for this suite.
Dec 20 13:10:10.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 13:10:10.702: INFO: namespace kubectl-1754 deletion completed in 8.521228071s

• [SLOW TEST:9.729 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 20 13:10:10.705: INFO: >>> kubeConfig: /tmp/kubeconfig-773140870
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2035
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 20 13:10:11.220: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b3f012f5-4ef5-45c0-a02a-0257c9662324" in namespace "downward-api-2035" to be "success or failure"
Dec 20 13:10:11.366: INFO: Pod "downwardapi-volume-b3f012f5-4ef5-45c0-a02a-0257c9662324": Phase="Pending", Reason="", readiness=false. Elapsed: 145.820691ms
Dec 20 13:10:13.375: INFO: Pod "downwardapi-volume-b3f012f5-4ef5-45c0-a02a-0257c9662324": Phase="Pending", Reason="", readiness=false. Elapsed: 2.155169792s
Dec 20 13:10:15.383: INFO: Pod "downwardapi-volume-b3f012f5-4ef5-45c0-a02a-0257c9662324": Phase="Pending", Reason="", readiness=false. Elapsed: 4.162652064s
Dec 20 13:10:17.391: INFO: Pod "downwardapi-volume-b3f012f5-4ef5-45c0-a02a-0257c9662324": Phase="Pending", Reason="", readiness=false. Elapsed: 6.170941335s
Dec 20 13:10:19.398: INFO: Pod "downwardapi-volume-b3f012f5-4ef5-45c0-a02a-0257c9662324": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.177535181s
STEP: Saw pod success
Dec 20 13:10:19.398: INFO: Pod "downwardapi-volume-b3f012f5-4ef5-45c0-a02a-0257c9662324" satisfied condition "success or failure"
Dec 20 13:10:19.509: INFO: Trying to get logs from node metakube-worker-457fd-578f899757-428w5 pod downwardapi-volume-b3f012f5-4ef5-45c0-a02a-0257c9662324 container client-container: <nil>
STEP: delete the pod
Dec 20 13:10:19.891: INFO: Waiting for pod downwardapi-volume-b3f012f5-4ef5-45c0-a02a-0257c9662324 to disappear
Dec 20 13:10:19.919: INFO: Pod downwardapi-volume-b3f012f5-4ef5-45c0-a02a-0257c9662324 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 20 13:10:19.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2035" for this suite.
Dec 20 13:10:28.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 20 13:10:28.400: INFO: namespace downward-api-2035 deletion completed in 8.418376386s

• [SLOW TEST:17.695 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSDec 20 13:10:28.403: INFO: Running AfterSuite actions on all nodes
Dec 20 13:10:28.403: INFO: Running AfterSuite actions on node 1
Dec 20 13:10:28.403: INFO: Skipping dumping logs from cluster

Ran 274 of 4732 Specs in 8872.759 seconds
SUCCESS! -- 274 Passed | 0 Failed | 0 Pending | 4458 Skipped
PASS

Ginkgo ran 1 suite in 2h27m54.741054695s
Test Suite Passed
